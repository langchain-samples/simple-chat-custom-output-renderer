[
  {
    "role": "user",
    "id": "c28cb5a9-9fbf-4518-a3b9-624c8300a20a",
    "content": "Every time i use deep agents cli does it starts with empty folders and memories and starting config "
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "cb192690-0160-4c33-bd1f-7b884ace75b8",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"cli\"}"
        }
      },
      {
        "type": "function",
        "id": "3bf408bf-8534-4233-ac21-67bb0bcca2c4",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"persistence\"}"
        }
      },
      {
        "type": "function",
        "id": "2bb7fb75-e4d6-4140-ba14-1e70553c8af1",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"all\"}"
        }
      }
    ],
    "id": "lc_run--019b3847-ff3c-70b1-a0dc-6856b9c2bbc5",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "cb192690-0160-4c33-bd1f-7b884ace75b8",
    "id": "32ca3f34-7d46-4628-b799-9028bd5d66ea",
    "content": "Result 1:\nTitle: LangGraph CLI\nLink: https://docs.langchain.comlangsmith/cli\nContent: LangGraph CLI\nLangGraph CLI is a command-line tool for building and running the Agent Server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.\n\nLangGraph CLI\nLangGraph CLI\nLangGraph CLI is a command-line tool for building and running the Agent Server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.\n\nUsing Wolfi base images\nYou can specify the Linux distribution for your base image using the image_distro field. Valid options are debian , wolfi , bookworm , or bullseye . Wolfi is the recommended option as it provides smaller and more secure images. This is available in langgraph-cli>=0.2.11. {\n  \"$schema\": \"https://langgra.ph/schema.json\",\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"chat\": \"chat.graph:graph\"\n  },\n  \"image_distro\": \"wolfi\"\n}\n\n---\n\nResult 2:\nTitle: Deep Agents CLI\nLink: https://docs.langchain.comoss/javascript/deepagents/cli\nContent: A terminal interface for building agents with persistent memory. Agents maintain context across sessions, learn project conventions, and execute code with approval controls.\n\nThe Deep Agents CLI has the following built-in capabilities:\n\n* <Icon icon=\"file\" size={16} /> **File operations** - read, write, and edit files in your project with tools that enable agents to manage and modify code and documentation.\n* <Icon icon=\"terminal\" size={16} /> **Shell command execution** - execute shell commands to run tests, build projects, manage dependencies, and interact with version control systems.\n* <Icon icon=\"magnifying-glass\" size={16} /> **Web search** - search the web for up-to-date information and documentation (requires Tavily API key).\n* <Icon icon=\"globe\" size={16} /> **HTTP requests** - make HTTP requests to APIs and external services for data fetching and integration tasks.\n* <Icon icon=\"list-check\" size={16} /> **Task planning and tracking** - break down complex tasks into discrete steps and track progress through the built-in todo system.\n* <Icon icon=\"brain\" size={16} /> **Memory storage and retrieval** - store and retrieve information across sessions, enabling agents to remember project conventions and learned patterns.\n* <Icon icon=\"head-side\" size={16} /> **Human-in-the-loop** - require human approval for sensitive tool operations.\n\n<Tip>\n  [Watch the demo video](https://youtu.be/IrnacLa9PJc?si=3yUnPbxnm2yaqVQb) to see how the Deep Agents CLI works.\n</Tip>\n\n## Quick start\n\n<Steps>\n  <Step title=\"Set your API key\" icon=\"key\">\n    Export as an environment variable:\n\n    ```bash  theme={null}\n    export ANTHROPIC_API_KEY=\"your-api-key\"\n    ```\n\n    Or create a `.env` file in your project root:\n\n    ```bash  theme={null}\n    ANTHROPIC_API_KEY=your-api-key\n    ```\n  </Step>\n\n  <Step title=\"Run the CLI\" icon=\"terminal\">\n    ```bash  theme={null}\n    uvx deepagents-cli\n    ```\n  </Step>\n\n  <Step title=\"Give the agent a task\" icon=\"message\">\n    ```bash  theme={null}\n    > Create a Python script that prints \"Hello, World!\"\n    ```\n\n    The agent proposes changes with diffs for your approval before modifying files.\n  </Step>\n</Steps>\n\n<Accordion title=\"Additional installation and configuration options\">\n  Install locally if needed:\n\n  <CodeGroup>\n    ```bash pip theme={null}\n    pip install deepagents-cli\n    ```\n\n    ```bash uv theme={null}\n    uv add deepagents-cli\n    ```\n  </CodeGroup>\n\n  The CLI uses Anthropic Claude Sonnet 4 by default. To use OpenAI:\n\n  ```bash  theme={null}\n  export OPENAI_API_KEY=\"your-key\"\n  ```\n\n  Enable web search (optional):\n\n  ```bash  theme={null}\n  export TAVILY_API_KEY=\"your-key\"\n  ```\n\n  API keys can be set as environment variables or in a `.env` file.\n</Accordion>\n\n## Configuration\n\n<AccordionGroup>\n  <Accordion title=\"Command-line options\" icon=\"flag\">\n    | Option                 | Description                                                 |\n    | ---------------------- | ----------------------------------------------------------- |\n    | `--agent NAME`         | Use named agent with separate memory                        |\n    | `--auto-approve`       | Skip tool confirmation prompts (toggle with `Ctrl+T`)       |\n    | `--sandbox TYPE`       | Execute in remote sandbox: `modal`, `daytona`, or `runloop` |\n    | `--sandbox-id ID`      | Reuse existing sandbox                                      |\n    | `--sandbox-setup PATH` | Run setup script in sandbox                                 |\n  </Accordion>\n\n  <Accordion title=\"CLI commands\" icon=\"terminal\">\n    | Command                                         | Description                             |\n    | ----------------------------------------------- | --------------------------------------- |\n    | `deepagents list`                               | List all agents                         |\n    | `deepagents help`                               | Show help                               |\n    | `deepagents reset --agent NAME`                 | Clear agent memory and reset to default |\n    | `deepagents reset --agent NAME --target SOURCE` | Copy memory from another agent          |\n  </Accordion>\n</AccordionGroup>\n\n## Interactive mode\n\n<AccordionGroup>\n  <Accordion title=\"Slash commands\" icon=\"slash\">\n    Use these commands within the CLI session:\n\n    * `/tokens` - Display token usage\n    * `/clear` - Clear conversation history\n    * `/exit` - Exit the CLI\n  </Accordion>\n\n  <Accordion title=\"Bash commands\" icon=\"terminal\">\n    Execute shell commands directly by prefixing with `!`:\n\n    ```bash  theme={null}\n    !git status\n    !npm test\n    !ls -la\n    ```\n  </Accordion>\n\n  <Accordion title=\"Keyboard shortcuts\" icon=\"keyboard\">\n    | Shortcut    | Action              |\n    | ----------- | ------------------- |\n    | `Enter`     | Submit              |\n    | `Alt+Enter` | Newline             |\n    | `Ctrl+E`    | External editor     |\n    | `Ctrl+T`    | Toggle auto-approve |\n    | `Ctrl+C`    | Interrupt           |\n    | `Ctrl+D`    | Exit                |\n  </Accordion>\n</AccordionGroup>\n\n## Set project conventions with memories\n\nAgents store information in `~/.deepagents/AGENT_NAME/memories/` as markdown files using a memory-first protocol:\n\n1. **Research**: Searches memory for relevant context before starting tasks\n2. **Response**: Checks memory when uncertain during execution\n3. **Learning**: Automatically saves new information for future sessions\n\nOrganize memories by topic with descriptive filenames:\n\n```\n~/.deepagents/backend-dev/memories/\n\u251c\u2500\u2500 api-conventions.md\n\u251c\u2500\u2500 database-schema.md\n\u2514\u2500\u2500 deployment-process.md\n```\n\nTeach the agent conventions once:\n\n```bash  theme={null}\nuvx deepagents-cli --agent backend-dev\n> Our API uses snake_case and includes created_at/updated_at timestamps\n```\n\nIt remembers for future sessions:\n\n```bash  theme={null}\n> Create a /users endpoint\n# Applies conventions without prompting\n```\n\n## Use remote sandboxes\n\nExecute code in isolated remote environments for safety and flexibility. Remote sandboxes provide the following benefits:\n\n* **Safety**: Protect your local machine from potentially harmful code execution\n* **Clean environments**: Use specific dependencies or OS configurations without local setup\n* **Parallel execution**: Run multiple agents simultaneously in isolated environments\n* **Long-running tasks**: Execute time-intensive operations without blocking your machine\n* **Reproducibility**: Ensure consistent execution environments across teams\n\nTo use a remote sandbox, follow these steps:\n\n1. Configure your sandbox provider ([Runloop](https://www.runloop.ai/), [Daytona](https://www.daytona.io/), or [Modal](https://modal.com/)):\n\n   ```bash  theme={null}\n   # Runloop\n   export RUNLOOP_API_KEY=\"your-key\"\n\n   # Daytona\n   export DAYTONA_API_KEY=\"your-key\"\n\n   # Modal\n   modal setup\n   ```\n\n2. Run the CLI with a sandbox:\n\n   ```bash  theme={null}\n   uvx deepagents-cli --sandbox runloop --sandbox-setup ./setup.sh\n   ```\n\n   The agent runs locally but executes all code operations in the remote sandbox. Optional setup scripts can configure environment variables, clone repositories, and prepare dependencies.\n\n3. (Optional) Create a `setup.sh` file to configure your sandbox environment:\n\n   ```bash  theme={null}\n   #!/bin/bash\n   set -e\n\n   # Clone repository using GitHub token\n   git clone https://x-access-token:${GITHUB_TOKEN}@github.com/username/repo.git $HOME/workspace\n   cd $HOME/workspace\n\n   # Make environment variables persistent\n   cat >> ~/.bashrc <<'EOF'\n   export GITHUB_TOKEN=\"${GITHUB_TOKEN}\"\n   export OPENAI_API_KEY=\"${OPENAI_API_KEY}\"\n   cd $HOME/workspace\n   EOF\n\n   source ~/.bashrc\n   ```\n\n   Store secrets in a local `.env` file for the setup script to access.\n\n<Warning>\n  Sandboxes isolate code execution, but agents remain vulnerable to prompt injection with untrusted inputs. Use human-in-the-loop approval, short-lived secrets, and trusted setup scripts only. Note that sandbox APIs are evolving rapidly, and we expect more providers to support proxies that help mitigate prompt injection and secrets management concerns.\n</Warning>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/cli.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Deep Agents CLI\nLink: https://docs.langchain.comoss/python/deepagents/cli\nContent: A terminal interface for building agents with persistent memory. Agents maintain context across sessions, learn project conventions, and execute code with approval controls.\n\nThe Deep Agents CLI has the following built-in capabilities:\n\n* <Icon icon=\"file\" size={16} /> **File operations** - read, write, and edit files in your project with tools that enable agents to manage and modify code and documentation.\n* <Icon icon=\"terminal\" size={16} /> **Shell command execution** - execute shell commands to run tests, build projects, manage dependencies, and interact with version control systems.\n* <Icon icon=\"magnifying-glass\" size={16} /> **Web search** - search the web for up-to-date information and documentation (requires Tavily API key).\n* <Icon icon=\"globe\" size={16} /> **HTTP requests** - make HTTP requests to APIs and external services for data fetching and integration tasks.\n* <Icon icon=\"list-check\" size={16} /> **Task planning and tracking** - break down complex tasks into discrete steps and track progress through the built-in todo system.\n* <Icon icon=\"brain\" size={16} /> **Memory storage and retrieval** - store and retrieve information across sessions, enabling agents to remember project conventions and learned patterns.\n* <Icon icon=\"head-side\" size={16} /> **Human-in-the-loop** - require human approval for sensitive tool operations.\n\n<Tip>\n  [Watch the demo video](https://youtu.be/IrnacLa9PJc?si=3yUnPbxnm2yaqVQb) to see how the Deep Agents CLI works.\n</Tip>\n\n## Quick start\n\n<Steps>\n  <Step title=\"Set your API key\" icon=\"key\">\n    Export as an environment variable:\n\n    ```bash  theme={null}\n    export ANTHROPIC_API_KEY=\"your-api-key\"\n    ```\n\n    Or create a `.env` file in your project root:\n\n    ```bash  theme={null}\n    ANTHROPIC_API_KEY=your-api-key\n    ```\n  </Step>\n\n  <Step title=\"Run the CLI\" icon=\"terminal\">\n    ```bash  theme={null}\n    uvx deepagents-cli\n    ```\n  </Step>\n\n  <Step title=\"Give the agent a task\" icon=\"message\">\n    ```bash  theme={null}\n    > Create a Python script that prints \"Hello, World!\"\n    ```\n\n    The agent proposes changes with diffs for your approval before modifying files.\n  </Step>\n</Steps>\n\n<Accordion title=\"Additional installation and configuration options\">\n  Install locally if needed:\n\n  <CodeGroup>\n    ```bash pip theme={null}\n    pip install deepagents-cli\n    ```\n\n    ```bash uv theme={null}\n    uv add deepagents-cli\n    ```\n  </CodeGroup>\n\n  The CLI uses Anthropic Claude Sonnet 4 by default. To use OpenAI:\n\n  ```bash  theme={null}\n  export OPENAI_API_KEY=\"your-key\"\n  ```\n\n  Enable web search (optional):\n\n  ```bash  theme={null}\n  export TAVILY_API_KEY=\"your-key\"\n  ```\n\n  API keys can be set as environment variables or in a `.env` file.\n</Accordion>\n\n## Configuration\n\n<AccordionGroup>\n  <Accordion title=\"Command-line options\" icon=\"flag\">\n    | Option                 | Description                                                 |\n    | ---------------------- | ----------------------------------------------------------- |\n    | `--agent NAME`         | Use named agent with separate memory                        |\n    | `--auto-approve`       | Skip tool confirmation prompts (toggle with `Ctrl+T`)       |\n    | `--sandbox TYPE`       | Execute in remote sandbox: `modal`, `daytona`, or `runloop` |\n    | `--sandbox-id ID`      | Reuse existing sandbox                                      |\n    | `--sandbox-setup PATH` | Run setup script in sandbox                                 |\n  </Accordion>\n\n  <Accordion title=\"CLI commands\" icon=\"terminal\">\n    | Command                                         | Description                             |\n    | ----------------------------------------------- | --------------------------------------- |\n    | `deepagents list`                               | List all agents                         |\n    | `deepagents help`                               | Show help                               |\n    | `deepagents reset --agent NAME`                 | Clear agent memory and reset to default |\n    | `deepagents reset --agent NAME --target SOURCE` | Copy memory from another agent          |\n  </Accordion>\n</AccordionGroup>\n\n## Interactive mode\n\n<AccordionGroup>\n  <Accordion title=\"Slash commands\" icon=\"slash\">\n    Use these commands within the CLI session:\n\n    * `/tokens` - Display token usage\n    * `/clear` - Clear conversation history\n    * `/exit` - Exit the CLI\n  </Accordion>\n\n  <Accordion title=\"Bash commands\" icon=\"terminal\">\n    Execute shell commands directly by prefixing with `!`:\n\n    ```bash  theme={null}\n    !git status\n    !npm test\n    !ls -la\n    ```\n  </Accordion>\n\n  <Accordion title=\"Keyboard shortcuts\" icon=\"keyboard\">\n    | Shortcut    | Action              |\n    | ----------- | ------------------- |\n    | `Enter`     | Submit              |\n    | `Alt+Enter` | Newline             |\n    | `Ctrl+E`    | External editor     |\n    | `Ctrl+T`    | Toggle auto-approve |\n    | `Ctrl+C`    | Interrupt           |\n    | `Ctrl+D`    | Exit                |\n  </Accordion>\n</AccordionGroup>\n\n## Set project conventions with memories\n\nAgents store information in `~/.deepagents/AGENT_NAME/memories/` as markdown files using a memory-first protocol:\n\n1. **Research**: Searches memory for relevant context before starting tasks\n2. **Response**: Checks memory when uncertain during execution\n3. **Learning**: Automatically saves new information for future sessions\n\nOrganize memories by topic with descriptive filenames:\n\n```\n~/.deepagents/backend-dev/memories/\n\u251c\u2500\u2500 api-conventions.md\n\u251c\u2500\u2500 database-schema.md\n\u2514\u2500\u2500 deployment-process.md\n```\n\nTeach the agent conventions once:\n\n```bash  theme={null}\nuvx deepagents-cli --agent backend-dev\n> Our API uses snake_case and includes created_at/updated_at timestamps\n```\n\nIt remembers for future sessions:\n\n```bash  theme={null}\n> Create a /users endpoint\n# Applies conventions without prompting\n```\n\n## Use remote sandboxes\n\nExecute code in isolated remote environments for safety and flexibility. Remote sandboxes provide the following benefits:\n\n* **Safety**: Protect your local machine from potentially harmful code execution\n* **Clean environments**: Use specific dependencies or OS configurations without local setup\n* **Parallel execution**: Run multiple agents simultaneously in isolated environments\n* **Long-running tasks**: Execute time-intensive operations without blocking your machine\n* **Reproducibility**: Ensure consistent execution environments across teams\n\nTo use a remote sandbox, follow these steps:\n\n1. Configure your sandbox provider ([Runloop](https://www.runloop.ai/), [Daytona](https://www.daytona.io/), or [Modal](https://modal.com/)):\n\n   ```bash  theme={null}\n   # Runloop\n   export RUNLOOP_API_KEY=\"your-key\"\n\n   # Daytona\n   export DAYTONA_API_KEY=\"your-key\"\n\n   # Modal\n   modal setup\n   ```\n\n2. Run the CLI with a sandbox:\n\n   ```bash  theme={null}\n   uvx deepagents-cli --sandbox runloop --sandbox-setup ./setup.sh\n   ```\n\n   The agent runs locally but executes all code operations in the remote sandbox. Optional setup scripts can configure environment variables, clone repositories, and prepare dependencies.\n\n3. (Optional) Create a `setup.sh` file to configure your sandbox environment:\n\n   ```bash  theme={null}\n   #!/bin/bash\n   set -e\n\n   # Clone repository using GitHub token\n   git clone https://x-access-token:${GITHUB_TOKEN}@github.com/username/repo.git $HOME/workspace\n   cd $HOME/workspace\n\n   # Make environment variables persistent\n   cat >> ~/.bashrc <<'EOF'\n   export GITHUB_TOKEN=\"${GITHUB_TOKEN}\"\n   export OPENAI_API_KEY=\"${OPENAI_API_KEY}\"\n   cd $HOME/workspace\n   EOF\n\n   source ~/.bashrc\n   ```\n\n   Store secrets in a local `.env` file for the setup script to access.\n\n<Warning>\n  Sandboxes isolate code execution, but agents remain vulnerable to prompt injection with untrusted inputs. Use human-in-the-loop approval, short-lived secrets, and trusted setup scripts only. Note that sandbox APIs are evolving rapidly, and we expect more providers to support proxies that help mitigate prompt injection and secrets management concerns.\n</Warning>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/cli.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "3bf408bf-8534-4233-ac21-67bb0bcca2c4",
    "id": "91025a49-021b-46aa-8c38-ae8bbd63e585",
    "content": "Result 1:\nTitle: Build a multi-source knowledge base with routing\nLink: https://docs.langchain.comoss/javascript/langchain/multi-agent/router-knowledge-base\nContent: Full persistence approach\nIf you need the router itself to maintain state\u2014for example, to use previous search results in routing decisions\u2014use persistence to store message history at the router level. Stateful routers add complexity. When routing to different agents across turns, conversations may feel inconsistent if agents have different tones or prompts. Consider the handoffs pattern or subagents pattern instead\u2014both provide clearer semantics for multi-turn conversations with different agents.\n\n9. Advanced: Stateful routers\nThe router we've built so far is stateless \u2014each request is handled independently with no memory between calls. For multi-turn conversations, you need a stateful approach.\n\nTool wrapper approach\nThe simplest way to add conversation memory is to wrap the stateless router as a tool that a conversational agent can call: import { MemorySaver } from \"@langchain/langgraph\";\n\nconst searchKnowledgeBase = tool(\n  async ({ query }) => {\n    const result = await workflow.invoke({ query });\n    return result.finalAnswer;\n  },\n  {\n    name: \"search_knowledge_base\",\n    description: `Search across multiple knowledge sources (GitHub, Notion, Slack).\nUse this to find information about code, documentation, or team discussions.`,\n    schema: z.object({\n      query: z.string().describe(\"The search query\"),\n    }),\n  }\n);\n\nconst conversationalAgent = createAgent({\n  model: llm,\n  tools: [searchKnowledgeBase],\n  systemPrompt: `\nYou are a helpful assistant that answers questions about our organization.\nUse the search_knowledge_base tool to find information across our code,\ndocumentation, and team discussions.\n  `.trim(),\n  checkpointer: new MemorySaver(),\n});\n\n---\n\nResult 2:\nTitle: Build a multi-source knowledge base with routing\nLink: https://docs.langchain.comoss/python/langchain/multi-agent/router-knowledge-base\nContent: Full persistence approach\nIf you need the router itself to maintain state\u2014for example, to use previous search results in routing decisions\u2014use persistence to store message history at the router level. Stateful routers add complexity. When routing to different agents across turns, conversations may feel inconsistent if agents have different tones or prompts. Consider the handoffs pattern or subagents pattern instead\u2014both provide clearer semantics for multi-turn conversations with different agents.\n\n9. Advanced: Stateful routers\nThe router we've built so far is stateless \u2014each request is handled independently with no memory between calls. For multi-turn conversations, you need a stateful approach.\n\nTool wrapper approach\nThe simplest way to add conversation memory is to wrap the stateless router as a tool that a conversational agent can call: from langgraph.checkpoint.memory import InMemorySaver\n\n\n@tool\ndef search_knowledge_base(query: str) -> str:\n    \"\"\"Search across multiple knowledge sources (GitHub, Notion, Slack).\n\n    Use this to find information about code, documentation, or team discussions.\n    \"\"\"\n    result = workflow.invoke({\"query\": query})\n    return result[\"final_answer\"]\n\n\nconversational_agent = create_agent(\n    model,\n    tools=[search_knowledge_base],\n    system_prompt=(\n        \"You are a helpful assistant that answers questions about our organization. \"\n        \"Use the search_knowledge_base tool to find information across our code, \"\n        \"documentation, and team discussions.\"\n    ),\n    checkpointer=InMemorySaver(),\n)\n\n---\n\nResult 3:\nTitle: Persistence\nLink: https://docs.langchain.comoss/python/langgraph/persistence\nContent: Persistence\nPersistence\nLangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread , which can be accessed after graph execution. Because threads allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail. Agent Server handles checkpointing automatically When using the Agent Server , you don't need to implement or configure checkpointers manually. The server handles all persistence infrastructure for you behind the scenes.\n\nPersistence\nLangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread , which can be accessed after graph execution. Because threads allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail. Agent Server handles checkpointing automatically When using the Agent Server , you don't need to implement or configure checkpointers manually. The server handles all persistence infrastructure for you behind the scenes.\n\nMemory Store\nA state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence. But, what if we want to retain some information across threads ? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user! With checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable. LangGraph API handles stores automatically When using the LangGraph API, you don't need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "2bb7fb75-e4d6-4140-ba14-1e70553c8af1",
    "id": "2c8090b1-4da6-4524-8774-c00adbe826af",
    "content": "{\n  \"collections\": \"all\",\n  \"total\": 73,\n  \"articles\": [\n    {\n      \"id\": \"7a5d1732-c25b-4a26-9686-52ddfd17926e\",\n      \"title\": \"Merging LangSmith Runs from Remote Graph\",\n      \"url\": \"https://support.langchain.com/articles/9499391881-merging-langsmith-runs-from-remote-graph\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"796cd8bc-a7d9-419d-8141-83998b267e90\",\n      \"title\": \"Resolving common issues when upgrading LangSmith Helm chart from 0.11.x to 0.12.x?\",\n      \"url\": \"https://support.langchain.com/articles/5255204348-resolving-common-issues-when-upgrading-langsmith-helm-chart-from-0-11-x-to-0-12-x\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"adeafe65-c1b6-4620-b3aa-044873fd8c9c\",\n      \"title\": \"Why are my traces being automatically upgraded to extended retention?\",\n      \"url\": \"https://support.langchain.com/articles/3206725817-why-are-my-traces-being-automatically-upgraded-to-extended-retention\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"73778fa5-7613-4254-b687-7b0288ac9903\",\n      \"title\": \"Why am I getting 429 Rate Limit Errors during load testing on self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8364157755-why-am-i-getting-429-rate-limit-errors-during-load-testing-on-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"27a421b4-e877-4a0f-8f38-7b719b189785\",\n      \"title\": \"PagerDuty Alert Severity Not Respecting Configuration\",\n      \"url\": \"https://support.langchain.com/articles/8222693126-pagerduty-alert-severity-not-respecting-configuration\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"adf84afd-7d07-4e11-8645-a3c9a1686f08\",\n      \"title\": \"Session Timeout Configuration for LangSmith Self-Hosted\",\n      \"url\": \"https://support.langchain.com/articles/9515017706-session-timeout-configuration-for-langsmith-self-hosted\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2fcf3091-0038-4226-b56f-4559d6cb1dbc\",\n      \"title\": \"Cleaning ClickHouse System Log Tables\",\n      \"url\": \"https://support.langchain.com/articles/9482219057-cleaning-clickhouse-system-log-tables\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"065735b3-432b-41ce-8469-1494cb753727\",\n      \"title\": \"Why are my agent health checks failing with GCP Gateway API on GKE?\",\n      \"url\": \"https://support.langchain.com/articles/3603461301-why-are-my-agent-health-checks-failing-with-gcp-gateway-api-on-gke\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f9775b3-e85d-4368-ad1b-21f9a8f7552e\",\n      \"title\": \"Evaluator Creation with LiteLLM Proxy - Missing API Key Error\",\n      \"url\": \"https://support.langchain.com/articles/2378929317-evaluator-creation-with-litellm-proxy-missing-api-key-error\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3a81a902-1bca-463d-ab2d-abefd38f8721\",\n      \"title\": \"Why did my evaluator token usage and costs increase after upgrading LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/7643067789-why-did-my-evaluator-token-usage-and-costs-increase-after-upgrading-langsmith\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"61f0452e-5198-4524-9c94-a9cf476ea5bc\",\n      \"title\": \"How to Create a Locked-Down Deployment with Restricted Access\",\n      \"url\": \"https://support.langchain.com/articles/5382256129-how-to-create-a-locked-down-deployment-with-restricted-access\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"c0615bd9-c4a2-4a54-af03-0ee07470ee43\",\n      \"title\": \"How do I configure LangSmith pricing match patterns for AWS Bedrock inference profiles?\",\n      \"url\": \"https://support.langchain.com/articles/4241511543-how-do-i-configure-langsmith-pricing-match-patterns-for-aws-bedrock-inference-profiles\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"365a0d3f-d784-407d-b741-fe51fa8356d4\",\n      \"title\": \"LangSmith Deployments not working with Valkey\",\n      \"url\": \"https://support.langchain.com/articles/2265174426-why-are-my-langgraph-requests-hanging-when-using-aws-elasticache-with-valkey\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"818454c3-a656-4811-a7ca-56cb13df2355\",\n      \"title\": \"How do I enable KMS encryption for S3 storage in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9381186430-how-do-i-enable-kms-encryption-for-s3-storage-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a3c79a7d-390f-43e6-8234-8b55a7f191dd\",\n      \"title\": \"Streaming Failures When Using Shared Custom Redis Instances\",\n      \"url\": \"https://support.langchain.com/articles/3842219806-streaming-failures-when-using-shared-custom-redis-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"55a2c6fe-adf8-4b55-bcb9-969b465241b7\",\n      \"title\": \"Why are my agent threads getting deleted automatically?\",\n      \"url\": \"https://support.langchain.com/articles/7993792111-why-are-my-threads-being-marked-as-temporary-and-getting-deleted-automatically\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b4ba8c34-6d1d-4653-8123-c2eeee2d34e5\",\n      \"title\": \"How do I set up ABAC to restrict user access to specific projects?\",\n      \"url\": \"https://support.langchain.com/articles/6539560119-how-do-i-set-up-abac-to-restrict-user-access-to-specific-projects\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b42f572e-0490-4bb8-9c81-9f065f0c83ed\",\n      \"title\": \"Productionizing and Scaling Self-Hosted LangSmith - Best Practices\",\n      \"url\": \"https://support.langchain.com/articles/8478795211-productionizing-and-scaling-self-hosted-langsmith-best-practices\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"d270716f-edc0-49d9-b5d1-497b5d0e4e6d\",\n      \"title\": \"Private Network Access Blocking LangSmith Studio Connection\",\n      \"url\": \"https://support.langchain.com/articles/1897181200-private-network-access-blocking-langsmith-studio-connection\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"b20fd0bf-1ae9-445c-86ba-5a223112756f\",\n      \"title\": \"How to setup private container registry authentication for LangGraph agent deployments\",\n      \"url\": \"https://support.langchain.com/articles/8606755009-how-to-setup-private-container-registry-authentication-for-langgraph-cloud-agent-deployments\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f6f6fa0-610c-4a15-a4b9-987e932f6a3a\",\n      \"title\": \"How to setup path-based routing for self-hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/2352997668-how-to-setup-path-based-routing-for-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"cc4be529-34c6-458b-8bb3-621fab0e2ab4\",\n      \"title\": \"Best practices for scaling Self-Hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/5142502915-best-practices-for-scaling-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3cec875c-3c8d-4951-b83b-0ac5b8ba7c7c\",\n      \"title\": \"LangSmith Deployments agent is running, but the UI shows it as \\u201cUnsuccessful\\u201d\",\n      \"url\": \"https://support.langchain.com/articles/8394994213-langsmith-deployments-agent-is-running-but-the-ui-shows-it-as-unsuccessful\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"57055dee-474d-4635-93a3-79439570f6ac\",\n      \"title\": \"How do I prevent traces from mixing between projects during concurrent requests?\",\n      \"url\": \"https://support.langchain.com/articles/4784014353-how-do-i-prevent-traces-from-mixing-between-projects-during-concurrent-requests\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9619ff10-34d0-49ea-a7ee-cdbbddafc1e4\",\n      \"title\": \"How do I resolve \\\"too many connections\\\" PostgreSQL errors when scaling platform instances?\",\n      \"url\": \"https://support.langchain.com/articles/9651943867-how-do-i-resolve-too-many-connections-postgresql-errors-when-scaling-platform-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"b740be98-677d-486f-ba77-12258712f70a\",\n      \"title\": \"How do I use the LangSmith Control Plane API to manage deployments?\",\n      \"url\": \"https://support.langchain.com/articles/8184800662-how-do-i-use-the-langgraph-platform-control-plane-api-to-manage-deployments\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"52d389ff-3918-46a8-9ccf-55fe972077bf\",\n      \"title\": \"Why is my LangSmith agent queue Deployment timing out after 600 seconds?\",\n      \"url\": \"https://support.langchain.com/articles/7898522976-why-is-my-queue-deployment-timing-out-after-600-seconds\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"1caf7aa7-5382-43f5-869c-27212fe9f287\",\n      \"title\": \"How do I resolve ClickHouse migration errors after upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/6463985661-how-do-i-resolve-clickhouse-migration-errors-after-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0ff2089a-501c-45b8-b557-4947639c5072\",\n      \"title\": \"How do I disable public sharing for traces in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5345880639-how-do-i-disable-public-sharing-for-traces-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\",\n      \"title\": \"What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\",\n      \"url\": \"https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"bef0ce3c-94c2-4999-b563-ff016fa3884f\",\n      \"title\": \"How do I get LangChain to complete security questionnaires for vendor assessments?\",\n      \"url\": \"https://support.langchain.com/articles/8555158040-how-do-i-get-langchain-to-complete-security-questionnaires-for-vendor-assessments\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"fe2b3c9e-8fa1-49dd-a52d-df92d88050c4\",\n      \"title\": \"How do I update billing information on an existing invoice?\",\n      \"url\": \"https://support.langchain.com/articles/5832977338-how-do-i-update-billing-information-on-an-existing-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"6ea47321-cd15-49a7-a972-be33139a5fe3\",\n      \"title\": \"Can I use an external database cluster to host multiple self-hosted LangSmith instances?\",\n      \"url\": \"https://support.langchain.com/articles/5989664339-can-i-use-an-external-database-cluster-to-host-multiple-self-hosted-langsmith-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"4a127c79-3504-4afa-befa-ec974b85c5f8\",\n      \"title\": \"Why am I being charged for node executions and standby minutes on my development deployment?\",\n      \"url\": \"https://support.langchain.com/articles/2626278629-why-am-i-being-charged-for-node-executions-on-my-development-deployment\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"a442595a-454c-4208-8854-d51f632a3d0f\",\n      \"title\": \"Managing ClickHouse Storage\",\n      \"url\": \"https://support.langchain.com/articles/2929899751-managing-clickhouse-storage\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"c6c77d41-4376-4452-b7a0-fd42c1428242\",\n      \"title\": \"How do I deploy an MCP server on LangSmith Deployments with my agent?\",\n      \"url\": \"https://support.langchain.com/articles/8743137454-how-do-i-deploy-an-mcp-server-on-langsmith-deployments-with-my-agent\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"e5b95386-5971-4e48-be5b-9d9c7dcfe8e3\",\n      \"title\": \"How do I archive a prompt instead of deleting it?\",\n      \"url\": \"https://support.langchain.com/articles/6404244010-how-do-i-archive-a-prompt-instead-of-deleting-it\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"7444d2d0-3f90-42d8-aa4c-b36fa7665e05\",\n      \"title\": \"How do I resolve Redis disk space issues in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5370519454-how-do-i-resolve-redis-disk-space-issues-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"1076b041-020f-4738-acd0-d8ecc07f19ed\",\n      \"title\": \"How do I add existing organization members to new workspaces programmatically via API?\",\n      \"url\": \"https://support.langchain.com/articles/5797508505-how-do-i-add-existing-organization-members-to-new-workspaces-programmatically-via-api\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a91a5cf4-a8a3-4ad5-b9ef-9d0d534d0ba4\",\n      \"title\": \"ClickHouse: Drop Old Data\",\n      \"url\": \"https://support.langchain.com/articles/2379935803-clickhouse-drop-old-data\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5886208c-1f87-4672-87c4-053f2773c161\",\n      \"title\": \"How do I filter out traces that contain a specific node name in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8185473222-how-do-i-filter-out-traces-that-contain-a-specific-node-name-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b170d29c-925a-4aab-b7a0-354f47b7dc77\",\n      \"title\": \"Can I enforce Google SSO-only authentication for my organization?\",\n      \"url\": \"https://support.langchain.com/articles/9820106567-can-i-enforce-google-sso-only-authentication-for-my-organization\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b5128a24-178a-4063-9337-c273af5ac617\",\n      \"title\": \"Why is my trace exploration slow in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9719508217-why-is-my-trace-exploration-slow-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"50634017-ff08-43bb-b7ae-7518aab83623\",\n      \"title\": \"How do I get access to LangSmith Agent Builder?\",\n      \"url\": \"https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"4ad14b9c-abe8-47b0-a416-81dbe2a3a127\",\n      \"title\": \"Why are some experiment runs missing when processing large datasets in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/3506757366-why-are-some-experiment-runs-missing-when-processing-large-datasets-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"40305680-01d3-4506-8257-16db939e39bf\",\n      \"title\": \"How do I get help with billing issues and payment problems?\",\n      \"url\": \"https://support.langchain.com/articles/3779255598-how-do-i-get-help-with-billing-issues-and-payment-problems\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"9273a5a9-0a53-4d5a-917a-c19ebeec64fe\",\n      \"title\": \"How do I get a copy of my LangSmith invoice?\",\n      \"url\": \"https://support.langchain.com/articles/6317483716-how-do-i-get-a-copy-of-my-langsmith-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"ebea1a08-4488-4cac-9c65-75442f3de254\",\n      \"title\": \"Adding FastAPI dependency for custom middleware in LangGraph Platform\",\n      \"url\": \"https://support.langchain.com/articles/7593416758-adding-fastapi-dependency-for-custom-middleware-in-langgraph-platform\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"d37491ce-7f3c-4575-91d5-09c62129248e\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/8574277609-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"48541052-b69e-4e78-ae8b-c2ef4fe26af1\",\n      \"title\": \"Resolving LangGraph and LangSmith version compatibility issues in production\",\n      \"url\": \"https://support.langchain.com/articles/5102702588-resolving-langgraph-and-langsmith-version-compatibility-issues-in-production\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0b6ce386-e341-47a1-8be9-ae350de16843\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/3873263156-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"68fbf7da-954d-454e-8ad4-73e1f1f63b74\",\n      \"title\": \"LangSmith Studio vs Agent Chat UI: When to Use What\",\n      \"url\": \"https://support.langchain.com/articles/1353449293-langsmith-studio-vs-agent-chat-ui\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f7dc0a0c-fa3f-4319-b820-ae470d89877f\",\n      \"title\": \"Why is token-by-token streaming not working after upgrading LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/7150806184-why-is-token-by-token-streaming-not-working-after-upgrading-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"ada8fd1b-4800-434e-8d6f-b5186b170941\",\n      \"title\": \"LangSmith Studio URL Redirects to Error Page\",\n      \"url\": \"https://support.langchain.com/articles/4270790436-langsmith-studio-url-redirects-to-error-page\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f41e0fb9-7ed0-4d9b-85b0-fe88fd805f02\",\n      \"title\": \"Why am I getting service interruption errors when using text search filters?\",\n      \"url\": \"https://support.langchain.com/articles/6919527577-why-am-i-getting-service-interruption-errors-when-using-text-search-filters\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\",\n      \"title\": \"Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\",\n      \"url\": \"https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b1e1ee29-1124-4a92-8f9f-eaa4a679df05\",\n      \"title\": \"Why am I getting a parsing token error when upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/7019615393-why-am-i-getting-a-token-parsing-error-when-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"da8e3156-8405-4c12-a347-96d6590da06c\",\n      \"title\": \"LangSmith upgrade fails with SSL certificate verification error when connecting to Azure Cache for Redis\",\n      \"url\": \"https://support.langchain.com/articles/7558740603-langsmith-helm-upgrade-fails-with-ssl-certificate-verification-error-when-connecting-to-azure-cache-for-redis\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"af6b64a9-ed4a-4993-b54d-d4913eeb1d26\",\n      \"title\": \"How do I change my email address in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9223968956-how-do-i-change-my-email-address-in-langsmith\",\n      \"collection\": \"General\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3848-0ece-7562-b19f-d18bb752794f",
    "content": "**No, the Deep Agents CLI is designed to persist memories and context across sessions by default.**\n\nWhen you run the CLI, it uses a \"memory-first\" protocol to research existing context before starting tasks and automatically saves new information to your local machine. This allows agents to \"learn\" project conventions and remember your preferences over time.\n\n## Persistent Storage Location\n\nAll agent data, including memories and learned patterns, is stored locally on your machine:\n\n```bash\n# Data is organized by agent name in your home directory\n~/.deepagents/AGENT_NAME/memories/\n\u251c\u2500\u2500 api-conventions.md\n\u251c\u2500\u2500 database-schema.md\n\u2514\u2500\u2500 project-context.md\n```\n\nIf you want to maintain separate contexts for different projects, you should use the `--agent` flag to give each a unique name:\n\n```bash\n# Starts or resumes a specific persistent agent\nuvx deepagents-cli --agent backend-dev\n```\n\n## How to Reset State\n\nIf you actually want to start with a fresh state (empty memories and default config), you must explicitly run the reset command:\n\n```bash\n# Clears all saved memories for the specified agent\ndeepagents reset --agent NAME\n```\n\n## Configuration Persistence\n\nConfiguration like API keys (Anthropic, OpenAI, Tavily) is typically managed via environment variables or a `.env` file in your project root. These remain until you change or delete them. If you use a remote sandbox (`--sandbox`), the execution environment is fresh each time unless you provide a `setup.sh` script to configure it.\n\n**Relevant docs:**\n\n- [Deep Agents CLI Overview](https://docs.langchain.com/oss/python/deepagents/cli)\n- [Setting Project Conventions with Memories](https://docs.langchain.com/oss/python/deepagents/cli#set-project-conventions-with-memories)"
  },
  {
    "role": "user",
    "id": "05af360a-ac25-4b00-89e4-472a000b7ff2",
    "content": "So how can i also use skills but then with normal deep agents"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "76d47d99-0cf8-49ff-b9ed-3a0648c5825d",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"skills\"}"
        }
      },
      {
        "type": "function",
        "id": "295a491b-0180-4656-882f-07009a3484df",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"tools\"}"
        }
      },
      {
        "type": "function",
        "id": "96647ae4-7864-46b8-b542-4af36d63ecc2",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"all\"}"
        }
      }
    ],
    "id": "lc_run--019b384a-a0a2-7a52-976c-92f84137434b",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "76d47d99-0cf8-49ff-b9ed-3a0648c5825d",
    "id": "2f78bb7a-f6f4-4ca6-85d4-af02a5e594ee",
    "content": "Result 1:\nTitle: Skills\nLink: https://docs.langchain.comoss/javascript/langchain/multi-agent/skills\nContent: In the **skills** architecture, specialized capabilities are packaged as invokable \"skills\" that augment an [agent's](/oss/javascript/langchain/agents) behavior. Skills are primarily prompt-driven specializations that an agent can invoke on-demand.\n\n<Tip>\n  This pattern is conceptually identical to [llms.txt](https://llmstxt.org/) (introduced by Jeremy Howard), which uses tool calling for progressive disclosure of documentation. The skills pattern applies the same approach to specialized prompts and domain knowledge rather than just documentation pages.\n</Tip>\n\n```mermaid  theme={null}\ngraph LR\n    A[User] --> B[Agent]\n    B --> C[Skill A]\n    B --> D[Skill B]\n    B --> E[Skill C]\n    B --> A\n```\n\n## Key characteristics\n\n* Prompt-driven specialization: Skills are primarily defined by specialized prompts\n* Progressive disclosure: Skills become available based on context or user needs\n* Team distribution: Different teams can develop and maintain skills independently\n* Lightweight composition: Skills are simpler than full sub-agents\n\n## When to use\n\nUse the skills pattern when you want a single [agent](/oss/javascript/langchain/agents) with many possible specializations, you don't need to enforce specific constraints between skills, or different teams need to develop capabilities independently. Common examples include coding assistants (skills for different languages or tasks), knowledge bases (skills for different domains), and creative assistants (skills for different formats).\n\n## Basic implementation\n\n```typescript  theme={null}\nimport { tool, createAgent } from \"langchain\";\nimport * as z from \"zod\";\n\nconst loadSkill = tool(\n  async ({ skillName }) => {\n    // Load skill content from file/database\n    return \"\";\n  },\n  {\n    name: \"load_skill\",\n    description: `Load a specialized skill.\n\nAvailable skills:\n- write_sql: SQL query writing expert\n- review_legal_doc: Legal document reviewer\n\nReturns the skill's prompt and context.`,\n    schema: z.object({\n      skillName: z\n        .string()\n        .describe(\"Name of skill to load\")\n    })\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [loadSkill],\n  prompt: (\n    \"You are a helpful assistant. \" +\n    \"You have access to two skills: \" +\n    \"write_sql and review_legal_doc. \" +\n    \"Use load_skill to access them.\"\n  ),\n});\n```\n\nFor a complete implementation, see the tutorial below.\n\n<Card title=\"Tutorial: Build a SQL assistant with on-demand skills\" icon=\"wand-magic-sparkles\" href=\"/oss/javascript/langchain/multi-agent/skills-sql-assistant\" arrow cta=\"Learn more\">\n  Learn how to implement skills with progressive disclosure, where the agent loads specialized prompts and schemas on-demand rather than upfront.\n</Card>\n\n## Extending the pattern\n\nWhen writing custom implementations, you can extend the basic skills pattern in several ways:\n\n* **Dynamic tool registration**: Combine progressive disclosure with state management to register new [tools](/oss/javascript/langchain/tools) as skills load. For example, loading a \"database\\_admin\" skill could both add specialized context and register database-specific tools (backup, restore, migrate). This uses the same tool-and-state mechanisms used across multi-agent patterns\u2014tools updating state to dynamically change agent capabilities.\n\n* **Hierarchical skills**: Skills can define other skills in a tree structure, creating nested specializations. For instance, loading a \"data\\_science\" skill might make available sub-skills like \"pandas\\_expert\", \"visualization\", and \"statistical\\_analysis\". Each sub-skill can be loaded independently as needed, allowing for fine-grained progressive disclosure of domain knowledge. This hierarchical approach helps manage large knowledge bases by organizing capabilities into logical groupings that can be discovered and loaded on-demand.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent/skills.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Skills\nLink: https://docs.langchain.comoss/python/langchain/multi-agent/skills\nContent: In the **skills** architecture, specialized capabilities are packaged as invokable \"skills\" that augment an [agent's](/oss/python/langchain/agents) behavior. Skills are primarily prompt-driven specializations that an agent can invoke on-demand.\n\n<Tip>\n  This pattern is conceptually identical to [llms.txt](https://llmstxt.org/) (introduced by Jeremy Howard), which uses tool calling for progressive disclosure of documentation. The skills pattern applies the same approach to specialized prompts and domain knowledge rather than just documentation pages.\n</Tip>\n\n```mermaid  theme={null}\ngraph LR\n    A[User] --> B[Agent]\n    B --> C[Skill A]\n    B --> D[Skill B]\n    B --> E[Skill C]\n    B --> A\n```\n\n## Key characteristics\n\n* Prompt-driven specialization: Skills are primarily defined by specialized prompts\n* Progressive disclosure: Skills become available based on context or user needs\n* Team distribution: Different teams can develop and maintain skills independently\n* Lightweight composition: Skills are simpler than full sub-agents\n\n## When to use\n\nUse the skills pattern when you want a single [agent](/oss/python/langchain/agents) with many possible specializations, you don't need to enforce specific constraints between skills, or different teams need to develop capabilities independently. Common examples include coding assistants (skills for different languages or tasks), knowledge bases (skills for different domains), and creative assistants (skills for different formats).\n\n## Basic implementation\n\n```python  theme={null}\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n@tool\ndef load_skill(skill_name: str) -> str:\n    \"\"\"Load a specialized skill prompt.\n\n    Available skills:\n    - write_sql: SQL query writing expert\n    - review_legal_doc: Legal document reviewer\n\n    Returns the skill's prompt and context.\n    \"\"\"\n    # Load skill content from file/database\n    ...\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[load_skill],\n    system_prompt=(\n        \"You are a helpful assistant. \"\n        \"You have access to two skills: \"\n        \"write_sql and review_legal_doc. \"\n        \"Use load_skill to access them.\"\n    ),\n)\n```\n\nFor a complete implementation, see the tutorial below.\n\n<Card title=\"Tutorial: Build a SQL assistant with on-demand skills\" icon=\"wand-magic-sparkles\" href=\"/oss/python/langchain/multi-agent/skills-sql-assistant\" arrow cta=\"Learn more\">\n  Learn how to implement skills with progressive disclosure, where the agent loads specialized prompts and schemas on-demand rather than upfront.\n</Card>\n\n## Extending the pattern\n\nWhen writing custom implementations, you can extend the basic skills pattern in several ways:\n\n* **Dynamic tool registration**: Combine progressive disclosure with state management to register new [tools](/oss/python/langchain/tools) as skills load. For example, loading a \"database\\_admin\" skill could both add specialized context and register database-specific tools (backup, restore, migrate). This uses the same tool-and-state mechanisms used across multi-agent patterns\u2014tools updating state to dynamically change agent capabilities.\n\n* **Hierarchical skills**: Skills can define other skills in a tree structure, creating nested specializations. For instance, loading a \"data\\_science\" skill might make available sub-skills like \"pandas\\_expert\", \"visualization\", and \"statistical\\_analysis\". Each sub-skill can be loaded independently as needed, allowing for fine-grained progressive disclosure of domain knowledge. This hierarchical approach helps manage large knowledge bases by organizing capabilities into logical groupings that can be discovered and loaded on-demand.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent/skills.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Build a SQL assistant with on-demand skills\nLink: https://docs.langchain.comoss/python/langchain/multi-agent/skills-sql-assistant\nContent: 1. Define skills\nFirst, define the structure for skills. Each skill has a name, a brief description (shown in the system prompt), and full content (loaded on-demand): Now define example skills for a SQL query assistant. The skills are designed to be lightweight in description (shown to the agent upfront) but detailed in content (loaded only when needed):\n\nBuild a SQL assistant with on-demand skills\nBuild a SQL assistant with on-demand skills\nThis tutorial shows how to use progressive disclosure - a context management technique where the agent loads information on-demand rather than upfront - to implement skills (specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task. Use case: Imagine building an agent to help write SQL queries across different business verticals in a large enterprise. Your organization might have separate datastores for each vertical, or a single monolithic database with thousands of tables. Either way, loading all schemas upfront would overwhelm the context window. Progressive disclosure solves this by loading only the relevant schema when needed. This architecture also enables different product owners and stakeholders to independently contribute and maintain skills for their specific business verticals. What you'll build: A SQL query assistant with two skills (sales analytics and inventory management). The agent sees lightweight skill descriptions in its system prompt, then loads full database schemas and business logic through tool calls only when relevant to the user's query. For a more complete example of a SQL agent with query execution, error correction, and validation, see our SQL Agent tutorial . This tutorial focuses on the progressive disclosure pattern which can be applied to any domain. Progressive disclosure was popularized by Anthropic as a technique for building scalable agent skills systems. This approach uses a three-level architecture (metadata \u2192 core content \u2192 detailed resources) where agents load information only as needed. For more on this technique, see Equipping agents for the real world with Agent Skills.\n\nBuild a SQL assistant with on-demand skills\nThis tutorial shows how to use progressive disclosure - a context management technique where the agent loads information on-demand rather than upfront - to implement skills (specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task. Use case: Imagine building an agent to help write SQL queries across different business verticals in a large enterprise. Your organization might have separate datastores for each vertical, or a single monolithic database with thousands of tables. Either way, loading all schemas upfront would overwhelm the context window. Progressive disclosure solves this by loading only the relevant schema when needed. This architecture also enables different product owners and stakeholders to independently contribute and maintain skills for their specific business verticals. What you'll build: A SQL query assistant with two skills (sales analytics and inventory management). The agent sees lightweight skill descriptions in its system prompt, then loads full database schemas and business logic through tool calls only when relevant to the user's query. For a more complete example of a SQL agent with query execution, error correction, and validation, see our SQL Agent tutorial . This tutorial focuses on the progressive disclosure pattern which can be applied to any domain. Progressive disclosure was popularized by Anthropic as a technique for building scalable agent skills systems. This approach uses a three-level architecture (metadata \u2192 core content \u2192 detailed resources) where agents load information only as needed. For more on this technique, see Equipping agents for the real world with Agent Skills.\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "295a491b-0180-4656-882f-07009a3484df",
    "id": "8e4b0482-ecf4-4f90-b343-8f8f6ba5751f",
    "content": "Result 1:\nTitle: Tools and Toolkits\nLink: https://docs.langchain.comoss/javascript/integrations/tools/index\nContent: [Tools](/oss/javascript/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n\nA [toolkit](/oss/javascript/langchain/tools#toolkits) is a collection of tools meant to be used together.\n\n## Integration Platforms\n\nThe following platforms provide access to multiple tools and services through a unified interface:\n\n| Tool/Toolkit                                            | Number of Integrations | Pricing             | Key Features                                               |\n| ------------------------------------------------------- | ---------------------- | ------------------- | ---------------------------------------------------------- |\n| [Composio](/oss/javascript/integrations/tools/composio) | 500+                   | Free tier available | OAuth handling, event-driven workflows, multi-user support |\n\n## All Tools and Toolkits\n\n<Columns cols={3}>\n  <Card title=\"Azure Container Apps Dynamic Sessions\" icon=\"link\" href=\"/oss/javascript/integrations/tools/azure_dynamic_sessions\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Connery Action Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/connery\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Composio\" icon=\"link\" href=\"/oss/javascript/integrations/tools/composio\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dall-E Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/dalle\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Decodo Tools\" icon=\"link\" href=\"/oss/javascript/integrations/tools/decodo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Discord Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/discord_tool\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DuckDuckGoSearch\" icon=\"link\" href=\"/oss/javascript/integrations/tools/duckduckgo_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ExaSearchResults\" icon=\"link\" href=\"/oss/javascript/integrations/tools/exa_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FalkorDB\" icon=\"link\" href=\"/oss/javascript/integrations/tools/falkordb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gmail Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_gmail\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GOAT\" icon=\"link\" href=\"/oss/javascript/integrations/tools/goat\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Calendar Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_calendar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Places Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_places\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Routes Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_routes\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Scholar\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_scholar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Trends Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_trends\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"JigsawStack Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/jigsawstack\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Agent with AWS Lambda\" icon=\"link\" href=\"/oss/javascript/integrations/tools/lambda_agent\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Python interpreter tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/pyinterpreter\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SearchApi tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/searchapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Searxng Search tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/searxng\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SerpAPI\" icon=\"link\" href=\"/oss/javascript/integrations/tools/serpapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"StackExchange Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/stackexchange\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Stagehand AI Web Automation Toolkit\" icon=\"link\" href=\"/oss/javascript/integrations/tools/stagehand\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Crawl\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_crawl\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Extract\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_extract\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Map\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_map\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Search\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Web Browser Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/webbrowser\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wikipedia tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/wikipedia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"WolframAlpha Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/wolframalpha\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n<Info>\n  If you'd like to write your own tool, see [this how-to](/oss/javascript/langchain/tools#customize-tool-properties). If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).\n</Info>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/tools/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Tools\nLink: https://docs.langchain.comoss/javascript/langchain/tools\nContent: Tools extend what [agents](/oss/javascript/langchain/agents) can do\u2014letting them fetch real-time data, execute code, query external databases, and take actions in the world.\n\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a [chat model](/oss/javascript/langchain/models). The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\n\n<Tip>\n  For details on how models handle tool calls, see [Tool calling](/oss/javascript/langchain/models#tool-calling).\n</Tip>\n\n## Create tools\n\n### Basic tool definition\n\nThe simplest way to create a tool is by importing the `tool` function from the `langchain` package. You can use [zod](https://zod.dev/) to define the tool's input schema:\n\n```ts  theme={null}\nimport * as z from \"zod\"\nimport { tool } from \"langchain\"\n\nconst searchDatabase = tool(\n  ({ query, limit }) => `Found ${limit} results for '${query}'`,\n  {\n    name: \"search_database\",\n    description: \"Search the customer database for records matching the query.\",\n    schema: z.object({\n      query: z.string().describe(\"Search terms to look for\"),\n      limit: z.number().describe(\"Maximum number of results to return\"),\n    }),\n  }\n);\n```\n\n<Note>\n  **Server-side tool use**\n\n  Some chat models (e.g., [OpenAI](/oss/javascript/integrations/chat/openai), [Anthropic](/oss/javascript/integrations/chat/anthropic), and [Gemini](/oss/javascript/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/javascript/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/javascript/integrations/providers/overview) to learn how to access these tools with your specific chat model.\n</Note>\n\n## Accessing Context\n\n<Info>\n  **Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\n  The runtime context provides a structured way to supply runtime data, such as DB connections, user IDs, or config, into your tools. This avoids global state and keeps tools testable and reusable.\n</Info>\n\n#### Context\n\nTools can access an agent's runtime context through the `config` parameter:\n\n```ts  theme={null}\nimport * as z from \"zod\"\nimport { ChatOpenAI } from \"@langchain/openai\"\nimport { createAgent } from \"langchain\"\n\nconst getUserName = tool(\n  (_, config) => {\n    return config.context.user_name\n  },\n  {\n    name: \"get_user_name\",\n    description: \"Get the user's name.\",\n    schema: z.object({}),\n  }\n);\n\nconst contextSchema = z.object({\n  user_name: z.string(),\n});\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserName],\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"What is my name?\" }]\n  },\n  {\n    context: { user_name: \"John Smith\" }\n  }\n);\n```\n\n#### Memory (Store)\n\nAccess persistent data across conversations using the store. The store is accessed via `config.store` and allows you to save and retrieve user-specific or application-specific data.\n\n```ts expandable theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst store = new InMemoryStore();\n\n// Access memory\nconst getUserInfo = tool(\n  async ({ user_id }) => {\n    const value = await store.get([\"users\"], user_id);\n    console.log(\"get_user_info\", user_id, value);\n    return value;\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({\n      user_id: z.string(),\n    }),\n  }\n);\n\n// Update memory\nconst saveUserInfo = tool(\n  async ({ user_id, name, age, email }) => {\n    console.log(\"save_user_info\", user_id, name, age, email);\n    await store.put([\"users\"], user_id, { name, age, email });\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info.\",\n    schema: z.object({\n      user_id: z.string(),\n      name: z.string(),\n      age: z.number(),\n      email: z.string(),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserInfo, saveUserInfo],\n  store,\n});\n\n// First session: save user info\nawait agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\",\n    },\n  ],\n});\n\n// Second session: get user info\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"Get user info for user with id 'abc123'\" },\n  ],\n});\n\nconsole.log(result);\n// Here is the user info for user with ID \"abc123\":\n// - Name: Foo\n// - Age: 25\n// - Email: foo@langchain.dev\n```\n\n#### Stream Writer\n\nStream custom updates from tools as they execute using `config.streamWriter`. This is useful for providing real-time feedback to users about what a tool is doing.\n\n```ts  theme={null}\nimport * as z from \"zod\";\nimport { tool, ToolRuntime } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }, config: ToolRuntime) => {\n    const writer = config.writer;\n\n    // Stream custom updates as the tool executes\n    if (writer) {\n      writer(`Looking up data for city: ${city}`);\n      writer(`Acquired data for city: ${city}`);\n    }\n\n    return `It's always sunny in ${city}!`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather for a given city.\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Tools\nLink: https://docs.langchain.comoss/python/langchain/tools\nContent: Tools extend what [agents](/oss/python/langchain/agents) can do\u2014letting them fetch real-time data, execute code, query external databases, and take actions in the world.\n\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a [chat model](/oss/python/langchain/models). The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\n\n<Tip>\n  For details on how models handle tool calls, see [Tool calling](/oss/python/langchain/models#tool-calling).\n</Tip>\n\n## Create tools\n\n### Basic tool definition\n\nThe simplest way to create a tool is with the [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) decorator. By default, the function's docstring becomes the tool's description that helps the model understand when to use it:\n\n```python  theme={null}\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str, limit: int = 10) -> str:\n    \"\"\"Search the customer database for records matching the query.\n\n    Args:\n        query: Search terms to look for\n        limit: Maximum number of results to return\n    \"\"\"\n    return f\"Found {limit} results for '{query}'\"\n```\n\nType hints are **required** as they define the tool's input schema. The docstring should be informative and concise to help the model understand the tool's purpose.\n\n<Note>\n  **Server-side tool use**\n\n  Some chat models (e.g., [OpenAI](/oss/python/integrations/chat/openai), [Anthropic](/oss/python/integrations/chat/anthropic), and [Gemini](/oss/python/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/python/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/python/integrations/providers/overview) to learn how to access these tools with your specific chat model.\n</Note>\n\n### Customize tool properties\n\n#### Custom tool name\n\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\n\n```python  theme={null}\n@tool(\"web_search\")  # Custom name\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nprint(search.name)  # web_search\n```\n\n#### Custom tool description\n\nOverride the auto-generated tool description for clearer model guidance:\n\n```python  theme={null}\n@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\ndef calc(expression: str) -> str:\n    \"\"\"Evaluate mathematical expressions.\"\"\"\n    return str(eval(expression))\n```\n\n### Advanced schema definition\n\nDefine complex inputs with Pydantic models or JSON schemas:\n\n<CodeGroup>\n  ```python Pydantic model theme={null}\n  from pydantic import BaseModel, Field\n  from typing import Literal\n\n  class WeatherInput(BaseModel):\n      \"\"\"Input for weather queries.\"\"\"\n      location: str = Field(description=\"City name or coordinates\")\n      units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n          default=\"celsius\",\n          description=\"Temperature unit preference\"\n      )\n      include_forecast: bool = Field(\n          default=False,\n          description=\"Include 5-day forecast\"\n      )\n\n  @tool(args_schema=WeatherInput)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n\n  ```python JSON Schema theme={null}\n  weather_schema = {\n      \"type\": \"object\",\n      \"properties\": {\n          \"location\": {\"type\": \"string\"},\n          \"units\": {\"type\": \"string\"},\n          \"include_forecast\": {\"type\": \"boolean\"}\n      },\n      \"required\": [\"location\", \"units\", \"include_forecast\"]\n  }\n\n  @tool(args_schema=weather_schema)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n</CodeGroup>\n\n### Reserved argument names\n\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\n\n| Parameter name | Purpose                                                                |\n| -------------- | ---------------------------------------------------------------------- |\n| `config`       | Reserved for passing `RunnableConfig` to tools internally              |\n| `runtime`      | Reserved for `ToolRuntime` parameter (accessing state, context, store) |\n\nTo access runtime information, use the [`ToolRuntime`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.ToolRuntime) parameter instead of naming your own arguments `config` or `runtime`.\n\n## Accessing Context\n\n<Info>\n  **Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\n  Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.\n</Info>\n\nTools can access runtime information through the `ToolRuntime` parameter, which provides:\n\n* **State** - Mutable data that flows through execution (e.g., messages, counters, custom fields)\n* **Context** - Immutable configuration like user IDs, session details, or application-specific configuration\n* **Store** - Persistent long-term memory across conversations\n* **Stream Writer** - Stream custom updates as tools execute\n* **Config** - `RunnableConfig` for the execution\n* **Tool Call ID** - ID of the current tool call\n\n```mermaid  theme={null}\ngraph LR\n    %% Runtime Context\n    subgraph \"\ud83d\udd27 Tool Runtime Context\"\n        A[Tool Call] --> B[ToolRuntime]\n        B --> C[State Access]\n        B --> D[Context Access]\n        B --> E[Store Access]\n        B --> F[Stream Writer]\n    end\n\n    %% Available Resources\n    subgraph \"\ud83d\udcca Available Resources\"\n        C --> G[Messages]\n        C --> H[Custom State]\n        D --> I[User ID]\n        D --> J[Session Info]\n        E --> K[Long-term Memory]\n        E --> L[User Preferences]\n    end\n\n    %% Tool Capabilities\n    subgraph \"\u26a1 Enhanced Tool Capabilities\"\n        M[Context-Aware Tools]\n        N[Stateful Tools]\n        O[Memory-Enabled Tools]\n        P[Streaming Tools]\n    end\n\n    %% Connections\n    G --> M\n    H --> N\n    I --> M\n    J --> M\n    K --> O\n    L --> O\n    F --> P\n```\n\n### `ToolRuntime`\n\nUse `ToolRuntime` to access all runtime information in a single parameter. Simply add `runtime: ToolRuntime` to your tool signature, and it will be automatically injected without being exposed to the LLM.\n\n<Info>\n  **`ToolRuntime`**: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate [`InjectedState`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedState), [`InjectedStore`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedStore), [`get_runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.get_runtime), and [`InjectedToolCallId`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.InjectedToolCallId) annotations.\n\n  The runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.\n</Info>\n\n**Accessing state:**\n\nTools can access the current graph state using `ToolRuntime`:\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n# Access the current conversation state\n@tool\ndef summarize_conversation(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Summarize the conversation so far.\"\"\"\n    messages = runtime.state[\"messages\"]\n\n    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n\n    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\n\n# Access custom state fields\n@tool\ndef get_user_preference(\n    pref_name: str,\n    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model\n) -> str:\n    \"\"\"Get a user preference value.\"\"\"\n    preferences = runtime.state.get(\"user_preferences\", {})\n    return preferences.get(pref_name, \"Not set\")\n```\n\n<Warning>\n  The `runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `runtime` is *not* included in the request.\n</Warning>\n\n**Updating state:**\n\nUse [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) to update the agent's state or control the graph's execution flow:\n\n```python  theme={null}\nfrom langgraph.types import Command\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langchain.tools import tool, ToolRuntime\n\n# Update the conversation history by removing all messages\n@tool\ndef clear_conversation() -> Command:\n    \"\"\"Clear the conversation history.\"\"\"\n\n    return Command(\n        update={\n            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],\n        }\n    )\n\n# Update the user_name in the agent state\n@tool\ndef update_user_name(\n    new_name: str,\n    runtime: ToolRuntime\n) -> Command:\n    \"\"\"Update the user's name.\"\"\"\n    return Command(update={\"user_name\": new_name})\n```\n\n#### Context\n\nAccess immutable configuration and contextual data like user IDs, session details, or application-specific configuration through `runtime.context`.\n\nTools can access runtime context through `ToolRuntime`:\n\n```python  theme={null}\nfrom dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\nUSER_DATABASE = {\n    \"user123\": {\n        \"name\": \"Alice Johnson\",\n        \"account_type\": \"Premium\",\n        \"balance\": 5000,\n        \"email\": \"alice@example.com\"\n    },\n    \"user456\": {\n        \"name\": \"Bob Smith\",\n        \"account_type\": \"Standard\",\n        \"balance\": 1200,\n        \"email\": \"bob@example.com\"\n    }\n}\n\n@dataclass\nclass UserContext:\n    user_id: str\n\n@tool\ndef get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n    \"\"\"Get the current user's account information.\"\"\"\n    user_id = runtime.context.user_id\n\n    if user_id in USER_DATABASE:\n        user = USER_DATABASE[user_id]\n        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n    return \"User not found\"\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nagent = create_agent(\n    model,\n    tools=[get_account_info],\n    context_schema=UserContext,\n    system_prompt=\"You are a financial assistant.\"\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n    context=UserContext(user_id=\"user123\")\n)\n```\n\n#### Memory (Store)\n\nAccess persistent data across conversations using the store. The store is accessed via `runtime.store` and allows you to save and retrieve user-specific or application-specific data.\n\nTools can access and update the store through `ToolRuntime`:\n\n```python expandable theme={null}\nfrom typing import Any\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\n# Access memory\n@tool\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n    \"\"\"Look up user info.\"\"\"\n    store = runtime.store\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\n# Update memory\n@tool\ndef save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n    \"\"\"Save user info.\"\"\"\n    store = runtime.store\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nstore = InMemoryStore()\nagent = create_agent(\n    model,\n    tools=[get_user_info, save_user_info],\n    store=store\n)\n\n# First session: save user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n})\n\n# Second session: get user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email: foo@langchain.dev\n```\n\n#### Stream Writer\n\nStream custom updates from tools as they execute using `runtime.stream_writer`. This is useful for providing real-time feedback to users about what a tool is doing.\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather(city: str, runtime: ToolRuntime) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = runtime.stream_writer\n\n    # Stream custom updates as the tool executes\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n\n    return f\"It's always sunny in {city}!\"\n```\n\n<Note>\n  If you use `runtime.stream_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/python/langchain/streaming) for more details.\n</Note>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "96647ae4-7864-46b8-b542-4af36d63ecc2",
    "id": "d9357d92-096b-42af-9dd7-91d41d033c0f",
    "content": "{\n  \"collections\": \"all\",\n  \"total\": 73,\n  \"articles\": [\n    {\n      \"id\": \"7a5d1732-c25b-4a26-9686-52ddfd17926e\",\n      \"title\": \"Merging LangSmith Runs from Remote Graph\",\n      \"url\": \"https://support.langchain.com/articles/9499391881-merging-langsmith-runs-from-remote-graph\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"796cd8bc-a7d9-419d-8141-83998b267e90\",\n      \"title\": \"Resolving common issues when upgrading LangSmith Helm chart from 0.11.x to 0.12.x?\",\n      \"url\": \"https://support.langchain.com/articles/5255204348-resolving-common-issues-when-upgrading-langsmith-helm-chart-from-0-11-x-to-0-12-x\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"adeafe65-c1b6-4620-b3aa-044873fd8c9c\",\n      \"title\": \"Why are my traces being automatically upgraded to extended retention?\",\n      \"url\": \"https://support.langchain.com/articles/3206725817-why-are-my-traces-being-automatically-upgraded-to-extended-retention\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"73778fa5-7613-4254-b687-7b0288ac9903\",\n      \"title\": \"Why am I getting 429 Rate Limit Errors during load testing on self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8364157755-why-am-i-getting-429-rate-limit-errors-during-load-testing-on-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"27a421b4-e877-4a0f-8f38-7b719b189785\",\n      \"title\": \"PagerDuty Alert Severity Not Respecting Configuration\",\n      \"url\": \"https://support.langchain.com/articles/8222693126-pagerduty-alert-severity-not-respecting-configuration\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"adf84afd-7d07-4e11-8645-a3c9a1686f08\",\n      \"title\": \"Session Timeout Configuration for LangSmith Self-Hosted\",\n      \"url\": \"https://support.langchain.com/articles/9515017706-session-timeout-configuration-for-langsmith-self-hosted\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2fcf3091-0038-4226-b56f-4559d6cb1dbc\",\n      \"title\": \"Cleaning ClickHouse System Log Tables\",\n      \"url\": \"https://support.langchain.com/articles/9482219057-cleaning-clickhouse-system-log-tables\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"065735b3-432b-41ce-8469-1494cb753727\",\n      \"title\": \"Why are my agent health checks failing with GCP Gateway API on GKE?\",\n      \"url\": \"https://support.langchain.com/articles/3603461301-why-are-my-agent-health-checks-failing-with-gcp-gateway-api-on-gke\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f9775b3-e85d-4368-ad1b-21f9a8f7552e\",\n      \"title\": \"Evaluator Creation with LiteLLM Proxy - Missing API Key Error\",\n      \"url\": \"https://support.langchain.com/articles/2378929317-evaluator-creation-with-litellm-proxy-missing-api-key-error\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3a81a902-1bca-463d-ab2d-abefd38f8721\",\n      \"title\": \"Why did my evaluator token usage and costs increase after upgrading LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/7643067789-why-did-my-evaluator-token-usage-and-costs-increase-after-upgrading-langsmith\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"61f0452e-5198-4524-9c94-a9cf476ea5bc\",\n      \"title\": \"How to Create a Locked-Down Deployment with Restricted Access\",\n      \"url\": \"https://support.langchain.com/articles/5382256129-how-to-create-a-locked-down-deployment-with-restricted-access\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"c0615bd9-c4a2-4a54-af03-0ee07470ee43\",\n      \"title\": \"How do I configure LangSmith pricing match patterns for AWS Bedrock inference profiles?\",\n      \"url\": \"https://support.langchain.com/articles/4241511543-how-do-i-configure-langsmith-pricing-match-patterns-for-aws-bedrock-inference-profiles\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"365a0d3f-d784-407d-b741-fe51fa8356d4\",\n      \"title\": \"LangSmith Deployments not working with Valkey\",\n      \"url\": \"https://support.langchain.com/articles/2265174426-why-are-my-langgraph-requests-hanging-when-using-aws-elasticache-with-valkey\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"818454c3-a656-4811-a7ca-56cb13df2355\",\n      \"title\": \"How do I enable KMS encryption for S3 storage in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9381186430-how-do-i-enable-kms-encryption-for-s3-storage-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a3c79a7d-390f-43e6-8234-8b55a7f191dd\",\n      \"title\": \"Streaming Failures When Using Shared Custom Redis Instances\",\n      \"url\": \"https://support.langchain.com/articles/3842219806-streaming-failures-when-using-shared-custom-redis-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"55a2c6fe-adf8-4b55-bcb9-969b465241b7\",\n      \"title\": \"Why are my agent threads getting deleted automatically?\",\n      \"url\": \"https://support.langchain.com/articles/7993792111-why-are-my-threads-being-marked-as-temporary-and-getting-deleted-automatically\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b4ba8c34-6d1d-4653-8123-c2eeee2d34e5\",\n      \"title\": \"How do I set up ABAC to restrict user access to specific projects?\",\n      \"url\": \"https://support.langchain.com/articles/6539560119-how-do-i-set-up-abac-to-restrict-user-access-to-specific-projects\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b42f572e-0490-4bb8-9c81-9f065f0c83ed\",\n      \"title\": \"Productionizing and Scaling Self-Hosted LangSmith - Best Practices\",\n      \"url\": \"https://support.langchain.com/articles/8478795211-productionizing-and-scaling-self-hosted-langsmith-best-practices\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"d270716f-edc0-49d9-b5d1-497b5d0e4e6d\",\n      \"title\": \"Private Network Access Blocking LangSmith Studio Connection\",\n      \"url\": \"https://support.langchain.com/articles/1897181200-private-network-access-blocking-langsmith-studio-connection\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"b20fd0bf-1ae9-445c-86ba-5a223112756f\",\n      \"title\": \"How to setup private container registry authentication for LangGraph agent deployments\",\n      \"url\": \"https://support.langchain.com/articles/8606755009-how-to-setup-private-container-registry-authentication-for-langgraph-cloud-agent-deployments\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f6f6fa0-610c-4a15-a4b9-987e932f6a3a\",\n      \"title\": \"How to setup path-based routing for self-hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/2352997668-how-to-setup-path-based-routing-for-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"cc4be529-34c6-458b-8bb3-621fab0e2ab4\",\n      \"title\": \"Best practices for scaling Self-Hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/5142502915-best-practices-for-scaling-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3cec875c-3c8d-4951-b83b-0ac5b8ba7c7c\",\n      \"title\": \"LangSmith Deployments agent is running, but the UI shows it as \\u201cUnsuccessful\\u201d\",\n      \"url\": \"https://support.langchain.com/articles/8394994213-langsmith-deployments-agent-is-running-but-the-ui-shows-it-as-unsuccessful\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"57055dee-474d-4635-93a3-79439570f6ac\",\n      \"title\": \"How do I prevent traces from mixing between projects during concurrent requests?\",\n      \"url\": \"https://support.langchain.com/articles/4784014353-how-do-i-prevent-traces-from-mixing-between-projects-during-concurrent-requests\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9619ff10-34d0-49ea-a7ee-cdbbddafc1e4\",\n      \"title\": \"How do I resolve \\\"too many connections\\\" PostgreSQL errors when scaling platform instances?\",\n      \"url\": \"https://support.langchain.com/articles/9651943867-how-do-i-resolve-too-many-connections-postgresql-errors-when-scaling-platform-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"b740be98-677d-486f-ba77-12258712f70a\",\n      \"title\": \"How do I use the LangSmith Control Plane API to manage deployments?\",\n      \"url\": \"https://support.langchain.com/articles/8184800662-how-do-i-use-the-langgraph-platform-control-plane-api-to-manage-deployments\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"52d389ff-3918-46a8-9ccf-55fe972077bf\",\n      \"title\": \"Why is my LangSmith agent queue Deployment timing out after 600 seconds?\",\n      \"url\": \"https://support.langchain.com/articles/7898522976-why-is-my-queue-deployment-timing-out-after-600-seconds\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"1caf7aa7-5382-43f5-869c-27212fe9f287\",\n      \"title\": \"How do I resolve ClickHouse migration errors after upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/6463985661-how-do-i-resolve-clickhouse-migration-errors-after-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0ff2089a-501c-45b8-b557-4947639c5072\",\n      \"title\": \"How do I disable public sharing for traces in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5345880639-how-do-i-disable-public-sharing-for-traces-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\",\n      \"title\": \"What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\",\n      \"url\": \"https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"bef0ce3c-94c2-4999-b563-ff016fa3884f\",\n      \"title\": \"How do I get LangChain to complete security questionnaires for vendor assessments?\",\n      \"url\": \"https://support.langchain.com/articles/8555158040-how-do-i-get-langchain-to-complete-security-questionnaires-for-vendor-assessments\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"fe2b3c9e-8fa1-49dd-a52d-df92d88050c4\",\n      \"title\": \"How do I update billing information on an existing invoice?\",\n      \"url\": \"https://support.langchain.com/articles/5832977338-how-do-i-update-billing-information-on-an-existing-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"6ea47321-cd15-49a7-a972-be33139a5fe3\",\n      \"title\": \"Can I use an external database cluster to host multiple self-hosted LangSmith instances?\",\n      \"url\": \"https://support.langchain.com/articles/5989664339-can-i-use-an-external-database-cluster-to-host-multiple-self-hosted-langsmith-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"4a127c79-3504-4afa-befa-ec974b85c5f8\",\n      \"title\": \"Why am I being charged for node executions and standby minutes on my development deployment?\",\n      \"url\": \"https://support.langchain.com/articles/2626278629-why-am-i-being-charged-for-node-executions-on-my-development-deployment\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"a442595a-454c-4208-8854-d51f632a3d0f\",\n      \"title\": \"Managing ClickHouse Storage\",\n      \"url\": \"https://support.langchain.com/articles/2929899751-managing-clickhouse-storage\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"c6c77d41-4376-4452-b7a0-fd42c1428242\",\n      \"title\": \"How do I deploy an MCP server on LangSmith Deployments with my agent?\",\n      \"url\": \"https://support.langchain.com/articles/8743137454-how-do-i-deploy-an-mcp-server-on-langsmith-deployments-with-my-agent\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"e5b95386-5971-4e48-be5b-9d9c7dcfe8e3\",\n      \"title\": \"How do I archive a prompt instead of deleting it?\",\n      \"url\": \"https://support.langchain.com/articles/6404244010-how-do-i-archive-a-prompt-instead-of-deleting-it\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"7444d2d0-3f90-42d8-aa4c-b36fa7665e05\",\n      \"title\": \"How do I resolve Redis disk space issues in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5370519454-how-do-i-resolve-redis-disk-space-issues-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"1076b041-020f-4738-acd0-d8ecc07f19ed\",\n      \"title\": \"How do I add existing organization members to new workspaces programmatically via API?\",\n      \"url\": \"https://support.langchain.com/articles/5797508505-how-do-i-add-existing-organization-members-to-new-workspaces-programmatically-via-api\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a91a5cf4-a8a3-4ad5-b9ef-9d0d534d0ba4\",\n      \"title\": \"ClickHouse: Drop Old Data\",\n      \"url\": \"https://support.langchain.com/articles/2379935803-clickhouse-drop-old-data\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5886208c-1f87-4672-87c4-053f2773c161\",\n      \"title\": \"How do I filter out traces that contain a specific node name in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8185473222-how-do-i-filter-out-traces-that-contain-a-specific-node-name-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b170d29c-925a-4aab-b7a0-354f47b7dc77\",\n      \"title\": \"Can I enforce Google SSO-only authentication for my organization?\",\n      \"url\": \"https://support.langchain.com/articles/9820106567-can-i-enforce-google-sso-only-authentication-for-my-organization\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b5128a24-178a-4063-9337-c273af5ac617\",\n      \"title\": \"Why is my trace exploration slow in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9719508217-why-is-my-trace-exploration-slow-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"50634017-ff08-43bb-b7ae-7518aab83623\",\n      \"title\": \"How do I get access to LangSmith Agent Builder?\",\n      \"url\": \"https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"4ad14b9c-abe8-47b0-a416-81dbe2a3a127\",\n      \"title\": \"Why are some experiment runs missing when processing large datasets in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/3506757366-why-are-some-experiment-runs-missing-when-processing-large-datasets-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"40305680-01d3-4506-8257-16db939e39bf\",\n      \"title\": \"How do I get help with billing issues and payment problems?\",\n      \"url\": \"https://support.langchain.com/articles/3779255598-how-do-i-get-help-with-billing-issues-and-payment-problems\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"9273a5a9-0a53-4d5a-917a-c19ebeec64fe\",\n      \"title\": \"How do I get a copy of my LangSmith invoice?\",\n      \"url\": \"https://support.langchain.com/articles/6317483716-how-do-i-get-a-copy-of-my-langsmith-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"ebea1a08-4488-4cac-9c65-75442f3de254\",\n      \"title\": \"Adding FastAPI dependency for custom middleware in LangGraph Platform\",\n      \"url\": \"https://support.langchain.com/articles/7593416758-adding-fastapi-dependency-for-custom-middleware-in-langgraph-platform\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"d37491ce-7f3c-4575-91d5-09c62129248e\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/8574277609-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"48541052-b69e-4e78-ae8b-c2ef4fe26af1\",\n      \"title\": \"Resolving LangGraph and LangSmith version compatibility issues in production\",\n      \"url\": \"https://support.langchain.com/articles/5102702588-resolving-langgraph-and-langsmith-version-compatibility-issues-in-production\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0b6ce386-e341-47a1-8be9-ae350de16843\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/3873263156-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"68fbf7da-954d-454e-8ad4-73e1f1f63b74\",\n      \"title\": \"LangSmith Studio vs Agent Chat UI: When to Use What\",\n      \"url\": \"https://support.langchain.com/articles/1353449293-langsmith-studio-vs-agent-chat-ui\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f7dc0a0c-fa3f-4319-b820-ae470d89877f\",\n      \"title\": \"Why is token-by-token streaming not working after upgrading LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/7150806184-why-is-token-by-token-streaming-not-working-after-upgrading-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"ada8fd1b-4800-434e-8d6f-b5186b170941\",\n      \"title\": \"LangSmith Studio URL Redirects to Error Page\",\n      \"url\": \"https://support.langchain.com/articles/4270790436-langsmith-studio-url-redirects-to-error-page\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f41e0fb9-7ed0-4d9b-85b0-fe88fd805f02\",\n      \"title\": \"Why am I getting service interruption errors when using text search filters?\",\n      \"url\": \"https://support.langchain.com/articles/6919527577-why-am-i-getting-service-interruption-errors-when-using-text-search-filters\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\",\n      \"title\": \"Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\",\n      \"url\": \"https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b1e1ee29-1124-4a92-8f9f-eaa4a679df05\",\n      \"title\": \"Why am I getting a parsing token error when upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/7019615393-why-am-i-getting-a-token-parsing-error-when-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"da8e3156-8405-4c12-a347-96d6590da06c\",\n      \"title\": \"LangSmith upgrade fails with SSL certificate verification error when connecting to Azure Cache for Redis\",\n      \"url\": \"https://support.langchain.com/articles/7558740603-langsmith-helm-upgrade-fails-with-ssl-certificate-verification-error-when-connecting-to-azure-cache-for-redis\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"af6b64a9-ed4a-4993-b54d-d4913eeb1d26\",\n      \"title\": \"How do I change my email address in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9223968956-how-do-i-change-my-email-address-in-langsmith\",\n      \"collection\": \"General\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b384a-b057-7f30-a248-53f66014e3e4",
    "content": "**You can use the skills pattern with standard LangChain agents by implementing a progressive disclosure system where specialized prompts and tools are loaded on-demand.**\n\nIn the \"skills\" architecture, your main agent has access to a `load_skill` tool. Instead of cramming all possible instructions into one massive system prompt, the agent calls this tool to retrieve specific domain knowledge or instructions only when they become relevant.\n\n## Implementing Skills in LangChain\n\nDefine a tool that fetches specialized content (like a SQL schema or legal guidelines) from a database or file system:\n\n```python\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n@tool\ndef load_skill(skill_name: str) -> str:\n    \"\"\"Load a specialized skill prompt.\n    \n    Available skills:\n    - write_sql: Expert in database schemas and complex joins\n    - technical_writing: Expert in API documentation standards\n    \"\"\"\n    # In a real app, fetch from a DB or file\n    skills = {\n        \"write_sql\": \"When writing SQL, always use snake_case and include 'created_at'...\",\n        \"technical_writing\": \"Use active voice and follow the Google Developer style guide...\"\n    }\n    return skills.get(skill_name, \"Skill not found.\")\n\n# The agent starts with awareness of the skills but not the full content\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[load_skill],\n    system_prompt=(\n        \"You are a helpful assistant. You have access to specialized skills: \"\n        \"write_sql and technical_writing. Use load_skill to access them when needed.\"\n    )\n)\n```\n\n## Progressive Disclosure Advantages\n\nThis approach mimics how the Deep Agents CLI manages large contexts by separating high-level discovery from deep domain expertise:\n\n- **Scalability**: You can have hundreds of skills without overwhelming the context window.\n- **Independence**: Different teams can update specific skill contents (e.g., updating a database schema) without re-deploying the main agent.\n- **Dynamic Tools**: You can extend this pattern to dynamically register new tools based on which skill is loaded. For example, loading a `database_admin` skill could register `run_query` and `backup_table` tools.\n\nTo persist these skills across sessions like the CLI does, you should pair this architecture with a `checkpointer` or `store` to save the state of which skills have already been loaded.\n\n**Relevant docs:**\n\n- [Multi-agent Skills Pattern](https://docs.langchain.com/oss/python/langchain/multi-agent/skills)\n- [Tutorial: SQL Assistant with On-Demand Skills](https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant)\n- [Customizing Tools in LangChain](https://docs.langchain.com/oss/python/langchain/tools)"
  },
  {
    "role": "user",
    "id": "961d3c5d-b1cf-40ac-a19c-56248b74f898",
    "content": "Show me the code in deep agents cli. Is that available "
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "d5f6284b-b44d-4b17-bce3-f423977f5aeb",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"deepagents-cli repo\"}"
        }
      },
      {
        "type": "function",
        "id": "bc91d8ff-5fb8-4bb2-913e-db276690efe2",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"all\"}"
        }
      }
    ],
    "id": "lc_run--019b384b-9f98-7e01-b273-918f7dfc2a0c",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "d5f6284b-b44d-4b17-bce3-f423977f5aeb",
    "id": "a4e2e3d0-808c-4f8b-a11b-03353046349d",
    "content": "Result 1:\nTitle: Deep Agents CLI\nLink: https://docs.langchain.comoss/javascript/deepagents/cli\nContent: A terminal interface for building agents with persistent memory. Agents maintain context across sessions, learn project conventions, and execute code with approval controls.\n\nThe Deep Agents CLI has the following built-in capabilities:\n\n* <Icon icon=\"file\" size={16} /> **File operations** - read, write, and edit files in your project with tools that enable agents to manage and modify code and documentation.\n* <Icon icon=\"terminal\" size={16} /> **Shell command execution** - execute shell commands to run tests, build projects, manage dependencies, and interact with version control systems.\n* <Icon icon=\"magnifying-glass\" size={16} /> **Web search** - search the web for up-to-date information and documentation (requires Tavily API key).\n* <Icon icon=\"globe\" size={16} /> **HTTP requests** - make HTTP requests to APIs and external services for data fetching and integration tasks.\n* <Icon icon=\"list-check\" size={16} /> **Task planning and tracking** - break down complex tasks into discrete steps and track progress through the built-in todo system.\n* <Icon icon=\"brain\" size={16} /> **Memory storage and retrieval** - store and retrieve information across sessions, enabling agents to remember project conventions and learned patterns.\n* <Icon icon=\"head-side\" size={16} /> **Human-in-the-loop** - require human approval for sensitive tool operations.\n\n<Tip>\n  [Watch the demo video](https://youtu.be/IrnacLa9PJc?si=3yUnPbxnm2yaqVQb) to see how the Deep Agents CLI works.\n</Tip>\n\n## Quick start\n\n<Steps>\n  <Step title=\"Set your API key\" icon=\"key\">\n    Export as an environment variable:\n\n    ```bash  theme={null}\n    export ANTHROPIC_API_KEY=\"your-api-key\"\n    ```\n\n    Or create a `.env` file in your project root:\n\n    ```bash  theme={null}\n    ANTHROPIC_API_KEY=your-api-key\n    ```\n  </Step>\n\n  <Step title=\"Run the CLI\" icon=\"terminal\">\n    ```bash  theme={null}\n    uvx deepagents-cli\n    ```\n  </Step>\n\n  <Step title=\"Give the agent a task\" icon=\"message\">\n    ```bash  theme={null}\n    > Create a Python script that prints \"Hello, World!\"\n    ```\n\n    The agent proposes changes with diffs for your approval before modifying files.\n  </Step>\n</Steps>\n\n<Accordion title=\"Additional installation and configuration options\">\n  Install locally if needed:\n\n  <CodeGroup>\n    ```bash pip theme={null}\n    pip install deepagents-cli\n    ```\n\n    ```bash uv theme={null}\n    uv add deepagents-cli\n    ```\n  </CodeGroup>\n\n  The CLI uses Anthropic Claude Sonnet 4 by default. To use OpenAI:\n\n  ```bash  theme={null}\n  export OPENAI_API_KEY=\"your-key\"\n  ```\n\n  Enable web search (optional):\n\n  ```bash  theme={null}\n  export TAVILY_API_KEY=\"your-key\"\n  ```\n\n  API keys can be set as environment variables or in a `.env` file.\n</Accordion>\n\n## Configuration\n\n<AccordionGroup>\n  <Accordion title=\"Command-line options\" icon=\"flag\">\n    | Option                 | Description                                                 |\n    | ---------------------- | ----------------------------------------------------------- |\n    | `--agent NAME`         | Use named agent with separate memory                        |\n    | `--auto-approve`       | Skip tool confirmation prompts (toggle with `Ctrl+T`)       |\n    | `--sandbox TYPE`       | Execute in remote sandbox: `modal`, `daytona`, or `runloop` |\n    | `--sandbox-id ID`      | Reuse existing sandbox                                      |\n    | `--sandbox-setup PATH` | Run setup script in sandbox                                 |\n  </Accordion>\n\n  <Accordion title=\"CLI commands\" icon=\"terminal\">\n    | Command                                         | Description                             |\n    | ----------------------------------------------- | --------------------------------------- |\n    | `deepagents list`                               | List all agents                         |\n    | `deepagents help`                               | Show help                               |\n    | `deepagents reset --agent NAME`                 | Clear agent memory and reset to default |\n    | `deepagents reset --agent NAME --target SOURCE` | Copy memory from another agent          |\n  </Accordion>\n</AccordionGroup>\n\n## Interactive mode\n\n<AccordionGroup>\n  <Accordion title=\"Slash commands\" icon=\"slash\">\n    Use these commands within the CLI session:\n\n    * `/tokens` - Display token usage\n    * `/clear` - Clear conversation history\n    * `/exit` - Exit the CLI\n  </Accordion>\n\n  <Accordion title=\"Bash commands\" icon=\"terminal\">\n    Execute shell commands directly by prefixing with `!`:\n\n    ```bash  theme={null}\n    !git status\n    !npm test\n    !ls -la\n    ```\n  </Accordion>\n\n  <Accordion title=\"Keyboard shortcuts\" icon=\"keyboard\">\n    | Shortcut    | Action              |\n    | ----------- | ------------------- |\n    | `Enter`     | Submit              |\n    | `Alt+Enter` | Newline             |\n    | `Ctrl+E`    | External editor     |\n    | `Ctrl+T`    | Toggle auto-approve |\n    | `Ctrl+C`    | Interrupt           |\n    | `Ctrl+D`    | Exit                |\n  </Accordion>\n</AccordionGroup>\n\n## Set project conventions with memories\n\nAgents store information in `~/.deepagents/AGENT_NAME/memories/` as markdown files using a memory-first protocol:\n\n1. **Research**: Searches memory for relevant context before starting tasks\n2. **Response**: Checks memory when uncertain during execution\n3. **Learning**: Automatically saves new information for future sessions\n\nOrganize memories by topic with descriptive filenames:\n\n```\n~/.deepagents/backend-dev/memories/\n\u251c\u2500\u2500 api-conventions.md\n\u251c\u2500\u2500 database-schema.md\n\u2514\u2500\u2500 deployment-process.md\n```\n\nTeach the agent conventions once:\n\n```bash  theme={null}\nuvx deepagents-cli --agent backend-dev\n> Our API uses snake_case and includes created_at/updated_at timestamps\n```\n\nIt remembers for future sessions:\n\n```bash  theme={null}\n> Create a /users endpoint\n# Applies conventions without prompting\n```\n\n## Use remote sandboxes\n\nExecute code in isolated remote environments for safety and flexibility. Remote sandboxes provide the following benefits:\n\n* **Safety**: Protect your local machine from potentially harmful code execution\n* **Clean environments**: Use specific dependencies or OS configurations without local setup\n* **Parallel execution**: Run multiple agents simultaneously in isolated environments\n* **Long-running tasks**: Execute time-intensive operations without blocking your machine\n* **Reproducibility**: Ensure consistent execution environments across teams\n\nTo use a remote sandbox, follow these steps:\n\n1. Configure your sandbox provider ([Runloop](https://www.runloop.ai/), [Daytona](https://www.daytona.io/), or [Modal](https://modal.com/)):\n\n   ```bash  theme={null}\n   # Runloop\n   export RUNLOOP_API_KEY=\"your-key\"\n\n   # Daytona\n   export DAYTONA_API_KEY=\"your-key\"\n\n   # Modal\n   modal setup\n   ```\n\n2. Run the CLI with a sandbox:\n\n   ```bash  theme={null}\n   uvx deepagents-cli --sandbox runloop --sandbox-setup ./setup.sh\n   ```\n\n   The agent runs locally but executes all code operations in the remote sandbox. Optional setup scripts can configure environment variables, clone repositories, and prepare dependencies.\n\n3. (Optional) Create a `setup.sh` file to configure your sandbox environment:\n\n   ```bash  theme={null}\n   #!/bin/bash\n   set -e\n\n   # Clone repository using GitHub token\n   git clone https://x-access-token:${GITHUB_TOKEN}@github.com/username/repo.git $HOME/workspace\n   cd $HOME/workspace\n\n   # Make environment variables persistent\n   cat >> ~/.bashrc <<'EOF'\n   export GITHUB_TOKEN=\"${GITHUB_TOKEN}\"\n   export OPENAI_API_KEY=\"${OPENAI_API_KEY}\"\n   cd $HOME/workspace\n   EOF\n\n   source ~/.bashrc\n   ```\n\n   Store secrets in a local `.env` file for the setup script to access.\n\n<Warning>\n  Sandboxes isolate code execution, but agents remain vulnerable to prompt injection with untrusted inputs. Use human-in-the-loop approval, short-lived secrets, and trusted setup scripts only. Note that sandbox APIs are evolving rapidly, and we expect more providers to support proxies that help mitigate prompt injection and secrets management concerns.\n</Warning>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/cli.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Deep Agents CLI\nLink: https://docs.langchain.comoss/python/deepagents/cli\nContent: A terminal interface for building agents with persistent memory. Agents maintain context across sessions, learn project conventions, and execute code with approval controls.\n\nThe Deep Agents CLI has the following built-in capabilities:\n\n* <Icon icon=\"file\" size={16} /> **File operations** - read, write, and edit files in your project with tools that enable agents to manage and modify code and documentation.\n* <Icon icon=\"terminal\" size={16} /> **Shell command execution** - execute shell commands to run tests, build projects, manage dependencies, and interact with version control systems.\n* <Icon icon=\"magnifying-glass\" size={16} /> **Web search** - search the web for up-to-date information and documentation (requires Tavily API key).\n* <Icon icon=\"globe\" size={16} /> **HTTP requests** - make HTTP requests to APIs and external services for data fetching and integration tasks.\n* <Icon icon=\"list-check\" size={16} /> **Task planning and tracking** - break down complex tasks into discrete steps and track progress through the built-in todo system.\n* <Icon icon=\"brain\" size={16} /> **Memory storage and retrieval** - store and retrieve information across sessions, enabling agents to remember project conventions and learned patterns.\n* <Icon icon=\"head-side\" size={16} /> **Human-in-the-loop** - require human approval for sensitive tool operations.\n\n<Tip>\n  [Watch the demo video](https://youtu.be/IrnacLa9PJc?si=3yUnPbxnm2yaqVQb) to see how the Deep Agents CLI works.\n</Tip>\n\n## Quick start\n\n<Steps>\n  <Step title=\"Set your API key\" icon=\"key\">\n    Export as an environment variable:\n\n    ```bash  theme={null}\n    export ANTHROPIC_API_KEY=\"your-api-key\"\n    ```\n\n    Or create a `.env` file in your project root:\n\n    ```bash  theme={null}\n    ANTHROPIC_API_KEY=your-api-key\n    ```\n  </Step>\n\n  <Step title=\"Run the CLI\" icon=\"terminal\">\n    ```bash  theme={null}\n    uvx deepagents-cli\n    ```\n  </Step>\n\n  <Step title=\"Give the agent a task\" icon=\"message\">\n    ```bash  theme={null}\n    > Create a Python script that prints \"Hello, World!\"\n    ```\n\n    The agent proposes changes with diffs for your approval before modifying files.\n  </Step>\n</Steps>\n\n<Accordion title=\"Additional installation and configuration options\">\n  Install locally if needed:\n\n  <CodeGroup>\n    ```bash pip theme={null}\n    pip install deepagents-cli\n    ```\n\n    ```bash uv theme={null}\n    uv add deepagents-cli\n    ```\n  </CodeGroup>\n\n  The CLI uses Anthropic Claude Sonnet 4 by default. To use OpenAI:\n\n  ```bash  theme={null}\n  export OPENAI_API_KEY=\"your-key\"\n  ```\n\n  Enable web search (optional):\n\n  ```bash  theme={null}\n  export TAVILY_API_KEY=\"your-key\"\n  ```\n\n  API keys can be set as environment variables or in a `.env` file.\n</Accordion>\n\n## Configuration\n\n<AccordionGroup>\n  <Accordion title=\"Command-line options\" icon=\"flag\">\n    | Option                 | Description                                                 |\n    | ---------------------- | ----------------------------------------------------------- |\n    | `--agent NAME`         | Use named agent with separate memory                        |\n    | `--auto-approve`       | Skip tool confirmation prompts (toggle with `Ctrl+T`)       |\n    | `--sandbox TYPE`       | Execute in remote sandbox: `modal`, `daytona`, or `runloop` |\n    | `--sandbox-id ID`      | Reuse existing sandbox                                      |\n    | `--sandbox-setup PATH` | Run setup script in sandbox                                 |\n  </Accordion>\n\n  <Accordion title=\"CLI commands\" icon=\"terminal\">\n    | Command                                         | Description                             |\n    | ----------------------------------------------- | --------------------------------------- |\n    | `deepagents list`                               | List all agents                         |\n    | `deepagents help`                               | Show help                               |\n    | `deepagents reset --agent NAME`                 | Clear agent memory and reset to default |\n    | `deepagents reset --agent NAME --target SOURCE` | Copy memory from another agent          |\n  </Accordion>\n</AccordionGroup>\n\n## Interactive mode\n\n<AccordionGroup>\n  <Accordion title=\"Slash commands\" icon=\"slash\">\n    Use these commands within the CLI session:\n\n    * `/tokens` - Display token usage\n    * `/clear` - Clear conversation history\n    * `/exit` - Exit the CLI\n  </Accordion>\n\n  <Accordion title=\"Bash commands\" icon=\"terminal\">\n    Execute shell commands directly by prefixing with `!`:\n\n    ```bash  theme={null}\n    !git status\n    !npm test\n    !ls -la\n    ```\n  </Accordion>\n\n  <Accordion title=\"Keyboard shortcuts\" icon=\"keyboard\">\n    | Shortcut    | Action              |\n    | ----------- | ------------------- |\n    | `Enter`     | Submit              |\n    | `Alt+Enter` | Newline             |\n    | `Ctrl+E`    | External editor     |\n    | `Ctrl+T`    | Toggle auto-approve |\n    | `Ctrl+C`    | Interrupt           |\n    | `Ctrl+D`    | Exit                |\n  </Accordion>\n</AccordionGroup>\n\n## Set project conventions with memories\n\nAgents store information in `~/.deepagents/AGENT_NAME/memories/` as markdown files using a memory-first protocol:\n\n1. **Research**: Searches memory for relevant context before starting tasks\n2. **Response**: Checks memory when uncertain during execution\n3. **Learning**: Automatically saves new information for future sessions\n\nOrganize memories by topic with descriptive filenames:\n\n```\n~/.deepagents/backend-dev/memories/\n\u251c\u2500\u2500 api-conventions.md\n\u251c\u2500\u2500 database-schema.md\n\u2514\u2500\u2500 deployment-process.md\n```\n\nTeach the agent conventions once:\n\n```bash  theme={null}\nuvx deepagents-cli --agent backend-dev\n> Our API uses snake_case and includes created_at/updated_at timestamps\n```\n\nIt remembers for future sessions:\n\n```bash  theme={null}\n> Create a /users endpoint\n# Applies conventions without prompting\n```\n\n## Use remote sandboxes\n\nExecute code in isolated remote environments for safety and flexibility. Remote sandboxes provide the following benefits:\n\n* **Safety**: Protect your local machine from potentially harmful code execution\n* **Clean environments**: Use specific dependencies or OS configurations without local setup\n* **Parallel execution**: Run multiple agents simultaneously in isolated environments\n* **Long-running tasks**: Execute time-intensive operations without blocking your machine\n* **Reproducibility**: Ensure consistent execution environments across teams\n\nTo use a remote sandbox, follow these steps:\n\n1. Configure your sandbox provider ([Runloop](https://www.runloop.ai/), [Daytona](https://www.daytona.io/), or [Modal](https://modal.com/)):\n\n   ```bash  theme={null}\n   # Runloop\n   export RUNLOOP_API_KEY=\"your-key\"\n\n   # Daytona\n   export DAYTONA_API_KEY=\"your-key\"\n\n   # Modal\n   modal setup\n   ```\n\n2. Run the CLI with a sandbox:\n\n   ```bash  theme={null}\n   uvx deepagents-cli --sandbox runloop --sandbox-setup ./setup.sh\n   ```\n\n   The agent runs locally but executes all code operations in the remote sandbox. Optional setup scripts can configure environment variables, clone repositories, and prepare dependencies.\n\n3. (Optional) Create a `setup.sh` file to configure your sandbox environment:\n\n   ```bash  theme={null}\n   #!/bin/bash\n   set -e\n\n   # Clone repository using GitHub token\n   git clone https://x-access-token:${GITHUB_TOKEN}@github.com/username/repo.git $HOME/workspace\n   cd $HOME/workspace\n\n   # Make environment variables persistent\n   cat >> ~/.bashrc <<'EOF'\n   export GITHUB_TOKEN=\"${GITHUB_TOKEN}\"\n   export OPENAI_API_KEY=\"${OPENAI_API_KEY}\"\n   cd $HOME/workspace\n   EOF\n\n   source ~/.bashrc\n   ```\n\n   Store secrets in a local `.env` file for the setup script to access.\n\n<Warning>\n  Sandboxes isolate code execution, but agents remain vulnerable to prompt injection with untrusted inputs. Use human-in-the-loop approval, short-lived secrets, and trusted setup scripts only. Note that sandbox APIs are evolving rapidly, and we expect more providers to support proxies that help mitigate prompt injection and secrets management concerns.\n</Warning>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/cli.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Deep Agents overview\nLink: https://docs.langchain.comoss/python/deepagents/overview\nContent: [`deepagents`](https://pypi.org/project/deepagents/) is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.\n\n## When to use deep agents\n\nUse deep agents when you need agents that can:\n\n* **Handle complex, multi-step tasks** that require planning and decomposition\n* **Manage large amounts of context** through file system tools\n* **Delegate work** to specialized subagents for context isolation\n* **Persist memory** across conversations and threads\n\nFor simpler use cases, consider using LangChain's [`create_agent`](/oss/python/langchain/agents) or building a custom [LangGraph](/oss/python/langgraph/overview) workflow.\n\n## Core capabilities\n\n<Card title=\"Planning and task decomposition\" icon=\"timeline\">\n  Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.\n</Card>\n\n<Card title=\"Context management\" icon=\"scissors\">\n  File system tools (`ls`, `read_file`, `write_file`, `edit_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.\n</Card>\n\n<Card title=\"Subagent spawning\" icon=\"people-group\">\n  A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent's context clean while still going deep on specific subtasks.\n</Card>\n\n<Card title=\"Long-term memory\" icon=\"database\">\n  Extend agents with persistent memory across threads using LangGraph's Store. Agents can save and retrieve information from previous conversations.\n</Card>\n\n## Relationship to the LangChain ecosystem\n\nDeep agents is built on top of:\n\n* [LangGraph](/oss/python/langgraph/overview) - Provides the underlying graph execution and state management\n* [LangChain](/oss/python/langchain/overview) - Tools and model integrations work seamlessly with deep agents\n* [LangSmith](/langsmith/home) - Observability, evaluation, and deployment\n\nDeep agents applications can be deployed via [LangSmith Deployment](/langsmith/deployments) and monitored with [LangSmith Observability](/langsmith/observability).\n\n## Get started\n\n<CardGroup cols={2}>\n  <Card title=\"Quickstart\" icon=\"rocket\" href=\"/oss/python/deepagents/quickstart\">\n    Build your first deep agent\n  </Card>\n\n  <Card title=\"Customization\" icon=\"sliders\" href=\"/oss/python/deepagents/customization\">\n    Learn about customization options\n  </Card>\n\n  <Card title=\"Middleware\" icon=\"layer-group\" href=\"/oss/python/deepagents/middleware\">\n    Understand the middleware architecture\n  </Card>\n\n  <Card title=\"Reference\" icon=\"arrow-up-right-from-square\" href=\"https://reference.langchain.com/python/deepagents/\">\n    See the `deepagents` API reference\n  </Card>\n</CardGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "bc91d8ff-5fb8-4bb2-913e-db276690efe2",
    "id": "409d440f-5fb3-403c-8479-68e2438a3410",
    "content": "{\n  \"collections\": \"all\",\n  \"total\": 73,\n  \"articles\": [\n    {\n      \"id\": \"7a5d1732-c25b-4a26-9686-52ddfd17926e\",\n      \"title\": \"Merging LangSmith Runs from Remote Graph\",\n      \"url\": \"https://support.langchain.com/articles/9499391881-merging-langsmith-runs-from-remote-graph\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"796cd8bc-a7d9-419d-8141-83998b267e90\",\n      \"title\": \"Resolving common issues when upgrading LangSmith Helm chart from 0.11.x to 0.12.x?\",\n      \"url\": \"https://support.langchain.com/articles/5255204348-resolving-common-issues-when-upgrading-langsmith-helm-chart-from-0-11-x-to-0-12-x\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"adeafe65-c1b6-4620-b3aa-044873fd8c9c\",\n      \"title\": \"Why are my traces being automatically upgraded to extended retention?\",\n      \"url\": \"https://support.langchain.com/articles/3206725817-why-are-my-traces-being-automatically-upgraded-to-extended-retention\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"73778fa5-7613-4254-b687-7b0288ac9903\",\n      \"title\": \"Why am I getting 429 Rate Limit Errors during load testing on self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8364157755-why-am-i-getting-429-rate-limit-errors-during-load-testing-on-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"27a421b4-e877-4a0f-8f38-7b719b189785\",\n      \"title\": \"PagerDuty Alert Severity Not Respecting Configuration\",\n      \"url\": \"https://support.langchain.com/articles/8222693126-pagerduty-alert-severity-not-respecting-configuration\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"adf84afd-7d07-4e11-8645-a3c9a1686f08\",\n      \"title\": \"Session Timeout Configuration for LangSmith Self-Hosted\",\n      \"url\": \"https://support.langchain.com/articles/9515017706-session-timeout-configuration-for-langsmith-self-hosted\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2fcf3091-0038-4226-b56f-4559d6cb1dbc\",\n      \"title\": \"Cleaning ClickHouse System Log Tables\",\n      \"url\": \"https://support.langchain.com/articles/9482219057-cleaning-clickhouse-system-log-tables\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"065735b3-432b-41ce-8469-1494cb753727\",\n      \"title\": \"Why are my agent health checks failing with GCP Gateway API on GKE?\",\n      \"url\": \"https://support.langchain.com/articles/3603461301-why-are-my-agent-health-checks-failing-with-gcp-gateway-api-on-gke\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f9775b3-e85d-4368-ad1b-21f9a8f7552e\",\n      \"title\": \"Evaluator Creation with LiteLLM Proxy - Missing API Key Error\",\n      \"url\": \"https://support.langchain.com/articles/2378929317-evaluator-creation-with-litellm-proxy-missing-api-key-error\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3a81a902-1bca-463d-ab2d-abefd38f8721\",\n      \"title\": \"Why did my evaluator token usage and costs increase after upgrading LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/7643067789-why-did-my-evaluator-token-usage-and-costs-increase-after-upgrading-langsmith\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"61f0452e-5198-4524-9c94-a9cf476ea5bc\",\n      \"title\": \"How to Create a Locked-Down Deployment with Restricted Access\",\n      \"url\": \"https://support.langchain.com/articles/5382256129-how-to-create-a-locked-down-deployment-with-restricted-access\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"c0615bd9-c4a2-4a54-af03-0ee07470ee43\",\n      \"title\": \"How do I configure LangSmith pricing match patterns for AWS Bedrock inference profiles?\",\n      \"url\": \"https://support.langchain.com/articles/4241511543-how-do-i-configure-langsmith-pricing-match-patterns-for-aws-bedrock-inference-profiles\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"365a0d3f-d784-407d-b741-fe51fa8356d4\",\n      \"title\": \"LangSmith Deployments not working with Valkey\",\n      \"url\": \"https://support.langchain.com/articles/2265174426-why-are-my-langgraph-requests-hanging-when-using-aws-elasticache-with-valkey\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"818454c3-a656-4811-a7ca-56cb13df2355\",\n      \"title\": \"How do I enable KMS encryption for S3 storage in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9381186430-how-do-i-enable-kms-encryption-for-s3-storage-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a3c79a7d-390f-43e6-8234-8b55a7f191dd\",\n      \"title\": \"Streaming Failures When Using Shared Custom Redis Instances\",\n      \"url\": \"https://support.langchain.com/articles/3842219806-streaming-failures-when-using-shared-custom-redis-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"55a2c6fe-adf8-4b55-bcb9-969b465241b7\",\n      \"title\": \"Why are my agent threads getting deleted automatically?\",\n      \"url\": \"https://support.langchain.com/articles/7993792111-why-are-my-threads-being-marked-as-temporary-and-getting-deleted-automatically\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b4ba8c34-6d1d-4653-8123-c2eeee2d34e5\",\n      \"title\": \"How do I set up ABAC to restrict user access to specific projects?\",\n      \"url\": \"https://support.langchain.com/articles/6539560119-how-do-i-set-up-abac-to-restrict-user-access-to-specific-projects\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b42f572e-0490-4bb8-9c81-9f065f0c83ed\",\n      \"title\": \"Productionizing and Scaling Self-Hosted LangSmith - Best Practices\",\n      \"url\": \"https://support.langchain.com/articles/8478795211-productionizing-and-scaling-self-hosted-langsmith-best-practices\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"d270716f-edc0-49d9-b5d1-497b5d0e4e6d\",\n      \"title\": \"Private Network Access Blocking LangSmith Studio Connection\",\n      \"url\": \"https://support.langchain.com/articles/1897181200-private-network-access-blocking-langsmith-studio-connection\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"b20fd0bf-1ae9-445c-86ba-5a223112756f\",\n      \"title\": \"How to setup private container registry authentication for LangGraph agent deployments\",\n      \"url\": \"https://support.langchain.com/articles/8606755009-how-to-setup-private-container-registry-authentication-for-langgraph-cloud-agent-deployments\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f6f6fa0-610c-4a15-a4b9-987e932f6a3a\",\n      \"title\": \"How to setup path-based routing for self-hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/2352997668-how-to-setup-path-based-routing-for-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"cc4be529-34c6-458b-8bb3-621fab0e2ab4\",\n      \"title\": \"Best practices for scaling Self-Hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/5142502915-best-practices-for-scaling-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3cec875c-3c8d-4951-b83b-0ac5b8ba7c7c\",\n      \"title\": \"LangSmith Deployments agent is running, but the UI shows it as \\u201cUnsuccessful\\u201d\",\n      \"url\": \"https://support.langchain.com/articles/8394994213-langsmith-deployments-agent-is-running-but-the-ui-shows-it-as-unsuccessful\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"57055dee-474d-4635-93a3-79439570f6ac\",\n      \"title\": \"How do I prevent traces from mixing between projects during concurrent requests?\",\n      \"url\": \"https://support.langchain.com/articles/4784014353-how-do-i-prevent-traces-from-mixing-between-projects-during-concurrent-requests\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9619ff10-34d0-49ea-a7ee-cdbbddafc1e4\",\n      \"title\": \"How do I resolve \\\"too many connections\\\" PostgreSQL errors when scaling platform instances?\",\n      \"url\": \"https://support.langchain.com/articles/9651943867-how-do-i-resolve-too-many-connections-postgresql-errors-when-scaling-platform-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"b740be98-677d-486f-ba77-12258712f70a\",\n      \"title\": \"How do I use the LangSmith Control Plane API to manage deployments?\",\n      \"url\": \"https://support.langchain.com/articles/8184800662-how-do-i-use-the-langgraph-platform-control-plane-api-to-manage-deployments\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"52d389ff-3918-46a8-9ccf-55fe972077bf\",\n      \"title\": \"Why is my LangSmith agent queue Deployment timing out after 600 seconds?\",\n      \"url\": \"https://support.langchain.com/articles/7898522976-why-is-my-queue-deployment-timing-out-after-600-seconds\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"1caf7aa7-5382-43f5-869c-27212fe9f287\",\n      \"title\": \"How do I resolve ClickHouse migration errors after upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/6463985661-how-do-i-resolve-clickhouse-migration-errors-after-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0ff2089a-501c-45b8-b557-4947639c5072\",\n      \"title\": \"How do I disable public sharing for traces in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5345880639-how-do-i-disable-public-sharing-for-traces-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\",\n      \"title\": \"What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\",\n      \"url\": \"https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"bef0ce3c-94c2-4999-b563-ff016fa3884f\",\n      \"title\": \"How do I get LangChain to complete security questionnaires for vendor assessments?\",\n      \"url\": \"https://support.langchain.com/articles/8555158040-how-do-i-get-langchain-to-complete-security-questionnaires-for-vendor-assessments\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"fe2b3c9e-8fa1-49dd-a52d-df92d88050c4\",\n      \"title\": \"How do I update billing information on an existing invoice?\",\n      \"url\": \"https://support.langchain.com/articles/5832977338-how-do-i-update-billing-information-on-an-existing-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"6ea47321-cd15-49a7-a972-be33139a5fe3\",\n      \"title\": \"Can I use an external database cluster to host multiple self-hosted LangSmith instances?\",\n      \"url\": \"https://support.langchain.com/articles/5989664339-can-i-use-an-external-database-cluster-to-host-multiple-self-hosted-langsmith-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"4a127c79-3504-4afa-befa-ec974b85c5f8\",\n      \"title\": \"Why am I being charged for node executions and standby minutes on my development deployment?\",\n      \"url\": \"https://support.langchain.com/articles/2626278629-why-am-i-being-charged-for-node-executions-on-my-development-deployment\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"a442595a-454c-4208-8854-d51f632a3d0f\",\n      \"title\": \"Managing ClickHouse Storage\",\n      \"url\": \"https://support.langchain.com/articles/2929899751-managing-clickhouse-storage\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"c6c77d41-4376-4452-b7a0-fd42c1428242\",\n      \"title\": \"How do I deploy an MCP server on LangSmith Deployments with my agent?\",\n      \"url\": \"https://support.langchain.com/articles/8743137454-how-do-i-deploy-an-mcp-server-on-langsmith-deployments-with-my-agent\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"e5b95386-5971-4e48-be5b-9d9c7dcfe8e3\",\n      \"title\": \"How do I archive a prompt instead of deleting it?\",\n      \"url\": \"https://support.langchain.com/articles/6404244010-how-do-i-archive-a-prompt-instead-of-deleting-it\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"7444d2d0-3f90-42d8-aa4c-b36fa7665e05\",\n      \"title\": \"How do I resolve Redis disk space issues in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5370519454-how-do-i-resolve-redis-disk-space-issues-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"1076b041-020f-4738-acd0-d8ecc07f19ed\",\n      \"title\": \"How do I add existing organization members to new workspaces programmatically via API?\",\n      \"url\": \"https://support.langchain.com/articles/5797508505-how-do-i-add-existing-organization-members-to-new-workspaces-programmatically-via-api\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a91a5cf4-a8a3-4ad5-b9ef-9d0d534d0ba4\",\n      \"title\": \"ClickHouse: Drop Old Data\",\n      \"url\": \"https://support.langchain.com/articles/2379935803-clickhouse-drop-old-data\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5886208c-1f87-4672-87c4-053f2773c161\",\n      \"title\": \"How do I filter out traces that contain a specific node name in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8185473222-how-do-i-filter-out-traces-that-contain-a-specific-node-name-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b170d29c-925a-4aab-b7a0-354f47b7dc77\",\n      \"title\": \"Can I enforce Google SSO-only authentication for my organization?\",\n      \"url\": \"https://support.langchain.com/articles/9820106567-can-i-enforce-google-sso-only-authentication-for-my-organization\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b5128a24-178a-4063-9337-c273af5ac617\",\n      \"title\": \"Why is my trace exploration slow in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9719508217-why-is-my-trace-exploration-slow-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"50634017-ff08-43bb-b7ae-7518aab83623\",\n      \"title\": \"How do I get access to LangSmith Agent Builder?\",\n      \"url\": \"https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"4ad14b9c-abe8-47b0-a416-81dbe2a3a127\",\n      \"title\": \"Why are some experiment runs missing when processing large datasets in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/3506757366-why-are-some-experiment-runs-missing-when-processing-large-datasets-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"40305680-01d3-4506-8257-16db939e39bf\",\n      \"title\": \"How do I get help with billing issues and payment problems?\",\n      \"url\": \"https://support.langchain.com/articles/3779255598-how-do-i-get-help-with-billing-issues-and-payment-problems\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"9273a5a9-0a53-4d5a-917a-c19ebeec64fe\",\n      \"title\": \"How do I get a copy of my LangSmith invoice?\",\n      \"url\": \"https://support.langchain.com/articles/6317483716-how-do-i-get-a-copy-of-my-langsmith-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"ebea1a08-4488-4cac-9c65-75442f3de254\",\n      \"title\": \"Adding FastAPI dependency for custom middleware in LangGraph Platform\",\n      \"url\": \"https://support.langchain.com/articles/7593416758-adding-fastapi-dependency-for-custom-middleware-in-langgraph-platform\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"d37491ce-7f3c-4575-91d5-09c62129248e\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/8574277609-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"48541052-b69e-4e78-ae8b-c2ef4fe26af1\",\n      \"title\": \"Resolving LangGraph and LangSmith version compatibility issues in production\",\n      \"url\": \"https://support.langchain.com/articles/5102702588-resolving-langgraph-and-langsmith-version-compatibility-issues-in-production\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0b6ce386-e341-47a1-8be9-ae350de16843\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/3873263156-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"68fbf7da-954d-454e-8ad4-73e1f1f63b74\",\n      \"title\": \"LangSmith Studio vs Agent Chat UI: When to Use What\",\n      \"url\": \"https://support.langchain.com/articles/1353449293-langsmith-studio-vs-agent-chat-ui\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f7dc0a0c-fa3f-4319-b820-ae470d89877f\",\n      \"title\": \"Why is token-by-token streaming not working after upgrading LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/7150806184-why-is-token-by-token-streaming-not-working-after-upgrading-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"ada8fd1b-4800-434e-8d6f-b5186b170941\",\n      \"title\": \"LangSmith Studio URL Redirects to Error Page\",\n      \"url\": \"https://support.langchain.com/articles/4270790436-langsmith-studio-url-redirects-to-error-page\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f41e0fb9-7ed0-4d9b-85b0-fe88fd805f02\",\n      \"title\": \"Why am I getting service interruption errors when using text search filters?\",\n      \"url\": \"https://support.langchain.com/articles/6919527577-why-am-i-getting-service-interruption-errors-when-using-text-search-filters\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\",\n      \"title\": \"Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\",\n      \"url\": \"https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b1e1ee29-1124-4a92-8f9f-eaa4a679df05\",\n      \"title\": \"Why am I getting a parsing token error when upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/7019615393-why-am-i-getting-a-token-parsing-error-when-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"da8e3156-8405-4c12-a347-96d6590da06c\",\n      \"title\": \"LangSmith upgrade fails with SSL certificate verification error when connecting to Azure Cache for Redis\",\n      \"url\": \"https://support.langchain.com/articles/7558740603-langsmith-helm-upgrade-fails-with-ssl-certificate-verification-error-when-connecting-to-azure-cache-for-redis\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"af6b64a9-ed4a-4993-b54d-d4913eeb1d26\",\n      \"title\": \"How do I change my email address in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9223968956-how-do-i-change-my-email-address-in-langsmith\",\n      \"collection\": \"General\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b384b-ad6e-74f2-8945-a3b173b16ff6",
    "content": "**Yes, the Deep Agents CLI is an open-source tool, and you can view its source code and the underlying `deepagents` library on GitHub.**\n\nThe CLI is built on top of the `deepagents` Python library, which provides core capabilities like planning, task decomposition, and context management using LangGraph.\n\n## Source Code Repositories\n\nYou can find the source code in the official LangChain AI organization:\n\n- **Library and CLI Source:** [langchain-ai/deepagents](https://github.com/langchain-ai/deepagents)\n- **Documentation Source:** [langchain-ai/docs](https://github.com/langchain-ai/docs/tree/main/src/oss/deepagents)\n\n## Core Implementation Details\n\nThe Deep Agents architecture uses several advanced LangGraph patterns that you can see in the source:\n\n```python\n# The CLI entry point typically handles:\n# 1. Environment and .env loading\n# 2. Command-line argument parsing (--agent, --sandbox)\n# 3. Initializing the LangGraph Checkpointer for memory\n# 4. Spawning the interactive shell session\n```\n\nThe underlying library uses a `write_todos` tool for planning and a `task` tool for spawning subagents, which are the primary mechanisms for handling complex, multi-step tasks.\n\n## Local Installation from Source\n\nIf you want to modify the CLI or run it from the latest source code, you can clone the repository:\n\n```bash\ngit clone https://github.com/langchain-ai/deepagents.git\ncd deepagents\npip install -e .\n```\n\nThis allows you to explore the internal logic for how agents research their local memory (`~/.deepagents/`) before responding and how they manage approval controls for sensitive tool operations.\n\n**Relevant docs:**\n\n- [Deep Agents CLI Documentation](https://docs.langchain.com/oss/python/deepagents/cli)\n- [Deep Agents Overview](https://docs.langchain.com/oss/python/deepagents/overview)\n- [Deep Agents API Reference](https://reference.langchain.com/python/deepagents/)"
  },
  {
    "role": "user",
    "id": "ec176bb1-7428-4d8e-917e-cb1562adda3b",
    "content": "How is skills being used in cli"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "010a2b15-b53b-4a97-9750-51c3884b6f43",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"skills\"}"
        }
      },
      {
        "type": "function",
        "id": "dd0b83f4-c997-4e6b-8897-ad9ebfd4cc87",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"memories\"}"
        }
      },
      {
        "type": "function",
        "id": "090a259a-88a0-490a-903d-5ab62fc5a4a9",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"OSS (LangChain and LangGraph)\"}"
        }
      }
    ],
    "id": "lc_run--019b384c-66d8-74f2-806a-36895b866618",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "010a2b15-b53b-4a97-9750-51c3884b6f43",
    "id": "24a36724-5150-47dd-a0df-c4e7e879a476",
    "content": "Result 1:\nTitle: Skills\nLink: https://docs.langchain.comoss/javascript/langchain/multi-agent/skills\nContent: In the **skills** architecture, specialized capabilities are packaged as invokable \"skills\" that augment an [agent's](/oss/javascript/langchain/agents) behavior. Skills are primarily prompt-driven specializations that an agent can invoke on-demand.\n\n<Tip>\n  This pattern is conceptually identical to [llms.txt](https://llmstxt.org/) (introduced by Jeremy Howard), which uses tool calling for progressive disclosure of documentation. The skills pattern applies the same approach to specialized prompts and domain knowledge rather than just documentation pages.\n</Tip>\n\n```mermaid  theme={null}\ngraph LR\n    A[User] --> B[Agent]\n    B --> C[Skill A]\n    B --> D[Skill B]\n    B --> E[Skill C]\n    B --> A\n```\n\n## Key characteristics\n\n* Prompt-driven specialization: Skills are primarily defined by specialized prompts\n* Progressive disclosure: Skills become available based on context or user needs\n* Team distribution: Different teams can develop and maintain skills independently\n* Lightweight composition: Skills are simpler than full sub-agents\n\n## When to use\n\nUse the skills pattern when you want a single [agent](/oss/javascript/langchain/agents) with many possible specializations, you don't need to enforce specific constraints between skills, or different teams need to develop capabilities independently. Common examples include coding assistants (skills for different languages or tasks), knowledge bases (skills for different domains), and creative assistants (skills for different formats).\n\n## Basic implementation\n\n```typescript  theme={null}\nimport { tool, createAgent } from \"langchain\";\nimport * as z from \"zod\";\n\nconst loadSkill = tool(\n  async ({ skillName }) => {\n    // Load skill content from file/database\n    return \"\";\n  },\n  {\n    name: \"load_skill\",\n    description: `Load a specialized skill.\n\nAvailable skills:\n- write_sql: SQL query writing expert\n- review_legal_doc: Legal document reviewer\n\nReturns the skill's prompt and context.`,\n    schema: z.object({\n      skillName: z\n        .string()\n        .describe(\"Name of skill to load\")\n    })\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [loadSkill],\n  prompt: (\n    \"You are a helpful assistant. \" +\n    \"You have access to two skills: \" +\n    \"write_sql and review_legal_doc. \" +\n    \"Use load_skill to access them.\"\n  ),\n});\n```\n\nFor a complete implementation, see the tutorial below.\n\n<Card title=\"Tutorial: Build a SQL assistant with on-demand skills\" icon=\"wand-magic-sparkles\" href=\"/oss/javascript/langchain/multi-agent/skills-sql-assistant\" arrow cta=\"Learn more\">\n  Learn how to implement skills with progressive disclosure, where the agent loads specialized prompts and schemas on-demand rather than upfront.\n</Card>\n\n## Extending the pattern\n\nWhen writing custom implementations, you can extend the basic skills pattern in several ways:\n\n* **Dynamic tool registration**: Combine progressive disclosure with state management to register new [tools](/oss/javascript/langchain/tools) as skills load. For example, loading a \"database\\_admin\" skill could both add specialized context and register database-specific tools (backup, restore, migrate). This uses the same tool-and-state mechanisms used across multi-agent patterns\u2014tools updating state to dynamically change agent capabilities.\n\n* **Hierarchical skills**: Skills can define other skills in a tree structure, creating nested specializations. For instance, loading a \"data\\_science\" skill might make available sub-skills like \"pandas\\_expert\", \"visualization\", and \"statistical\\_analysis\". Each sub-skill can be loaded independently as needed, allowing for fine-grained progressive disclosure of domain knowledge. This hierarchical approach helps manage large knowledge bases by organizing capabilities into logical groupings that can be discovered and loaded on-demand.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent/skills.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Skills\nLink: https://docs.langchain.comoss/python/langchain/multi-agent/skills\nContent: In the **skills** architecture, specialized capabilities are packaged as invokable \"skills\" that augment an [agent's](/oss/python/langchain/agents) behavior. Skills are primarily prompt-driven specializations that an agent can invoke on-demand.\n\n<Tip>\n  This pattern is conceptually identical to [llms.txt](https://llmstxt.org/) (introduced by Jeremy Howard), which uses tool calling for progressive disclosure of documentation. The skills pattern applies the same approach to specialized prompts and domain knowledge rather than just documentation pages.\n</Tip>\n\n```mermaid  theme={null}\ngraph LR\n    A[User] --> B[Agent]\n    B --> C[Skill A]\n    B --> D[Skill B]\n    B --> E[Skill C]\n    B --> A\n```\n\n## Key characteristics\n\n* Prompt-driven specialization: Skills are primarily defined by specialized prompts\n* Progressive disclosure: Skills become available based on context or user needs\n* Team distribution: Different teams can develop and maintain skills independently\n* Lightweight composition: Skills are simpler than full sub-agents\n\n## When to use\n\nUse the skills pattern when you want a single [agent](/oss/python/langchain/agents) with many possible specializations, you don't need to enforce specific constraints between skills, or different teams need to develop capabilities independently. Common examples include coding assistants (skills for different languages or tasks), knowledge bases (skills for different domains), and creative assistants (skills for different formats).\n\n## Basic implementation\n\n```python  theme={null}\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n@tool\ndef load_skill(skill_name: str) -> str:\n    \"\"\"Load a specialized skill prompt.\n\n    Available skills:\n    - write_sql: SQL query writing expert\n    - review_legal_doc: Legal document reviewer\n\n    Returns the skill's prompt and context.\n    \"\"\"\n    # Load skill content from file/database\n    ...\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[load_skill],\n    system_prompt=(\n        \"You are a helpful assistant. \"\n        \"You have access to two skills: \"\n        \"write_sql and review_legal_doc. \"\n        \"Use load_skill to access them.\"\n    ),\n)\n```\n\nFor a complete implementation, see the tutorial below.\n\n<Card title=\"Tutorial: Build a SQL assistant with on-demand skills\" icon=\"wand-magic-sparkles\" href=\"/oss/python/langchain/multi-agent/skills-sql-assistant\" arrow cta=\"Learn more\">\n  Learn how to implement skills with progressive disclosure, where the agent loads specialized prompts and schemas on-demand rather than upfront.\n</Card>\n\n## Extending the pattern\n\nWhen writing custom implementations, you can extend the basic skills pattern in several ways:\n\n* **Dynamic tool registration**: Combine progressive disclosure with state management to register new [tools](/oss/python/langchain/tools) as skills load. For example, loading a \"database\\_admin\" skill could both add specialized context and register database-specific tools (backup, restore, migrate). This uses the same tool-and-state mechanisms used across multi-agent patterns\u2014tools updating state to dynamically change agent capabilities.\n\n* **Hierarchical skills**: Skills can define other skills in a tree structure, creating nested specializations. For instance, loading a \"data\\_science\" skill might make available sub-skills like \"pandas\\_expert\", \"visualization\", and \"statistical\\_analysis\". Each sub-skill can be loaded independently as needed, allowing for fine-grained progressive disclosure of domain knowledge. This hierarchical approach helps manage large knowledge bases by organizing capabilities into logical groupings that can be discovered and loaded on-demand.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent/skills.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Build a SQL assistant with on-demand skills\nLink: https://docs.langchain.comoss/python/langchain/multi-agent/skills-sql-assistant\nContent: 1. Define skills\nFirst, define the structure for skills. Each skill has a name, a brief description (shown in the system prompt), and full content (loaded on-demand): Now define example skills for a SQL query assistant. The skills are designed to be lightweight in description (shown to the agent upfront) but detailed in content (loaded only when needed):\n\nBuild a SQL assistant with on-demand skills\nBuild a SQL assistant with on-demand skills\nThis tutorial shows how to use progressive disclosure - a context management technique where the agent loads information on-demand rather than upfront - to implement skills (specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task. Use case: Imagine building an agent to help write SQL queries across different business verticals in a large enterprise. Your organization might have separate datastores for each vertical, or a single monolithic database with thousands of tables. Either way, loading all schemas upfront would overwhelm the context window. Progressive disclosure solves this by loading only the relevant schema when needed. This architecture also enables different product owners and stakeholders to independently contribute and maintain skills for their specific business verticals. What you'll build: A SQL query assistant with two skills (sales analytics and inventory management). The agent sees lightweight skill descriptions in its system prompt, then loads full database schemas and business logic through tool calls only when relevant to the user's query. For a more complete example of a SQL agent with query execution, error correction, and validation, see our SQL Agent tutorial . This tutorial focuses on the progressive disclosure pattern which can be applied to any domain. Progressive disclosure was popularized by Anthropic as a technique for building scalable agent skills systems. This approach uses a three-level architecture (metadata \u2192 core content \u2192 detailed resources) where agents load information only as needed. For more on this technique, see Equipping agents for the real world with Agent Skills.\n\nBuild a SQL assistant with on-demand skills\nThis tutorial shows how to use progressive disclosure - a context management technique where the agent loads information on-demand rather than upfront - to implement skills (specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task. Use case: Imagine building an agent to help write SQL queries across different business verticals in a large enterprise. Your organization might have separate datastores for each vertical, or a single monolithic database with thousands of tables. Either way, loading all schemas upfront would overwhelm the context window. Progressive disclosure solves this by loading only the relevant schema when needed. This architecture also enables different product owners and stakeholders to independently contribute and maintain skills for their specific business verticals. What you'll build: A SQL query assistant with two skills (sales analytics and inventory management). The agent sees lightweight skill descriptions in its system prompt, then loads full database schemas and business logic through tool calls only when relevant to the user's query. For a more complete example of a SQL agent with query execution, error correction, and validation, see our SQL Agent tutorial . This tutorial focuses on the progressive disclosure pattern which can be applied to any domain. Progressive disclosure was popularized by Anthropic as a technique for building scalable agent skills systems. This approach uses a three-level architecture (metadata \u2192 core content \u2192 detailed resources) where agents load information only as needed. For more on this technique, see Equipping agents for the real world with Agent Skills.\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "dd0b83f4-c997-4e6b-8897-ad9ebfd4cc87",
    "id": "1e161161-f1b7-45b2-b69d-093f00ed16f8",
    "content": "Result 1:\nTitle: Memory overview\nLink: https://docs.langchain.comoss/javascript/concepts/memory\nContent: Memory overview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. This conceptual guide covers two types of memory, based on their recall scope: Short-term memory , or thread -scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's state . State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step. Long-term memory stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread . Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores ( reference doc ) to let you save and recall long-term memories.\n\nMemory overview\nMemory overview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. This conceptual guide covers two types of memory, based on their recall scope: Short-term memory , or thread -scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's state . State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step. Long-term memory stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread . Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores ( reference doc ) to let you save and recall long-term memories.\n\nWriting memories\nThere are two primary methods for agents to write memories: \"in the hot path\" and \"in the background\".\n\n---\n\nResult 2:\nTitle: Short-term memory\nLink: https://docs.langchain.comoss/javascript/langchain/short-term-memory\nContent: Short-term memory\nOverview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. Short term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation. Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today's LLMs; a full history may not fit inside an LLM's context window, resulting in an context loss or errors. Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs. Chat models accept context using messages , which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or \"forget\" stale information.\n\nOverview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. Short term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation. Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today's LLMs; a full history may not fit inside an LLM's context window, resulting in an context loss or errors. Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs. Chat models accept context using messages , which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or \"forget\" stale information.\n\nAccess memory\nYou can access and modify the short-term memory (state) of an agent in several ways:\n\n---\n\nResult 3:\nTitle: Memory overview\nLink: https://docs.langchain.comoss/python/concepts/memory\nContent: Memory overview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. This conceptual guide covers two types of memory, based on their recall scope: Short-term memory , or thread -scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's state . State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step. Long-term memory stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread . Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores ( reference doc ) to let you save and recall long-term memories.\n\nMemory overview\nMemory overview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. This conceptual guide covers two types of memory, based on their recall scope: Short-term memory , or thread -scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's state . State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step. Long-term memory stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread . Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores ( reference doc ) to let you save and recall long-term memories.\n\nWriting memories\nThere are two primary methods for agents to write memories: \"in the hot path\" and \"in the background\".\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "090a259a-88a0-490a-903d-5ab62fc5a4a9",
    "id": "34066319-f0fe-4a27-8a84-6ff429be1c8b",
    "content": "{\n  \"collections\": \"OSS (LangChain and LangGraph)\",\n  \"total\": 4,\n  \"articles\": [\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b384c-8253-7102-8e8a-c5aa5d4f2166",
    "content": "**In the Deep Agents CLI, \"skills\" are implemented through a combination of the Memories system and the Subagent (Task) tool.**\n\nInstead of a single \"skills\" command, the CLI uses a \"memory-first\" protocol and a delegation model to manage specialized capabilities without overwhelming the main agent's context.\n\n## 1. Domain Knowledge via Memories\n\nThe CLI treats markdown files in your `~/.deepagents/AGENT_NAME/memories/` directory as its primary \"skill\" source. These are not just plain logs; they are structured domain knowledge that the agent researches before every task.\n\n- **Researching**: Before starting a task, the agent automatically searches these files for relevant context.\n- **Learning**: As you teach the agent new conventions (e.g., \"Always use async/await for database calls\"), it writes them to these memory files.\n- **Organization**: You can organize these by topic (e.g., `api-conventions.md`, `testing-setup.md`) to effectively \"equip\" the agent with different skills.\n\n```bash\n# Data location for your \"skills\" (memories)\n~/.deepagents/my-coding-agent/memories/\n\u251c\u2500\u2500 deployment-flow.md\n\u2514\u2500\u2500 code-style.md\n```\n\n## 2. Capability Delegation via Subagents\n\nThe CLI includes a built-in `task` tool that allows the primary agent to spawn specialized subagents. This is the implementation of the \"hierarchical skills\" pattern:\n\n- **Context Isolation**: Instead of the main agent doing everything, it delegates a specific sub-task (like \"Write a unit test for this function\") to a subagent.\n- **Specialization**: The subagent only receives the context needed for that specific task, preventing the main conversation from getting cluttered with technical details.\n- **Recursion**: Subagents can themselves use the CLI's tools and memories to complete their work.\n\n## 3. Tool-Based Progressive Disclosure\n\nThe CLI uses the progressive disclosure pattern mentioned in the general skills architecture. It doesn't load every file or instruction at once. Instead:\n\n1. **Discovery**: The agent sees a high-level summary of what is available.\n2. **Invocation**: It uses file-system tools (`read_file`, `ls`) or the `task` tool only when the user query requires that specific \"skill.\"\n3. **Execution**: It loads the full content or spawns the worker only at the moment of need.\n\nYou can view the implementation of these tools in the [deepagents GitHub repository](https://github.com/langchain-ai/deepagents), specifically looking at the `planning` and `filesystem` tool implementations.\n\n**Relevant docs:**\n\n- [Deep Agents CLI Overview](https://docs.langchain.com/oss/python/deepagents/cli)\n- [Set Project Conventions with Memories](https://docs.langchain.com/oss/python/deepagents/cli#set-project-conventions-with-memories)\n- [Planning and Subagent Capabilities](https://docs.langchain.com/oss/python/deepagents/overview#core-capabilities)"
  },
  {
    "role": "user",
    "id": "4980c04c-42c1-446d-b2c0-b02e49fa9af2",
    "content": "But i have a bunch of skills written myself that i always want to include"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "e11b6762-8847-4aaa-be0e-c155508d52ba",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"customization\"}"
        }
      },
      {
        "type": "function",
        "id": "73875157-5372-4130-89b2-6dcd2d5af9d6",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"tools\"}"
        }
      },
      {
        "type": "function",
        "id": "a0cf7695-220c-4dbf-bfec-cb58e40ee223",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"OSS (LangChain and LangGraph)\"}"
        }
      }
    ],
    "id": "lc_run--019b384e-4336-7ed0-a2a2-0cf4bc6bbf46",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "e11b6762-8847-4aaa-be0e-c155508d52ba",
    "id": "0d4e207a-e9fc-4405-9f1d-ced3530206a2",
    "content": "Result 1:\nTitle: How to customize the Dockerfile\nLink: https://docs.langchain.comlangsmith/custom-docker\nContent: Users can add an array of additional lines to add to the Dockerfile following the import from the parent LangGraph image. In order to do this, you simply need to modify your `langgraph.json` file by passing in the commands you want run to the `dockerfile_lines` key. For example, if we wanted to use `Pillow` in our graph you would need to add the following dependencies:\n\n```\n{\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"openai_agent\": \"./openai_agent.py:agent\",\n    },\n    \"env\": \"./.env\",\n    \"dockerfile_lines\": [\n        \"RUN apt-get update && apt-get install -y libjpeg-dev zlib1g-dev libpng-dev\",\n        \"RUN pip install Pillow\"\n    ]\n}\n```\n\nThis would install the system packages required to use Pillow if we were working with `jpeg` or `png` image formats.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-docker.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Custom output rendering\nLink: https://docs.langchain.comlangsmith/custom-output-rendering\nContent: Custom output rendering allows you to visualize run outputs and dataset reference outputs using your own custom HTML pages. This is particularly useful for:\n\n* **Domain-specific formatting**: Display medical records, legal documents, or other specialized data types in their native format.\n* **Custom visualizations**: Create charts, graphs, or diagrams from numeric or structured output data.\n\nIn this page you'll learn how to:\n\n* **[Configure custom rendering](#configure-custom-output-rendering)** in the LangSmith UI.\n* **[Build a custom renderer](#build-a-custom-renderer)** to display output data.\n* **[Understand where custom rendering appears](#where-custom-rendering-appears)** in LangSmith.\n\n## Configure custom output rendering\n\nConfigure custom rendering at two levels:\n\n* **For datasets**: Apply custom rendering to all runs associated with that dataset, wherever they appear\u2014in experiments, run detail panes, or annotation queues.\n* **For annotation queues**: Apply custom rendering to all runs within a specific annotation queue, regardless of which dataset they come from. This takes precedence over dataset-level configuration.\n\n### For tracing projects\n\nTo configure custom output rendering for a tracing project:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=034f982fa3174c1649e5188bbb11ca03\" alt=\"Tracing project settings showing custom output rendering configuration\" data-og-width=\"1325\" width=\"1325\" data-og-height=\"1207\" height=\"1207\" data-path=\"langsmith/images/tracing-project-custom-output-rendering-settings.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=280&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=9192b74a8df6dac268f17acee40817ab 280w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=560&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=106cf65662788e212505ac39d46d60ab 560w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=840&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=247529f2b0a3326a346145b9476cb709 840w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=1100&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=efcd6238cd47a17055a7ebcc214b3b30 1100w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=1650&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=4e0a42d7a9d939b0cd2731814116cba9 1650w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=2500&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=2519e4389455995b458e317fc23caa83 2500w\" />\n\n1. Navigate to the **Tracing Projects** page.\n2. Click on an existing tracing project or create a new one.\n3. In the edit tracing project pane, scroll to the **Custom Output Rendering** section.\n4. Toggle **Enable custom output rendering**.\n5. Enter the webpage URL in the **URL** field.\n6. Click **Save**.\n\n### For datasets\n\nTo configure custom output rendering for a dataset:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=7daf042ebae80eec20cd90a25c1d6087\" alt=\"Dataset page with three-dot menu showing Custom Output Rendering option\" data-og-width=\"3456\" width=\"3456\" data-og-height=\"2156\" height=\"2156\" data-path=\"langsmith/images/custom-output-rendering-menu.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=432f6a5bb6c79797c17fa8faa74169b1 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=c77ec3be9ea681180d5c82d2588814b5 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=3f8fdb966f809ad049585efe3271182b 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=3bdd2738b8dc1d17eeea3951536dedc8 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=5428362058ab8e8f4f71009779e3134f 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a31530b7bd16f2fe6f8c6b622f55d9de 2500w\" />\n\n1. Navigate to your dataset in the **Datasets & Experiments** page.\n2. Click **\u22ee** (three-dot menu) in the top right corner.\n3. Select **Custom Output Rendering**.\n4. Toggle **Enable custom output rendering**.\n5. Enter the webpage URL in the **URL** field.\n6. Click **Save**.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=bffd3b40ca14bbebc05c998d1cb5fa7e\" alt=\"Custom Output Rendering modal with fields filled in\" data-og-width=\"3456\" width=\"3456\" data-og-height=\"2156\" height=\"2156\" data-path=\"langsmith/images/custom-output-rendering-modal.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=2fd8bdfcce6fdf5e9bd865590a8e0f79 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=c110a35a03909e146be961ac5386c888 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=1959a6a045a04e0d3290de4379a358bc 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=4a154ab114547ff7e243acb36c9bd9a3 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=c2eae83b1d6ec160a37fd998511e9794 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=025b243cf994c130439ef55f6dc5e856 2500w\" />\n\n### For annotation queues\n\nTo configure custom output rendering for an annotation queue:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=579aad04fa6990b220514280eef799f4\" alt=\"Annotation queue settings showing custom output rendering configuration\" data-og-width=\"3456\" width=\"3456\" data-og-height=\"1914\" height=\"1914\" data-path=\"langsmith/images/annotation-queue-custom-output-rendering-settings.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=280&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=525d5f11afb5bf8a5fd86f5de5063d72 280w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=560&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=0d58600ddf45ff1e7bbad5cc31489f24 560w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=840&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=d5aaa11700f5f58bcf157dc0fd32877a 840w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=1100&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=51794d75925516c728177e91689dbdcd 1100w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=1650&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=cf7c47a38a18a548f627dc56e94abfde 1650w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=2500&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=bcd50a9547635188f2e7e458af0baa46 2500w\" />\n\n1. Navigate to the **Annotation Queues** page.\n2. Click on an existing annotation queue or create a new one.\n3. In the annotation queue settings pane, scroll to the **Custom Output Rendering** section.\n4. Toggle **Enable custom output rendering**.\n5. Enter the webpage URL in the **URL** field.\n6. Click **Save** or **Create**.\n\n<Info>When custom rendering settings are applied at multiple levels, the precedence is as follows: annotation queue > dataset > tracing project.</Info>\n\n## Build a custom renderer\n\n### Understand the message format\n\nYour HTML page will receive output data via the [postMessage API](https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage). LangSmith sends messages with the following structure:\n\n```typescript  theme={null}\n{\n  type: \"output\" | \"reference\",\n  data: {\n    // The outputs (actual output or reference output)\n    // Structure varies based on your application\n  },\n  metadata: {\n    inputs: {\n      // The inputs that generated this output\n      // Structure varies based on your application\n    }\n  }\n}\n```\n\n* `type`: Indicates whether this is an actual output (`\"output\"`) or a reference output (`\"reference\"`).\n* `data`: The output data itself.\n* `metadata.inputs`: The input data that generated this output, provided for context.\n\n<Note>**Message delivery timing**: LangSmith uses an exponential backoff retry mechanism to ensure your page receives the data even if it loads slowly. Messages are sent up to 6 times with increasing delays (100ms, 200ms, 400ms, 800ms, 1600ms, 3200ms).</Note>\n\n### Example implementation\n\nThis example listens for incoming postMessage events and displays them on the page. Each message is numbered and formatted as JSON, making it easy to inspect the data structure LangSmith sends to your renderer.\n\n```html  theme={null}\n<!DOCTYPE html>\n<html>\n    <head>\n        <meta charset=\"UTF-8\" />\n        <title>PostMessage Echo</title>\n        <link rel=\"stylesheet\" href=\"https://unpkg.com/sakura.css/css/sakura.css\" />\n    </head>\n    <body>\n        <h1>PostMessage Messages</h1>\n        <div id=\"messages\"></div>\n        <script>\n            let count = 0;\n            window.addEventListener(\"message\", (event) => {\n                count++;\n                const header = document.createElement(\"h3\");\n                header.appendChild(document.createTextNode(`Message ${count}`));\n                const code = document.createElement(\"code\");\n                code.appendChild(document.createTextNode(JSON.stringify(event.data, null, 2)));\n                const pre = document.createElement(\"pre\");\n                pre.appendChild(code);\n                document.getElementById(\"messages\").appendChild(header);\n                document.getElementById(\"messages\").appendChild(pre);\n            });\n        </script>\n    </body>\n</html>\n```\n\n## Where custom rendering appears\n\nWhen enabled, your custom rendering will replace the default output view in:\n\n* **Experiment comparison view**: When comparing outputs across multiple experiments:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=6f1fd9d3ca4be55aa9a0b40140771e08\" alt=\"Experiment comparison view showing custom rendering\" data-og-width=\"3456\" width=\"3456\" data-og-height=\"2156\" height=\"2156\" data-path=\"langsmith/images/custom-output-rendering-experiment-comparison.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=4010749197ec99216b6cbc004f8e2aed 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=3eb5a8825d76c92f531c34912c476fb5 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=1977c3020b1b2150beefc4980db857f9 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=2f8f86c1ad7ff02b65f78cd43bead446 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=979e5426ee86673cd088aef614c52a6e 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=292b4ac1322414a059b2ba5a4285caf3 2500w\" />\n\n* **Run detail panes**: When viewing runs that are associated with a dataset:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=abec759e27bb3dfa827354d13746cf61\" alt=\"Run detail pane showing custom rendering\" data-og-width=\"3456\" width=\"3456\" data-og-height=\"2156\" height=\"2156\" data-path=\"langsmith/images/custom-output-rendering-run-details.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=9d4d8e9e4d3e2f9e856d9da23e16d805 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=1adfc78a0ba06a3b9b18433fc2e1f82c 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a882e8fb03c81526070c0e9f49de157b 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=39d6a85d5578c58cf2c6853ce1d5e704 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=d9c37910f147b76685b018a620aa9d40 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=24538e055a2363976a7f3ce5d0111356 2500w\" />\n\n* **Annotation queues**: When reviewing runs in annotation queues:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=8d1b66541ea7dcd0246354fca1568719\" alt=\"Annotation queue showing custom rendering\" data-og-width=\"3456\" width=\"3456\" data-og-height=\"2156\" height=\"2156\" data-path=\"langsmith/images/custom-output-rendering-annotation-queue.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a42373180e5ab88ee80c3a670102f290 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=47be4504dd9d4a0c191cc426d7eaa7c4 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=36eda0a27d923057fbb4f0fc661e311e 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=82593a5f663686eb61d9b90d9b1b8c39 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=dec8e45c1c77bd1cdd9b3aec093bf566 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a8a978b3ec3d119fb16002d1a432c91a 2500w\" />\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-output-rendering.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Customize user management\nLink: https://docs.langchain.comlangsmith/self-host-user-management\nContent: <Note>\n  This guide assumes you have read the [admin guide](/langsmith/administration-overview) and [organization setup guide](/langsmith/set-up-a-workspace#set-up-an-organization).\n</Note>\n\nLangSmith offers additional customization features for user management using feature flags.\n\n## Features\n\n### Workspace level invites to an organization\n\nThe default behavior in LangSmith requires a user to be an Organization Admin in order to invite new users to an organization. For self-hosted customers that would like to delegate this responsibility to workspace Admins, a feature flag may be set that enables workspace Admins to invite new users to the organization as well as their specific workspace **at the workspace level**.\n\nOnce this feature is enabled via the configuration option below, workspace Admins may add new users in the `Workspace members` tab under `Settings` > `Workspaces`. Both of the following cases are supported when inviting at the workspace level, while the organization level invite functions the same as before.\n\n1. Invite users who are NOT already active in the organization: this will add the users as pending to the organization and specific workspace\n2. Invite users who ARE already active in the organization: adds the users directly to the workspace as an active member (no pending state).\n\nAdmins may invite users for both cases at the same time.\n\n#### Configuration\n\n<CodeGroup>\n  ```yaml Helm theme={null}\n  config:\n    workspaceScopeOrgInvitesEnabled: true\n  ```\n\n  ```bash Docker theme={null}\n  # In your .env file\n  WORKSPACE_SCOPE_ORG_INVITES_ENABLED=\"true\"\n  ```\n</CodeGroup>\n\n### SSO New Member Login Flow\n\nAs of helm **v0.11.10**, self-hosted deployments using OAuth SSO will no longer need to manually add members in LangSmith settings for them to join. Deployments will have a <b>default</b> organization, to which new users will automatically be added upon their first login to LangSmith.\n\nFor your **default** organization, you can set which workspace(s) and workspace role is assigned to new members. For **non-default** organizations, the invitation flow remains the same.\nOnce a user joins an organization, any changes to their workspaces or roles beyond the default organization settings must be managed either through LangSmith settings (as before) or via SCIM.\n\n<Note>\n  By default, all new users are added to the organization\u2019s initially provisioned workspace (**Workspace 1** by default) with the **Workspace Editor** role.\n</Note>\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=e7274ed7fdd47fe7c4c1f514d78f3ac7\" alt=\"Update SSO Member Settings\" data-og-width=\"1769\" width=\"1769\" data-og-height=\"1251\" height=\"1251\" data-path=\"langsmith/images/sso-member-settings-update.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=280&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=2acde74eb4c622771decfe6d750f7c35 280w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=560&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=47bcef317de7189eda2743a66ead2070 560w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=840&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=eaeb449a5dc84d481fee92b7bdd0e163 840w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1100&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=dd18c7a8aa8ee1fc2cce7d418334c713 1100w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1650&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=a7d778198cdb93c3813b92341fc08b70 1650w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=2500&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=6029314be79253612b1465581144b170 2500w\" />\n\n<Note>\n  To change your default organization, use **Set Default Organization** in the organization selector dropdown. (Org Admin permissions required in both the source and target organization.)\n</Note>\n\n### Disabling Organization Creating\n\nBy default, any user can create an organization in LangSmith. For self-hosted customers, an admin may want to restrict this ability after setting up initial organizations. This feature flag allows an admin to disable the ability for users to create new organizations.\n\n#### Configuration\n\n<Note>\n  The `userOrgCreationDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).\n</Note>\n\n<CodeGroup>\n  ```yaml Helm theme={null}\n  config:\n    userOrgCreationDisabled: true\n  ```\n\n  ```bash Docker theme={null}\n  # In your .env file\n  FF_ORG_CREATION_DISABLED=\"true\"\n  ```\n</CodeGroup>\n\n### Disabling Personal Organizations\n\nBy default, any user who logs in to LangSmith will have a personal organization created for them. For self-hosted customers, an admin may want to restrict this ability. This feature flag allows an admin to disable the ability for users to create personal organizations.\n\n#### Configuration\n\n<Note>\n  The `personalOrgsDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).\n</Note>\n\n<CodeGroup>\n  ```yaml Helm theme={null}\n  config:\n    personalOrgsDisabled: true\n  ```\n\n  ```bash Docker theme={null}\n  # In your .env file\n  PERSONAL_ORGS_DISABLED=\"true\"\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-user-management.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "73875157-5372-4130-89b2-6dcd2d5af9d6",
    "id": "4c462984-e31f-4774-9028-51c387c23bf8",
    "content": "Result 1:\nTitle: Tools and Toolkits\nLink: https://docs.langchain.comoss/javascript/integrations/tools/index\nContent: [Tools](/oss/javascript/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n\nA [toolkit](/oss/javascript/langchain/tools#toolkits) is a collection of tools meant to be used together.\n\n## Integration Platforms\n\nThe following platforms provide access to multiple tools and services through a unified interface:\n\n| Tool/Toolkit                                            | Number of Integrations | Pricing             | Key Features                                               |\n| ------------------------------------------------------- | ---------------------- | ------------------- | ---------------------------------------------------------- |\n| [Composio](/oss/javascript/integrations/tools/composio) | 500+                   | Free tier available | OAuth handling, event-driven workflows, multi-user support |\n\n## All Tools and Toolkits\n\n<Columns cols={3}>\n  <Card title=\"Azure Container Apps Dynamic Sessions\" icon=\"link\" href=\"/oss/javascript/integrations/tools/azure_dynamic_sessions\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Connery Action Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/connery\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Composio\" icon=\"link\" href=\"/oss/javascript/integrations/tools/composio\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dall-E Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/dalle\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Decodo Tools\" icon=\"link\" href=\"/oss/javascript/integrations/tools/decodo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Discord Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/discord_tool\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DuckDuckGoSearch\" icon=\"link\" href=\"/oss/javascript/integrations/tools/duckduckgo_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ExaSearchResults\" icon=\"link\" href=\"/oss/javascript/integrations/tools/exa_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FalkorDB\" icon=\"link\" href=\"/oss/javascript/integrations/tools/falkordb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gmail Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_gmail\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GOAT\" icon=\"link\" href=\"/oss/javascript/integrations/tools/goat\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Calendar Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_calendar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Places Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_places\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Routes Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_routes\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Scholar\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_scholar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Trends Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_trends\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"JigsawStack Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/jigsawstack\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Agent with AWS Lambda\" icon=\"link\" href=\"/oss/javascript/integrations/tools/lambda_agent\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Python interpreter tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/pyinterpreter\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SearchApi tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/searchapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Searxng Search tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/searxng\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SerpAPI\" icon=\"link\" href=\"/oss/javascript/integrations/tools/serpapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"StackExchange Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/stackexchange\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Stagehand AI Web Automation Toolkit\" icon=\"link\" href=\"/oss/javascript/integrations/tools/stagehand\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Crawl\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_crawl\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Extract\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_extract\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Map\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_map\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Search\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Web Browser Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/webbrowser\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wikipedia tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/wikipedia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"WolframAlpha Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/wolframalpha\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n<Info>\n  If you'd like to write your own tool, see [this how-to](/oss/javascript/langchain/tools#customize-tool-properties). If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).\n</Info>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/tools/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Tools\nLink: https://docs.langchain.comoss/javascript/langchain/tools\nContent: Tools extend what [agents](/oss/javascript/langchain/agents) can do\u2014letting them fetch real-time data, execute code, query external databases, and take actions in the world.\n\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a [chat model](/oss/javascript/langchain/models). The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\n\n<Tip>\n  For details on how models handle tool calls, see [Tool calling](/oss/javascript/langchain/models#tool-calling).\n</Tip>\n\n## Create tools\n\n### Basic tool definition\n\nThe simplest way to create a tool is by importing the `tool` function from the `langchain` package. You can use [zod](https://zod.dev/) to define the tool's input schema:\n\n```ts  theme={null}\nimport * as z from \"zod\"\nimport { tool } from \"langchain\"\n\nconst searchDatabase = tool(\n  ({ query, limit }) => `Found ${limit} results for '${query}'`,\n  {\n    name: \"search_database\",\n    description: \"Search the customer database for records matching the query.\",\n    schema: z.object({\n      query: z.string().describe(\"Search terms to look for\"),\n      limit: z.number().describe(\"Maximum number of results to return\"),\n    }),\n  }\n);\n```\n\n<Note>\n  **Server-side tool use**\n\n  Some chat models (e.g., [OpenAI](/oss/javascript/integrations/chat/openai), [Anthropic](/oss/javascript/integrations/chat/anthropic), and [Gemini](/oss/javascript/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/javascript/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/javascript/integrations/providers/overview) to learn how to access these tools with your specific chat model.\n</Note>\n\n## Accessing Context\n\n<Info>\n  **Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\n  The runtime context provides a structured way to supply runtime data, such as DB connections, user IDs, or config, into your tools. This avoids global state and keeps tools testable and reusable.\n</Info>\n\n#### Context\n\nTools can access an agent's runtime context through the `config` parameter:\n\n```ts  theme={null}\nimport * as z from \"zod\"\nimport { ChatOpenAI } from \"@langchain/openai\"\nimport { createAgent } from \"langchain\"\n\nconst getUserName = tool(\n  (_, config) => {\n    return config.context.user_name\n  },\n  {\n    name: \"get_user_name\",\n    description: \"Get the user's name.\",\n    schema: z.object({}),\n  }\n);\n\nconst contextSchema = z.object({\n  user_name: z.string(),\n});\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserName],\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"What is my name?\" }]\n  },\n  {\n    context: { user_name: \"John Smith\" }\n  }\n);\n```\n\n#### Memory (Store)\n\nAccess persistent data across conversations using the store. The store is accessed via `config.store` and allows you to save and retrieve user-specific or application-specific data.\n\n```ts expandable theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst store = new InMemoryStore();\n\n// Access memory\nconst getUserInfo = tool(\n  async ({ user_id }) => {\n    const value = await store.get([\"users\"], user_id);\n    console.log(\"get_user_info\", user_id, value);\n    return value;\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({\n      user_id: z.string(),\n    }),\n  }\n);\n\n// Update memory\nconst saveUserInfo = tool(\n  async ({ user_id, name, age, email }) => {\n    console.log(\"save_user_info\", user_id, name, age, email);\n    await store.put([\"users\"], user_id, { name, age, email });\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info.\",\n    schema: z.object({\n      user_id: z.string(),\n      name: z.string(),\n      age: z.number(),\n      email: z.string(),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserInfo, saveUserInfo],\n  store,\n});\n\n// First session: save user info\nawait agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\",\n    },\n  ],\n});\n\n// Second session: get user info\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"Get user info for user with id 'abc123'\" },\n  ],\n});\n\nconsole.log(result);\n// Here is the user info for user with ID \"abc123\":\n// - Name: Foo\n// - Age: 25\n// - Email: foo@langchain.dev\n```\n\n#### Stream Writer\n\nStream custom updates from tools as they execute using `config.streamWriter`. This is useful for providing real-time feedback to users about what a tool is doing.\n\n```ts  theme={null}\nimport * as z from \"zod\";\nimport { tool, ToolRuntime } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }, config: ToolRuntime) => {\n    const writer = config.writer;\n\n    // Stream custom updates as the tool executes\n    if (writer) {\n      writer(`Looking up data for city: ${city}`);\n      writer(`Acquired data for city: ${city}`);\n    }\n\n    return `It's always sunny in ${city}!`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather for a given city.\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Tools\nLink: https://docs.langchain.comoss/python/langchain/tools\nContent: Tools extend what [agents](/oss/python/langchain/agents) can do\u2014letting them fetch real-time data, execute code, query external databases, and take actions in the world.\n\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a [chat model](/oss/python/langchain/models). The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\n\n<Tip>\n  For details on how models handle tool calls, see [Tool calling](/oss/python/langchain/models#tool-calling).\n</Tip>\n\n## Create tools\n\n### Basic tool definition\n\nThe simplest way to create a tool is with the [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) decorator. By default, the function's docstring becomes the tool's description that helps the model understand when to use it:\n\n```python  theme={null}\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str, limit: int = 10) -> str:\n    \"\"\"Search the customer database for records matching the query.\n\n    Args:\n        query: Search terms to look for\n        limit: Maximum number of results to return\n    \"\"\"\n    return f\"Found {limit} results for '{query}'\"\n```\n\nType hints are **required** as they define the tool's input schema. The docstring should be informative and concise to help the model understand the tool's purpose.\n\n<Note>\n  **Server-side tool use**\n\n  Some chat models (e.g., [OpenAI](/oss/python/integrations/chat/openai), [Anthropic](/oss/python/integrations/chat/anthropic), and [Gemini](/oss/python/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/python/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/python/integrations/providers/overview) to learn how to access these tools with your specific chat model.\n</Note>\n\n### Customize tool properties\n\n#### Custom tool name\n\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\n\n```python  theme={null}\n@tool(\"web_search\")  # Custom name\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nprint(search.name)  # web_search\n```\n\n#### Custom tool description\n\nOverride the auto-generated tool description for clearer model guidance:\n\n```python  theme={null}\n@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\ndef calc(expression: str) -> str:\n    \"\"\"Evaluate mathematical expressions.\"\"\"\n    return str(eval(expression))\n```\n\n### Advanced schema definition\n\nDefine complex inputs with Pydantic models or JSON schemas:\n\n<CodeGroup>\n  ```python Pydantic model theme={null}\n  from pydantic import BaseModel, Field\n  from typing import Literal\n\n  class WeatherInput(BaseModel):\n      \"\"\"Input for weather queries.\"\"\"\n      location: str = Field(description=\"City name or coordinates\")\n      units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n          default=\"celsius\",\n          description=\"Temperature unit preference\"\n      )\n      include_forecast: bool = Field(\n          default=False,\n          description=\"Include 5-day forecast\"\n      )\n\n  @tool(args_schema=WeatherInput)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n\n  ```python JSON Schema theme={null}\n  weather_schema = {\n      \"type\": \"object\",\n      \"properties\": {\n          \"location\": {\"type\": \"string\"},\n          \"units\": {\"type\": \"string\"},\n          \"include_forecast\": {\"type\": \"boolean\"}\n      },\n      \"required\": [\"location\", \"units\", \"include_forecast\"]\n  }\n\n  @tool(args_schema=weather_schema)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n</CodeGroup>\n\n### Reserved argument names\n\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\n\n| Parameter name | Purpose                                                                |\n| -------------- | ---------------------------------------------------------------------- |\n| `config`       | Reserved for passing `RunnableConfig` to tools internally              |\n| `runtime`      | Reserved for `ToolRuntime` parameter (accessing state, context, store) |\n\nTo access runtime information, use the [`ToolRuntime`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.ToolRuntime) parameter instead of naming your own arguments `config` or `runtime`.\n\n## Accessing Context\n\n<Info>\n  **Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\n  Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.\n</Info>\n\nTools can access runtime information through the `ToolRuntime` parameter, which provides:\n\n* **State** - Mutable data that flows through execution (e.g., messages, counters, custom fields)\n* **Context** - Immutable configuration like user IDs, session details, or application-specific configuration\n* **Store** - Persistent long-term memory across conversations\n* **Stream Writer** - Stream custom updates as tools execute\n* **Config** - `RunnableConfig` for the execution\n* **Tool Call ID** - ID of the current tool call\n\n```mermaid  theme={null}\ngraph LR\n    %% Runtime Context\n    subgraph \"\ud83d\udd27 Tool Runtime Context\"\n        A[Tool Call] --> B[ToolRuntime]\n        B --> C[State Access]\n        B --> D[Context Access]\n        B --> E[Store Access]\n        B --> F[Stream Writer]\n    end\n\n    %% Available Resources\n    subgraph \"\ud83d\udcca Available Resources\"\n        C --> G[Messages]\n        C --> H[Custom State]\n        D --> I[User ID]\n        D --> J[Session Info]\n        E --> K[Long-term Memory]\n        E --> L[User Preferences]\n    end\n\n    %% Tool Capabilities\n    subgraph \"\u26a1 Enhanced Tool Capabilities\"\n        M[Context-Aware Tools]\n        N[Stateful Tools]\n        O[Memory-Enabled Tools]\n        P[Streaming Tools]\n    end\n\n    %% Connections\n    G --> M\n    H --> N\n    I --> M\n    J --> M\n    K --> O\n    L --> O\n    F --> P\n```\n\n### `ToolRuntime`\n\nUse `ToolRuntime` to access all runtime information in a single parameter. Simply add `runtime: ToolRuntime` to your tool signature, and it will be automatically injected without being exposed to the LLM.\n\n<Info>\n  **`ToolRuntime`**: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate [`InjectedState`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedState), [`InjectedStore`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedStore), [`get_runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.get_runtime), and [`InjectedToolCallId`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.InjectedToolCallId) annotations.\n\n  The runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.\n</Info>\n\n**Accessing state:**\n\nTools can access the current graph state using `ToolRuntime`:\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n# Access the current conversation state\n@tool\ndef summarize_conversation(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Summarize the conversation so far.\"\"\"\n    messages = runtime.state[\"messages\"]\n\n    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n\n    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\n\n# Access custom state fields\n@tool\ndef get_user_preference(\n    pref_name: str,\n    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model\n) -> str:\n    \"\"\"Get a user preference value.\"\"\"\n    preferences = runtime.state.get(\"user_preferences\", {})\n    return preferences.get(pref_name, \"Not set\")\n```\n\n<Warning>\n  The `runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `runtime` is *not* included in the request.\n</Warning>\n\n**Updating state:**\n\nUse [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) to update the agent's state or control the graph's execution flow:\n\n```python  theme={null}\nfrom langgraph.types import Command\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langchain.tools import tool, ToolRuntime\n\n# Update the conversation history by removing all messages\n@tool\ndef clear_conversation() -> Command:\n    \"\"\"Clear the conversation history.\"\"\"\n\n    return Command(\n        update={\n            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],\n        }\n    )\n\n# Update the user_name in the agent state\n@tool\ndef update_user_name(\n    new_name: str,\n    runtime: ToolRuntime\n) -> Command:\n    \"\"\"Update the user's name.\"\"\"\n    return Command(update={\"user_name\": new_name})\n```\n\n#### Context\n\nAccess immutable configuration and contextual data like user IDs, session details, or application-specific configuration through `runtime.context`.\n\nTools can access runtime context through `ToolRuntime`:\n\n```python  theme={null}\nfrom dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\nUSER_DATABASE = {\n    \"user123\": {\n        \"name\": \"Alice Johnson\",\n        \"account_type\": \"Premium\",\n        \"balance\": 5000,\n        \"email\": \"alice@example.com\"\n    },\n    \"user456\": {\n        \"name\": \"Bob Smith\",\n        \"account_type\": \"Standard\",\n        \"balance\": 1200,\n        \"email\": \"bob@example.com\"\n    }\n}\n\n@dataclass\nclass UserContext:\n    user_id: str\n\n@tool\ndef get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n    \"\"\"Get the current user's account information.\"\"\"\n    user_id = runtime.context.user_id\n\n    if user_id in USER_DATABASE:\n        user = USER_DATABASE[user_id]\n        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n    return \"User not found\"\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nagent = create_agent(\n    model,\n    tools=[get_account_info],\n    context_schema=UserContext,\n    system_prompt=\"You are a financial assistant.\"\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n    context=UserContext(user_id=\"user123\")\n)\n```\n\n#### Memory (Store)\n\nAccess persistent data across conversations using the store. The store is accessed via `runtime.store` and allows you to save and retrieve user-specific or application-specific data.\n\nTools can access and update the store through `ToolRuntime`:\n\n```python expandable theme={null}\nfrom typing import Any\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\n# Access memory\n@tool\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n    \"\"\"Look up user info.\"\"\"\n    store = runtime.store\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\n# Update memory\n@tool\ndef save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n    \"\"\"Save user info.\"\"\"\n    store = runtime.store\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nstore = InMemoryStore()\nagent = create_agent(\n    model,\n    tools=[get_user_info, save_user_info],\n    store=store\n)\n\n# First session: save user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n})\n\n# Second session: get user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email: foo@langchain.dev\n```\n\n#### Stream Writer\n\nStream custom updates from tools as they execute using `runtime.stream_writer`. This is useful for providing real-time feedback to users about what a tool is doing.\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather(city: str, runtime: ToolRuntime) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = runtime.stream_writer\n\n    # Stream custom updates as the tool executes\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n\n    return f\"It's always sunny in {city}!\"\n```\n\n<Note>\n  If you use `runtime.stream_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/python/langchain/streaming) for more details.\n</Note>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "a0cf7695-220c-4dbf-bfec-cb58e40ee223",
    "id": "56dd2e9a-af9e-4d76-8163-796d6dbd02d2",
    "content": "{\n  \"collections\": \"OSS (LangChain and LangGraph)\",\n  \"total\": 4,\n  \"articles\": [\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b384e-55d2-71a3-85b3-83a6deeb2add",
    "content": "**You can include your own custom skills in the Deep Agents CLI by saving them as markdown files in the agent's memories directory or by extending the library with custom Python tools.**\n\nThe CLI is designed to be highly extensible, allowing you to \"teach\" it your specific workflows and technical skills in two primary ways:\n\n## Option 1: Knowledge-Based Skills (Memories)\n\nIf your skills are primarily instructions, domain knowledge, or technical conventions, save them as `.md` files in the agent's local memory path. The agent will research these files before starting any task.\n\n```bash\n# Path for your custom skill files\n~/.deepagents/my-agent-name/memories/\n\u251c\u2500\u2500 database-migrations.md  # Skill: How to run your specific DB migrations\n\u251c\u2500\u2500 pr-standards.md         # Skill: Your team's code review checklist\n\u2514\u2500\u2500 dev-server-setup.md     # Skill: Custom server startup commands\n```\n\nSimply teach the agent via the CLI once, and it will persist these to the memories directory:\n\n```bash\nuvx deepagents-cli --agent my-agent\n> Here is how we run our database migrations: [your detailed instructions]\n```\n\n## Option 2: Code-Based Skills (Custom Tools)\n\nIf your skills are Python functions (e.g., calling a custom API or running a specific script), you can wrap the `deepagents` library in your own Python script. This allows you to append your custom tools to the agent's built-in set:\n\n```python\nfrom deepagents import DeepAgent\nfrom langchain.tools import tool\n\n@tool\ndef trigger_deploy(environment: str):\n    \"\"\"Trigger a deployment to 'staging' or 'production'.\"\"\"\n    # Your custom deployment logic here\n    return f\"Deploying to {environment}...\"\n\n# Initialize the agent with your custom tools\nagent = DeepAgent(\n    name=\"custom-expert\",\n    tools=[trigger_deploy]  # Your skills added here\n)\n\n# Start the interactive CLI session with your custom setup\nagent.start_cli()\n```\n\n## Option 3: Tool-Level Skills (MCP)\n\nThe Deep Agents CLI supports **Model Context Protocol (MCP)** servers. If you have \"skills\" packaged as an MCP server (e.g., for Jira, Slack, or a custom internal tool), you can connect them to the CLI, and the agent will use them as native capabilities.\n\n**Relevant docs:**\n\n- [Deep Agents Overview](https://docs.langchain.com/oss/python/deepagents/overview)\n- [Set Project Conventions with Memories](https://docs.langchain.com/oss/python/deepagents/cli#set-project-conventions-with-memories)\n- [Creating and Customizing Tools](https://docs.langchain.com/oss/python/langchain/tools)\n- [How to Connect MCPs with LangChain](https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain)"
  },
  {
    "role": "user",
    "id": "c98bfa8b-f38a-4a6f-bac5-b6318ef9b94e",
    "content": "Show how option 3 works"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "5242d7cb-fdcd-4262-ad3f-182aff837bdd",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"query\": \"mcp\"}"
        }
      },
      {
        "type": "function",
        "id": "d0fd1314-e992-43c8-a5f5-46c36f84ea65",
        "function": {
          "name": "get_article_content",
          "arguments": "{\"article_id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\"}"
        }
      }
    ],
    "id": "lc_run--019b3850-072c-74f2-bd1e-1dd8689811d1",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "5242d7cb-fdcd-4262-ad3f-182aff837bdd",
    "id": "c4aa9c4e-820d-4080-a694-fcbeed64300d",
    "content": "Result 1:\nTitle: MCP endpoint in Agent Server\nLink: https://docs.langchain.comlangsmith/server-mcp\nContent: The Model Context Protocol (MCP) is an open protocol for describing tools and data sources in a model-agnostic format, enabling LLMs to discover and use them via a structured API.\n\n[Agent Server](/langsmith/agent-server) implements MCP using the [Streamable HTTP transport](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/transports/#streamable-http). This allows LangGraph **agents** to be exposed as **MCP tools**, making them usable with any MCP-compliant client supporting Streamable HTTP.\n\nThe MCP endpoint is available at `/mcp` on [Agent Server](/langsmith/agent-server).\n\nYou can set up [custom authentication middleware](/langsmith/custom-auth) to authenticate a user with an MCP server to get access to user-scoped tools within your LangSmith deployment.\n\nAn example architecture for this flow:\n\n```mermaid  theme={null}\nsequenceDiagram\n  %% Actors\n  participant ClientApp as Client\n  participant AuthProv  as Auth Provider\n  participant LangGraph as Agent Server\n  participant SecretStore as Secret Store\n  participant MCPServer as MCP Server\n\n  %% Platform login / AuthN\n  ClientApp  ->> AuthProv: 1. Login (username / password)\n  AuthProv   -->> ClientApp: 2. Return token\n  ClientApp  ->> LangGraph: 3. Request with token\n\n  Note over LangGraph: 4. Validate token (@auth.authenticate)\n  LangGraph  -->> AuthProv: 5. Fetch user info\n  AuthProv   -->> LangGraph: 6. Confirm validity\n\n  %% Fetch user tokens from secret store\n  LangGraph  ->> SecretStore: 6a. Fetch user tokens\n  SecretStore -->> LangGraph: 6b. Return tokens\n\n  Note over LangGraph: 7. Apply access control (@auth.on.*)\n\n  %% MCP round-trip\n  Note over LangGraph: 8. Build MCP client with user token\n  LangGraph  ->> MCPServer: 9. Call MCP tool (with header)\n  Note over MCPServer: 10. MCP validates header and runs tool\n  MCPServer  -->> LangGraph: 11. Tool response\n\n  %% Return to caller\n  LangGraph  -->> ClientApp: 12. Return resources / tool output\n```\n\n## Requirements\n\nTo use MCP, ensure you have the following dependencies installed:\n\n* `langgraph-api >= 0.2.3`\n* `langgraph-sdk >= 0.1.61`\n\nInstall them with:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install \"langgraph-api>=0.2.3\" \"langgraph-sdk>=0.1.61\"\n  ```\n\n  ```bash uv theme={null}\n  uv add \"langgraph-api>=0.2.3\" \"langgraph-sdk>=0.1.61\"\n  ```\n</CodeGroup>\n\n## Usage overview\n\nTo enable MCP:\n\n* Upgrade to use langgraph-api>=0.2.3. If you are deploying LangSmith, this will be done for you automatically if you create a new revision.\n* MCP tools (agents) will be automatically exposed.\n* Connect with any MCP-compliant client that supports Streamable HTTP.\n\n### Client\n\nUse an MCP-compliant client to connect to the Agent Server. The following examples show how to connect using different programming languages.\n\n<Tabs>\n  <Tab title=\"JavaScript/TypeScript\">\n    ```bash  theme={null}\n    npm install @modelcontextprotocol/sdk\n    ```\n\n    > **Note**\n    > Replace `serverUrl` with your Agent Server URL and configure authentication headers as needed.\n\n    ```js  theme={null}\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StreamableHTTPClientTransport } from \"@modelcontextprotocol/sdk/client/streamableHttp.js\";\n\n    // Connects to the LangGraph MCP endpoint\n    async function connectClient(url) {\n        const baseUrl = new URL(url);\n        const client = new Client({\n            name: 'streamable-http-client',\n            version: '1.0.0'\n        });\n\n        const transport = new StreamableHTTPClientTransport(baseUrl);\n        await client.connect(transport);\n\n        console.log(\"Connected using Streamable HTTP transport\");\n        console.log(JSON.stringify(await client.listTools(), null, 2));\n        return client;\n    }\n\n    const serverUrl = \"http://localhost:2024/mcp\";\n\n    connectClient(serverUrl)\n        .then(() => {\n            console.log(\"Client connected successfully\");\n        })\n        .catch(error => {\n            console.error(\"Failed to connect client:\", error);\n        });\n    ```\n  </Tab>\n\n  <Tab title=\"Python\">\n    Install the adapter with:\n\n    ```bash  theme={null}\n    pip install langchain-mcp-adapters\n    ```\n\n    Here is an example of how to connect to a remote MCP endpoint and use an agent as a tool:\n\n    ```python  theme={null}\n    # Create server parameters for stdio connection\n    from mcp import ClientSession\n    from mcp.client.streamable_http import streamablehttp_client\n    import asyncio\n\n    from langchain_mcp_adapters.tools import load_mcp_tools\n    from langchain.agents import create_agent\n\n\n    server_params = {\n        \"url\": \"https://mcp-finance-agent.xxx.us.langgraph.app/mcp\",\n        \"headers\": {\n            \"X-Api-Key\":\"lsv2_pt_your_api_key\"\n        }\n    }\n\n    async def main():\n        async with streamablehttp_client(**server_params) as (read, write, _):\n            async with ClientSession(read, write) as session:\n                # Initialize the connection\n                await session.initialize()\n\n                # Load the remote graph as if it was a tool\n                tools = await load_mcp_tools(session)\n\n                # Create and run a react agent with the tools\n                agent = create_agent(\"gpt-4.1\", tools)\n\n                # Invoke the agent with a message\n                agent_response = await agent.ainvoke({\"messages\": \"What can the finance agent do for me?\"})\n                print(agent_response)\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n  </Tab>\n</Tabs>\n\n## Expose an agent as MCP tool\n\nWhen deployed, your agent will appear as a tool in the MCP endpoint\nwith this configuration:\n\n* **Tool name**: The agent's name.\n* **Tool description**: The agent's description.\n* **Tool input schema**: The agent's input schema.\n\n### Setting name and description\n\nYou can set the name and description of your agent in `langgraph.json`:\n\n```json  theme={null}\n{\n    \"graphs\": {\n        \"my_agent\": {\n            \"path\": \"./my_agent/agent.py:graph\",\n            \"description\": \"A description of what the agent does\"\n        }\n    },\n    \"env\": \".env\"\n}\n```\n\nAfter deployment, you can update the name and description using the LangGraph SDK.\n\n### Schema\n\nDefine clear, minimal input and output schemas to avoid exposing unnecessary internal complexity to the LLM.\n\nThe default [MessagesState](/oss/python/langgraph/graph-api#messagesstate) uses `AnyMessage`, which supports many message types but is too general for direct LLM exposure.\n\nInstead, define **custom agents or workflows** that use explicitly typed input and output structures.\n\nFor example, a workflow answering documentation questions might look like this:\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# Define input schema\nclass InputState(TypedDict):\n    question: str\n\n# Define output schema\nclass OutputState(TypedDict):\n    answer: str\n\n# Combine input and output\nclass OverallState(InputState, OutputState):\n    pass\n\n# Define the processing node\ndef answer_node(state: InputState):\n    # Replace with actual logic and do something useful\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\n\n# Build the graph with explicit schemas\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\nbuilder.add_node(answer_node)\nbuilder.add_edge(START, \"answer_node\")\nbuilder.add_edge(\"answer_node\", END)\ngraph = builder.compile()\n\n# Run the graph\nprint(graph.invoke({\"question\": \"hi\"}))\n```\n\nFor more details, see the [low-level concepts guide](/oss/python/langgraph/graph-api#state).\n\n## Use user-scoped MCP tools in your deployment\n\n<Tip>\n  **Prerequisites**\n  You have added your own [custom auth middleware](/langsmith/custom-auth) that populates the `langgraph_auth_user` object, making it accessible through configurable context for every node in your graph.\n</Tip>\n\nTo make user-scoped tools available to your LangSmith deployment, start with implementing a snippet like the following:\n\n```python  theme={null}\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\ndef mcp_tools_node(state, config):\n    user = config[\"configurable\"].get(\"langgraph_auth_user\")\n         , user[\"github_token\"], user[\"email\"], etc.\n\n    client = MultiServerMCPClient({\n        \"github\": {\n            \"transport\": \"streamable_http\", # (1)\n            \"url\": \"https://my-github-mcp-server/mcp\", # (2)\n            \"headers\": {\n                \"Authorization\": f\"Bearer {user['github_token']}\"\n            }\n        }\n    })\n    tools = await client.get_tools() # (3)\n\n    # Your tool-calling logic here\n\n    tool_messages = ...\n    return {\"messages\": tool_messages}\n```\n\n1. MCP only supports adding headers to requests made to `streamable_http` and `sse` `transport` servers.\n2. Your MCP server URL.\n3. Get available tools from your MCP server.\n\n*This can also be done by [rebuilding your graph at runtime](/langsmith/graph-rebuild) to have a different configuration for a new run*\n\n## Session behavior\n\nThe current LangGraph MCP implementation does not support sessions. Each `/mcp` request is stateless and independent.\n\n## Authentication\n\nThe `/mcp` endpoint uses the same authentication as the rest of the LangGraph API. Refer to the [authentication guide](/langsmith/auth) for setup details.\n\n## Disable MCP\n\nTo disable the MCP endpoint, set `disable_mcp` to `true` in your `langgraph.json` configuration file:\n\n```json  theme={null}\n{\n  \"$schema\": \"https://langgra.ph/schema.json\",\n  \"http\": {\n    \"disable_mcp\": true\n  }\n}\n```\n\nThis will prevent the server from exposing the `/mcp` endpoint.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/server-mcp.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Tools\nLink: https://docs.langchain.comoss/javascript/integrations/tools/anthropic\nContent: MCP Toolset\nThe MCP toolset ( mcpToolset_20251120 ) enables Claude to connect to remote MCP (Model Context Protocol) servers directly from the Messages API without implementing a separate MCP client. This allows Claude to use tools provided by MCP servers. Key features: Direct API integration - Connect to MCP servers without implementing an MCP client Tool calling support - Access MCP tools through the Messages API Flexible tool configuration - Enable all tools, allowlist specific tools, or denylist unwanted tools Per-tool configuration - Configure individual tools with custom settings OAuth authentication - Support for OAuth Bearer tokens for authenticated servers Multiple servers - Connect to multiple MCP servers in a single request Allowlist pattern - Enable only specific tools: Denylist pattern - Disable specific tools: Multiple MCP servers: With Tool Search - Use deferred loading for on-demand tool discovery: const response = await llm.invoke(\"Find and use the right tool\", {\n  mcp_servers: [\n    {\n      type: \"url\",\n      url: \"https://example.com/sse\",\n      name: \"example-mcp\",\n    },\n  ],\n  tools: [\n    tools.toolSearchRegex_20251119(),\n    tools.mcpToolset_20251120({\n      serverName: \"example-mcp\",\n      defaultConfig: { deferLoading: true },\n    }),\n  ],\n});\n\nMCP Toolset\nThe MCP toolset ( mcpToolset_20251120 ) enables Claude to connect to remote MCP (Model Context Protocol) servers directly from the Messages API without implementing a separate MCP client. This allows Claude to use tools provided by MCP servers. Key features: Direct API integration - Connect to MCP servers without implementing an MCP client Tool calling support - Access MCP tools through the Messages API Flexible tool configuration - Enable all tools, allowlist specific tools, or denylist unwanted tools Per-tool configuration - Configure individual tools with custom settings OAuth authentication - Support for OAuth Bearer tokens for authenticated servers Multiple servers - Connect to multiple MCP servers in a single request Allowlist pattern - Enable only specific tools: Denylist pattern - Disable specific tools: Multiple MCP servers: With Tool Search - Use deferred loading for on-demand tool discovery: For more information, see Anthropic's MCP Connector documentation. Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nMCP Toolset\nThe MCP toolset ( mcpToolset_20251120 ) enables Claude to connect to remote MCP (Model Context Protocol) servers directly from the Messages API without implementing a separate MCP client. This allows Claude to use tools provided by MCP servers. Key features: Direct API integration - Connect to MCP servers without implementing an MCP client Tool calling support - Access MCP tools through the Messages API Flexible tool configuration - Enable all tools, allowlist specific tools, or denylist unwanted tools Per-tool configuration - Configure individual tools with custom settings OAuth authentication - Support for OAuth Bearer tokens for authenticated servers Multiple servers - Connect to multiple MCP servers in a single request Allowlist pattern - Enable only specific tools: Denylist pattern - Disable specific tools: Multiple MCP servers: const response = await llm.invoke(\"Use tools from both servers\", {\n  mcp_servers: [\n    {\n      type: \"url\",\n      url: \"https://mcp.example1.com/sse\",\n      name: \"mcp-server-1\",\n      authorization_token: \"TOKEN1\",\n    },\n    {\n      type: \"url\",\n      url: \"https://mcp.example2.com/sse\",\n      name: \"mcp-server-2\",\n      authorization_token: \"TOKEN2\",\n    },\n  ],\n  tools: [\n    tools.mcpToolset_20251120({ serverName: \"mcp-server-1\" }),\n    tools.mcpToolset_20251120({\n      serverName: \"mcp-server-2\",\n      defaultConfig: { deferLoading: true },\n    }),\n  ],\n});\n\n---\n\nResult 3:\nTitle: Model Context Protocol (MCP)\nLink: https://docs.langchain.comoss/javascript/langchain/mcp\nContent: [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`@langchain/mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters) library.\n\n## Quickstart\n\nInstall the `@langchain/mcp-adapters` library:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/mcp-adapters\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/mcp-adapters\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/mcp-adapters\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/mcp-adapters\n  ```\n</CodeGroup>\n\n`@langchain/mcp-adapters` enables agents to use tools defined across one or more MCP servers.\n\n<Note>\n  `MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up. See the [stateful sessions](#stateful-sessions) section for more details.\n</Note>\n\n```ts Accessing multiple MCP servers icon=\"server\" theme={null}\nimport { MultiServerMCPClient } from \"@langchain/mcp-adapters\";  // [!code highlight]\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { createAgent } from \"langchain\";\n\nconst client = new MultiServerMCPClient({  // [!code highlight]\n    math: {\n        transport: \"stdio\",  // Local subprocess communication\n        command: \"node\",\n        // Replace with absolute path to your math_server.js file\n        args: [\"/path/to/math_server.js\"],\n    },\n    weather: {\n        transport: \"streamable_http\",  // HTTP-based remote server\n        // Ensure you start your weather server on port 8000\n        url: \"http://localhost:8000/mcp\",\n    },\n});\n\nconst tools = await client.getTools();  // [!code highlight]\nconst agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools,  // [!code highlight]\n});\n\nconst mathResponse = await agent.invoke({\n    messages: [{ role: \"user\", content: \"what's (3 + 5) x 12?\" }],\n});\n\nconst weatherResponse = await agent.invoke({\n    messages: [{ role: \"user\", content: \"what is the weather in nyc?\" }],\n});\n```\n\n## Custom servers\n\nTo create your own MCP servers, you can use the `@modelcontextprotocol/sdk` library. This library provides a simple way to define [tools](https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions) and run them as servers.\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @modelcontextprotocol/sdk\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @modelcontextprotocol/sdk\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @modelcontextprotocol/sdk\n  ```\n\n  ```bash bun theme={null}\n  bun add @modelcontextprotocol/sdk\n  ```\n</CodeGroup>\n\nTo test your agent with MCP tool servers, use the following examples:\n\n```typescript title=\"Math server (stdio transport)\" icon=\"floppy-disk\" theme={null}\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport {\n    CallToolRequestSchema,\n    ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\n\nconst server = new Server(\n    {\n        name: \"math-server\",\n        version: \"0.1.0\",\n    },\n    {\n        capabilities: {\n            tools: {},\n        },\n    }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => {\n    return {\n        tools: [\n        {\n            name: \"add\",\n            description: \"Add two numbers\",\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    a: {\n                        type: \"number\",\n                        description: \"First number\",\n                    },\n                    b: {\n                        type: \"number\",\n                        description: \"Second number\",\n                    },\n                },\n                required: [\"a\", \"b\"],\n            },\n        },\n        {\n            name: \"multiply\",\n            description: \"Multiply two numbers\",\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    a: {\n                        type: \"number\",\n                        description: \"First number\",\n                    },\n                    b: {\n                        type: \"number\",\n                        description: \"Second number\",\n                    },\n                },\n                required: [\"a\", \"b\"],\n            },\n        },\n        ],\n    };\n});\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n    switch (request.params.name) {\n        case \"add\": {\n            const { a, b } = request.params.arguments as { a: number; b: number };\n            return {\n                content: [\n                {\n                    type: \"text\",\n                    text: String(a + b),\n                },\n                ],\n            };\n        }\n        case \"multiply\": {\n            const { a, b } = request.params.arguments as { a: number; b: number };\n            return {\n                content: [\n                {\n                    type: \"text\",\n                    text: String(a * b),\n                },\n                ],\n            };\n        }\n        default:\n            throw new Error(`Unknown tool: ${request.params.name}`);\n    }\n});\n\nasync function main() {\n    const transport = new StdioServerTransport();\n    await server.connect(transport);\n    console.error(\"Math MCP server running on stdio\");\n}\n\nmain();\n```\n\n```typescript title=\"Weather server (SSE transport)\" icon=\"wifi\" theme={null}\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { SSEServerTransport } from \"@modelcontextprotocol/sdk/server/sse.js\";\nimport {\n    CallToolRequestSchema,\n    ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\nimport express from \"express\";\n\nconst app = express();\napp.use(express.json());\n\nconst server = new Server(\n    {\n        name: \"weather-server\",\n        version: \"0.1.0\",\n    },\n    {\n        capabilities: {\n            tools: {},\n        },\n    }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => {\n    return {\n        tools: [\n        {\n            name: \"get_weather\",\n            description: \"Get weather for location\",\n            inputSchema: {\n            type: \"object\",\n            properties: {\n                location: {\n                type: \"string\",\n                description: \"Location to get weather for\",\n                },\n            },\n            required: [\"location\"],\n            },\n        },\n        ],\n    };\n});\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n    switch (request.params.name) {\n        case \"get_weather\": {\n            const { location } = request.params.arguments as { location: string };\n            return {\n                content: [\n                    {\n                        type: \"text\",\n                        text: `It's always sunny in ${location}`,\n                    },\n                ],\n            };\n        }\n        default:\n            throw new Error(`Unknown tool: ${request.params.name}`);\n    }\n});\n\napp.post(\"/mcp\", async (req, res) => {\n    const transport = new SSEServerTransport(\"/mcp\", res);\n    await server.connect(transport);\n});\n\nconst PORT = process.env.PORT || 8000;\napp.listen(PORT, () => {\n    console.log(`Weather MCP server running on port ${PORT}`);\n});\n```\n\n## Transports\n\nMCP supports different transport mechanisms for client-server communication.\n\n### HTTP\n\nThe `http` transport (also referred to as `streamable-http`) uses HTTP requests for client-server communication. See the [MCP HTTP transport specification](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http) for more details.\n\n```typescript  theme={null}\nconst client = new MultiServerMCPClient({\n    weather: {\n        transport: \"sse\",\n        url: \"http://localhost:8000/mcp\",\n    },\n});\n```\n\n#### Passing headers\n\n#### Authentication\n\n### stdio\n\nClient launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\n\n<Note>\n  Unlike HTTP transports, `stdio` connections are inherently **stateful**\u2014the subprocess persists for the lifetime of the client connection. However, when using `MultiServerMCPClient` without explicit session management, each tool call still creates a new session. See [stateful sessions](#stateful-sessions) for managing persistent connections.\n</Note>\n\n```typescript  theme={null}\nconst client = new MultiServerMCPClient({\n    math: {\n        transport: \"stdio\",\n        command: \"node\",\n        args: [\"/path/to/math_server.js\"],\n    },\n});\n```\n\n## Core features\n\n### Tools\n\n[Tools](https://modelcontextprotocol.io/docs/concepts/tools) allow MCP servers to expose executable functions that LLMs can invoke to perform actions\u2014such as querying databases, calling APIs, or interacting with external systems. LangChain converts MCP tools into LangChain [tools](/oss/javascript/langchain/tools), making them directly usable in any LangChain agent or workflow.\n\n#### Loading tools\n\nUse `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:\n\n```typescript  theme={null}\nimport { MultiServerMCPClient } from \"@langchain/mcp-adapters\";\nimport { createAgent } from \"langchain\";\n\nconst client = new MultiServerMCPClient({...});\nconst tools = await client.getTools();  // [!code highlight]\nconst agent = createAgent({ model: \"claude-sonnet-4-5-20250929\", tools });\n```\n\n## Additional resources\n\n* [MCP documentation](https://modelcontextprotocol.io/introduction)\n\n* [MCP Transport documentation](https://modelcontextprotocol.io/docs/concepts/transports)\n\n* [`@langchain/mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters/)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/mcp.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "get_article_content",
    "tool_call_id": "d0fd1314-e992-43c8-a5f5-46c36f84ea65",
    "id": "4a672392-90f8-44ac-9e93-79744068f6f2",
    "content": "ID: ae959c68-a996-4426-8c8e-ef3fa1dd88cc\nTitle: How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\nURL: https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\nCollection: Customer Support Knowledge Base\n\nContent:\n<h1 style=\"padding-left: 0px!important;\" level=\"1\" id=\"\">Context</h1><p style=\"padding-left: 0px!important;\">You want to integrate JIRA, Slack, and Confluence Model Context Protocol (MCP) servers with LangChain to enable AI agents to interact with these services using natural language. This integration allows you to streamline workflows and increase productivity by letting AI handle technical execution instead of manually navigating interfaces.</p><h1 style=\"padding-left: 0px!important;\" level=\"1\" id=\"\">Answer</h1><p style=\"padding-left: 0px!important;\">Yes, LangChain has full support for MCP integration. Here's how to connect each service:</p><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">General MCP Integration with LangChain</h2><p style=\"padding-left: 0px!important;\">LangChain supports MCP through the <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://github.com/langchain-ai/langchain-mcp-adapters\">langchain-mcp-adapters</a> library. You can integrate existing MCP servers using the following code:</p><pre data-language=\"plaintext\"><code class=\"language-plaintext\">from langchain_mcp_adapters import MultiServerMCPClient\n\nclient = MultiServerMCPClient({\n    \"your-mcp-server\": {\n        \"transport\": \"streamable_http\",\n        \"url\": \"http://your-mcp-server-url/mcp\"\n    }\n})\n\ntools = await client.get_tools()</code></pre><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">Slack Integration</h2><p style=\"padding-left: 0px!important;\">For Slack, you have two options:</p><ol><li><p style=\"padding-left: 0px!important;\"><strong>LangSmith Agent Builder Slack App (Recommended):</strong> Use the <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://docs.langchain.com/langsmith/agent-builder-slack-app\">LangSmith Agent Builder Slack App</a> which provides:</p><ul><li><p style=\"padding-left: 0px!important;\">Send DMs and post to channels</p></li><li><p style=\"padding-left: 0px!important;\">Read threads</p></li><li><p style=\"padding-left: 0px!important;\">No custom MCP setup needed</p></li><li><p style=\"padding-left: 0px!important;\">Slack data remains private (not used for training)</p></li></ul></li><li><p style=\"padding-left: 0px!important;\"><strong>Custom MCP Integration:</strong> If you have an existing Slack MCP server, use the general MCP integration method above.</p></li></ol><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">Confluence Integration</h2><p style=\"padding-left: 0px!important;\">For Confluence, you can use LangChain's built-in <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://docs.langchain.com/oss/python/integrations/providers/confluence\">Confluence integration</a> or connect via MCP if you have a Confluence MCP server.</p><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">JIRA Integration</h2><p style=\"padding-left: 0px!important;\">For JIRA, especially self-hosted instances, you can use MCP servers. If you need to connect to a self-hosted JIRA instance on a private network, ensure your MCP server has proper network access and authentication configured.</p><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">Example MCP Configuration</h2><p style=\"padding-left: 0px!important;\">Here's an example configuration for connecting to an MCP server with authentication:</p><pre data-language=\"plaintext\"><code class=\"language-plaintext\">{\n  \"mcpServers\": {\n    \"your-service\": {\n      \"url\": \"https://your-mcp-server.com/mcp/sse\",\n      \"transport\": \"sse\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_API_TOKEN\"\n      }\n    }\n  }\n}</code></pre><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">Deployment on LangSmith</h2><p style=\"padding-left: 0px!important;\">If you're deploying agents on LangSmith, refer to the guide on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://support.langchain.com/articles/8743137454-how-do-i-deploy-an-mcp-server-on-langsmith-deployments-with-my-agent\">how to deploy an MCP server on LangSmith deployments</a>.</p><p style=\"padding-left: 0px!important;\"><strong>Sources:</strong></p><ul><li><p style=\"padding-left: 0px!important;\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://docs.langchain.com/oss/python/langchain/mcp\">LangChain MCP Documentation</a></p></li><li><p style=\"padding-left: 0px!important;\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://github.com/langchain-ai/langchain-mcp-adapters\">langchain-mcp-adapters library</a></p></li><li><p style=\"padding-left: 0px!important;\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://docs.langchain.com/langsmith/agent-builder-slack-app\">LangSmith Agent Builder Slack App</a></p></li><li><p style=\"padding-left: 0px!important;\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://docs.langchain.com/oss/python/integrations/providers/confluence\">LangChain Confluence Integration</a></p></li></ul><p style=\"padding-left: 0px!important;\"></p>"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3850-1967-7422-b7d4-b33e7b9e8f5b",
    "content": "**Option 3 works by using an MCP client to load remote or local tools and passing them directly into your agent's toolset.**\n\nThe Model Context Protocol (MCP) standardizes how agents discover and call tools. To use your custom skills via MCP, you use the `MultiServerMCPClient` from the `langchain-mcp-adapters` library to connect to one or more tool servers (e.g., Jira, Slack, or your own custom service).\n\n## 1. Connect to MCP Servers\n\nYou can connect to local servers using a `stdio` subprocess or remote servers using `sse` (Server-Sent Events) or `http`:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\n# 1. Define your MCP servers\nclient = MultiServerMCPClient({\n    \"my-local-skill\": {\n        \"transport\": \"stdio\",\n        \"command\": \"node\",\n        \"args\": [\"/path/to/your/server.js\"],\n    },\n    \"my-remote-skill\": {\n        \"transport\": \"sse\",\n        \"url\": \"https://your-api.com/mcp\",\n        \"headers\": {\n            \"Authorization\": \"Bearer YOUR_TOKEN\"\n        }\n    }\n})\n\n# 2. Retrieve the tools from all connected servers\ntools = await client.get_tools()\n```\n\n## 2. Integrate with DeepAgent\n\nOnce you have the `tools` list from the MCP client, you provide it to the `DeepAgent` constructor. This makes those external skills available to the agent as native tools:\n\n```python\nfrom deepagents import DeepAgent\n\n# Create the agent with your combined MCP tools\nagent = DeepAgent(\n    name=\"skill-expert\",\n    tools=tools,  # Your custom MCP skills are now available here\n    system_prompt=\"You are an expert equipped with Jira and custom math tools.\"\n)\n\n# Launch the interactive CLI session\nagent.start_cli()\n```\n\n## 3. How it looks in the CLI\n\nWhen you run your agent, it will \"discover\" these tools. If you ask a question related to your custom skill, the agent will:\n\n1.  Identify the relevant MCP tool from the schema.\n2.  Formulate the tool call based on your custom server's definition.\n3.  Receive the response from your MCP server and incorporate it into the conversation.\n\nThis allows you to keep your \"skills\" in a separate, dedicated server (perhaps written in a different language like TypeScript) while using the `deepagents` CLI as your primary interface.\n\n**Relevant docs:**\n\n- [Model Context Protocol (MCP) in LangChain](https://docs.langchain.com/oss/python/langchain/mcp)\n- [How to Connect MCPs with LangChain](https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain)\n- [Deep Agents CLI Overview](https://docs.langchain.com/oss/python/deepagents/cli)"
  }
]