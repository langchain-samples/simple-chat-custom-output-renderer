[
  {
    "role": "user",
    "id": "3525e8d2-d980-4075-bdd7-4a163ab90601",
    "content": "estaba leyendo mira esto import * as z from \"zod\"\nimport { ChatOpenAI } from \"@langchain/openai\"\nimport { createAgent } from \"langchain\"\n\nconst getUserName = tool(\n  (_, config) => {\n    return config.context.user_name\n  },\n  {\n    name: \"get_user_name\",\n    description: \"Get the user's name.\",\n    schema: z.object({}),\n  }\n);\n\nconst contextSchema = z.object({\n  user_name: z.string(),\n});\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserName],\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"What is my name?\" }]\n  },\n  {\n    context: { user_name: \"John Smith\" }\n  }\n);  em lo  explicas  es decir  alem del tool que le permite usar esa funcion   puede agregarle mas poder guardando cosa ?"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_46062593",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"javascript\", \"page_size\": 5, \"query\": \"contextSchema\"}"
        }
      },
      {
        "type": "function",
        "id": "call_33249699",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"javascript\", \"page_size\": 5, \"query\": \"createAgent\"}"
        }
      },
      {
        "type": "function",
        "id": "call_85810906",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"javascript\", \"page_size\": 5, \"query\": \"context\"}"
        }
      },
      {
        "type": "function",
        "id": "call_99871985",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"OSS (LangChain and LangGraph),SDKs and APIs\"}"
        }
      }
    ],
    "id": "lc_run--019b382b-1550-7543-9400-53fa1a7a33bb",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_46062593",
    "id": "acde96a8-536a-4e22-95fa-b1d5623d94a0",
    "content": "Result 1:\nTitle: Custom middleware\nLink: https://docs.langchain.comoss/javascript/langchain/middleware/custom\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\n\n## Hooks\n\nMiddleware provides two styles of hooks to intercept agent execution:\n\n<CardGroup cols={2}>\n  <Card title=\"Node-style hooks\" icon=\"share-nodes\" href=\"#node-style-hooks\">\n    Run sequentially at specific execution points.\n  </Card>\n\n  <Card title=\"Wrap-style hooks\" icon=\"container-storage\" href=\"#wrap-style-hooks\">\n    Run around each model or tool call.\n  </Card>\n</CardGroup>\n\n### Node-style hooks\n\nRun sequentially at specific execution points. Use for logging, validation, and state updates.\n\n**Available hooks:**\n\n* `beforeAgent` - Before agent starts (once per invocation)\n* `beforeModel` - Before each model call\n* `afterModel` - After each model response\n* `afterAgent` - After agent completes (once per invocation)\n\n**Example:**\n\n```typescript  theme={null}\nimport { createMiddleware, AIMessage } from \"langchain\";\n\nconst createMessageLimitMiddleware = (maxMessages: number = 50) => {\n  return createMiddleware({\n    name: \"MessageLimitMiddleware\",\n    beforeModel: (state) => {\n      if (state.messages.length === maxMessages) {\n        return {\n          messages: [new AIMessage(\"Conversation limit reached.\")],\n          jumpTo: \"end\",\n        };\n      }\n      return;\n    },\n    afterModel: (state) => {\n      const lastMessage = state.messages[state.messages.length - 1];\n      console.log(`Model returned: ${lastMessage.content}`);\n      return;\n    },\n  });\n};\n```\n\n### Wrap-style hooks\n\nIntercept execution and control when the handler is called. Use for retries, caching, and transformation.\n\nYou decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\n\n**Available hooks:**\n\n* `wrapModelCall` - Around each model call\n* `wrapToolCall` - Around each tool call\n\n**Example:**\n\n```typescript  theme={null}\nimport { createMiddleware } from \"langchain\";\n\nconst createRetryMiddleware = (maxRetries: number = 3) => {\n  return createMiddleware({\n    name: \"RetryMiddleware\",\n    wrapModelCall: (request, handler) => {\n      for (let attempt = 0; attempt < maxRetries; attempt++) {\n        try {\n          return handler(request);\n        } catch (e) {\n          if (attempt === maxRetries - 1) {\n            throw e;\n          }\n          console.log(`Retry ${attempt + 1}/${maxRetries} after error: ${e}`);\n        }\n      }\n      throw new Error(\"Unreachable\");\n    },\n  });\n};\n```\n\n## Create middleware\n\nUse the `createMiddleware` function to define custom middleware:\n\n```typescript  theme={null}\nimport { createMiddleware } from \"langchain\";\n\nconst loggingMiddleware = createMiddleware({\n  name: \"LoggingMiddleware\",\n  beforeModel: (state) => {\n    console.log(`About to call model with ${state.messages.length} messages`);\n    return;\n  },\n  afterModel: (state) => {\n    const lastMessage = state.messages[state.messages.length - 1];\n    console.log(`Model returned: ${lastMessage.content}`);\n    return;\n  },\n});\n```\n\n## Custom state schema\n\nMiddleware can extend the agent's state with custom properties. This enables middleware to:\n\n* **Track state across execution**: Maintain counters, flags, or other values that persist throughout the agent's execution lifecycle\n\n* **Share data between hooks**: Pass information from `beforeModel` to `afterModel` or between different middleware instances\n\n* **Implement cross-cutting concerns**: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic\n\n* **Make conditional decisions**: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically\n\n```typescript  theme={null}\nimport { createMiddleware, createAgent, HumanMessage } from \"langchain\";\nimport * as z from \"zod\";\n\nconst callCounterMiddleware = createMiddleware({\n  name: \"CallCounterMiddleware\",\n  stateSchema: z.object({\n    modelCallCount: z.number().default(0),\n    userId: z.string().optional(),\n  }),\n  beforeModel: (state) => {\n    if (state.modelCallCount > 10) {\n      return { jumpTo: \"end\" };\n    }\n    return;\n  },\n  afterModel: (state) => {\n    return { modelCallCount: state.modelCallCount + 1 };\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [...],\n  middleware: [callCounterMiddleware],\n});\n\nconst result = await agent.invoke({\n  messages: [new HumanMessage(\"Hello\")],\n  modelCallCount: 0,\n  userId: \"user-123\",\n});\n```\n\nState fields can be either public or private. Fields that start with an underscore (`_`) are considered private and will not be included in the agent's result. Only public fields (those without a leading underscore) are returned.\n\nThis is useful for storing internal middleware state that shouldn't be exposed to the caller, such as temporary tracking variables or internal flags:\n\n```typescript  theme={null}\nconst middleware = createMiddleware({\n  name: \"ExampleMiddleware\",\n  stateSchema: z.object({\n    // Public field - included in invoke result\n    publicCounter: z.number().default(0),\n    // Private field - excluded from invoke result\n    _internalFlag: z.boolean().default(false),\n  }),\n  afterModel: (state) => {\n    // Both fields are accessible during execution\n    if (state._internalFlag) {\n      return { publicCounter: state.publicCounter + 1 };\n    }\n    return { _internalFlag: true };\n  },\n});\n\nconst result = await agent.invoke({\n  messages: [new HumanMessage(\"Hello\")],\n  publicCounter: 0\n});\n\n// result only contains publicCounter, not _internalFlag\nconsole.log(result.publicCounter); // 1\nconsole.log(result._internalFlag); // undefined\n```\n\n## Custom context\n\nMiddleware can define a custom context schema to access per-invocation metadata. Unlike state, context is read-only and not persisted between invocations. This makes it ideal for:\n\n* **User information**: Pass user ID, roles, or preferences that don't change during execution\n* **Configuration overrides**: Provide per-invocation settings like rate limits or feature flags\n* **Tenant/workspace context**: Include organization-specific data for multi-tenant applications\n* **Request metadata**: Pass request IDs, API keys, or other metadata needed by middleware\n\nDefine a context schema using Zod and access it via `runtime.context` in middleware hooks. Required fields in the context schema will be enforced at the TypeScript level, ensuring you must provide them when calling `agent.invoke()`.\n\n```typescript  theme={null}\nimport { createAgent, createMiddleware, HumanMessage } from \"langchain\";\nimport * as z from \"zod\";\n\nconst contextSchema = z.object({\n  userId: z.string(),\n  tenantId: z.string(),\n  apiKey: z.string().optional(),\n});\n\nconst userContextMiddleware = createMiddleware({\n  name: \"UserContextMiddleware\",\n  contextSchema,\n  wrapModelCall: (request, handler) => {\n    // Access context from runtime\n    const { userId, tenantId } = request.runtime.context;\n\n    // Add user context to system message\n    const contextText = `User ID: ${userId}, Tenant: ${tenantId}`;\n    const newSystemMessage = request.systemMessage.concat(contextText);\n\n    return handler({\n      ...request,\n      systemMessage: newSystemMessage,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  middleware: [userContextMiddleware],\n  tools: [],\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  { messages: [new HumanMessage(\"Hello\")] },\n  // Required fields (userId, tenantId) must be provided\n  {\n    context: {\n      userId: \"user-123\",\n      tenantId: \"acme-corp\",\n    },\n  }\n);\n```\n\n**Required context fields**: When you define required fields in your `contextSchema` (fields without `.optional()` or `.default()`), TypeScript will enforce that these fields must be provided during `agent.invoke()` calls. This ensures type safety and prevents runtime errors from missing required context.\n\n```typescript  theme={null}\n// This will cause a TypeScript error if userId or tenantId are missing\nconst result = await agent.invoke(\n  { messages: [new HumanMessage(\"Hello\")] },\n  { context: { userId: \"user-123\" } } // Error: tenantId is required\n);\n```\n\n## Execution order\n\nWhen using multiple middleware, understand how they execute:\n\n```typescript  theme={null}\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  middleware: [middleware1, middleware2, middleware3],\n  tools: [...],\n});\n```\n\n<Accordion title=\"Execution flow\">\n  **Before hooks run in order:**\n\n  1. `middleware1.before_agent()`\n  2. `middleware2.before_agent()`\n  3. `middleware3.before_agent()`\n\n  **Agent loop starts**\n\n  4. `middleware1.before_model()`\n  5. `middleware2.before_model()`\n  6. `middleware3.before_model()`\n\n  **Wrap hooks nest like function calls:**\n\n  7. `middleware1.wrap_model_call()` \u2192 `middleware2.wrap_model_call()` \u2192 `middleware3.wrap_model_call()` \u2192 model\n\n  **After hooks run in reverse order:**\n\n  8. `middleware3.after_model()`\n  9. `middleware2.after_model()`\n  10. `middleware1.after_model()`\n\n  **Agent loop ends**\n\n  11. `middleware3.after_agent()`\n  12. `middleware2.after_agent()`\n  13. `middleware1.after_agent()`\n</Accordion>\n\n**Key rules:**\n\n* `before_*` hooks: First to last\n* `after_*` hooks: Last to first (reverse)\n* `wrap_*` hooks: Nested (first middleware wraps all others)\n\n## Agent jumps\n\nTo exit early from middleware, return a dictionary with `jump_to`:\n\n**Available jump targets:**\n\n* `'end'`: Jump to the end of the agent execution (or the first `after_agent` hook)\n* `'tools'`: Jump to the tools node\n* `'model'`: Jump to the model node (or the first `before_model` hook)\n\n```typescript  theme={null}\nimport { createAgent, createMiddleware, AIMessage } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  middleware: [\n    createMiddleware({\n      name: \"BlockedContentMiddleware\",\n      beforeModel: {\n        canJumpTo: [\"end\"],\n        hook: (state) => {\n          if (state.messages.at(-1)?.content.includes(\"BLOCKED\")) {\n            return {\n              messages: [new AIMessage(\"I cannot respond to that request.\")],\n              jumpTo: \"end\" as const,\n            };\n          }\n          return;\n        },\n      },\n    }),\n  ],\n});\n\nconst result = await agent.invoke({\n    messages: \"Hello, world! BLOCKED\"\n});\n\n/**\n * Expected output:\n * I cannot respond to that request.\n */\nconsole.log(result.messages.at(-1)?.content);\n```\n\n## Best practices\n\n1. Keep middleware focused - each should do one thing well\n2. Handle errors gracefully - don't let middleware errors crash the agent\n3. **Use appropriate hook types**:\n   * Node-style for sequential logic (logging, validation)\n   * Wrap-style for control flow (retry, fallback, caching)\n4. Clearly document any custom state properties\n5. Unit test middleware independently before integrating\n6. Consider execution order - place critical middleware first in the list\n7. Use built-in middleware when possible\n\n## Examples\n\n### Dynamic model selection\n\n```typescript  theme={null}\nimport { createMiddleware, initChatModel } from \"langchain\";\n\nconst dynamicModelMiddleware = createMiddleware({\n  name: \"DynamicModelMiddleware\",\n  wrapModelCall: (request, handler) => {\n    const modifiedRequest = { ...request };\n    if (request.messages.length > 10) {\n      modifiedRequest.model = initChatModel(\"gpt-4o\");\n    } else {\n      modifiedRequest.model = initChatModel(\"gpt-4o-mini\");\n    }\n    return handler(modifiedRequest);\n  },\n});\n```\n\n### Tool call monitoring\n\n```typescript  theme={null}\nimport { createMiddleware } from \"langchain\";\n\nconst toolMonitoringMiddleware = createMiddleware({\n  name: \"ToolMonitoringMiddleware\",\n  wrapToolCall: (request, handler) => {\n    console.log(`Executing tool: ${request.toolCall.name}`);\n    console.log(`Arguments: ${JSON.stringify(request.toolCall.args)}`);\n    try {\n      const result = handler(request);\n      console.log(\"Tool completed successfully\");\n      return result;\n    } catch (e) {\n      console.log(`Tool failed: ${e}`);\n      throw e;\n    }\n  },\n});\n```\n\n### Dynamically selecting tools\n\nSelect relevant tools at runtime to improve performance and accuracy.\n\n**Benefits:**\n\n* **Shorter prompts** - Reduce complexity by exposing only relevant tools\n* **Better accuracy** - Models choose correctly from fewer options\n* **Permission control** - Dynamically filter tools based on user access\n\n```typescript  theme={null}\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst toolSelectorMiddleware = createMiddleware({\n  name: \"ToolSelector\",\n  wrapModelCall: (request, handler) => {\n    // Select a small, relevant subset of tools based on state/context\n    const relevantTools = selectRelevantTools(request.state, request.runtime);\n    const modifiedRequest = { ...request, tools: relevantTools };\n    return handler(modifiedRequest);\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: allTools,\n  middleware: [toolSelectorMiddleware],\n});\n```\n\n### Working with system messages\n\nModify system messages in middleware using the `systemMessage` field in `ModelRequest`. It contains a [`SystemMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.SystemMessage.html) object (even if the agent was created with a string [`systemPrompt`](https://reference.langchain.com/javascript/types/langchain.index.CreateAgentParams.html#systemprompt)).\n\n**Example: Chaining middleware** - Different middleware can use different approaches:\n\n```typescript  theme={null}\nimport { createMiddleware, SystemMessage, createAgent } from \"langchain\";\n\n// Middleware 1: Uses systemMessage with simple concatenation\nconst myMiddleware = createMiddleware({\n  name: \"MyMiddleware\",\n  wrapModelCall: async (request, handler) => {\n    return handler({\n      ...request,\n      systemMessage: request.systemMessage.concat(`Additional context.`),\n    });\n  },\n});\n\n// Middleware 2: Uses systemMessage with structured content (preserves structure)\nconst myOtherMiddleware = createMiddleware({\n  name: \"MyOtherMiddleware\",\n  wrapModelCall: async (request, handler) => {\n    return handler({\n      ...request,\n      systemMessage: request.systemMessage.concat(\n        new SystemMessage({\n          content: [\n            {\n              type: \"text\",\n              text: \" More additional context. This will be cached.\",\n              cache_control: { type: \"ephemeral\", ttl: \"5m\" },\n            },\n          ],\n        })\n      ),\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"anthropic:claude-3-5-sonnet\",\n  systemPrompt: \"You are a helpful assistant.\",\n  middleware: [myMiddleware, myOtherMiddleware],\n});\n```\n\nThe resulting system message will be:\n\n```typescript  theme={null}\nnew SystemMessage({\n  content: [\n    { type: \"text\", text: \"You are a helpful assistant.\" },\n    { type: \"text\", text: \"Additional context.\" },\n    {\n        type: \"text\",\n        text: \" More additional context. This will be cached.\",\n        cache_control: { type: \"ephemeral\", ttl: \"5m\" },\n    },\n  ],\n});\n```\n\nUse [`SystemMessage.concat`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.SystemMessage.html#concat) to preserve cache control metadata or structured content blocks created by other middleware.\n\n## Additional resources\n\n* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/)\n* [Built-in middleware](/oss/javascript/langchain/middleware/built-in)\n* [Testing agents](/oss/javascript/langchain/test)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/custom.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Runtime\nLink: https://docs.langchain.comoss/javascript/langchain/runtime\nContent: ## Overview\n\nLangChain's `createAgent` runs on LangGraph's runtime under the hood.\n\nLangGraph exposes a [`Runtime`](https://reference.langchain.com/javascript/interfaces/_langchain_langgraph.index.Runtime.html) object with the following information:\n\n1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation\n2. **Store**: a [BaseStore](https://reference.langchain.com/javascript/classes/_langchain_langgraph-checkpoint.BaseStore.html) instance used for [long-term memory](/oss/javascript/langchain/long-term-memory)\n3. **Stream writer**: an object used for streaming information via the `\"custom\"` stream mode\n\n<Tip>\n  The runtime context is how you thread data through your agent. Rather than storing things in global state, you can attach values \u2014 like a database connection, user session, or configuration \u2014 to the context and access them inside tools and middleware. This keeps things stateless, testable, and reusable.\n</Tip>\n\nYou can access the runtime information within [tools](#inside-tools) and [middleware](#inside-middleware).\n\n## Access\n\nWhen creating an agent with `createAgent`, you can specify a `contextSchema` to define the structure of the `context` stored in the agent [`Runtime`](https://reference.langchain.com/javascript/interfaces/_langchain_langgraph.index.Runtime.html).\n\nWhen invoking the agent, pass the `context` argument with the relevant configuration for the run:\n\n```ts  theme={null}\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst contextSchema = z.object({ // [!code highlight]\n  userName: z.string(), // [!code highlight]\n}); // [!code highlight]\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  contextSchema, // [!code highlight]\n});\n\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"What's my name?\" }] },\n  { context: { userName: \"John Smith\" } } // [!code highlight]\n);\n```\n\n### Inside tools\n\nYou can access the runtime information inside tools to:\n\n* Access the context\n* Read or write long-term memory\n* Write to the [custom stream](/oss/javascript/langchain/streaming#custom-updates) (ex, tool progress / updates)\n\nUse the `runtime` parameter to access the [`Runtime`](https://reference.langchain.com/javascript/interfaces/_langchain_langgraph.index.Runtime.html) object inside a tool.\n\n```ts  theme={null}\nimport * as z from \"zod\";\nimport { tool } from \"langchain\";\nimport { type ToolRuntime } from \"@langchain/core/tools\"; // [!code highlight]\n\nconst contextSchema = z.object({\n  userName: z.string(),\n});\n\nconst fetchUserEmailPreferences = tool(\n  async (_, runtime: ToolRuntime<any, typeof contextSchema>) => { // [!code highlight]\n    const userName = runtime.context?.userName; // [!code highlight]\n    if (!userName) {\n      throw new Error(\"userName is required\");\n    }\n\n    let preferences = \"The user prefers you to write a brief and polite email.\";\n    if (runtime.store) { // [!code highlight]\n      const memory = await runtime.store?.get([\"users\"], userName); // [!code highlight]\n      if (memory) {\n        preferences = memory.value.preferences;\n      }\n    }\n    return preferences;\n  },\n  {\n    name: \"fetch_user_email_preferences\",\n    description: \"Fetch the user's email preferences.\",\n    schema: z.object({}),\n  }\n);\n```\n\n### Inside middleware\n\nYou can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.\n\nUse the `runtime` parameter to access the [`Runtime`](https://reference.langchain.com/javascript/interfaces/_langchain_langgraph.index.Runtime.html) object inside middleware.\n\n```ts  theme={null}\nimport * as z from \"zod\";\nimport { createAgent, createMiddleware, SystemMessage } from \"langchain\";\n\nconst contextSchema = z.object({\n  userName: z.string(),\n});\n\n// Dynamic prompt middleware\nconst dynamicPromptMiddleware = createMiddleware({\n  name: \"DynamicPrompt\",\n  contextSchema,\n  beforeModel: (state, runtime) => { // [!code highlight]\n    const userName = runtime.context?.userName; // [!code highlight]\n    if (!userName) {\n      throw new Error(\"userName is required\");\n    }\n\n    const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`;\n    return {\n      messages: [new SystemMessage(systemMsg), ...state.messages],\n    };\n  },\n});\n\n// Logging middleware\nconst loggingMiddleware = createMiddleware({\n  name: \"Logging\",\n  contextSchema,\n  beforeModel: (state, runtime) => {  // [!code highlight]\n    console.log(`Processing request for user: ${runtime.context?.userName}`);  // [!code highlight]\n    return;\n  },\n  afterModel: (state, runtime) => {  // [!code highlight]\n    console.log(`Completed request for user: ${runtime.context?.userName}`);  // [!code highlight]\n    return;\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [dynamicPromptMiddleware, loggingMiddleware],  // [!code highlight]\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"What's my name?\" }] },\n  { context: { userName: \"John Smith\" } }\n);\n\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/runtime.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Graph API overview\nLink: https://docs.langchain.comoss/javascript/langgraph/graph-api\nContent: Runtime context\nWhen creating a graph, you can specify a contextSchema for runtime context passed to nodes. This is useful for passing\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection. import * as z from \"zod\";\n\nconst ContextSchema = z.object({\n  llm: z.union([z.literal(\"openai\"), z.literal(\"anthropic\")]),\n});\n\nconst graph = new StateGraph(State, ContextSchema);\n\nRuntime context\nWhen creating a graph, you can specify a contextSchema for runtime context passed to nodes. This is useful for passing\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection. You can then pass this configuration into the graph using the context property. You can then access and use this context inside a node or conditional edge: See this guide for a full breakdown on configuration.\n\nRuntime context\nWhen creating a graph, you can specify a contextSchema for runtime context passed to nodes. This is useful for passing\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection. You can then pass this configuration into the graph using the context property. const config = { context: { llm: \"anthropic\" } };\n\nawait graph.invoke(inputs, config);\n\n---\n\nResult 4:\nTitle: Context overview\nLink: https://docs.langchain.comoss/python/concepts/context\nContent: **Context engineering** is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:\n\n1. By **mutability**:\n\n* **Static context**: Immutable data that doesn't change during execution (e.g., user metadata, database connections, tools)\n* **Dynamic context**: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\n\n2. By **lifetime**:\n\n* **Runtime context**: Data scoped to a single run or invocation\n* **Cross-conversation context**: Data that persists across multiple conversations or sessions\n\n<Tip>\n  Runtime context refers to local context: data and dependencies your code needs to run. It does **not** refer to:\n\n  * The LLM context, which is the data passed into the LLM's prompt.\n  * The \"context window\", which is the maximum number of tokens that can be passed to the LLM.\n\n  Runtime context is a form of dependency injection and can be used to optimize the LLM context. It lets to provide dependencies (like database connections, user IDs, or API clients) to your tools and nodes at runtime rather than hardcoding them. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.\n</Tip>\n\nLangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:\n\n| Context type                                                                                | Description                                            | Mutability | Lifetime           | Access method                           |\n| ------------------------------------------------------------------------------------------- | ------------------------------------------------------ | ---------- | ------------------ | --------------------------------------- |\n| [**Static runtime context**](#static-runtime-context)                                       | User metadata, tools, db connections passed at startup | Static     | Single run         | `context` argument to `invoke`/`stream` |\n| [**Dynamic runtime context (state)**](#dynamic-runtime-context-state)                       | Mutable data that evolves during a single run          | Dynamic    | Single run         | LangGraph state object                  |\n| [**Dynamic cross-conversation context (store)**](#dynamic-cross-conversation-context-store) | Persistent data shared across conversations            | Dynamic    | Cross-conversation | LangGraph store                         |\n\n## Static runtime context\n\n**Static runtime context** represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the `context` argument to `invoke`/`stream`. This data does not change during execution.\n\n```python  theme={null}\n@dataclass\nclass ContextSchema:\n    user_name: str\n\ngraph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]},\n    context={\"user_name\": \"John Smith\"}  # [!code highlight]\n)\n```\n\n<Tabs>\n  <Tab title=\"Agent prompt\">\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\n    @dataclass\n    class ContextSchema:\n        user_name: str\n\n    @dynamic_prompt  # [!code highlight]\n    def personalized_prompt(request: ModelRequest) -> str:  # [!code highlight]\n        user_name = request.runtime.context.user_name\n        return f\"You are a helpful assistant. Address the user as {user_name}.\"\n\n    agent = create_agent(\n        model=\"claude-sonnet-4-5-20250929\",\n        tools=[get_weather],\n        middleware=[personalized_prompt],\n        context_schema=ContextSchema\n    )\n\n    agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        context=ContextSchema(user_name=\"John Smith\")  # [!code highlight]\n    )\n    ```\n\n    See [Agents](/oss/python/langchain/agents) for details.\n  </Tab>\n\n  <Tab title=\"Workflow node\">\n    ```python  theme={null}\n    from langgraph.runtime import Runtime\n\n    def node(state: State, runtime: Runtime[ContextSchema]):  # [!code highlight]\n        user_name = runtime.context.user_name\n        ...\n    ```\n\n    * See [the Graph API](/oss/python/langgraph/graph-api#add-runtime-configuration) for details.\n  </Tab>\n\n  <Tab title=\"In a tool\">\n    ```python  theme={null}\n    from langchain.tools import tool, ToolRuntime\n\n    @tool\n    def get_user_email(runtime: ToolRuntime[ContextSchema]) -> str:\n        \"\"\"Retrieve user information based on user ID.\"\"\"\n        # simulate fetching user info from a database\n        email = get_user_email_from_db(runtime.context.user_name)  # [!code highlight]\n        return email\n    ```\n\n    See the [tool calling guide](/oss/python/langchain/tools#configuration) for details.\n  </Tab>\n</Tabs>\n\n<Tip>\n  The `Runtime` object can be used to access static context and other utilities like the active store and stream writer.\n  See the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) documentation for details.\n</Tip>\n\n<a id=\"state\" />\n\n## Dynamic runtime context\n\n**Dynamic runtime context** represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as [short-term memory](/oss/python/concepts/memory) during a run.\n\n<Tabs>\n  <Tab title=\"In an agent\">\n    Example shows how to incorporate state into an agent **prompt**.\n\n    State can also be accessed by the agent's **tools**, which can read or update the state as needed. See [tool calling guide](/oss/python/langchain/tools#short-term-memory) for details.\n\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n    from langchain.agents import AgentState\n\n\n    class CustomState(AgentState):  # [!code highlight]\n        user_name: str\n\n    @dynamic_prompt  # [!code highlight]\n    def personalized_prompt(request: ModelRequest) -> str:  # [!code highlight]\n        user_name = request.state.get(\"user_name\", \"User\")\n        return f\"You are a helpful assistant. User's name is {user_name}\"\n\n    agent = create_agent(\n        model=\"claude-sonnet-4-5-20250929\",\n        tools=[...],\n        state_schema=CustomState,  # [!code highlight]\n        middleware=[personalized_prompt],  # [!code highlight]\n    )\n\n    agent.invoke({\n        \"messages\": \"hi!\",\n        \"user_name\": \"John Smith\"\n    })\n    ```\n  </Tab>\n\n  <Tab title=\"In a workflow\">\n    ```python  theme={null}\n    from typing_extensions import TypedDict\n    from langchain.messages import AnyMessage\n    from langgraph.graph import StateGraph\n\n    class CustomState(TypedDict):  # [!code highlight]\n        messages: list[AnyMessage]\n        extra_field: int\n\n    def node(state: CustomState):  # [!code highlight]\n        messages = state[\"messages\"]\n        ...\n        return {  # [!code highlight]\n            \"extra_field\": state[\"extra_field\"] + 1  # [!code highlight]\n        }\n\n    builder = StateGraph(State)\n    builder.add_node(node)\n    builder.set_entry_point(\"node\")\n    graph = builder.compile()\n    ```\n  </Tab>\n</Tabs>\n\n<Tip>\n  **Turning on memory**\n  Please see the [memory guide](/oss/python/langgraph/add-memory) for more details on how to enable memory. This is a powerful feature that allows you to persist the agent's state across multiple invocations. Otherwise, the state is scoped only to a single run.\n</Tip>\n\n<a id=\"store\" />\n\n## Dynamic cross-conversation context\n\n**Dynamic cross-conversation context** represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as [long-term memory](/oss/python/concepts/memory#long-term-memory) across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).\n\n## See also\n\n* [Memory conceptual overview](/oss/python/concepts/memory)\n* [Short-term memory in LangChain](/oss/python/langchain/short-term-memory)\n* [Long-term memory in LangChain](/oss/python/langchain/long-term-memory)\n* [Memory in LangGraph](/oss/python/langgraph/add-memory)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/concepts/context.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Graph API overview\nLink: https://docs.langchain.comoss/python/langgraph/graph-api\nContent: Runtime context\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection. @dataclass\nclass ContextSchema:\n    llm_provider: str = \"openai\"\n\ngraph = StateGraph(State, context_schema=ContextSchema)\n\nRuntime context\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection. You can then pass this context into the graph using the context parameter of the invoke method. You can then access and use this context inside a node or conditional edge: from langgraph.runtime import Runtime\n\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\n    llm = get_llm(runtime.context.llm_provider)\n    # ...\n\nRuntime context\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection. You can then pass this context into the graph using the context parameter of the invoke method. You can then access and use this context inside a node or conditional edge: See this guide for a full breakdown on configuration.\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_33249699",
    "id": "d9879e3d-0a54-406f-a384-91f2aa689d25",
    "content": "Result 1:\nTitle: Agents\nLink: https://docs.langchain.comoss/javascript/langchain/agents\nContent: Agents combine language models with [tools](/oss/javascript/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n`createAgent()` provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid  theme={null}\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n  `createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/javascript/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n\n  Learn more about the [Graph API](/oss/javascript/langgraph/graph-api).\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/javascript/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: []\n});\n```\n\nModel identifier strings use the format `provider:model` (e.g. `\"openai:gpt-5\"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/javascript/integrations/providers/) to see available params and methods on your model.\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:\n\n```ts  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst basicModel = new ChatOpenAI({ model: \"gpt-4o-mini\" });\nconst advancedModel = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst dynamicModelSelection = createMiddleware({\n  name: \"DynamicModelSelection\",\n  wrapModelCall: (request, handler) => {\n    // Choose model based on conversation complexity\n    const messageCount = request.messages.length;\n\n    return handler({\n        ...request,\n        model: messageCount > 10 ? advancedModel : basicModel,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\", // Base model (used when messageCount \u2264 10)\n  tools,\n  middleware: [dynamicModelSelection],\n});\n```\n\nFor more details on middleware and advanced patterns, see the [middleware documentation](/oss/javascript/langchain/middleware).\n\n<Tip>\n  For model configuration details, see [Models](/oss/javascript/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/javascript/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/javascript/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({\n      query: z.string().describe(\"The query to search for\"),\n    }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [search, getWeather],\n});\n```\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\nTo customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:\n\n```ts wrap theme={null}\nimport { createAgent, createMiddleware, ToolMessage } from \"langchain\";\n\nconst handleToolErrors = createMiddleware({\n  name: \"HandleToolErrors\",\n  wrapToolCall: async (request, handler) => {\n    try {\n      return await handler(request);\n    } catch (error) {\n      // Return a custom error message to the model\n      return new ToolMessage({\n        content: `Tool error: Please check your input and try again. (${error})`,\n        tool_call_id: request.toolCall.id!,\n      });\n    }\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [handleToolErrors],\n});\n```\n\nThe agent will return a [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) with the custom error message when a tool fails.\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n  **Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n  ```\n  ================================ Human Message =================================\n\n  Find the most popular wireless headphones right now and check if they're in stock\n  ```\n\n  * **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n  * **Acting**: Call `search_products(\"wireless headphones\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    search_products (call_abc123)\n   Call ID: call_abc123\n    Args:\n      query: wireless headphones\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n  ```\n\n  * **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n  * **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    check_inventory (call_def456)\n   Call ID: call_def456\n    Args:\n      product_id: WH-1000XM5\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Product WH-1000XM5: 10 units in stock\n  ```\n\n  * **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n  * **Acting**: Produce final answer\n\n  ```\n  ================================== Ai Message ==================================\n\n  I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n  ```\n</Accordion>\n\n<Tip>\n  To learn more about tools, see [Tools](/oss/javascript/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:\n\n```ts wrap theme={null}\nconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n```\n\nWhen no `systemPrompt` is provided, the agent will infer its task from the messages directly.\n\nThe `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/javascript/integrations/chat/anthropic#prompt-caching):\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst literaryAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-5\",\n  systemPrompt: new SystemMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"You are an AI assistant tasked with analyzing literary works.\",\n      },\n      {\n        type: \"text\",\n        text: \"<the entire contents of 'Pride and Prejudice'>\",\n        cache_control: { type: \"ephemeral\" }\n      }\n    ]\n  })\n});\n\nconst result = await literaryAgent.invoke({\n  messages: [new HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]\n});\n```\n\nThe `cache_control` field with `{ type: \"ephemeral\" }` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/javascript/langchain/middleware).\n\n```typescript wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userRole: z.enum([\"expert\", \"beginner\"]),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [/* ... */],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\n      const userRole = runtime.context.userRole || \"user\";\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    }),\n  ],\n});\n\n// The system prompt will be set dynamically based on context\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Explain machine learning\" }] },\n  { context: { userRole: \"expert\" } }\n);\n```\n\n<Tip>\n  For more details on message types and formatting, see [Messages](/oss/javascript/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/javascript/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/javascript/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n```typescript  theme={null}\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"What's the weather in San Francisco?\" }],\n})\n```\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/javascript/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/javascript/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst ContactInfo = z.object({\n  name: z.string(),\n  email: z.string(),\n  phone: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  responseFormat: ContactInfo,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n    },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n// {\n//   name: 'John Doe',\n//   email: 'john@example.com',\n//   phone: '(555) 123-4567'\n// }\n```\n\n<Tip>\n  To learn about structured output, see [Structured output](/oss/javascript/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/javascript/langchain/short-term-memory) of the agent:\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport { createAgent } from \"langchain\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\n\nconst customAgentState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst CustomAgentState = createAgent({\n  model: \"gpt-4o\",\n  tools: [],\n  stateSchema: customAgentState,\n});\n```\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/javascript/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/javascript/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```ts  theme={null}\nconst stream = await agent.stream(\n  {\n    messages: [{\n      role: \"user\",\n      content: \"Search for AI news and summarize the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/javascript/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/javascript/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n<Tip>\n  For comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: LangChain overview\nLink: https://docs.langchain.comoss/javascript/langchain/overview\nContent: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/javascript/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/javascript/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain [agents](/oss/javascript/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\n## <Icon icon=\"wand-magic-sparkles\" /> Create an agent\n\n```ts  theme={null}\nimport * as z from \"zod\";\n// npm install @langchain/anthropic to call the model\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }) => `It's always sunny in ${city}!`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  },\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n\nSee the [Installation instructions](/oss/javascript/langchain/install) and [Quickstart guide](/oss/javascript/langchain/quickstart) to get started building your own agents and applications with LangChain.\n\n## <Icon icon=\"star\" size={20} /> Core benefits\n\n<Columns cols={2}>\n  <Card title=\"Standard model interface\" icon=\"arrows-rotate\" href=\"/oss/javascript/langchain/models\" arrow cta=\"Learn more\">\n    Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\n  </Card>\n\n  <Card title=\"Easy to use, highly flexible agent\" icon=\"wand-magic-sparkles\" href=\"/oss/javascript/langchain/agents\" arrow cta=\"Learn more\">\n    LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\n  </Card>\n\n  <Card title=\"Built on top of LangGraph\" icon=\"circle-nodes\" href=\"/oss/javascript/langgraph/overview\" arrow cta=\"Learn more\">\n    LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.\n  </Card>\n\n  <Card title=\"Debug with LangSmith\" icon=\"eye\" href=\"/langsmith/home\" arrow cta=\"Learn more\">\n    Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Build a SQL agent\nLink: https://docs.langchain.comoss/javascript/langchain/sql-agent\nContent: 5. Use createAgent\nUse createAgent to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful. Initialize the agent with a descriptive system prompt to customize its behavior: Now, create an agent with the model, tools, and prompt:\n\n5. Use createAgent\nUse createAgent to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful. Initialize the agent with a descriptive system prompt to customize its behavior: Now, create an agent with the model, tools, and prompt: import { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: [executeSql],\n  systemPrompt: getSystemPrompt,\n});\n\n\n5. Use createAgent\nUse createAgent to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful. Initialize the agent with a descriptive system prompt to customize its behavior: import { SystemMessage } from \"langchain\";\n\nconst getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.\n\nAuthoritative schema (do not invent columns/tables):\n${await getSchema()}\n\nRules:\n- Think step-by-step.\n- When you need data, call the tool \\`execute_sql\\` with ONE SELECT query.\n- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.\n- Limit to 5 rows unless user explicitly asks otherwise.\n- If the tool returns 'Error:', revise the SQL and try again.\n- Limit the number of attempts to 5.\n- If you are not successful after 5 attempts, return a note to the user.\n- Prefer explicit column lists; avoid SELECT *.\n`);\n\n---\n\nResult 4:\nTitle: LangGraph v1 migration guide\nLink: https://docs.langchain.comoss/javascript/migrate/langgraph-v1\nContent: This guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of what's new, see the [release notes](/oss/javascript/releases/langgraph-v1).\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph@latest @langchain/core@latest\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph@latest @langchain/core@latest\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph@latest @langchain/core@latest\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph@latest @langchain/core@latest\n  ```\n</CodeGroup>\n\n## Summary of changes\n\n| Area                             | What changed                                               |\n| -------------------------------- | ---------------------------------------------------------- |\n| React prebuilt                   | `createReactAgent` deprecated; use LangChain `createAgent` |\n| Interrupts                       | Typed interrupts supported via `interrupts` config         |\n| `toLangGraphEventStream` removed | Use `graph.stream` with the desired `encoding` format      |\n| `useStream`                      | Supports custom transports                                 |\n\n***\n\n## Deprecation: `createReactAgent` \u2192 `createAgent`\n\nLangGraph v1 deprecates the `createReactAgent` prebuilt. Use LangChain's `createAgent`, which runs on LangGraph and adds a flexible middleware system.\n\nSee the LangChain v1 docs for details:\n\n* [Release notes](/oss/javascript/releases/langchain-v1#createagent)\n* [Migration guide](/oss/javascript/migrate/langchain-v1#createagent)\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent } from \"langchain\";\n\n  const agent = createAgent({\n    model,\n    tools,\n    systemPrompt: \"You are a helpful assistant.\", // [!code highlight]\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n\n  const agent = createReactAgent({\n    model,\n    tools,\n    prompt: \"You are a helpful assistant.\", // [!code highlight]\n  });\n  ```\n</CodeGroup>\n\n***\n\n## Typed interrupts\n\nYou can now define interrupt types at graph construction to strictly type the values passed to and received from interrupts.\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { StateGraph, interrupt } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  const State = z.object({ foo: z.string() });\n\n  const graphConfig = {\n    interrupts: {\n      approve: interrupt<{ reason: string }, { messages: string[] }>(),\n    },\n  }\n\n  const graph = new StateGraph(State, graphConfig)\n    .addNode(\"node\", async (state, runtime) => {\n      const value = runtime.interrupt.approve({ reason: \"review\" }); // [!code highlight]\n      return { foo: value };\n    })\n    .compile();\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { StateGraph } from \"@langchain/langgraph\";\n\n  const graph = new StateGraph(State)\n    .addNode(\"node\", async (state, runtime) => {\n      const value = runtime.interrupt.approve({ reason: \"review\" }); // [!code highlight]\n      return state;\n    })\n    .compile();\n  ```\n</CodeGroup>\n\nSee [Interrupts](/oss/javascript/langgraph/interrupts) to learn more.\n\n***\n\n## Event stream encoding\n\nThe low-level `toLangGraphEventStream` helper is removed. Streaming responses are handled by the SDK; when using low-level clients, select the wire format via an `encoding` option passed to `graph.stream`.\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  const stream = await graph.stream(input, {\n    encoding: \"text/event-stream\",\n    streamMode: [\"values\", \"messages\"],\n  });\n\n  return new Response(stream, {\n    headers: { \"Content-Type\": \"text/event-stream\" }, // [!code highlight]\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  return toLangGraphEventStreamResponse({\n    stream: graph.streamEvents(input, {\n      version: \"v2\",\n      streamMode: [\"values\", \"messages\"],\n    }),\n  });\n  ```\n</CodeGroup>\n\n***\n\n## Breaking changes\n\n### Dropped Node 18 support\n\nAll LangGraph packages now require **Node.js 20 or higher**. Node.js 18 reached [end of life](https://nodejs.org/en/about/releases/) in March 2025.\n\n### New build outputs\n\nBuilds for all langgraph packages now use a bundler based approach instead of using raw typescript outputs. If you were importing files from the `dist/` directory (which is not recommended), you will need to update your imports to use the new module system.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/migrate/langgraph-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: What's new in LangChain v1\nLink: https://docs.langchain.comoss/javascript/releases/langchain-v1\nContent: **LangChain v1 is a focused, production-ready foundation for building agents.** We've streamlined the framework around three core improvements:\n\n<CardGroup cols={1}>\n  <Card title=\"createAgent\" icon=\"robot\" href=\"#create-agent\" arrow>\n    A new standard way to build agents in LangChain, replacing `createReactAgent` from LangGraph with a cleaner, more powerful API.\n  </Card>\n\n  <Card title=\"Standard content blocks\" icon=\"cube\" href=\"#standard-content-blocks\" arrow>\n    A new `contentBlocks` property that provides unified access to modern LLM features across all providers.\n  </Card>\n\n  <Card title=\"Simplified package\" icon=\"sitemap\" href=\"#simplified-package\" arrow>\n    The `langchain` package has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to `@langchain/classic`.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install langchain @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm install langchain @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add langchain @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add langchain @langchain/core\n  ```\n</CodeGroup>\n\nFor a complete list of changes, see the [migration guide](/oss/javascript/migrate/langchain-v1).\n\n## `createAgent`\n\n`createAgent` is the standard way to build agents in LangChain 1.0. It provides a simpler interface than the prebuilt `createReactAgent` exported from LangGraph while offering greater customization potential by using middleware.\n\n```ts  theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n  systemPrompt: \"You are a helpful assistant.\",\n});\n\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"What is the weather in Tokyo?\" },\n  ],\n});\n\nconsole.log(result.content);\n```\n\nUnder the hood, `createAgent` is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063\" alt=\"Core agent loop diagram\" className=\"rounded-lg\" data-og-width=\"300\" width=\"300\" data-og-height=\"268\" height=\"268\" data-path=\"oss/images/core_agent_loop.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=280&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=a4c4b766b6678ef52a6ed556b1a0b032 280w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=560&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=111869e6e99a52c0eff60a1ef7ddc49c 560w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=840&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=6c1e21de7b53bd0a29683aca09c6f86e 840w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1100&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=88bef556edba9869b759551c610c60f4 1100w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1650&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=9b0bdd138e9548eeb5056dc0ed2d4a4b 1650w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=2500&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=41eb4f053ed5e6b0ba5bad2badf6d755 2500w\" />\n</div>\n\nFor more information, see [Agents](/oss/javascript/langchain/agents).\n\n### Middleware\n\nMiddleware is the defining feature of `createAgent`. It makes `createAgent` highly customizable, raising the ceiling for what you can build.\n\nGreat agents require [context engineering](/oss/javascript/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n\n#### Prebuilt middleware\n\nLangChain provides a few [prebuilt middlewares](/oss/javascript/langchain/middleware#built-in-middleware) for common patterns, including:\n\n* `summarizationMiddleware`: Condense conversation history when it gets too long\n* `humanInTheLoopMiddleware`: Require approval for sensitive tool calls\n* `piiRedactionMiddleware`: Redact sensitive information before sending to the model\n\n```ts  theme={null}\nimport {\n  createAgent,\n  summarizationMiddleware,\n  humanInTheLoopMiddleware,\n  piiRedactionMiddleware,\n} from \"langchain\";\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [readEmail, sendEmail],\n  middleware: [\n    piiRedactionMiddleware({ patterns: [\"email\", \"phone\", \"ssn\"] }),\n    summarizationMiddleware({\n      model: \"claude-sonnet-4-5-20250929\",\n      trigger: { tokens: 500 },\n    }),\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        sendEmail: {\n          allowedDecisions: [\"approve\", \"edit\", \"reject\"],\n        },\n      },\n    }),\n  ],\n});\n```\n\n#### Custom middleware\n\nYou can also build custom middleware to fit your specific needs.\n\nBuild custom middleware by implementing any of these hooks using the `createMiddleware` function:\n\n| Hook            | When it runs             | Use cases                               |\n| --------------- | ------------------------ | --------------------------------------- |\n| `beforeAgent`   | Before calling the agent | Load memory, validate input             |\n| `beforeModel`   | Before each LLM call     | Update prompts, trim messages           |\n| `wrapModelCall` | Around each LLM call     | Intercept and modify requests/responses |\n| `wrapToolCall`  | Around each tool call    | Intercept and modify tool execution     |\n| `afterModel`    | After each LLM response  | Validate output, apply guardrails       |\n| `afterAgent`    | After agent completes    | Save results, cleanup                   |\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1\" alt=\"Middleware flow diagram\" className=\"rounded-lg\" data-og-width=\"500\" width=\"500\" data-og-height=\"560\" height=\"560\" data-path=\"oss/images/middleware_final.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w\" />\n</div>\n\nExample custom middleware:\n\n```ts  theme={null}\nimport { createMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userExpertise: z.enum([\"beginner\", \"expert\"]).default(\"beginner\"),\n})\n\nconst expertiseBasedToolMiddleware = createMiddleware({\n  wrapModelCall: async (request, handler) => {\n    const userLevel = request.runtime.context.userExpertise;\n    if (userLevel === \"expert\") {\n      const tools = [advancedSearch, dataAnalysis];\n      return handler(\n        request.replace(\"openai:gpt-5\", tools)\n      );\n    }\n    const tools = [simpleSearch, basicCalculator];\n    return handler(\n      request.replace(\"openai:gpt-5-nano\", tools)\n    );\n  },\n});\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [simpleSearch, advancedSearch, basicCalculator, dataAnalysis],\n  middleware: [expertiseBasedToolMiddleware],\n  contextSchema,\n});\n```\n\nFor more information, see [the complete middleware guide](/oss/javascript/langchain/middleware).\n\n### Built on LangGraph\n\nBecause `createAgent` is built on LangGraph, you automatically get built in support for long running and reliable agents via:\n\n<CardGroup cols={2}>\n  <Card title=\"Persistence\" icon=\"database\">\n    Conversations automatically persist across sessions with built-in checkpointing\n  </Card>\n\n  <Card title=\"Streaming\" icon=\"water\">\n    Stream tokens, tool calls, and reasoning traces in real-time\n  </Card>\n\n  <Card title=\"Human-in-the-loop\" icon=\"hand\">\n    Pause agent execution for human approval before sensitive actions\n  </Card>\n\n  <Card title=\"Time travel\" icon=\"clock-rotate-left\">\n    Rewind conversations to any point and explore alternate paths and prompts\n  </Card>\n</CardGroup>\n\nYou don't need to learn LangGraph to use these features\u2014they work out of the box.\n\n### Structured output\n\n`createAgent` has improved structured output generation:\n\n* **Main loop integration**: Structured output is now generated in the main loop instead of requiring an additional LLM call\n* **Structured output strategy**: Models can choose between calling tools or using provider-side structured output generation\n* **Cost reduction**: Eliminates extra expense from additional LLM calls\n\n```ts  theme={null}\nimport { createAgent } from \"langchain\";\nimport * as z from \"zod\";\n\nconst weatherSchema = z.object({\n  temperature: z.number(),\n  condition: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\",\n  tools: [getWeather],\n  responseFormat: weatherSchema,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"What is the weather in Tokyo?\" },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n```\n\n**Error handling**: Control error handling via the `handleErrors` parameter to `ToolStrategy`:\n\n* **Parsing errors**: Model generates data that doesn't match desired structure\n* **Multiple tool calls**: Model generates 2+ tool calls for structured output schemas\n\n***\n\n## Standard content blocks\n\n<Note>\n  1.0 releases are available for most packages. Only the following currently support new content blocks:\n\n  * `langchain`\n  * `@langchain/core`\n  * `@langchain/anthropic`\n  * `@langchain/openai`\n\n  Broader support for content blocks is planned.\n</Note>\n\n### Benefits\n\n* **Provider agnostic**: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider\n* **Type safe**: Full type hints for all content block types\n* **Backward compatible**: Standard content can be [loaded lazily](/oss/javascript/langchain/messages#standard-content-blocks), so there are no associated breaking changes\n\nFor more information, see our guide on [content blocks](/oss/javascript/langchain/messages#message-content)\n\n***\n\n## Simplified package\n\nLangChain v1 streamlines the `langchain` package namespace to focus on essential building blocks for agents. The package exposes only the most useful and relevant functionality:\n\nMost of these are re-exported from `@langchain/core` for convenience, which gives you a focused API surface for building agents.\n\n### `@langchain/classic`\n\nLegacy functionality has moved to [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic) to keep the core package lean and focused.\n\n#### What's in `@langchain/classic`\n\n* Legacy chains and chain implementations\n* Retrievers\n* The indexing API\n* [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) exports\n* Other deprecated functionality\n\nIf you use any of this functionality, install [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic):\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/classic\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm install @langchain/classic\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/classic\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/classic\n  ```\n</CodeGroup>\n\nThen update your imports:\n\n```typescript  theme={null}\nimport { ... } from \"langchain\"; // [!code --]\nimport { ... } from \"@langchain/classic\"; // [!code ++]\n\nimport { ... } from \"langchain/chains\"; // [!code --]\nimport { ... } from \"@langchain/classic/chains\"; // [!code ++]\n```\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langchainjs/issues) using the [`'v1'` label](https://github.com/langchain-ai/langchainjs/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangChain 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Middleware guide\" icon=\"puzzle-piece\" href=\"https://blog.langchain.com/agent-middleware/\">\n    Deep dive into middleware\n  </Card>\n\n  <Card title=\"Agents Documentation\" icon=\"book\" href=\"/oss/javascript/langchain/agents\" arrow>\n    Full agent documentation\n  </Card>\n\n  <Card title=\"Message Content\" icon=\"message\" href=\"/oss/javascript/langchain/messages#message-content\" arrow>\n    New content blocks API\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/javascript/migrate/langchain-v1\" arrow>\n    How to migrate to LangChain v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langchainjs\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/javascript/versioning) \u2013 Understanding version numbers\n* [Release policy](/oss/javascript/release-policy) \u2013 Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/releases/langchain-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_85810906",
    "id": "d800fa8d-1a84-4f8c-8189-192a69163972",
    "content": "Result 1:\nTitle: Context overview\nLink: https://docs.langchain.comoss/javascript/concepts/context\nContent: **Context engineering** is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:\n\n1. By **mutability**:\n\n* **Static context**: Immutable data that doesn't change during execution (e.g., user metadata, database connections, tools)\n* **Dynamic context**: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\n\n2. By **lifetime**:\n\n* **Runtime context**: Data scoped to a single run or invocation\n* **Cross-conversation context**: Data that persists across multiple conversations or sessions\n\n<Tip>\n  Runtime context refers to local context: data and dependencies your code needs to run. It does **not** refer to:\n\n  * The LLM context, which is the data passed into the LLM's prompt.\n  * The \"context window\", which is the maximum number of tokens that can be passed to the LLM.\n\n  The runtime context is how you thread data through your agent. Rather than storing things in global state, you can attach values \u2014 like a database connection, user session, or configuration \u2014 to the context and access them inside tools and middleware. This keeps things stateless, testable, and reusable. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.\n</Tip>\n\nLangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:\n\n| Context type                                                                                | Description                                   | Mutability | Lifetime           |\n| ------------------------------------------------------------------------------------------- | --------------------------------------------- | ---------- | ------------------ |\n| [**Config**](#config-static-context)                                                        | data passed at the start of a run             | Static     | Single run         |\n| [**Dynamic runtime context (state)**](#dynamic-runtime-context-state)                       | Mutable data that evolves during a single run | Dynamic    | Single run         |\n| [**Dynamic cross-conversation context (store)**](#dynamic-cross-conversation-context-store) | Persistent data shared across conversations   | Dynamic    | Cross-conversation |\n\n<a id=\"static-context\" />\n\n## Config\n\nConfig is for immutable data like user metadata or API keys. Use this when you have values that don't change mid-run.\n\nSpecify configuration using a key called **\"configurable\"** which is reserved for this purpose.\n\n```typescript  theme={null}\nawait graph.invoke(\n  { messages: [{ role: \"user\", content: \"hi!\" }] },\n  { configurable: { user_id: \"user_123\" } } // [!code highlight]\n);\n```\n\n<a id=\"state\" />\n\n## Dynamic runtime context\n\n**Dynamic runtime context** represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as [short-term memory](/oss/javascript/concepts/memory) during a run.\n\n<Tabs>\n  <Tab title=\"In an agent\">\n    Example shows how to incorporate state into an agent **prompt**.\n\n    State can also be accessed by the agent's **tools**, which can read or update the state as needed. See [tool calling guide](/oss/javascript/langchain/tools#short-term-memory) for details.\n\n    ```typescript  theme={null}\n    import { createAgent, createMiddleware } from \"langchain\";\n    import type { AgentState } from \"langchain\";\n    import * as z from \"zod\";\n\n    const CustomState = z.object({ // [!code highlight]\n      userName: z.string(),\n    });\n\n    const personalizedPrompt = createMiddleware({ // [!code highlight]\n      name: \"PersonalizedPrompt\",\n      stateSchema: CustomState,\n      wrapModelCall: (request, handler) => {\n        const userName = request.state.userName || \"User\";\n        const systemPrompt = `You are a helpful assistant. User's name is ${userName}`;\n        return handler({ ...request, systemPrompt });\n      },\n    });\n\n    const agent = createAgent({  // [!code highlight]\n      model: \"claude-sonnet-4-5-20250929\",\n      tools: [/* your tools here */],\n      middleware: [personalizedPrompt] as const, // [!code highlight]\n    });\n\n    await agent.invoke({\n      messages: [{ role: \"user\", content: \"hi!\" }],\n      userName: \"John Smith\",\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"In a workflow\">\n    ```typescript  theme={null}\n    import type { BaseMessage } from \"@langchain/core/messages\";\n    import { StateGraph, MessagesZodMeta, START } from \"@langchain/langgraph\";\n    import { registry } from \"@langchain/langgraph/zod\";\n    import * as z from \"zod\";\n\n    const CustomState = z.object({  // [!code highlight]\n      messages: z\n        .array(z.custom<BaseMessage>())\n        .register(registry, MessagesZodMeta),\n      extraField: z.number(),\n    });\n\n    const builder = new StateGraph(CustomState)\n      .addNode(\"node\", async (state) => {  // [!code highlight]\n        const messages = state.messages;\n        // ...\n        return {  // [!code highlight]\n          extraField: state.extraField + 1,\n        };\n      })\n      .addEdge(START, \"node\");\n\n    const graph = builder.compile();\n    ```\n  </Tab>\n</Tabs>\n\n<Tip>\n  **Turning on memory**\n  Please see the [memory guide](/oss/javascript/langgraph/add-memory) for more details on how to enable memory. This is a powerful feature that allows you to persist the agent's state across multiple invocations. Otherwise, the state is scoped only to a single run.\n</Tip>\n\n<a id=\"store\" />\n\n## Dynamic cross-conversation context\n\n**Dynamic cross-conversation context** represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as [long-term memory](/oss/javascript/concepts/memory#long-term-memory) across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).\n\n## See also\n\n* [Memory conceptual overview](/oss/javascript/concepts/memory)\n* [Short-term memory in LangChain](/oss/javascript/langchain/short-term-memory)\n* [Long-term memory in LangChain](/oss/javascript/langchain/long-term-memory)\n* [Memory in LangGraph](/oss/javascript/langgraph/add-memory)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/concepts/context.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Contributing to documentation\nLink: https://docs.langchain.comoss/javascript/contributing/documentation\nContent: Conceptual guides\nConceptual guide cover core concepts abstractly, providing deep understanding. Understanding-focused : Explain why things work as they do Broad perspective : Higher and wider view than other types Design-oriented : Explain decisions and trade-offs Context-rich : Use analogies and comparisons Focus on the \"why\" rather than the \"how\" Provides supplementary information not necessarily required for feature usage Can use analogies and reference alternatives Avoid blending in too much reference content Link to related tutorials and how-to guides Memory Context Graph API Functional API\n\nTutorials\nTutorials are longer form step-by-step guides that builds upon itself and takes users through a specific practical activity to build understanding. Tutorials are typically found on the Learn tab. We generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please open a new issue. Practical : Focus on practical activities to build understanding. Step-by-step : Break down the activity into smaller steps. Hands-on : Provide sequential, working code snippets. Supplementary : Provide additional context and information not necessarily required for feature usage. Code snippets should be sequential and working if the user follows the steps in order. Provide some context for the activity, but link to related conceptual guides and references for more detailed information. Semantic search RAG agent\n\nExamples\nMemory Context Graph API Functional API\n\n---\n\nResult 3:\nTitle: Subagents\nLink: https://docs.langchain.comoss/javascript/langchain/multi-agent/subagents\nContent: In the **subagents** architecture, a central main [agent](/oss/javascript/langchain/agents) (often referred to as a **supervisor**) coordinates subagents by calling them as [tools](/oss/javascript/langchain/tools). The main agent decides which subagent to invoke, what input to provide, and how to combine results. Subagents are stateless\u2014they don't remember past interactions, with all conversation memory maintained by the main agent. This provides [context](/oss/javascript/langchain/context-engineering) isolation: each subagent invocation works in a clean context window, preventing context bloat in the main conversation.\n\n```mermaid  theme={null}\ngraph LR\n    A[User] --> B[Main Agent]\n    B --> C[Subagent A]\n    B --> D[Subagent B]\n    B --> E[Subagent C]\n    C --> B\n    D --> B\n    E --> B\n    B --> F[User response]\n```\n\n## Key characteristics\n\n* Centralized control: All routing passes through the main agent\n* No direct user interaction: Subagents return results to the main agent, not the user\n* Subagents via tools: Subagents are invoked via tools\n* Parallel execution: The main agent can invoke multiple subagents in a single turn\n\n## When to use\n\nUse the subagents pattern when you have multiple distinct domains (e.g., calendar, email, CRM, database), subagents don't need to converse directly with users, or you want centralized workflow control. For simpler cases with just a few [tools](/oss/javascript/langchain/tools), use a [single agent](/oss/javascript/langchain/agents).\n\n## Basic implementation\n\nThe core mechanism wraps a subagent as a tool that the main agent can call:\n\n```typescript  theme={null}\nimport { createAgent, tool } from \"langchain\";\nimport { z } from \"zod\";\n\n// Create a subagent\nconst subagent = createAgent({ model: \"anthropic:claude-sonnet-4-20250514\", tools: [...] });\n\n// Wrap it as a tool\nconst callResearchAgent = tool(\n  async ({ query }) => {\n    const result = await subagent.invoke({\n      messages: [{ role: \"user\", content: query }]\n    });\n    return result.messages.at(-1)?.content;\n  },\n  {\n    name: \"research\",\n    description: \"Research a topic and return findings\",\n    schema: z.object({ query: z.string() })\n  }\n);\n\n// Main agent with subagent as a tool\nconst mainAgent = createAgent({ model: \"anthropic:claude-sonnet-4-20250514\", tools: [callResearchAgent] });\n```\n\n<Card title=\"Tutorial: Build a personal assistant with subagents\" icon=\"sitemap\" href=\"/oss/javascript/langchain/multi-agent/subagents-personal-assistant\" arrow cta=\"Learn more\">\n  Learn how to build a personal assistant using the subagents pattern, where a central main agent (supervisor) coordinates specialized worker agents.\n</Card>\n\n## Design decisions\n\nWhen implementing the subagents pattern, you'll make several key design choices. This table summarizes the options\u2014each is covered in detail in the sections below.\n\n| Decision                                  | Options                                      |\n| ----------------------------------------- | -------------------------------------------- |\n| [**Sync vs. async**](#sync-vs-async)      | Sync (blocking) vs. async (background)       |\n| [**Tool patterns**](#tool-patterns)       | Tool per agent vs. single dispatch tool      |\n| [**Subagent inputs**](#subagent-inputs)   | Query only vs. full context                  |\n| [**Subagent outputs**](#subagent-outputs) | Subagent result vs full conversation history |\n\n## Sync vs. async\n\nSubagent execution can be **synchronous** (blocking) or **asynchronous** (background). Your choice depends on whether the main agent needs the result to continue.\n\n| Mode      | Main agent behavior                         | Best for                               | Tradeoff                            |\n| --------- | ------------------------------------------- | -------------------------------------- | ----------------------------------- |\n| **Sync**  | Waits for subagent to complete              | Main agent needs result to continue    | Simple, but blocks the conversation |\n| **Async** | Continues while subagent runs in background | Independent tasks, user shouldn't wait | Responsive, but more complex        |\n\n<Tip>\n  Not to be confused with Python's `async`/`await`. Here, \"async\" means the main agent kicks off a background job (typically in a separate process or service) and continues without blocking.\n</Tip>\n\n### Synchronous (default)\n\nBy default, subagent calls are **synchronous**\u2014the main agent waits for each subagent to complete before continuing. Use sync when the main agent's next action depends on the subagent's result.\n\n```mermaid  theme={null}\nsequenceDiagram\n    participant User\n    participant Main Agent\n    participant Research Subagent\n\n    User->>Main Agent: \"What's the weather in Tokyo?\"\n    Main Agent->>Research Subagent: research(\"Tokyo weather\")\n    Note over Main Agent: Waiting for result...\n    Research Subagent-->>Main Agent: \"Currently 72\u00b0F, sunny\"\n    Main Agent-->>User: \"It's 72\u00b0F and sunny in Tokyo\"\n```\n\n**When to use sync:**\n\n* Main agent needs the subagent's result to formulate its response\n* Tasks have order dependencies (e.g., fetch data \u2192 analyze \u2192 respond)\n* Subagent failures should block the main agent's response\n\n**Tradeoffs:**\n\n* Simple implementation\u2014just call and wait\n* User sees no response until all subagents complete\n* Long-running tasks freeze the conversation\n\n### Asynchronous\n\nUse **asynchronous execution** when the subagent's work is independent\u2014the main agent doesn't need the result to continue conversing with the user. The main agent kicks off a background job and remains responsive.\n\n```mermaid  theme={null}\nsequenceDiagram\n    participant User\n    participant Main Agent\n    participant Job System\n    participant Contract Reviewer\n\n    User->>Main Agent: \"Review this M&A contract\"\n    Main Agent->>Job System: run_agent(\"legal_reviewer\", task)\n    Job System->>Contract Reviewer: Start agent\n    Job System-->>Main Agent: job_id: \"job_123\"\n    Main Agent-->>User: \"Started review (job_123)\"\n\n    Note over Contract Reviewer: Reviewing 150+ pages...\n\n    User->>Main Agent: \"What's the status?\"\n    Main Agent->>Job System: check_status(job_id)\n    Job System-->>Main Agent: \"running\"\n    Main Agent-->>User: \"Still reviewing contract...\"\n\n    Note over Contract Reviewer: Review completes\n\n    User->>Main Agent: \"Is it done yet?\"\n    Main Agent->>Job System: check_status(job_id)\n    Job System-->>Main Agent: \"completed\"\n    Main Agent->>Job System: get_result(job_id)\n    Job System-->>Main Agent: Contract analysis\n    Main Agent-->>User: \"Review complete: [findings]\"\n```\n\n**When to use async:**\n\n* Subagent work is independent of the main conversation flow\n* Users should be able to continue chatting while work happens\n* You want to run multiple independent tasks in parallel\n\n**Three-tool pattern:**\n\n1. **Start job**: Kicks off the background task, returns a job ID\n2. **Check status**: Returns current state (pending, running, completed, failed)\n3. **Get result**: Retrieves the completed result\n\n**Handling job completion:** When a job finishes, your application needs to notify the user. One approach: surface a notification that, when clicked, sends a `HumanMessage` like \"Check job\\_123 and summarize the results.\"\n\n## Tool patterns\n\nThere are two main ways to expose subagents as tools:\n\n| Pattern                                           | Best for                                                      | Trade-off                                         |\n| ------------------------------------------------- | ------------------------------------------------------------- | ------------------------------------------------- |\n| [**Tool per agent**](#tool-per-agent)             | Fine-grained control over each subagent's input/output        | More setup, but more customization                |\n| [**Single dispatch tool**](#single-dispatch-tool) | Many agents, distributed teams, convention over configuration | Simpler composition, less per-agent customization |\n\n### Tool per agent\n\n```mermaid  theme={null}\ngraph LR\n    A[User] --> B[Main Agent]\n    B --> C[Subagent A]\n    B --> D[Subagent B]\n    B --> E[Subagent C]\n    C --> B\n    D --> B\n    E --> B\n    B --> F[User response]\n```\n\nThe key idea is wrapping subagents as tools that the main agent can call:\n\n```typescript  theme={null}\nimport { createAgent, tool } from \"langchain\";\nimport * as z from \"zod\";\n\n// Create a sub-agent\nconst subagent = createAgent({...});  // [!code highlight]\n\n// Wrap it as a tool  // [!code highlight]\nconst callSubagent = tool(  // [!code highlight]\n  async ({ query }) => {  // [!code highlight]\n    const result = await subagent.invoke({\n      messages: [{ role: \"user\", content: query }]\n    });\n    return result.messages.at(-1)?.text;\n  },\n  {\n    name: \"subagent_name\",\n    description: \"subagent_description\",\n    schema: z.object({\n      query: z.string().describe(\"The query to send to subagent\")\n    })\n  }\n);\n\n// Main agent with subagent as a tool  // [!code highlight]\nconst mainAgent = createAgent({ model, tools: [callSubagent] });  // [!code highlight]\n```\n\nThe main agent invokes the subagent tool when it decides the task matches the subagent's description, receives the result, and continues orchestration. See [Context engineering](#context-engineering) for fine-grained control.\n\n### Single dispatch tool\n\nAn alternative approach uses a single parameterized tool to invoke ephemeral sub-agents for independent tasks. Unlike the [tool per agent](#tool-per-agent) approach where each sub-agent is wrapped as a separate tool, this uses a convention-based approach with a single `task` tool: the task description is passed as a human message to the sub-agent, and the sub-agent's final message is returned as the tool result.\n\nUse this approach when you want to distribute agent development across multiple teams, need to isolate complex tasks into separate context windows, need a scalable way to add new agents without modifying the coordinator, or prefer convention over customization. This approach trades flexibility in context engineering for simplicity in agent composition and strong context isolation.\n\n```mermaid  theme={null}\ngraph LR\n    A[User] --> B[Main Agent]\n    B --> C{task<br/>agent_name, description}\n    C -->|research| D[Research Agent]\n    C -->|writer| E[Writer Agent]\n    C -->|reviewer| F[Reviewer Agent]\n    D --> C\n    E --> C\n    F --> C\n    C --> B\n    B --> G[User response]\n```\n\n**Key characteristics:**\n\n* Single task tool: One parameterized tool that can invoke any registered sub-agent by name\n* Convention-based invocation: Agent selected by name, task passed as human message, final message returned as tool result\n* Team distribution: Different teams can develop and deploy agents independently\n* Agent discovery: Sub-agents can be discovered via system prompt (listing available agents) or through [progressive disclosure](/oss/javascript/langchain/multi-agent/skills-sql-assistant) (loading agent information on-demand via tools)\n\n<Tip>\n  An interesting aspect of this approach is that sub-agents may have the exact same capabilities as the main agent. In such cases, invoking a sub-agent is **really about context isolation** as the primary reason\u2014allowing complex, multi-step tasks to run in isolated context windows without bloating the main agent's conversation history. The sub-agent completes its work autonomously and returns only a concise summary, keeping the main thread focused and efficient.\n</Tip>\n\n<Accordion title=\"Agent registry with task dispatcher\">\n  ```typescript  theme={null}\n  import { tool, createAgent } from \"langchain\";\n  import * as z from \"zod\";\n\n  // Sub-agents developed by different teams\n  const researchAgent = createAgent({\n    model: \"gpt-4o\",\n    prompt: \"You are a research specialist...\",\n  });\n\n  const writerAgent = createAgent({\n    model: \"gpt-4o\",\n    prompt: \"You are a writing specialist...\",\n  });\n\n  // Registry of available sub-agents\n  const SUBAGENTS = {\n    research: researchAgent,\n    writer: writerAgent,\n  };\n\n  const task = tool(\n    async ({ agentName, description }) => {\n      const agent = SUBAGENTS[agentName];\n      const result = await agent.invoke({\n        messages: [\n          { role: \"user\", content: description }\n        ],\n      });\n      return result.messages.at(-1)?.content;\n    },\n    {\n      name: \"task\",\n      description: `Launch an ephemeral subagent.\n\n  Available agents:\n  - research: Research and fact-finding\n  - writer: Content creation and editing`,\n      schema: z.object({\n        agentName: z\n          .string()\n          .describe(\"Name of agent to invoke\"),\n        description: z\n          .string()\n          .describe(\"Task description\"),\n      }),\n    }\n  );\n\n  // Main coordinator agent\n  const mainAgent = createAgent({\n    model: \"gpt-4o\",\n    tools: [task],\n    prompt: (\n      \"You coordinate specialized sub-agents. \" +\n      \"Available: research (fact-finding), \" +\n      \"writer (content creation). \" +\n      \"Use the task tool to delegate work.\"\n    ),\n  });\n  ```\n</Accordion>\n\n## Context engineering\n\nControl how context flows between the main agent and its subagents:\n\n| Category                                  | Purpose                                                  | Impacts                      |\n| ----------------------------------------- | -------------------------------------------------------- | ---------------------------- |\n| [**Subagent specs**](#subagent-specs)     | Ensure subagents are invoked when they should be         | Main agent routing decisions |\n| [**Subagent inputs**](#subagent-inputs)   | Ensure subagents can execute well with optimized context | Subagent performance         |\n| [**Subagent outputs**](#subagent-outputs) | Ensure the supervisor can act on subagent results        | Main agent performance       |\n\nSee also our comprehensive guide on [context engineering](/oss/javascript/langchain/context-engineering) for agents.\n\n### Subagent specs\n\nThe **names** and **descriptions** associated with subagents are the primary way the main agent knows which subagents to invoke.\nThese are prompting levers\u2014choose them carefully.\n\n* **Name**: How the main agent refers to the sub-agent. Keep it clear and action-oriented (e.g., `research_agent`, `code_reviewer`).\n* **Description**: What the main agent knows about the sub-agent's capabilities. Be specific about what tasks it handles and when to use it.\n\nFor the [single dispatch tool](#single-dispatch-tool) design, the main agent needs to call the `task` tool with the name of the subagent to invoke. The available tools can be provided to the main agent via one of the following methods:\n\n* **System prompt enumeration**: List available agents in the system prompt.\n* **Enum constraint on dispatch tool**: For small agent lists, add an enum to the `agent_name` field.\n* **Tool-based discovery**: For large or dynamic agent registries, provide a separate tool (e.g., `list_agents` or `search_agents`) that returns available agents.\n\n### Subagent inputs\n\nCustomize what context the subagent receives to execute its task. Add input that isn't practical to capture in a static prompt\u2014full message history, prior results, or task metadata\u2014by pulling from the agent's state.\n\n```typescript Subagent inputs example expandable theme={null}\nimport { createAgent, tool, AgentState, ToolMessage } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Example of passing the full conversation history to the sub agent via the state.\nconst callSubagent1 = tool(\n  async ({query}) => {\n    const state = getCurrentTaskInput<AgentState>();\n    // Apply any logic needed to transform the messages into a suitable input\n    const subAgentInput = someLogic(query, state.messages);\n    const result = await subagent1.invoke({\n      messages: subAgentInput,\n      // You could also pass other state keys here as needed.\n      // Make sure to define these in both the main and subagent's\n      // state schemas.\n      exampleStateKey: state.exampleStateKey\n    });\n    return result.messages.at(-1)?.content;\n  },\n  {\n    name: \"subagent1_name\",\n    description: \"subagent1_description\",\n  }\n);\n```\n\n### Subagent outputs\n\nCustomize what the main agent receives back so it can make good decisions. Two strategies:\n\n1. **Prompt the sub-agent**: Specify exactly what should be returned. A common failure mode is that the sub-agent performs tool calls or reasoning but doesn't include results in its final message\u2014remind it that the supervisor only sees the final output.\n2. **Format in code**: Adjust or enrich the response before returning it. For example, pass specific state keys back in addition to the final text using a [`Command`](/oss/javascript/langgraph/graph-api#command).\n\n```typescript Subagent outputs example expandable theme={null}\nimport { tool, ToolMessage } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst callSubagent1 = tool(\n  async ({ query }, config) => {\n    const result = await subagent1.invoke({\n      messages: [{ role: \"user\", content: query }]\n    });\n\n    // Return a Command to update multiple state keys\n    return new Command({\n      update: {\n        // Pass back additional state from the subagent\n        exampleStateKey: result.exampleStateKey,\n        messages: [\n          new ToolMessage({\n            content: result.messages.at(-1)?.text,\n            tool_call_id: config.toolCall?.id!\n          })\n        ]\n      }\n    });\n  },\n  {\n    name: \"subagent1_name\",\n    description: \"subagent1_description\",\n    schema: z.object({\n      query: z.string().describe(\"The query to send to subagent1\")\n    })\n  }\n);\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent/subagents.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Context overview\nLink: https://docs.langchain.comoss/python/concepts/context\nContent: **Context engineering** is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:\n\n1. By **mutability**:\n\n* **Static context**: Immutable data that doesn't change during execution (e.g., user metadata, database connections, tools)\n* **Dynamic context**: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\n\n2. By **lifetime**:\n\n* **Runtime context**: Data scoped to a single run or invocation\n* **Cross-conversation context**: Data that persists across multiple conversations or sessions\n\n<Tip>\n  Runtime context refers to local context: data and dependencies your code needs to run. It does **not** refer to:\n\n  * The LLM context, which is the data passed into the LLM's prompt.\n  * The \"context window\", which is the maximum number of tokens that can be passed to the LLM.\n\n  Runtime context is a form of dependency injection and can be used to optimize the LLM context. It lets to provide dependencies (like database connections, user IDs, or API clients) to your tools and nodes at runtime rather than hardcoding them. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.\n</Tip>\n\nLangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:\n\n| Context type                                                                                | Description                                            | Mutability | Lifetime           | Access method                           |\n| ------------------------------------------------------------------------------------------- | ------------------------------------------------------ | ---------- | ------------------ | --------------------------------------- |\n| [**Static runtime context**](#static-runtime-context)                                       | User metadata, tools, db connections passed at startup | Static     | Single run         | `context` argument to `invoke`/`stream` |\n| [**Dynamic runtime context (state)**](#dynamic-runtime-context-state)                       | Mutable data that evolves during a single run          | Dynamic    | Single run         | LangGraph state object                  |\n| [**Dynamic cross-conversation context (store)**](#dynamic-cross-conversation-context-store) | Persistent data shared across conversations            | Dynamic    | Cross-conversation | LangGraph store                         |\n\n## Static runtime context\n\n**Static runtime context** represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the `context` argument to `invoke`/`stream`. This data does not change during execution.\n\n```python  theme={null}\n@dataclass\nclass ContextSchema:\n    user_name: str\n\ngraph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]},\n    context={\"user_name\": \"John Smith\"}  # [!code highlight]\n)\n```\n\n<Tabs>\n  <Tab title=\"Agent prompt\">\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\n    @dataclass\n    class ContextSchema:\n        user_name: str\n\n    @dynamic_prompt  # [!code highlight]\n    def personalized_prompt(request: ModelRequest) -> str:  # [!code highlight]\n        user_name = request.runtime.context.user_name\n        return f\"You are a helpful assistant. Address the user as {user_name}.\"\n\n    agent = create_agent(\n        model=\"claude-sonnet-4-5-20250929\",\n        tools=[get_weather],\n        middleware=[personalized_prompt],\n        context_schema=ContextSchema\n    )\n\n    agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n        context=ContextSchema(user_name=\"John Smith\")  # [!code highlight]\n    )\n    ```\n\n    See [Agents](/oss/python/langchain/agents) for details.\n  </Tab>\n\n  <Tab title=\"Workflow node\">\n    ```python  theme={null}\n    from langgraph.runtime import Runtime\n\n    def node(state: State, runtime: Runtime[ContextSchema]):  # [!code highlight]\n        user_name = runtime.context.user_name\n        ...\n    ```\n\n    * See [the Graph API](/oss/python/langgraph/graph-api#add-runtime-configuration) for details.\n  </Tab>\n\n  <Tab title=\"In a tool\">\n    ```python  theme={null}\n    from langchain.tools import tool, ToolRuntime\n\n    @tool\n    def get_user_email(runtime: ToolRuntime[ContextSchema]) -> str:\n        \"\"\"Retrieve user information based on user ID.\"\"\"\n        # simulate fetching user info from a database\n        email = get_user_email_from_db(runtime.context.user_name)  # [!code highlight]\n        return email\n    ```\n\n    See the [tool calling guide](/oss/python/langchain/tools#configuration) for details.\n  </Tab>\n</Tabs>\n\n<Tip>\n  The `Runtime` object can be used to access static context and other utilities like the active store and stream writer.\n  See the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) documentation for details.\n</Tip>\n\n<a id=\"state\" />\n\n## Dynamic runtime context\n\n**Dynamic runtime context** represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as [short-term memory](/oss/python/concepts/memory) during a run.\n\n<Tabs>\n  <Tab title=\"In an agent\">\n    Example shows how to incorporate state into an agent **prompt**.\n\n    State can also be accessed by the agent's **tools**, which can read or update the state as needed. See [tool calling guide](/oss/python/langchain/tools#short-term-memory) for details.\n\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n    from langchain.agents import AgentState\n\n\n    class CustomState(AgentState):  # [!code highlight]\n        user_name: str\n\n    @dynamic_prompt  # [!code highlight]\n    def personalized_prompt(request: ModelRequest) -> str:  # [!code highlight]\n        user_name = request.state.get(\"user_name\", \"User\")\n        return f\"You are a helpful assistant. User's name is {user_name}\"\n\n    agent = create_agent(\n        model=\"claude-sonnet-4-5-20250929\",\n        tools=[...],\n        state_schema=CustomState,  # [!code highlight]\n        middleware=[personalized_prompt],  # [!code highlight]\n    )\n\n    agent.invoke({\n        \"messages\": \"hi!\",\n        \"user_name\": \"John Smith\"\n    })\n    ```\n  </Tab>\n\n  <Tab title=\"In a workflow\">\n    ```python  theme={null}\n    from typing_extensions import TypedDict\n    from langchain.messages import AnyMessage\n    from langgraph.graph import StateGraph\n\n    class CustomState(TypedDict):  # [!code highlight]\n        messages: list[AnyMessage]\n        extra_field: int\n\n    def node(state: CustomState):  # [!code highlight]\n        messages = state[\"messages\"]\n        ...\n        return {  # [!code highlight]\n            \"extra_field\": state[\"extra_field\"] + 1  # [!code highlight]\n        }\n\n    builder = StateGraph(State)\n    builder.add_node(node)\n    builder.set_entry_point(\"node\")\n    graph = builder.compile()\n    ```\n  </Tab>\n</Tabs>\n\n<Tip>\n  **Turning on memory**\n  Please see the [memory guide](/oss/python/langgraph/add-memory) for more details on how to enable memory. This is a powerful feature that allows you to persist the agent's state across multiple invocations. Otherwise, the state is scoped only to a single run.\n</Tip>\n\n<a id=\"store\" />\n\n## Dynamic cross-conversation context\n\n**Dynamic cross-conversation context** represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as [long-term memory](/oss/python/concepts/memory#long-term-memory) across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).\n\n## See also\n\n* [Memory conceptual overview](/oss/python/concepts/memory)\n* [Short-term memory in LangChain](/oss/python/langchain/short-term-memory)\n* [Long-term memory in LangChain](/oss/python/langchain/long-term-memory)\n* [Memory in LangGraph](/oss/python/langgraph/add-memory)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/concepts/context.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Contributing to documentation\nLink: https://docs.langchain.comoss/python/contributing/documentation\nContent: Conceptual guides\nConceptual guide cover core concepts abstractly, providing deep understanding. Understanding-focused : Explain why things work as they do Broad perspective : Higher and wider view than other types Design-oriented : Explain decisions and trade-offs Context-rich : Use analogies and comparisons Focus on the \"why\" rather than the \"how\" Provides supplementary information not necessarily required for feature usage Can use analogies and reference alternatives Avoid blending in too much reference content Link to related tutorials and how-to guides Memory Context Graph API Functional API\n\nTutorials\nTutorials are longer form step-by-step guides that builds upon itself and takes users through a specific practical activity to build understanding. Tutorials are typically found on the Learn tab. We generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please open a new issue. Practical : Focus on practical activities to build understanding. Step-by-step : Break down the activity into smaller steps. Hands-on : Provide sequential, working code snippets. Supplementary : Provide additional context and information not necessarily required for feature usage. Code snippets should be sequential and working if the user follows the steps in order. Provide some context for the activity, but link to related conceptual guides and references for more detailed information. Semantic search RAG agent\n\nExamples\nMemory Context Graph API Functional API\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_99871985",
    "id": "7a6cffa1-ed48-44f2-9f3e-09f8f371ba69",
    "content": "{\n  \"collections\": \"OSS (LangChain and LangGraph),SDKs and APIs\",\n  \"total\": 14,\n  \"articles\": [\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b382b-21e6-7db0-87d3-cad524c759df",
    "content": "**S\u00ed, adem\u00e1s del tool puedes agregar m\u00e1s datos al `context` para que est\u00e9n disponibles en tools, middleware y otras partes del agente durante toda la ejecuci\u00f3n.**\n\nEl `contextSchema` define datos **est\u00e1ticos** (inmutables durante la invocaci\u00f3n) que pasas en el segundo argumento de `agent.invoke()`. Estos se inyectan autom\u00e1ticamente en el `runtime.context` y son accesibles desde:\n\n- **Tools**: `(_, runtime) => runtime.context.user_name`\n- **Middleware**: `beforeModel: (state, runtime) => runtime.context.user_name`\n\n```typescript\nimport * as z from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, tool } from \"langchain\";\n\n// Schema con M\u00daLTIPLES campos\nconst contextSchema = z.object({\n  user_name: z.string(),\n  user_id: z.string(),\n  preferences: z.object({\n    language: z.enum([\"es\", \"en\"]),\n    max_results: z.number().default(5)\n  }).optional()\n});\n\n// Tool que usa M\u00c1S context\nconst getUserPreferences = tool(\n  (_, runtime) => {\n    const { user_name, user_id, preferences } = runtime.context;\n    return `Usuario: ${user_name} (ID: ${user_id}). Prefiere: ${preferences?.language || 'es'}`;\n  },\n  {\n    name: \"get_user_preferences\",\n    description: \"Obtiene preferencias del usuario actual.\",\n    schema: z.object({})\n  }\n);\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserName, getUserPreferences],  // M\u00faltiples tools\n  contextSchema,  // Define la estructura\n});\n\n// Pasa M\u00c1S datos en invoke\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Dime mis preferencias\" }] },\n  {\n    context: {\n      user_name: \"John Smith\",\n      user_id: \"user-123\",\n      preferences: { language: \"es\", max_results: 10 }\n    }\n  }\n);\n```\n\n## Datos Din\u00e1micos (State)\n\nPara datos que **cambian** durante la ejecuci\u00f3n, usa `stateSchema`:\n\n```typescript\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst dynamicStateSchema = z.object({\n  messages: MessagesZodState.shape.messages,  // Historial autom\u00e1tico\n  user_points: z.number().default(0),         // Cambia durante ejecuci\u00f3n\n  session_id: z.string()\n});\n\n// Middleware que actualiza state din\u00e1mico\nconst pointsMiddleware = createMiddleware({\n  name: \"PointsTracker\",\n  stateSchema: dynamicStateSchema,\n  afterModel: (state) => {\n    // Incrementa puntos cada respuesta\n    return { user_points: state.user_points + 10 };\n  }\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [getUserName],\n  contextSchema,      // Datos est\u00e1ticos\n  stateSchema: dynamicStateSchema,  // Datos din\u00e1micos\n  middleware: [pointsMiddleware]\n});\n```\n\n**Tipos de datos disponibles:**\n\n| Tipo | Cu\u00e1ndo usar | Ejemplo | Acceso |\n|------|-------------|---------|--------|\n| **Context** | Datos fijos por invocaci\u00f3n | user_id, DB connections | `runtime.context` |\n| **State** | Cambia durante ejecuci\u00f3n | points, counters | `state.points` |\n| **Store** | Persiste entre sesiones | user profiles | `runtime.store.get()` |\n\n**Relevant docs:**\n- [Runtime Context](https://docs.langchain.com/oss/javascript/langchain/runtime#custom-context)\n- [Custom Middleware State](https://docs.langchain.com/oss/javascript/langchain/middleware/custom#custom-state-schema)\n- [Agents Context](https://docs.langchain.com/oss/javascript/langchain/agents#system-prompt)"
  },
  {
    "role": "user",
    "id": "45f48640-4dc9-4579-a002-8a50747cafee",
    "content": "denuevo primerp explcia   sencillamnete  solo genero el toll y le aso unsa funcion como se veria    dame teoria no mucho code"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b382d-1268-78e0-ba48-fd90c180f6ea",
    "content": "**El `context` es como una \"mochila\" de datos que le das al agente al inicio y que TODOS sus tools pueden usar.**\n\n## Teor\u00eda simple:\n\n1. **Defines la mochila** (`contextSchema`): Dices qu\u00e9 datos va a llevar (nombre, ID, preferencias, etc.)\n\n2. **Llenas la mochila** (en `invoke`): Pones los datos reales del usuario\n\n3. **Tools leen la mochila**: Cada tool recibe autom\u00e1ticamente `runtime.context` con todos los datos\n\n## Flujo:\n\n```\nUsuario pide \"mi nombre\" \n    \u2193\nAgent ejecuta tool \"get_name\"\n    \u2193\nTool lee: runtime.context.user_name = \"Juan\"\n    \u2193\nTool responde: \"Tu nombre es Juan\"\n```\n\n## \u00bfQu\u00e9 puedes guardar?\n\n```\ncontext = {\n  user_name: \"Juan\",\n  user_id: \"123\",\n  rol: \"admin\", \n  limite: 10,\n  db_connection: \"...\"\n}\n```\n\n**CUALQUIER dato** que necesiten tus tools est\u00e1 disponible autom\u00e1ticamente.\n\n## Ventaja clave:\n- **Un solo lugar** para datos del usuario\n- **Todos los tools** lo ven sin pas\u00e1rselo manualmente\n- **Type-safe** con Zod (no hay errores de tipado)\n\n**En resumen**: Tool + Context = Agent s\u00faper poderoso que conoce TODO sobre el usuario desde cualquier tool."
  },
  {
    "role": "user",
    "id": "ad5436f9-95f6-4fa4-87f8-d44c9c9fd1c7",
    "content": "de neuvo   desde el incio ensenam e  a  usar toll tipo que paramentro puede recibir sa instacion eso primero"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b382d-ffb5-79b2-a740-79d5c75a0591",
    "content": "# Tools desde CERO - Par\u00e1metros que RECIBE\n\n## 1. **Tool SIN par\u00e1metros** (lo m\u00e1s simple)\n\n```typescript\nconst miTool = tool(\n  () => \"Hola mundo\",  // \u2190 FUNCI\u00d3N SIN par\u00e1metros\n  {\n    name: \"saludar\",\n    description: \"Saluda siempre igual\"\n  }\n);\n```\n\n**Recibe:** **NADA** - solo ejecuta\n\n---\n\n## 2. **Tool CON par\u00e1metros** (lo normal)\n\n```typescript\nconst miTool = tool(\n  ({ ciudad }) => `Clima en ${ciudad}`,  // \u2190 DESESTRUCTURA par\u00e1metros\n  {\n    name: \"clima\",\n    schema: z.object({\n      ciudad: z.string()  // \u2190 DEFINE qu\u00e9 par\u00e1metros espera\n    })\n  }\n);\n```\n\n**Recibe:** `{ ciudad: \"Madrid\" }` - el agente lo llena\n\n---\n\n## 3. **Tool + CONTEXT** (lo PODEROSO)\n\n```typescript\nconst miTool = tool(\n  (_, runtime) => {  // \u2190 _ = ignora input, runtime = context gratis\n    const userName = runtime.context.user_name;  // \u2190 DATO DEL USUARIO\n    return `Hola ${userName}`;\n  },\n  {\n    name: \"saludo_personal\",\n    schema: z.object({})  // \u2190 Sin par\u00e1metros del usuario\n  }\n);\n```\n\n**Recibe:** \n- `_` = par\u00e1metros del usuario (vac\u00edos)\n- `runtime.context` = **DATOS DEL USUARIO GRATIS**\n\n---\n\n## \u00bfQu\u00e9 RECIBE cada tool AUTOM\u00c1TICAMENTE?\n\n| Par\u00e1metro | Qu\u00e9 es | Ejemplo |\n|-----------|--------|---------|\n| **`{ query }`** | Input del AGENTE | `{ query: \"Madrid\" }` |\n| **`_ , runtime`** | **Context del usuario** | `runtime.context.user_name` |\n| **`runtime.store`** | **Memoria persistente** | Datos guardados antes |\n\n## Orden de complejidad:\n\n```\n1. Tool simple \u2192 () => \"hola\"\n2. Tool con input \u2192 ({ciudad}) => ...\n3. Tool + Context \u2192 (_, runtime) => runtime.context.nombre\n4. Tool COMPLETO \u2192 ({ciudad}, runtime) => ...\n```\n\n**\u00bfCu\u00e1l usar?** Empieza con **#1 o #2**. Cuando necesites datos del usuario, pasa a **#3**.\n\n**La magia:** `runtime.context` tiene **TODO** lo que definiste en `contextSchema`. \u00a1Gratis para todos los tools!"
  },
  {
    "role": "user",
    "id": "07bac783-fbbc-422d-859c-17fd7cd871c4",
    "content": "eso teoris solo teori entcnes i  yo ya tengo una fucnion hecha en mi proyecto y quiero darle permido tio mi funcion se llama guardar y gaurda nombre como s el a doy al agent como tool ya esta  hecha"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3831-3852-74c1-b1e7-7174c865bcfa",
    "content": "# Teor\u00eda: Convertir TU funci\u00f3n `guardar()` en Tool\n\n## **PASO 1: Adaptar tu funci\u00f3n existente**\n\n**TU funci\u00f3n actual:**\n```typescript\nfunction guardar(nombre: string) {\n  // tu l\u00f3gica para guardar\n  console.log(`Guardando: ${nombre}`);\n}\n```\n\n**Como TOOL recibe:**\n```typescript\n// El AGENTE te pasa los datos as\u00ed:\nguardar({ nombre: \"Juan\" })  // \u2190 DESDESTRUCTURA\n```\n\n---\n\n## **PASO 2: Envolver en `tool()`**\n\n```typescript\nconst guardarTool = tool(\n  ({ nombre }) => {  // \u2190 RECIBE del agente\n    // LLAMA tu funci\u00f3n original\n    guardar(nombre);\n    return `Nombre ${nombre} guardado`;\n  },\n  {\n    name: \"guardar_nombre\",\n    description: \"Guarda un nombre en la base de datos\",\n    schema: z.object({\n      nombre: z.string()  // \u2190 DICE qu\u00e9 espera\n    })\n  }\n);\n```\n\n---\n\n## **PASO 3: Darle al agente**\n\n```typescript\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [guardarTool]  // \u2190 TU FUNCI\u00d3N AQU\u00cd\n});\n```\n\n---\n\n## **Flujo COMPLETO:**\n\n```\nUsuario: \"Guarda mi nombre Juan\"\n    \u2193\nAgente: \"Llamo tool guardar_nombre\"\n    \u2193\nTu tool: guardar(\"Juan\")  // \u2190 TU FUNCI\u00d3N ORIGINAL\n    \u2193\nAgente: \"Nombre Juan guardado\"\n```\n\n## **Teor\u00eda clave:**\n\n1. **Agente \u2192 Tool**: `{ nombre: \"Juan\" }`\n2. **Tool \u2192 Tu funci\u00f3n**: `guardar(\"Juan\")`\n3. **Tu funci\u00f3n \u2192 Tool**: `\"Guardado\"`\n4. **Tool \u2192 Agente**: `\"Nombre Juan guardado\"`\n\n**\u00a1Tu funci\u00f3n NO cambia! Solo la envuelves en `tool()` y el agente la usa autom\u00e1ticamente.**\n\n**Resultado:** Agente dice \"guarda Juan\" \u2192 Tu funci\u00f3n `guardar(\"Juan\")` se ejecuta. **Fin.**"
  },
  {
    "role": "user",
    "id": "53cfa2d9-ddbe-4cd2-afef-41470926f043",
    "content": "chevere  ahora entiendo todo gracias me xplcia eso import * as z from \"zod\";\nimport { tool, ToolRuntime } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }, config: ToolRuntime) => {\n    const writer = config.writer;\n\n    // Stream custom updates as the tool executes\n    if (writer) {  faiclmente super bien xpicado capricha nomas codigo \n      writer(`Looking up data for city: ${city}`);\n      writer(`Acquired data for city: ${city}`);\n    }\n\n    return `It's always sunny in ${city}!`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather for a given city.\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3833-3176-75a0-8391-a1c3e75df5bb",
    "content": "# `ToolRuntime` explicado S\u00daPER SIMPLE\n\n## **Qu\u00e9 es `ToolRuntime`?**\nEs un **paquete de superpoderes** que el agente **te da GRATIS** a cada tool.\n\n## **Par\u00e1metros del Tool (orden importante):**\n\n```typescript\nconst getWeather = tool(\n  ({ city }, runtime: ToolRuntime) => {  // \u2190 2 par\u00e1metros\n    // runtime = superpoderes del agente\n  }\n);\n```\n\n| # | Par\u00e1metro | Qu\u00e9 es | Ejemplo |\n|---|-----------|--------|---------|\n| **1** | `{ city }` | **Input del usuario** | `{ city: \"Madrid\" }` |\n| **2** | `runtime` | **Superpoderes** | `writer`, `context`, `store` |\n\n---\n\n## **Superpoderes dentro de `runtime`:**\n\n```typescript\nruntime = {\n  writer: \"Para streaming\",     // \u2190 Mensajes en vivo\n  context: \"Datos usuario\",     // \u2190 user_name, user_id\n  store: \"Memoria permanente\",  // \u2190 Datos guardados antes\n  // + m\u00e1s...\n}\n```\n\n---\n\n## **El `writer` explicado:**\n\n**SIN writer:**\n```\nUsuario: \"Clima Madrid\"\n\u2193 (3 segundos despu\u00e9s)\nAgente: \"Siempre hace sol en Madrid\"\n```\n\n**CON writer:**\n```\nUsuario: \"Clima Madrid\"\n\u2193 (inmediato)\nAgente: \"\ud83d\udd0d Buscando datos para Madrid...\"\n\u2193 (1 segundo)\nAgente: \"\u2705 Datos obtenidos para Madrid\"\n\u2193 (total 3 seg)\nAgente: \"Siempre hace sol en Madrid\"\n```\n\n## **Teor\u00eda del c\u00f3digo:**\n\n```typescript\n({ city }, runtime) => {\n  const writer = runtime.writer;  // \u2190 Extrae superpoder \"streaming\"\n  \n  if (writer) {\n    writer(\"Buscando...\");        // \u2190 Mensaje 1 EN VIVO\n    // ... tu l\u00f3gica real ...\n    writer(\"\u00a1Listo!\");           // \u2190 Mensaje 2 EN VIVO  \n  }\n}\n```\n\n**Resultado:** Usuario ve **progreso en tiempo real** mientras tu tool trabaja.\n\n## **Cu\u00e1ndo usar cada uno:**\n\n| Escenario | Usa |\n|-----------|-----|\n| **Tool simple** | `({city}) => ...` |\n| **+ Usuario data** | `(_, runtime) => runtime.context.nombre` |\n| **+ Streaming** | `({city}, runtime) => runtime.writer(\"progreso\")` |\n| **TODO** | `({city}, runtime) => todo()` |\n\n**Tu `getWeather` = Tool PRO con streaming en vivo. \ud83d\udcaa**"
  },
  {
    "role": "user",
    "id": "175ddc98-780d-484f-87a8-6dfcff1f23aa",
    "content": "me da un ejemlo lo mas sencllo del mnfo para entender"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3838-a91c-7520-b339-7588094341ff",
    "content": "# Ejemplo M\u00c1S SIMPLE del `writer`\n\n## **SIN writer** (aburrido)\n```\nUsuario: \"Clima Madrid\"\n\u23f3 Espera 3 segundos...\nAgente: \"Hace sol en Madrid\"\n```\n\n## **CON writer** (emocionante)\n```\nUsuario: \"Clima Madrid\"\n\u2193 INSTANT\u00c1 NEO\n\"\ud83d\udd0d Buscando clima...\"\n\u2193 1 segundo\n\"\u2705 \u00a1Datos encontrados!\"\n\u2193 INSTANT\u00c1NEO\n\"\u2600\ufe0f Hace sol en Madrid\"\n```\n\n## **C\u00f3digo SIMPLE (3 l\u00edneas)**\n\n```typescript\nconst clima = tool(\n  ({ ciudad }, runtime) => {\n    const writer = runtime.writer;  // \u2190 1. Toma el superpoder\n    \n    if (writer) writer(\"\ud83d\udd0d Buscando...\");  // \u2190 2. Mensaje 1\n    \n    // Tu l\u00f3gica real aqu\u00ed (3 segundos)\n    \n    if (writer) writer(\"\u2705 \u00a1Listo!\");      // \u2190 3. Mensaje 2\n    \n    return `\u2600\ufe0f Sol en ${ciudad}`;\n  },\n  { name: \"clima\", schema: z.object({ ciudad: z.string() }) }\n);\n```\n\n## **Flujo visual:**\n```\nUsuario pide \u2192 Tool inicia \u2192 writer(\"Buscando...\") \u2192 Usuario VE mensaje\n                           \u2193\n                     Tu l\u00f3gica (3s) \n                           \u2193\n                    writer(\"\u00a1Listo!\") \u2192 Usuario VE progreso\n                           \u2193\n                      return resultado \u2192 Respuesta final\n```\n\n**RESUMEN:** `writer` = **\"Dile al usuario qu\u00e9 est\u00e1s haciendo\"** mientras trabajas. \n\n**\u00a1Como un humano que dice \"espera, ya busco eso\"!** \ud83d\ude0e"
  },
  {
    "role": "user",
    "id": "1ddeb2c8-50e8-46eb-bc6a-bd5bec8f8193",
    "content": "pero compelto como quedaria  la funcion y el controlado si uso nest js pel e mlo deas ese mismo ejemplo"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3839-acc6-7c20-9467-6369d11aaa1a",
    "content": "# Ejemplo COMPLETO NestJS + Tool con `writer`\n\n## **1. Controller (NestJS)**\n\n```typescript\n// weather.controller.ts\nimport { Controller, Post, Body } from '@nestjs/common';\nimport { createAgent } from 'langchain';\n\n@Controller('weather')\nexport class WeatherController {\n  private agent;\n\n  constructor() {\n    // Crea el agente UNA vez\n    this.agent = createAgent({\n      model: \"gpt-4o\",\n      tools: [this.getWeatherTool()]  // \u2190 Tu tool aqu\u00ed\n    });\n  }\n\n  @Post()\n  async getWeather(@Body() body: { message: string }) {\n    const result = await this.agent.invoke({\n      messages: [{ role: \"user\", content: body.message }]\n    });\n    \n    return { response: result.messages.at(-1)?.content };\n  }\n\n  // \u2190 Tu tool se define AQU\u00cd\n  private getWeatherTool() {\n    return tool(\n      ({ ciudad }, runtime) => {\n        const writer = runtime.writer;\n        \n        if (writer) writer(\"\ud83d\udd0d Buscando clima...\");\n        \n        // SIMULA tu l\u00f3gica real (API externa, DB, etc)\n        const climaReal = this.buscarClimaReal(ciudad);\n        \n        if (writer) writer(\"\u2705 \u00a1Datos encontrados!\");\n        \n        return `\u2600\ufe0f Hace ${climaReal} en ${ciudad}`;\n      },\n      {\n        name: \"clima\",\n        description: \"Obtiene el clima de una ciudad\",\n        schema: z.object({\n          ciudad: z.string()\n        })\n      }\n    );\n  }\n\n  // \u2190 TU L\u00d3GICA REAL (API, DB, etc)\n  private buscarClimaReal(ciudad: string): string {\n    // Aqu\u00ed llamas tu API real, DB, etc\n    return \"25\u00b0C soleado\"; \n  }\n}\n```\n\n## **2. Module (NestJS)**\n\n```typescript\n// weather.module.ts\nimport { Module } from '@nestjs/common';\nimport { WeatherController } from './weather.controller';\n\n@Module({\n  controllers: [WeatherController],\n  imports: [] // Agrega tus servicios aqu\u00ed\n})\nexport class WeatherModule {}\n```\n\n## **3. Uso (Postman/cURL)**\n\n```bash\ncurl -X POST http://localhost:3000/weather \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"dame el clima de Madrid\"}'\n```\n\n## **4. Respuesta EN VIVO (stream)**\n\nPara ver el **streaming** del `writer`, usa `stream` en lugar de `invoke`:\n\n```typescript\n// En tu controller, CAMBIA esto:\n@Post('stream')\nasync getWeatherStream(@Body() body: { message: string }, @Res() res) {\n  res.set({\n    'Content-Type': 'text/plain',\n    'Transfer-Encoding': 'chunked',\n    'Cache-Control': 'no-cache',\n    'Connection': 'keep-alive'\n  });\n\n  const stream = await this.agent.stream({\n    messages: [{ role: \"user\", content: body.message }]\n  }, { streamMode: \"values\" });\n\n  for await (const chunk of stream) {\n    const message = chunk.messages.at(-1);\n    if (message?.content) {\n      res.write(message.content + '\\n');\n    }\n  }\n  res.end();\n}\n```\n\n## **Resultado STREAMING:**\n```\n\ud83d\udd0d Buscando clima...\n\u2705 \u00a1Datos encontrados!\n\u2600\ufe0f Hace 25\u00b0C soleado en Madrid\n```\n\n## **Flujo COMPLETO:**\n\n```\nPOST /weather \u2192 \"dame clima Madrid\"\n    \u2193\nAgente: \"Llamo tool clima {ciudad: 'Madrid'}\"\n    \u2193\nTool: writer(\"\ud83d\udd0d Buscando...\") \u2192 Usuario VE INSTANT\u00c1 NEO\n    \u2193\nTool: this.buscarClimaReal(\"Madrid\") \u2192 TU L\u00d3GICA REAL\n    \u2193\nTool: writer(\"\u2705 \u00a1Listo!\") \u2192 Usuario VE progreso\n    \u2193\nTool: return \"\u2600\ufe0f 25\u00b0C en Madrid\" \u2192 Respuesta final\n```\n\n**\u00a1Listo! Tu funci\u00f3n `buscarClimaReal()` se ejecuta Y el usuario ve progreso en vivo.** \ud83d\ude80"
  },
  {
    "role": "user",
    "id": "df943cee-5e56-4019-9409-ad2ccc883940",
    "content": "para ver  lo  mejro porque e smuy rpaido porque no demora me da la fcntion paar agregarle una que haga que demore  q segundo entr e cada line a d e codigo  repuesa"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3852-c90b-7860-a606-87d7aa925875",
    "content": "# Funci\u00f3n con **DELAYS** para ver el streaming LENTO (1 segundo entre cada l\u00ednea)\n\n## **Reemplaza `buscarClimaReal()` con ESTA:**\n\n```typescript\nprivate async buscarClimaReal(ciudad: string): Promise<string> {\n  const writer = runtime.writer;  // \u2190 IMPORTANTE: Necesitas runtime aqu\u00ed\n  \n  // \ud83d\udd0d PASO 1: Buscando en API (1 segundo)\n  if (writer) writer(\"\ud83c\udf10 Conectando con API de clima...\");\n  await new Promise(r => setTimeout(r, 1000));  // \u2190 1 SEGUNDO\n  \n  // \ud83d\udd0d PASO 2: Obteniendo datos (1 segundo)\n  if (writer) writer(\"\ud83d\udce1 Obteniendo datos meteorol\u00f3gicos...\");\n  await new Promise(r => setTimeout(r, 1000));  // \u2190 1 SEGUNDO\n  \n  // \u2705 PASO 3: Procesando (1 segundo)\n  if (writer) writer(\"\u2699\ufe0f Procesando pron\u00f3stico...\");\n  await new Promise(r => setTimeout(r, 1000));  // \u2190 1 SEGUNDO\n  \n  // L\u00d3GICA REAL (simulada)\n  const temperatura = Math.floor(Math.random() * 30) + 10;\n  const condiciones = [\"soleado \u2600\ufe0f\", \"nublado \u2601\ufe0f\", \"lluvioso \ud83c\udf27\ufe0f\", \"nevando \u2744\ufe0f\"];\n  const clima = condiciones[Math.floor(Math.random() * condiciones.length)];\n  \n  return `${temperatura}\u00b0C ${clima}`;\n}\n```\n\n## **Tool COMPLETO actualizado:**\n\n```typescript\nprivate getWeatherTool() {\n  return tool(\n    async ({ ciudad }, runtime: ToolRuntime) => {  // \u2190 ASYNC importante\n      const writer = runtime.writer;\n      \n      // Mensaje inicial\n      if (writer) writer(\"\ud83d\udd0d Buscando clima para \" + ciudad + \"...\");\n      await new Promise(r => setTimeout(r, 500));  // \u2190 Peque\u00f1o delay\n      \n      // \u2190 LLAMA tu funci\u00f3n con DELAYS\n      const climaReal = await this.buscarClimaRealConDelays(ciudad, runtime);\n      \n      if (writer) writer(\"\ud83c\udf89 \u00a1Pron\u00f3stico listo!\");\n      \n      return `\ud83c\udf24\ufe0f **${ciudad}**: ${climaReal}`;\n    },\n    {\n      name: \"clima\",\n      description: \"Obtiene el clima REAL de una ciudad (toma ~4 segundos)\",\n      schema: z.object({ ciudad: z.string() })\n    }\n  );\n}\n\n// \u2190 NUEVA FUNCI\u00d3N con delays\nprivate async buscarClimaRealConDelays(ciudad: string, runtime: ToolRuntime): Promise<string> {\n  const writer = runtime.writer;\n  \n  if (writer) writer(\"\ud83c\udf10 Conectando API...\");\n  await new Promise(r => setTimeout(r, 1000));\n  \n  if (writer) writer(\"\ud83d\udce1 Descargando datos...\");\n  await new Promise(r => setTimeout(r, 1000));\n  \n  if (writer) writer(\"\u2699\ufe0f Analizando pron\u00f3stico...\");\n  await new Promise(r => setTimeout(r, 1000));\n  \n  const temp = Math.floor(Math.random() * 20) + 15;\n  return `${temp}\u00b0C soleado`;\n}\n```\n\n## **Resultado que VER\u00c1S (LENTO):**\n\n```\nPOST /weather/stream \u2192 \"clima Madrid\"\n\n[0.5s] \ud83d\udd0d Buscando clima para Madrid...\n[1.5s] \ud83c\udf10 Conectando API...\n[2.5s] \ud83d\udce1 Descargando datos...\n[3.5s] \u2699\ufe0f Analizando pron\u00f3stico...\n[4.0s] \ud83c\udf89 \u00a1Pron\u00f3stico listo!\n[4.1s] \ud83c\udf24\ufe0f **Madrid**: 23\u00b0C soleado\n```\n\n## **Controller STREAM actualizado:**\n\n```typescript\n@Post('stream')\nasync getWeatherStream(@Body() body: { message: string }, @Res() res) {\n  res.set({\n    'Content-Type': 'text/event-stream',\n    'Cache-Control': 'no-cache',\n    'Connection': 'keep-alive'\n  });\n\n  const stream = await this.agent.stream({\n    messages: [{ role: \"user\", content: body.message }]\n  }, { streamMode: \"updates\" });  // \u2190 \"updates\" para writer\n\n  for await (const chunk of stream) {\n    // Cada writer() aparece aqu\u00ed\n    if (chunk.type === \"custom\") {\n      res.write(`data: ${chunk.data}\\n\\n`);\n      res.flush();  // \u2190 Fuerza env\u00edo inmediato\n    }\n  }\n  res.end();\n}\n```\n\n## **cURL para probar:**\n\n```bash\ncurl -N http://localhost:3000/weather/stream \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"dame el clima de Madrid\"}'\n```\n\n**\u00a1AHORA VER\u00c1S cada paso LENTO! Perfecto para debugging.** \u23f3\u2728"
  }
]