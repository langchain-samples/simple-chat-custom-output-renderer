[
  {
    "role": "user",
    "id": "a634fab3-747a-413d-aa8b-0f4d8eec70fc",
    "content": "give me an overview of the lang products and what the costs are for a small dev business"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_74499628",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"products\"}"
        }
      },
      {
        "type": "function",
        "id": "call_44835674",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"pricing\"}"
        }
      },
      {
        "type": "function",
        "id": "call_50451106",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"langchain langgraph langsmith\"}"
        }
      },
      {
        "type": "function",
        "id": "call_16328678",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"all\"}"
        }
      },
      {
        "type": "function",
        "id": "call_59279866",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"General,LangSmith Deployment\"}"
        }
      }
    ],
    "id": "lc_run--019b3852-89f0-70f3-9ef0-749d1f30c112",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_74499628",
    "id": "12550c7f-a209-4d61-8254-cb1242a34027",
    "content": "Result 1:\nTitle: Search for items within a namespace prefix.\nLink: https://docs.langchain.comlangsmith/agent-server-api/store/search-for-items-within-a-namespace-prefix\nContent: \n\n---\n\nResult 2:\nTitle: Agents\nLink: https://docs.langchain.comoss/javascript/langchain/agents\nContent: Agents combine language models with [tools](/oss/javascript/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n`createAgent()` provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid  theme={null}\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n  `createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/javascript/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n\n  Learn more about the [Graph API](/oss/javascript/langgraph/graph-api).\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/javascript/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: []\n});\n```\n\nModel identifier strings use the format `provider:model` (e.g. `\"openai:gpt-5\"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/javascript/integrations/providers/) to see available params and methods on your model.\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:\n\n```ts  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst basicModel = new ChatOpenAI({ model: \"gpt-4o-mini\" });\nconst advancedModel = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst dynamicModelSelection = createMiddleware({\n  name: \"DynamicModelSelection\",\n  wrapModelCall: (request, handler) => {\n    // Choose model based on conversation complexity\n    const messageCount = request.messages.length;\n\n    return handler({\n        ...request,\n        model: messageCount > 10 ? advancedModel : basicModel,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\", // Base model (used when messageCount \u2264 10)\n  tools,\n  middleware: [dynamicModelSelection],\n});\n```\n\nFor more details on middleware and advanced patterns, see the [middleware documentation](/oss/javascript/langchain/middleware).\n\n<Tip>\n  For model configuration details, see [Models](/oss/javascript/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/javascript/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/javascript/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({\n      query: z.string().describe(\"The query to search for\"),\n    }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [search, getWeather],\n});\n```\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\nTo customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:\n\n```ts wrap theme={null}\nimport { createAgent, createMiddleware, ToolMessage } from \"langchain\";\n\nconst handleToolErrors = createMiddleware({\n  name: \"HandleToolErrors\",\n  wrapToolCall: async (request, handler) => {\n    try {\n      return await handler(request);\n    } catch (error) {\n      // Return a custom error message to the model\n      return new ToolMessage({\n        content: `Tool error: Please check your input and try again. (${error})`,\n        tool_call_id: request.toolCall.id!,\n      });\n    }\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [handleToolErrors],\n});\n```\n\nThe agent will return a [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) with the custom error message when a tool fails.\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n  **Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n  ```\n  ================================ Human Message =================================\n\n  Find the most popular wireless headphones right now and check if they're in stock\n  ```\n\n  * **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n  * **Acting**: Call `search_products(\"wireless headphones\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    search_products (call_abc123)\n   Call ID: call_abc123\n    Args:\n      query: wireless headphones\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n  ```\n\n  * **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n  * **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    check_inventory (call_def456)\n   Call ID: call_def456\n    Args:\n      product_id: WH-1000XM5\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Product WH-1000XM5: 10 units in stock\n  ```\n\n  * **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n  * **Acting**: Produce final answer\n\n  ```\n  ================================== Ai Message ==================================\n\n  I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n  ```\n</Accordion>\n\n<Tip>\n  To learn more about tools, see [Tools](/oss/javascript/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:\n\n```ts wrap theme={null}\nconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n```\n\nWhen no `systemPrompt` is provided, the agent will infer its task from the messages directly.\n\nThe `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/javascript/integrations/chat/anthropic#prompt-caching):\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst literaryAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-5\",\n  systemPrompt: new SystemMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"You are an AI assistant tasked with analyzing literary works.\",\n      },\n      {\n        type: \"text\",\n        text: \"<the entire contents of 'Pride and Prejudice'>\",\n        cache_control: { type: \"ephemeral\" }\n      }\n    ]\n  })\n});\n\nconst result = await literaryAgent.invoke({\n  messages: [new HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]\n});\n```\n\nThe `cache_control` field with `{ type: \"ephemeral\" }` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/javascript/langchain/middleware).\n\n```typescript wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userRole: z.enum([\"expert\", \"beginner\"]),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [/* ... */],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\n      const userRole = runtime.context.userRole || \"user\";\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    }),\n  ],\n});\n\n// The system prompt will be set dynamically based on context\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Explain machine learning\" }] },\n  { context: { userRole: \"expert\" } }\n);\n```\n\n<Tip>\n  For more details on message types and formatting, see [Messages](/oss/javascript/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/javascript/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/javascript/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n```typescript  theme={null}\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"What's the weather in San Francisco?\" }],\n})\n```\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/javascript/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/javascript/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst ContactInfo = z.object({\n  name: z.string(),\n  email: z.string(),\n  phone: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  responseFormat: ContactInfo,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n    },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n// {\n//   name: 'John Doe',\n//   email: 'john@example.com',\n//   phone: '(555) 123-4567'\n// }\n```\n\n<Tip>\n  To learn about structured output, see [Structured output](/oss/javascript/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/javascript/langchain/short-term-memory) of the agent:\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport { createAgent } from \"langchain\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\n\nconst customAgentState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst CustomAgentState = createAgent({\n  model: \"gpt-4o\",\n  tools: [],\n  stateSchema: customAgentState,\n});\n```\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/javascript/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/javascript/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```ts  theme={null}\nconst stream = await agent.stream(\n  {\n    messages: [{\n      role: \"user\",\n      content: \"Search for AI news and summarize the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/javascript/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/javascript/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n<Tip>\n  For comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Google\nLink: https://docs.langchain.comoss/python/integrations/providers/google\nContent: Other Google products\nIntegrations with various Google services beyond the core Cloud Platform.\n\nGoogle\nThis page covers all LangChain integrations with Google Gemini , Google Cloud , and other Google products (such as Google Maps, YouTube, and more ). Unified SDK & Package Consolidation As of langchain-google-genai 4.0.0, this package uses the consolidated google-genai SDK and now supports both the Gemini Developer API and Vertex AI backends. The langchain-google-vertexai package remains supported for Vertex AI platform-specific features (Model Garden, Vector Search, evaluation services, etc.). Read the full announcement and migration guide. Not sure which package to use? Access Google Gemini models via the Gemini Developer API or Vertex AI . The backend is selected automatically based on your configuration. Gemini Developer API : Quick setup with API key, ideal for individual developers and rapid prototyping Vertex AI : Enterprise features with Google Cloud integration (requires GCP project) Use the langchain-google-genai package for chat models, LLMs, and embeddings. See integrations. Access Vertex AI platform-specific services beyond Gemini models: Model Garden (Llama, Mistral, Anthropic), evaluation services, and specialized vision models. Use the langchain-google-vertexai package for platform services and specific packages (e.g., langchain-google-community , langchain-google-cloud-sql-pg ) for other cloud services like databases and storage. See integrations. See Google's guide on migrating from the Gemini API to Vertex AI for more details on the differences. Integration packages for Gemini models and the Vertex AI platform are maintained in the langchain-google repository. You can find a host of LangChain integrations with other Google APIs and services in the langchain-google-community package (listed on this page) and the googleapis GitHub organization.\n\nTools\nGoogle Search Perform web searches using Google Custom Search Engine (CSE). Google Drive Tools for interacting with Google Drive. Google Finance Query financial data. Google Jobs Query job listings. Google Lens Perform visual searches. Google Places Search for places information. Google Scholar Search academic papers. Google Trends Query Google Trends data.\n\n---\n\nResult 4:\nTitle: Retrievers\nLink: https://docs.langchain.comoss/python/integrations/retrievers/index\nContent: A [retriever](/oss/python/langchain/retrieval#building-blocks) is an interface that returns documents given an unstructured query.\nIt is more general than a vector store.\nA retriever does not need to be able to store documents, only to return (or retrieve) them.\nRetrievers can be created from vector stores, but are also broad enough to include [Wikipedia search](/oss/python/integrations/retrievers/wikipedia/) and [Amazon Kendra](/oss/python/integrations/retrievers/amazon_kendra_retriever/).\n\nRetrievers accept a string query as input and return a list of [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects as output.\n\nNote that all [vector stores](/oss/python/integrations/vectorstores) can be cast to retrievers. Refer to the vector store [integration docs](/oss/python/integrations/vectorstores/) for available vector stores.\nThis page lists custom retrievers, implemented via subclassing BaseRetriever.\n\n## Bring-your-own documents\n\nThe below retrievers allow you to index and search a custom corpus of documents.\n\n| Retriever                                                                                | Self-host | Cloud offering | Package                                                                                                                                                                               |\n| ---------------------------------------------------------------------------------------- | --------- | -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AmazonKnowledgeBasesRetriever`](/oss/python/integrations/retrievers/bedrock)           | \u274c         | \u2705              | [`langchain-aws`](https://python.langchain.com/api_reference/aws/retrievers/langchain_aws.retrievers.bedrock.AmazonKnowledgeBasesRetriever.html)                                      |\n| [`AzureAISearchRetriever`](/oss/python/integrations/retrievers/azure_ai_search)          | \u274c         | \u2705              | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.azure_ai_search.AzureAISearchRetriever.html)                   |\n| [`ElasticsearchRetriever`](/oss/python/integrations/retrievers/elasticsearch_retriever)  | \u2705         | \u2705              | [`langchain-elasticsearch`](https://python.langchain.com/api_reference/elasticsearch/retrievers/langchain_elasticsearch.retrievers.ElasticsearchRetriever.html)                       |\n| [`VertexAISearchRetriever`](/oss/python/integrations/retrievers/google_vertex_ai_search) | \u274c         | \u2705              | [`langchain-google-community`](https://python.langchain.com/api_reference/google_community/vertex_ai_search/langchain_google_community.vertex_ai_search.VertexAISearchRetriever.html) |\n\n## External index\n\nThe below retrievers will search over an external index (e.g., constructed from Internet data or similar).\n\n| Retriever                                                                | Source                                                | Package                                                                                                                                                                 |\n| ------------------------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`ArxivRetriever`](/oss/python/integrations/retrievers/arxiv)            | Scholarly articles on [arxiv.org](https://arxiv.org/) | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html)                       |\n| [`TavilySearchAPIRetriever`](/oss/python/integrations/retrievers/tavily) | Internet search                                       | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.tavily_search_api.TavilySearchAPIRetriever.html) |\n| [`WikipediaRetriever`](/oss/python/integrations/retrievers/wikipedia)    | [Wikipedia](https://www.wikipedia.org/) articles      | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.wikipedia.WikipediaRetriever.html)               |\n\n## All retrievers\n\n<Columns cols={3}>\n  <Card title=\"Activeloop Deep Memory\" icon=\"link\" href=\"/oss/python/integrations/retrievers/activeloop\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Amazon Kendra\" icon=\"link\" href=\"/oss/python/integrations/retrievers/amazon_kendra_retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Arcee\" icon=\"link\" href=\"/oss/python/integrations/retrievers/arcee\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Arxiv\" icon=\"link\" href=\"/oss/python/integrations/retrievers/arxiv\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"AskNews\" icon=\"link\" href=\"/oss/python/integrations/retrievers/asknews\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Azure AI Search\" icon=\"link\" href=\"/oss/python/integrations/retrievers/azure_ai_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Bedrock (Knowledge Bases)\" icon=\"link\" href=\"/oss/python/integrations/retrievers/bedrock\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BM25\" icon=\"link\" href=\"/oss/python/integrations/retrievers/bm25\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Box\" icon=\"link\" href=\"/oss/python/integrations/retrievers/box\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"BREEBS (Open Knowledge)\" icon=\"link\" href=\"/oss/python/integrations/retrievers/breebs\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Chaindesk\" icon=\"link\" href=\"/oss/python/integrations/retrievers/chaindesk\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ChatGPT plugin\" icon=\"link\" href=\"/oss/python/integrations/retrievers/chatgpt-plugin\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cognee\" icon=\"link\" href=\"/oss/python/integrations/retrievers/cognee\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cohere reranker\" icon=\"link\" href=\"/oss/python/integrations/retrievers/cohere-reranker\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Cohere RAG\" icon=\"link\" href=\"/oss/python/integrations/retrievers/cohere\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Contextual AI Reranker\" icon=\"link\" href=\"/oss/python/integrations/retrievers/contextual\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dappier\" icon=\"link\" href=\"/oss/python/integrations/retrievers/dappier\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DocArray\" icon=\"link\" href=\"/oss/python/integrations/retrievers/docarray_retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dria\" icon=\"link\" href=\"/oss/python/integrations/retrievers/dria_index\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ElasticSearch BM25\" icon=\"link\" href=\"/oss/python/integrations/retrievers/elastic_search_bm25\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Elasticsearch\" icon=\"link\" href=\"/oss/python/integrations/retrievers/elasticsearch_retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Embedchain\" icon=\"link\" href=\"/oss/python/integrations/retrievers/embedchain\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FlashRank reranker\" icon=\"link\" href=\"/oss/python/integrations/retrievers/flashrank-reranker\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Fleet AI Context\" icon=\"link\" href=\"/oss/python/integrations/retrievers/fleet_context\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Galaxia\" icon=\"link\" href=\"/oss/python/integrations/retrievers/galaxia-retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Drive\" icon=\"link\" href=\"/oss/python/integrations/retrievers/google_drive\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Vertex AI Search\" icon=\"link\" href=\"/oss/python/integrations/retrievers/google_vertex_ai_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Graph RAG\" icon=\"link\" href=\"/oss/python/integrations/retrievers/graph_rag\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GreenNode\" icon=\"link\" href=\"/oss/python/integrations/retrievers/greennode_reranker\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"IBM watsonx.ai\" icon=\"link\" href=\"/oss/python/integrations/retrievers/ibm_watsonx_ranker\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"JaguarDB Vector Database\" icon=\"link\" href=\"/oss/python/integrations/retrievers/jaguar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Kay.ai\" icon=\"link\" href=\"/oss/python/integrations/retrievers/kay\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Kinetica Vectorstore\" icon=\"link\" href=\"/oss/python/integrations/retrievers/kinetica\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"kNN\" icon=\"link\" href=\"/oss/python/integrations/retrievers/knn\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LinkupSearchRetriever\" icon=\"link\" href=\"/oss/python/integrations/retrievers/linkup_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LLMLingua Document Compressor\" icon=\"link\" href=\"/oss/python/integrations/retrievers/llmlingua\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"LOTR (Merger Retriever)\" icon=\"link\" href=\"/oss/python/integrations/retrievers/merger_retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Metal\" icon=\"link\" href=\"/oss/python/integrations/retrievers/metal\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NanoPQ (Product Quantization)\" icon=\"link\" href=\"/oss/python/integrations/retrievers/nanopq\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nebius\" icon=\"link\" href=\"/oss/python/integrations/retrievers/nebius\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"needle\" icon=\"link\" href=\"/oss/python/integrations/retrievers/needle\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Nimble\" icon=\"link\" href=\"/oss/python/integrations/retrievers/nimble\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Outline\" icon=\"link\" href=\"/oss/python/integrations/retrievers/outline\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Permit\" icon=\"link\" href=\"/oss/python/integrations/retrievers/permit\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pinecone Hybrid Search\" icon=\"link\" href=\"/oss/python/integrations/retrievers/pinecone_hybrid_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Pinecone Rerank\" icon=\"link\" href=\"/oss/python/integrations/retrievers/pinecone_rerank\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"PubMed\" icon=\"link\" href=\"/oss/python/integrations/retrievers/pubmed\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Qdrant Sparse Vector\" icon=\"link\" href=\"/oss/python/integrations/retrievers/qdrant-sparse\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"RAGatouille\" icon=\"link\" href=\"/oss/python/integrations/retrievers/ragatouille\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"RePhraseQuery\" icon=\"link\" href=\"/oss/python/integrations/retrievers/re_phrase\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Rememberizer\" icon=\"link\" href=\"/oss/python/integrations/retrievers/rememberizer\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SEC filing\" icon=\"link\" href=\"/oss/python/integrations/retrievers/sec_filings\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SVM\" icon=\"link\" href=\"/oss/python/integrations/retrievers/svm\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TavilySearchAPI\" icon=\"link\" href=\"/oss/python/integrations/retrievers/tavily\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"TF-IDF\" icon=\"link\" href=\"/oss/python/integrations/retrievers/tf_idf\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"NeuralDB\" icon=\"link\" href=\"/oss/python/integrations/retrievers/thirdai_neuraldb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ValyuContext\" icon=\"link\" href=\"/oss/python/integrations/retrievers/valyu\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vectorize\" icon=\"link\" href=\"/oss/python/integrations/retrievers/vectorize\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Vespa\" icon=\"link\" href=\"/oss/python/integrations/retrievers/vespa\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wikipedia\" icon=\"link\" href=\"/oss/python/integrations/retrievers/wikipedia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"You.com\" icon=\"link\" href=\"/oss/python/integrations/retrievers/you-retriever\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zep Cloud\" icon=\"link\" href=\"/oss/python/integrations/retrievers/zep_cloud_memorystore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zep Open Source\" icon=\"link\" href=\"/oss/python/integrations/retrievers/zep_memorystore\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Zotero\" icon=\"link\" href=\"/oss/python/integrations/retrievers/zotero\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/retrievers/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Agents\nLink: https://docs.langchain.comoss/python/langchain/agents\nContent: Tool use in the ReAct loop\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer. Prompt: Identify the current most popular wireless headphones and verify availability. Reasoning : \"Popularity is time-sensitive, I need to use the provided search tool.\" Acting : Call search_products(\"wireless headphones\") ================================== Ai Message ==================================\nTool Calls:\n  search_products (call_abc123)\n Call ID: call_abc123\n  Args:\n    query: wireless headphones\n\nTool use in the ReAct loop\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer. Prompt: Identify the current most popular wireless headphones and verify availability. Reasoning : \"Popularity is time-sensitive, I need to use the provided search tool.\" Acting : Call search_products(\"wireless headphones\") ================================= Tool Message =================================\n\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n\nTool use in the ReAct loop\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer. Prompt: Identify the current most popular wireless headphones and verify availability. Reasoning : \"Popularity is time-sensitive, I need to use the provided search tool.\" Acting : Call search_products(\"wireless headphones\") Reasoning : \"I need to confirm availability for the top-ranked item before answering.\" Acting : Call check_inventory(\"WH-1000XM5\") ================================= Tool Message =================================\n\nProduct WH-1000XM5: 10 units in stock\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_44835674",
    "id": "8f1777b4-573b-4a1b-9d63-77caca1d7f0b",
    "content": "Result 1:\nTitle: LangSmith Agent Builder Slack App\nLink: https://docs.langchain.comlangsmith/agent-builder-slack-app\nContent: The LangSmith Agent Builder Slack app integrates your agents with Slack for secure, context-aware communication inside your Slack workspace.\n\nAfter installation, your agents will be able to:\n\n* Send direct messages.\n* Post to channels.\n* Read thread messages.\n* Reply in threads.\n* Read conversation history.\n\n## How to install\n\nTo install the LangSmith Agent Builder for Slack:\n\n1. Navigate to Agent Builder in your [LangSmith workspace](https://smith.langchain.com).\n2. Create or edit an agent.\n3. Add Slack as a trigger or enable Slack tools.\n4. When prompted, authorize the Slack connection.\n5. Follow the OAuth flow to grant permissions to your Slack workspace.\n\nThe app will be installed automatically when you complete the authorization, *but* you will still need to invite the app into the specific channels you want to use it in.\n\nTo invite the Slack bot, you can send the following message:\n\n```txt  theme={null}\n/invite @LangSmith Agent Builder\n```\n\n## Permissions\n\nThe LangSmith Agent Builder requires the following permissions to your Slack workspace:\n\n* **Send messages** - Send direct messages and post to channels\n* **Read messages** - Read channel history and thread messages\n* **View channels** - Access basic channel information\n* **View users** - Look up user information for messaging\n\nThese permissions enable agents to communicate effectively within your Slack workspace.\n\n## Privacy policy\n\nThe LangSmith Agent Builder Slack app collects, manages, and stores third-party data in accordance with our privacy policy. For full details on how your data is handled, please see [our privacy policy](https://www.langchain.com/privacy-policy).\n\n## AI components and disclaimers\n\nThe LangSmith Agent Builder uses Large Language Models (LLMs) to power AI agents that interact with users in Slack. While these models are powerful, they have the potential to generate inaccurate responses, summaries, or other outputs.\n\n### What you should know\n\n* **AI-generated content**: All responses from agents are generated by AI and may contain errors or inaccuracies. Always verify important information.\n* **Data usage**: Slack data is not used to train LLMs. Your workspace data remains private and is only used to provide agent functionality.\n* **Transparency**: The Agent Builder is transparent about the actions it will take once added to your workspace, as outlined in the permissions section above.\n\n### Technical details\n\nThe Agent Builder uses the following approach to AI:\n\n* **Model**: Uses LLMs provided through the LangSmith platform\n* **Data retention**: User data is retained according to LangSmith's data retention policies\n* **Data tenancy**: Data is handled according to your LangSmith organization settings\n* **Data residency**: Data residency follows your LangSmith configuration\n\nFor more information about AI safety and best practices, see the [Agent Builder documentation](/langsmith/agent-builder).\n\n## Pricing\n\nThe LangSmith Agent Builder Slack app itself does not have any direct pricing. However, agent runs and traces are billed through the [LangSmith platform](https://smith.langchain.com) according to your organization's plan.\n\nFor current pricing information, see the [LangSmith pricing page](https://www.langchain.com/pricing).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-builder-slack-app.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Cost tracking\nLink: https://docs.langchain.comlangsmith/cost-tracking\nContent: Create a new or modify an existing model price entry\nTo modify the default model prices, create a new entry with the same model, provider and match pattern as the default entry. To create a  new entry  in the model pricing map, click on the  + Model  button in the top right corner.   Here, you can specify the following fields: Model Name : The human-readable name of the model. Input Price : The cost per 1M input tokens for the model. This number is multiplied by the number of tokens in the prompt to calculate the prompt cost. Input Price Breakdown  (Optional): The breakdown of price for each different type of input token, e.g.  cache_read ,  video ,  audio Output Price : The cost per 1M output tokens for the model. This number is multiplied by the number of tokens in the completion to calculate the completion cost. Output Price Breakdown  (Optional): The breakdown of price for each different type of output token, e.g.  reasoning ,  image , etc. Model Activation Date  (Optional): The date from which the pricing is applicable. Only runs after this date will apply this model price. Match Pattern : A regex pattern to match the model name. This is used to match the value for  ls_model_name  in the run metadata. Provider  (Optional): The provider of the model. If specified, this is matched against  ls_provider  in the run metadata. Once you have set up the model pricing map, LangSmith will automatically calculate and aggregate the token-based costs for traces based on the token counts provided in the LLM invocations.\n\nLLM calls: Automatically track costs based on token counts\nTo compute cost automatically from token usage, you need to provide token counts , the model and provider and the model price. Follow the instructions below if you\u2019re using model providers whose responses don\u2019t follow the same patterns as one of OpenAI or Anthropic. These steps are only required if you are not: Calling LLMs with LangChain Using @traceable to trace LLM calls to OpenAI, Anthropic or models that follow an OpenAI-compliant format Using LangSmith wrappers for OpenAI or Anthropic. 1. Send token counts Many models include token counts as part of the response. You must extract this information and include it in your run using one of the following methods: Set a usage_metadata field on the run's metadata. The advantage of this approach is that you do not need to change your traced function\u2019s runtime outputs Include the usage_metadata key directly within the object returned by your traced function. LangSmith will extract it from the output. In either case, the usage metadata should contain a subset of the following LangSmith-recognized fields: The following fields in the usage_metadata dict are recognized by LangSmith. You can view the full Python types or TypeScript interfaces directly. input_tokens Number of tokens used in the model input. Sum of all input token types. output_tokens Number of tokens used in the model response. Sum of all output token types. total_tokens Number of tokens used in the input and output. Optional, can be inferred. Sum of input_tokens + output_tokens. input_token_details Breakdown of input token types. Keys are token-type strings, values are counts. Example {\"cache_read\": 5}. Known fields include: audio , text , image , cache_read , cache_creation . Additional fields are possible depending on the model or provider. output_token_details Breakdown of output token types. Keys are token-type strings, values are counts. Example {\"reasoning\": 5}. Known fields include: audio , text , image , reasoning . Additional fields are possible depending on the model or provider. input_cost Cost of the input tokens. output_cost Cost of the output tokens. total_cost Cost of the tokens. Optional, can be inferred.  Sum of input_cost + output_cost. input_cost_details Details of the input cost. Keys are token-type strings, values are cost amounts. output_cost_details Details of the output cost. Keys are token-type strings, values are cost amounts. Cost Calculations The cost for a run is computed greedily from most-to-least specific token type. Suppose you set a price of $2 per 1M input tokens with a detailed price of $1 per 1M cache_read input tokens, and $3 per 1M output tokens. If you uploaded the following usage metadata: Then, the token costs would be computed as follows: 2. Specify model name When using a custom model, the following fields need to be specified in a run's metadata in order to associate token counts with costs. It's also helpful to provide these metadata fields to identify the model when viewing traces and when filtering. ls_provider : The provider of the model, e.g., \u201copenai\u201d, \u201canthropic\u201d ls_model_name : The name of the model, e.g., \u201cgpt-4o-mini\u201d, \u201cclaude-3-opus-20240229\u201d 3. Set model prices A model pricing map is used to map model names to their per-token prices to compute costs from token counts. LangSmith's model pricing table is used for this. The table comes with pricing information for most OpenAI, Anthropic, and Gemini models. You can add prices for other models , or overwrite pricing for default models if you have custom pricing. For models that have different pricing for different token types (e.g., multimodal or cached tokens), you can specify a breakdown of prices for each token type. Hovering over the ... next to the input/output prices shows you the price breakdown by token type. Updates to the model pricing map are not reflected in the costs for traces already logged. We do not currently support backfilling model pricing changes. To modify the default model prices, create a new entry with the same model, provider and match pattern as the default entry. To create a new entry in the model pricing map, click on the + Model button in the top right corner. Here, you can specify the following fields: Model Name : The human-readable name of the model. Input Price : The cost per 1M input tokens for the model. This number is multiplied by the number of tokens in the prompt to calculate the prompt cost. Input Price Breakdown (Optional): The breakdown of price for each different type of input token, e.g. cache_read , video , audio Output Price : The cost per 1M output tokens for the model. This number is multiplied by the number of tokens in the completion to calculate the completion cost. Output Price Breakdown (Optional): The breakdown of price for each different type of output token, e.g. reasoning , image , etc. Model Activation Date (Optional): The date from which the pricing is applicable. Only runs after this date will apply this model price. Match Pattern : A regex pattern to match the model name. This is used to match the value for ls_model_name in the run metadata. Provider (Optional): The provider of the model. If specified, this is matched against ls_provider in the run metadata. Once you have set up the model pricing map, LangSmith will automatically calculate and aggregate the token-based costs for traces based on the token counts provided in the LLM invocations.\n\nLLM calls: Sending costs directly\nIf your model follows a non-linear pricing scheme, we recommend calculating costs client-side and sending them to LangSmith as usage_metadata. Gemini 3 Pro Preview and Gemini 2.5 Pro follow a pricing scheme with a stepwise cost function. We support this pricing scheme for Gemini by default. For any other models with non-linear pricing, you will need to follow these instructions to calculate costs.\n\n---\n\nResult 3:\nTitle: Frequently Asked Questions\nLink: https://docs.langchain.comlangsmith/pricing-faq\nContent: ## Questions and Answers\n\n### I've been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?\n\nIf you've been using LangSmith already, your usage will be billable starting in July 2024. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by [contacting our sales team](https://www.langchain.com/contact-sales).\n\n### Which plan is right for me?\n\nIf you're an individual developer, the Developer plan is a great choice for small projects.\n\nFor teams that want to collaborate in LangSmith, check out the Plus plan. **If you are an early-stage startup building an AI application**, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our [Startup Contact Form](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form) for more details.\n\nIf you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our [Sales Contact Form](https://www.langchain.com/contact-sales) for more details.\n\n### What is a seat?\n\nA seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.\n\n### What is a trace?\n\nA trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an [example](https://smith.langchain.com/public/17c24270-9f74-47e7-b70c-d508afc448fa/r) of a single trace.\n\n### What is an Agent Run?\n\nAn Agent Run is one end-to-end invocation of a LangGraph agent deployed via LangSmith Deployment. Nodes and subgraphs within a single agent execution are not charged separately. Calls to other LangGraph agents (through RemoteGraph or the LangGraph SDK or the API directly) are charged separately, to the deployment that hosts the agent being called. An interrupt for human-in-the-loop creates a separate Agent Run when resuming.\n\nAgent Runs are billed at \\$0.005 each. For high-volume usage, please [contact our sales team](https://www.langchain.com/contact-sales) to discuss custom pricing options.\n\n### What is an ingested event?\n\nAn ingested event is any distinct, trace-related data sent to LangSmith. This includes:\n\n* Inputs, outputs and metadata sent at the start of a run step within a trace\n* Inputs, outputs and metadata sent at the end of a run step within a trace\n* Feedback on run steps or traces\n\n### I've hit my rate or usage limits. What can I do?\n\nWhen you first sign up for a LangSmith account, you get a Personal organization that is limited to 5000 monthly traces. To continue sending traces after reaching this limit, upgrade to the Developer or Plus plans by adding a credit card. Head to [Plans and Billing](https://smith.langchain.com/settings/payments) to upgrade.\n\nSimilarly, if you've hit the rate limits on your current plan, you can upgrade to a higher plan to get higher limits, or contact support via [support.langchain.com](https://support.langchain.com) with questions.\n\n### I have a developer account, can I upgrade my account to the Plus or Enterprise plan?\n\nYes, Developer plan users can easily upgrade to the Plus plan on the [Plans and Billing](https://smith.langchain.com/settings/payments) page. For the Enterprise plan, please [contact our sales team](https://www.langchain.com/contact-sales) to discuss your needs.\n\n### How does billing work?\n\n**Seats**\n\nSeats are billed monthly on the first of the month. Additional seats purchased mid-month are pro-rated and billed within one day of the purchase. Seats removed mid-month will not be credited.\n\n**Traces**\n\nAs long as you have a card on file in your account, we'll service your traces and bill you on the first of the month for traces that you submitted in the previous month. You will be able to set usage limits if you so choose to limit the maximum charges you could incur in any given month.\n\n### Can I limit how much I spend on tracing?\n\nYou can set limits on the number of traces that can be sent to LangSmith per month on the [Usage configuration](https://smith.langchain.com/settings/payments) page.\n\n<Note>\n  While we do show you the dollar value of your usage limit for convenience, this limit evaluated in terms of number of traces instead of dollar amount. For example, if you are approved for our startup plan tier where you are given a generous allotment of free traces, your usage limit will not automatically change.\n\n  You are not currently able to set a spend limit in the product.\n</Note>\n\n### How can I track my usage so far this month?\n\nUnder the Settings section for your Organization you will see subsection for **Usage**. There, you will be able to see a graph of the daily number of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day.\n\n### I have a question about my bill...\n\nCustomers on the Developer and Plus plan tiers should contact support via [support.langchain.com](https://support.langchain.com). Customers on the Enterprise plan should contact their sales representative directly.\n\nEnterprise plan customers are billed annually by invoice.\n\n### What can I expect from Support?\n\nOn the Developer plan, community-based support is available on [LangChain community Slack](https://www.langchain.com/join-community).\n\nOn the Plus plan, you will also receive preferential support via [support.langchain.com](https://support.langchain.com) for LangSmith-related questions only and we'll do our best to respond within the next business day.\n\nOn the Enterprise plan, you'll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, we'll also support deployments and new releases with our infra engineering team on-call.\n\n### Where is my data stored?\n\nYou may choose to sign up in either the US or EU region. See the [cloud architecture reference](/langsmith/cloud#cloud-architecture-and-scalability) for more details. If you're on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment.\n\n### Which security frameworks is LangSmith compliant with?\n\nWe are SOC 2 Type II, GDPR, and HIPAA compliant.\n\nYou can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). Please note we only enter into BAAs with customers on our Enterprise plan.\n\n### Will you train on the data that I send LangSmith?\n\nWe will not train on your data, and you own all rights to your data. See [LangSmith Terms of Service](https://langchain.dev/terms-of-service) for more information.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pricing-faq.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Pricing plans\nLink: https://docs.langchain.comlangsmith/pricing-plans\nContent: ***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pricing-plans.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Regions FAQ\nLink: https://docs.langchain.comlangsmith/regions-faq\nContent: <Note>\n  See the [cloud architecture reference](/langsmith/cloud#architecture) for additional details.\n</Note>\n\n## Legal and compliance\n\n#### *What privacy and data protection frameworks does LangSmith, including its EU instance, comply with?*\n\nLangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). If you would like to sign a Data Processing Addendum (DPA) with us, please contact support via [support.langchain.com](https://support.langchain.com). Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan.\n\n#### *My company isn't based in the EU, can I still have my data hosted there?*\n\nYes, you can host your LangSmith data in the EU instance independent of your location.\n\n#### *Do you have a legal entity in the EU that we can contract with?*\n\nWe do not have a legal entity in the EU for customer contracting today.\n\n#### *Do different legal terms apply if I choose the EU region?*\n\nThe terms are the same for the EU and US regions.\n\n## Features\n\n#### *How do I use the EU instance?*\n\nFollow the instructions [here](/langsmith/create-account-api-key) to create an account and an API key (make sure to change the region to EU in the dropdown)\n\n#### *Are there any functional differences between US and EU cloud-managed LangSmith?*\n\nThere may be a small delay between launches to each region depending on the feature. Besides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa.\n\n#### *Can an organization have workspaces in different regions?*\n\nLangSmith does not support this at the moment, but if you are interested, please contact support via [support.langchain.com](https://support.langchain.com) and share your use case.\n\n#### *Can I connect an EU organization to a US organization and share billing?*\n\nLangSmith does not support this at the moment, but if you are interested, please contact support via [support.langchain.com](https://support.langchain.com) and share your use case.\n\n#### *What data will be stored in my selected region?*\n\nSee the [cloud architecture reference](/langsmith/cloud#architecture) for details.\n\n#### *How can I see my organization's region?*\n\nCheck your URL - any organizations on [https://eu.smith.langchain.com](https://eu.smith.langchain.com) are in the EU, and any on [https://smith.langchain.com](https://smith.langchain.com) are in the US.\n\n#### *Can I switch my organization from the US to EU or vice versa?*\n\nWe do not support migration between regions at this time, but if you are interested in this feature, please contact support via [support.langchain.com](https://support.langchain.com).\n\n## Plans and pricing\n\n#### *Is the EU region available on all LangSmith plans?*\n\nYes, you can sign up for the EU region on all plans including free plans.\n\n#### *Is pricing different for the EU region compared to the US region?*\n\nNo, pricing is the same for the EU and US regions.\n\n#### *What currency is used for payment if I use the EU region?*\n\nAll LangSmith plans are paid in USD.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/regions-faq.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_50451106",
    "id": "a4097108-b3bb-443e-9261-d946c4ebcc4a",
    "content": "Result 1:\nTitle: Contributing to code\nLink: https://docs.langchain.comoss/javascript/contributing/code\nContent: Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.\n\n## Getting started\n\n<Note>\n  Before submitting large **new features or refactors**, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work.\n\n  This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. See the quickstart guide below.\n</Note>\n\n### Quick fix: submit a bugfix\n\nFor simple bugfixes, you can get started immediately:\n\n<Steps>\n  <Step title=\"Reproduce the issue\">\n    Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification\n  </Step>\n\n  <Step title=\"Fork the repository\">\n    Fork the [LangChain](https://github.com/langchain-ai/langchainjs) or [LangGraph](https://github.com/langchain-ai/langgraphjs) repo to your personal GitHub account\n  </Step>\n\n  <Step title=\"Clone and setup\">\n    ```bash  theme={null}\n    git clone https://github.com/your-username/name-of-forked-repo.git\n\n    # For instance, for LangChain:\n    git clone https://github.com/parrot123/langchainjs.git\n\n    # For LangGraph:\n    git clone https://github.com/parrot123/langgraphjs.git\n    ```\n\n    ```bash  theme={null}\n    # Inside your repo, install dependencies\n    pnpm install\n    # Create a build for all packages to resolve workspace dependencies\n    pnpm build\n    ```\n  </Step>\n\n  <Step title=\"Create a branch\">\n    Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later.\n\n    ```bash  theme={null}\n    git checkout -b your-username/short-bugfix-name\n    ```\n  </Step>\n\n  <Step title=\"Write failing tests\">\n    Add [unit tests](#test-writing-guidelines) that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions\n  </Step>\n\n  <Step title=\"Make your changes\">\n    Fix the bug while following our [code quality standards](#code-quality-standards). Make the **minimal change necessary** to resolve the issue\n  </Step>\n\n  <Step title=\"Run build\">\n    Run the build command to ensure the package still builds properly\n\n    ```bash  theme={null}\n    pnpm build\n    # or build a specific workspace package\n    pnpm --filter @langchain/core build\n    ```\n  </Step>\n\n  <Step title=\"Verify the fix\">\n    Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR\n\n    ```bash  theme={null}\n    pnpm lint\n    pnpm test\n\n    # For bugfixes involving integrations, also run:\n    pnpm test:int\n\n    # Or run tests in a specific workspace package\n    cd libs/langchain-core\n    pnpm test\n    pnpm lint\n\n    # Or run tests for a specific package from the root of the repo\n    pnpm --filter @langchain/core test\n    pnpm --filter @langchain/core lint\n    ```\n  </Step>\n\n  <Step title=\"Document the change\">\n    Update docstrings if behavior changes, add comments for complex logic\n  </Step>\n\n  <Step title=\"Submit a pull request\">\n    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #ISSUE_NUMBER`) so that the issue is automatically closed when your PR is merged.\n  </Step>\n</Steps>\n\n### Full development setup\n\nFor ongoing development or larger contributions:\n\n1. Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations\n2. Set up your environment following our [setup guide](#development-environment) below\n3. Understand the [repository structure](#repository-structure) and package organization\n4. Learn our [development workflow](#development-workflow) including testing and linting\n\n***\n\n## Contribution guidelines\n\nBefore you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a \"first contribution\" to your resume (or if you're just looking for a quick win) you might be better off doing a boot-camp or an online tutorial.\n\nContributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it's important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you're willing to take the time to do things well.\n\n### Backwards compatibility\n\n<Warning>\n  Breaking changes to public APIs are not allowed except for critical security fixes.\n\n  See our [versioning policy](/oss/javascript/versioning) for details on major version releases.\n</Warning>\n\nMaintain compatibility via:\n\n<AccordionGroup>\n  <Accordion title=\"Stable interfaces\">\n    **Always preserve**:\n\n    * Function signatures and parameter names\n    * Class interfaces and method names\n    * Return value structure and types\n    * Import paths for public APIs\n  </Accordion>\n\n  <Accordion title=\"Safe changes\">\n    **Acceptable modifications**:\n\n    * Adding new optional parameters/type parameters\n\n    * Adding new methods to classes\n\n    * Improving performance without changing behavior\n\n    * Adding new modules or functions\n  </Accordion>\n\n  <Accordion title=\"Before making changes\">\n    * **Would this break existing user code?**\n\n    * Check if your target is public\n\n    * Are there existing usage patterns in tests?\n  </Accordion>\n</AccordionGroup>\n\n### New features\n\nWe aim to keep the bar high for new features. We generally don't accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies.\n\nIn general, feature contribution requirements include:\n\n<Steps>\n  <Step title=\"Design discussion\">\n    Open an issue describing:\n\n    * The problem you're solving\n    * Proposed API design\n    * Expected usage patterns\n  </Step>\n\n  <Step title=\"Implementation\">\n    * Follow existing code patterns\n    * Include comprehensive tests and documentation\n    * Consider security implications\n  </Step>\n\n  <Step title=\"Integration considerations\">\n    * How does this interact with existing features?\n    * Are there performance implications?\n    * Does this introduce new dependencies?\n\n    We will reject features that are likely to lead to security vulnerabilities or reports.\n  </Step>\n</Steps>\n\n### Security guidelines\n\n<Warning>\n  Security is paramount. Never introduce vulnerabilities or unsafe patterns.\n</Warning>\n\nSecurity checklist:\n\n<AccordionGroup>\n  <Accordion title=\"Input validation\">\n    * Validate and sanitize all user inputs\n\n    * Properly escape data in templates and queries\n\n    * Never use `eval()`, as this can lead to arbitrary code execution vulnerabilities\n  </Accordion>\n\n  <Accordion title=\"Error handling\">\n    * Use specific exception types\n    * Don't expose sensitive information in error messages\n    * Implement proper resource cleanup\n  </Accordion>\n\n  <Accordion title=\"Dependencies\">\n    * Avoid adding hard dependencies\n    * Keep optional dependencies minimal\n    * Review third-party packages for security issues\n  </Accordion>\n</AccordionGroup>\n\n***\n\n## Development environment\n\n<Warning>\n  Our JS/TS projects uses [`pnpm`](https://pnpm.io/) for dependency management. Make sure you have the latest version installed, or run `corepack enable` (on Node 24+) to setup the required pnpm version.\n</Warning>\n\nOnce you've reviewed the [contribution guidelines](#contribution-guidelines), set up a development environment for the package(s) you're working on.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <AccordionGroup>\n      <Accordion title=\"Core abstractions\">\n        For changes to `langchain-core`:\n\n        ```bash  theme={null}\n        pnpm install\n        # Run tests for a package from the package directory\n        cd libs/langchain-core\n        pnpm test\n\n        # Or run tests for a package from the root of the repo\n        pnpm --filter @langchain/core test\n\n        ```\n      </Accordion>\n\n      <Accordion title=\"Main package\">\n        For changes to `langchain`:\n\n        ```bash  theme={null}\n        pnpm install\n        pnpm --filter langchain test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Provider packages\">\n        For changes to [partner integrations](/oss/javascript/integrations/providers/overview):\n\n        ```bash  theme={null}\n        pnpm install\n        pnpm --filter @langchain/{provider} test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Community packages\">\n        For changes to [community integrations](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-community):\n\n        ```bash  theme={null}\n        pnpm install\n        pnpm --filter @langchain/community test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Repository structure\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    LangChain is organized as a monorepo with multiple packages:\n\n    <AccordionGroup>\n      <Accordion title=\"Core packages\" defaultOpen>\n        * **[`langchain`](https://github.com/langchain-ai/langchainjs/tree/main/langchain#readme)** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic\n        * **[`@langchain/core`](https://github.com/langchain-ai/langchainjs/tree/main/langchain-core#readme)** (located in `libs/langchain-core/`): Base interfaces and core abstractions\n      </Accordion>\n\n      <Accordion title=\"Partner packages\">\n        Located in `libs/providers/`, these are independently versioned packages for specific integrations. For example:\n\n        * **[`@langchain/openai`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-openai#readme)**: [OpenAI](/oss/javascript/integrations/providers/openai) integrations\n        * **[`@langchain/anthropic`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-anthropic#readme)**: [Anthropic](/oss/javascript/integrations/providers/anthropic) integrations\n        * **[`@langchain/google-genai`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-google-genai#readme)**: [Google Generative AI](/oss/javascript/integrations/chat/google_generative_ai) integrations\n      </Accordion>\n\n      <Accordion title=\"Supporting packages\">\n        * **[`@langchain/textsplitters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-textsplitters#readme)**: Text splitting utilities\n        * **[`@langchain/standard-tests`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-standard-tests#readme)**: Standard test suites for integrations\n        * **[`langchain-community`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-community)**: Community maintained integrations\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Development workflow\n\n### Testing requirements\n\n<Info>\n  Directories are relative to the package you're working in.\n</Info>\n\nEvery code change must include comprehensive tests.\n\n#### Unit tests\n\n**Location**: `src/tests/FILENAME_BEING_TESTED.test.ts`\n\n**Requirements**:\n\n* No network calls allowed\n* Test all code paths including edge cases\n* Use mocks for external dependencies\n\n```bash  theme={null}\n# Run the entire test suite\npnpm test\n\n# Or run a specific test file\npnpm test src/tests/FILENAME_BEING_TESTED.test.ts\n\n# Or run a specific test function\npnpm test -t \"the test that should be run\"\n```\n\n#### Integration tests\n\n**Location**: `src/tests/FILENAME_BEING_TESTED.int.test.ts`\n\nIntegration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.\n\nNot every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.\n\n**Requirements**:\n\n* Test real integrations with external services\n* Use environment variables for API keys\n* Skip gracefully if credentials unavailable\n\n```bash  theme={null}\npnpm test:int\n```\n\n### Code quality standards\n\nContributions must adhere to the following quality requirements:\n\n<Tabs>\n  <Tab title=\"Type hints\">\n    **Required**: Complete types for all functions\n\n    ```typescript  theme={null}\n    function processDocuments(\n        docs: Document[],\n        processor: DocumentProcessor,\n        batchSize: number = 100\n    ): ProcessingResult {\n        // ...\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"Documentation\">\n    **Required**: [JSDocs](https://jsdoc.app/about-getting-started) for all exported functions and interfaces\n\n    ```typescript  theme={null}\n    /**\n     * Document processing instance.\n     */\n    interface FooDocumentProcessor {\n        /**\n         * Process documents in batches.\n         *\n         * @param docs - List of documents to process.\n         * @returns Processing results with success/failure counts.\n         */\n        process(docs: Document[]): ProcessingResult;\n    }\n\n    /**\n     * Process documents in batches.\n     *\n     * @param docs - List of documents to process.\n     * @param processor - Document processing instance.\n     * @param batchSize - Number of documents per batch.\n     * @returns Processing results with success/failure counts.\n     */\n    export function processDocuments(\n        docs: Document[],\n        processor: DocumentProcessor,\n        batchSize: number = 100\n    ): ProcessingResult {\n        // ...\n    }\n    ```\n\n    * Document all parameters and return values\n    * Include usage examples for complex functions\n    * Document raised exceptions\n    * Focus on \"why\" rather than \"what\"\n  </Tab>\n\n  <Tab title=\"Code style\">\n    **Automated**: Formatting and linting:\n\n    ```bash  theme={null}\n    pnpm lint    # Check style and types\n    pnpm format  # Apply formatting\n    ```\n\n    **Standards**:\n\n    * Descriptive variable names\n    * Break up complex functions (aim for fewer than 20 lines)\n    * Follow existing patterns in the codebase\n  </Tab>\n</Tabs>\n\n***\n\n## Testing and validation\n\n### Running tests locally\n\nBefore submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <Steps>\n      <Step title=\"Unit tests\">\n        ```bash  theme={null}\n        pnpm test\n        ```\n\n        All unit tests must pass\n      </Step>\n\n      <Step title=\"Integration tests\">\n        ```bash  theme={null}\n        pnpm test:int\n        ```\n\n        (Run if your changes affect integrations)\n      </Step>\n\n      <Step title=\"Formatting\">\n        ```bash  theme={null}\n        pnpm format\n        pnpm lint\n        ```\n\n        Code must pass all style checks\n      </Step>\n\n      <Step title=\"PR submission\">\n        Push your branch and open a pull request. Follow the provided form template. Note related issues using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.\n      </Step>\n    </Steps>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n### Test writing guidelines\n\nIn order to write effective tests, there's a few good practices to follow:\n\n* Encapsulate the test in a `describe` block that describes the component being tested\n* Use natural language to describe the test name\n* Be exhaustive with assertions\n* Only use snapshots for reasonably sized data objects\n\n<Tabs>\n  <Tab title=\"Unit tests\">\n    ```typescript  theme={null}\n    describe(\"DocumentProcessor\", () => {\n        it(\"Should handle empty document list\", () => {\n            const processor = new DocumentProcessor();\n            const result = processor.process([]);\n\n            expect(result.success).toBe(true);\n            expect(result.processedCount).toBe(0);\n            expect(result.errors).toHaveLength(0);\n        });\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"Integration tests\">\n    ```typescript  theme={null}\n    describe(\"ChatOpenAI\", () => {\n        it(\"Should test with real API\", () => {\n            const chat = new ChatOpenAI();\n            const response = chat.invoke(\"Hello\");\n        });\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"Mock usage\">\n    ```typescript  theme={null}\n    describe(\"APIService\", () => {\n        it(\"Should call with retry\", () => {\n            const mockClient = new MockClient();\n            const service = new APIService(client: mockClient);\n            const result = service.callWithRetry();\n        });\n    });\n    ```\n  </Tab>\n</Tabs>\n\n## Getting help\n\nOur goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).\n\n<Check>\n  You're now ready to contribute high-quality code to LangChain!\n</Check>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Implement a LangChain integration\nLink: https://docs.langchain.comoss/javascript/contributing/implement-langchain\nContent: Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.\n\nLangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/javascript/integrations/chat), [tools](/oss/javascript/integrations/tools), [retrievers](/oss/javascript/integrations/retrievers), and more.\n\nYour integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.\n\n<Tabs>\n  <Tab title=\"Chat Models\">\n    Chat models are subclasses of the [`BaseChatModel`](https://reference.langchain.com/javascript/classes/_langchain_core.language_models_chat_models.BaseChatModel.html) class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.\n\n    <Warning>\n      The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/javascript/langchain/models) for details on how LangChain chat models function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Tools\">\n    Tools are used in 2 main ways:\n\n    1. To define an \"input schema\" or \"args schema\" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a \"tool call\", or parameters to call the tool with.\n    2. To take a \"tool call\" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage.\n\n    The Tools class must inherit from the [`BaseTool`](https://reference.langchain.com/javascript/classes/_langchain_core.tools.StructuredTool.html) base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.\n\n    <Warning>\n      The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/javascript/langchain/tools) for details on how LangChain tools function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Retrievers\">\n    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.\n\n    <Warning>\n      The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/javascript/integrations/retrievers) for details on how LangChain retrievers function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Vector Stores\">\n    All vector stores must inherit from the [`VectorStore`](https://reference.langchain.com/javascript/classes/_langchain_core.vectorstores.VectorStore.html) base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.\n\n    See the [vector store integration guide](/oss/javascript/integrations/vectorstores) for details on implementing a vector store integration.\n\n    <Warning>\n      The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/javascript/integrations/vectorstores) for details on how LangChain vector stores function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Embeddings\">\n    Embedding models are subclasses of the [`Embeddings`](https://reference.langchain.com/javascript/classes/_langchain_core.embeddings.Embeddings.html) class.\n\n    <Warning>\n      The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/javascript/integrations/text_embedding) for details on how LangChain embedding models function.\n    </Warning>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: OpenAIEmbeddings\nLink: https://docs.langchain.comoss/javascript/integrations/text_embedding/openai\nContent: This will help you get started with OpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `OpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n| Class                                                                                           | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/openai/) |                                             Downloads                                             |                                             Version                                            |\n| :---------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |\n| [OpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   \u274c   |                                          \u2705                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\\&label=%20&) |\n\n## Setup\n\nTo access OpenAIEmbeddings embedding models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.\n\n### Credentials\n\nHead to [platform.openai.com](https://platform.openai.com) to sign up to OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:\n\n```bash  theme={null}\nexport OPENAI_API_KEY=\"your-api-key\"\n```\n\nIf you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:\n\n```bash  theme={null}\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n### Installation\n\nThe LangChain OpenAIEmbeddings integration lives in the `@langchain/openai` package:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/openai @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/openai @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/openai @langchain/core\n  ```\n</CodeGroup>\n\n## Instantiation\n\nNow we can instantiate our model object and generate chat completions:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n  batchSize: 512, // Default value if omitted is 512. Max is 2048\n  model: \"text-embedding-3-large\",\n});\n```\n\nIf you're part of an organization, you can set `process.env.OPENAI_ORGANIZATION` to your OpenAI organization id, or pass it in as `organization` when\ninitializing the model.\n\n## Indexing and Retrieval\n\nEmbedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [**Learn** tab](/oss/javascript/learn/).\n\nBelow, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/oss/javascript/integrations/vectorstores/memory).\n\n```typescript  theme={null}\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n```output  theme={null}\nLangChain is the framework for building context-aware reasoning applications\n```\n\n## Direct Usage\n\nUnder the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.\n\nYou can directly call these methods to get embeddings for your own use cases.\n\n### Embed single texts\n\nYou can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:\n\n```typescript  theme={null}\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n```output  theme={null}\n[\n    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,\n   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,\n    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,\n   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,\n   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,\n   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,\n    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,\n   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,\n  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,\n  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,\n  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,\n    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,\n    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,\n   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,\n    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,\n    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,\n     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,\n   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,\n   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,\n     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397\n]\n```\n\n### Embed multiple texts\n\nYou can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:\n\n```typescript  theme={null}\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n```output  theme={null}\n[\n    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,\n   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,\n    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,\n   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,\n   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,\n   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,\n    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,\n   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,\n  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,\n  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,\n  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,\n    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,\n    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,\n   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,\n    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,\n    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,\n     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,\n   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,\n   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,\n     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397\n]\n[\n   -0.010181213,   0.023419594,   -0.04215527, -0.0015320902,  -0.023573855,\n  -0.0091644935,  -0.014893179,   0.019016149,  -0.023475688,  0.0010219777,\n    0.009255648,    0.03996757,   -0.04366983,   -0.01640774,  -0.020194141,\n    0.019408813,  -0.027977299,  -0.022017224,   0.013539891,  -0.007769135,\n    0.032647192,  -0.015089511,  -0.022900717,   0.023798235,   0.026084099,\n   -0.024625633,   0.035003178,  -0.017978394,  -0.049615882,   0.013364594,\n    0.031132633,   0.019142363,   0.023195215,  -0.038396914,   0.005584942,\n   -0.031946007,   0.053682756, -0.0036356465,   0.011240003,  0.0056690844,\n  -0.0062791156,   0.044146635,  -0.037387207,    0.01300699,   0.018946031,\n   0.0050415234,   0.029618073,  -0.021750772,  -0.000649473, 0.00026951815,\n   -0.014710871,  -0.029814405,    0.04204308,  -0.014710871,  0.0039616977,\n   -0.021512369,   0.054608323,   0.021484323,    0.02790718,  -0.010573876,\n   -0.023952495,  -0.035143413,  -0.048802506, -0.0075798146,   0.023279356,\n   -0.022690361,  -0.016590048,  0.0060477243,   0.014100839,   0.005476258,\n   -0.017221114, -0.0100059165,  -0.017922299,  -0.021989176,    0.01830094,\n     0.05516927,   0.001033372,  0.0017310516,   -0.00960624,  -0.037864015,\n    0.013063084,   0.006591143,  -0.010160177,  0.0011394264,    0.04953174,\n    0.004806626,   0.029421741,  -0.037751824,   0.003618117,   0.007162609,\n    0.027696826, -0.0021070621,  -0.024485396, -0.0042141243,   -0.02801937,\n   -0.019605145,   0.016281527,  -0.035143413,    0.01640774,   0.042323552\n]\n```\n\n## Specifying dimensions\n\nWith the `text-embedding-3` class of models, you can specify the size of the embeddings you want returned. For example by default `text-embedding-3-large` returns embeddings of dimension 3072:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddingsDefaultDimensions = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n});\n\nconst vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments([\"some text\"]);\nconsole.log(vectorsDefaultDimensions[0].length);\n```\n\n```output  theme={null}\n3072\n```\n\nBut by passing in `dimensions: 1024` we can reduce the size of our embeddings to 1024:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings1024 = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n  dimensions: 1024,\n});\n\nconst vectors1024 = await embeddings1024.embedDocuments([\"some text\"]);\nconsole.log(vectors1024[0].length);\n```\n\n```output  theme={null}\n1024\n```\n\n## Custom URLs\n\nYou can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst model = new OpenAIEmbeddings({\n  configuration: {\n    baseURL: \"https://your_custom_url.com\",\n  },\n});\n```\n\nYou can also pass other `ClientOptions` parameters accepted by the official SDK.\n\nIf you are hosting on Azure OpenAI, see the [dedicated page instead](/oss/javascript/integrations/text_embedding/azure_openai).\n\n***\n\n## API reference\n\nFor detailed documentation of all OpenAIEmbeddings features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/openai.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Contributing to code\nLink: https://docs.langchain.comoss/python/contributing/code\nContent: Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.\n\n## Getting started\n\n<Note>\n  Before submitting large **new features or refactors**, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work.\n\n  This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. See the quickstart guide below.\n</Note>\n\n### Quick fix: submit a bugfix\n\nFor simple bugfixes, you can get started immediately:\n\n<Steps>\n  <Step title=\"Reproduce the issue\">\n    Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification\n  </Step>\n\n  <Step title=\"Fork the repository\">\n    Fork the [LangChain](https://github.com/langchain-ai/langchain) or [LangGraph](https://github.com/langchain-ai/langgraph) repo to your <Tooltip tip=\"If you fork to an organization account, maintainers will be unable to make edits.\">personal GitHub account</Tooltip>\n  </Step>\n\n  <Step title=\"Clone and setup\">\n    ```bash  theme={null}\n    git clone https://github.com/your-username/name-of-forked-repo.git\n\n    # For instance, for LangChain:\n    git clone https://github.com/parrot123/langchain.git\n\n    # For LangGraph:\n    git clone https://github.com/parrot123/langgraph.git\n    ```\n\n    ```bash  theme={null}\n    # Inside your repo, install dependencies\n    uv sync --all-groups\n    ```\n\n    You will need to install [`uv`](https://docs.astral.sh/uv/) if you haven't previously\n  </Step>\n\n  <Step title=\"Create a branch\">\n    Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later.\n\n    ```bash  theme={null}\n    git checkout -b your-username/short-bugfix-name\n    ```\n  </Step>\n\n  <Step title=\"Write failing tests\">\n    Add [unit tests](#test-writing-guidelines) that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions\n  </Step>\n\n  <Step title=\"Make your changes\">\n    Fix the bug while following our [code quality standards](#code-quality-standards). Make the **minimal change necessary** to resolve the issue\n  </Step>\n\n  <Step title=\"Verify the fix\">\n    Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR\n\n    ```bash  theme={null}\n    make lint\n    make test\n\n    # For bugfixes involving integrations, also run:\n    make integration_tests\n    # (You may need to set up API testing credentials)\n    ```\n  </Step>\n\n  <Step title=\"Document the change\">\n    Update docstrings if behavior changes, add comments for complex logic\n  </Step>\n\n  <Step title=\"Submit a pull request\">\n    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #ISSUE_NUMBER`) so that the issue is automatically closed when your PR is merged.\n  </Step>\n</Steps>\n\n### Full development setup\n\nFor ongoing development or larger contributions:\n\n1. Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations\n2. Set up your environment following our [setup guide](#development-environment) below\n3. Understand the [repository structure](#repository-structure) and package organization\n4. Learn our [development workflow](#development-workflow) including testing and linting\n\n***\n\n## Contribution guidelines\n\nBefore you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a \"first contribution\" to your resume (or if you're just looking for a quick win) you might be better off doing a boot-camp or an online tutorial.\n\nContributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it's important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you're willing to take the time to do things well.\n\n### Backwards compatibility\n\n<Warning>\n  Breaking changes to public APIs are not allowed except for critical security fixes.\n\n  See our [versioning policy](/oss/python/versioning) for details on major version releases.\n</Warning>\n\nMaintain compatibility via:\n\n<AccordionGroup>\n  <Accordion title=\"Stable interfaces\">\n    **Always preserve**:\n\n    * Function signatures and parameter names\n    * Class interfaces and method names\n    * Return value structure and types\n    * Import paths for public APIs\n  </Accordion>\n\n  <Accordion title=\"Safe changes\">\n    **Acceptable modifications**:\n\n    * Adding new optional parameters\n\n    * Adding new methods to classes\n\n    * Improving performance without changing behavior\n\n    * Adding new modules or functions\n  </Accordion>\n\n  <Accordion title=\"Before making changes\">\n    * **Would this break existing user code?**\n\n    * Check if your target is public\n\n    * If needed, is it exported in `__init__.py`?\n\n    * Are there existing usage patterns in tests?\n  </Accordion>\n</AccordionGroup>\n\n### New features\n\nWe aim to keep the bar high for new features. We generally don't accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies.\n\nIn general, feature contribution requirements include:\n\n<Steps>\n  <Step title=\"Design discussion\">\n    Open an issue describing:\n\n    * The problem you're solving\n    * Proposed API design\n    * Expected usage patterns\n  </Step>\n\n  <Step title=\"Implementation\">\n    * Follow existing code patterns\n    * Include comprehensive tests and documentation\n    * Consider security implications\n  </Step>\n\n  <Step title=\"Integration considerations\">\n    * How does this interact with existing features?\n    * Are there performance implications?\n    * Does this introduce new dependencies?\n\n    We will reject features that are likely to lead to security vulnerabilities or reports.\n  </Step>\n</Steps>\n\n### Security guidelines\n\n<Warning>\n  Security is paramount. Never introduce vulnerabilities or unsafe patterns.\n</Warning>\n\nSecurity checklist:\n\n<AccordionGroup>\n  <Accordion title=\"Input validation\">\n    * Validate and sanitize all user inputs\n    * Properly escape data in templates and queries\n    * Never use `eval()`, `exec()`, or `pickle` on user data, as this can lead to arbitrary code execution vulnerabilities\n  </Accordion>\n\n  <Accordion title=\"Error handling\">\n    * Use specific exception types\n    * Don't expose sensitive information in error messages\n    * Implement proper resource cleanup\n  </Accordion>\n\n  <Accordion title=\"Dependencies\">\n    * Avoid adding hard dependencies\n    * Keep optional dependencies minimal\n    * Review third-party packages for security issues\n  </Accordion>\n</AccordionGroup>\n\n***\n\n## Development environment\n\n<Warning>\n  Our Python projects use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management. Make sure you have the latest version installed.\n</Warning>\n\nOnce you've reviewed the [contribution guidelines](#contribution-guidelines), set up a development environment for the package(s) you're working on.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <AccordionGroup>\n      <Accordion title=\"Core abstractions\">\n        For changes to `langchain-core`:\n\n        ```bash  theme={null}\n        cd libs/core\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Main package\">\n        For changes to `langchain`:\n\n        ```bash  theme={null}\n        cd libs/langchain\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Partner packages\">\n        For changes to [partner integrations](/oss/python/integrations/providers/overview):\n\n        ```bash  theme={null}\n        cd libs/partners/langchain-{partner}\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Community packages\">\n        For changes to community integrations (located in a [separate repo](https://github.com/langchain-ai/langchain-community)):\n\n        ```bash  theme={null}\n        cd libs/community/langchain_community/path/to/integration\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Repository structure\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    LangChain is organized as a monorepo with multiple packages:\n\n    <AccordionGroup>\n      <Accordion title=\"Core packages\" defaultOpen>\n        * **[`langchain`](https://github.com/langchain-ai/langchain/tree/master/libs/langchain#readme)** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic\n        * **[`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core#readme)** (located in `libs/core/`): Base interfaces and core abstractions\n      </Accordion>\n\n      <Accordion title=\"Partner packages\">\n        Located in `libs/partners/`, these are independently versioned packages for specific integrations. For example:\n\n        * **[`langchain-openai`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai#readme)**: [OpenAI](/oss/python/integrations/providers/openai) integrations\n        * **[`langchain-anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic#readme)**: [Anthropic](/oss/python/integrations/providers/anthropic) integrations\n        * **[`langchain-google-genai`](https://github.com/langchain-ai/langchain-google/)**: [Google Generative AI](/oss/python/integrations/chat/google_generative_ai) integrations\n\n        Many partner packages are in external repositories. Please check the [list of integrations](/oss/python/integrations/providers/overview) for details.\n      </Accordion>\n\n      <Accordion title=\"Supporting packages\">\n        * **[`langchain-text-splitters`](https://github.com/langchain-ai/langchain/tree/master/libs/text-splitters#readme)**: Text splitting utilities\n        * **[`langchain-standard-tests`](https://github.com/langchain-ai/langchain/tree/master/libs/standard-tests#readme)**: Standard test suites for integrations\n        * **[`langchain-cli`](https://github.com/langchain-ai/langchain/tree/master/libs/cli#readme)**: Command line interface\n        * **[`langchain-community`](https://github.com/langchain-ai/langchain-community)**: Community maintained integrations (located in a separate repo)\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Development workflow\n\n### Testing requirements\n\n<Info>\n  Directories are relative to the package you're working in.\n</Info>\n\nEvery code change must include comprehensive tests.\n\n#### Unit tests\n\n**Location**: `tests/unit_tests/`\n\n**Requirements**:\n\n* No network calls allowed\n* Test all code paths including edge cases\n* Use mocks for external dependencies\n\n```bash  theme={null}\nmake test\n\n# Or directly:\nuv run --group test pytest tests/unit_tests\n```\n\n#### Integration tests\n\n**Location**: `tests/integration_tests/`\n\nIntegration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.\n\nNot every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.\n\n**Requirements**:\n\n* Test real integrations with external services\n* Use environment variables for API keys\n* Skip gracefully if credentials unavailable\n\n```bash  theme={null}\nmake integration_tests\n```\n\n### Code quality standards\n\nContributions must adhere to the following quality requirements:\n\n<Tabs>\n  <Tab title=\"Type hints\">\n    **Required**: Complete type annotations for all functions\n\n    ```python  theme={null}\n    def process_documents(\n        docs: list[Document],\n        processor: DocumentProcessor,\n        *,\n        batch_size: int = 100\n    ) -> ProcessingResult:\n        \"\"\"Process documents in batches.\n\n        Args:\n            docs: List of documents to process.\n            processor: Document processing instance.\n            batch_size: Number of documents per batch.\n\n        Returns:\n            Processing results with success/failure counts.\n        \"\"\"\n    ```\n  </Tab>\n\n  <Tab title=\"Documentation\">\n    **Required**: [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) for all public functions\n\n    * Document all parameters and return values\n    * Include usage examples for complex functions\n    * Document raised exceptions\n    * Focus on \"why\" rather than \"what\"\n  </Tab>\n\n  <Tab title=\"Code style\">\n    **Automated**: Formatting and linting via [`ruff`](https://docs.astral.sh/ruff/)\n\n    ```bash  theme={null}\n    make format  # Apply formatting\n    make lint    # Check style and types\n    ```\n\n    **Standards**:\n\n    * Descriptive variable names\n    * Break up complex functions (aim for fewer than 20 lines)\n    * Follow existing patterns in the codebase\n  </Tab>\n</Tabs>\n\n***\n\n## Testing and validation\n\n### Running tests locally\n\nBefore submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <Steps>\n      <Step title=\"Unit tests\">\n        ```bash  theme={null}\n        make test\n        ```\n\n        All unit tests must pass\n      </Step>\n\n      <Step title=\"Integration tests\">\n        ```bash  theme={null}\n        make integration_tests\n        ```\n\n        (Run if your changes affect integrations)\n      </Step>\n\n      <Step title=\"Formatting\">\n        ```bash  theme={null}\n        make format\n        make lint\n        ```\n\n        Code must pass all style checks\n      </Step>\n\n      <Step title=\"Type checking\">\n        ```bash  theme={null}\n        make type_check\n        ```\n\n        All type hints must be valid\n      </Step>\n\n      <Step title=\"PR submission\">\n        Push your branch and open a pull request. Follow the provided form template. Note related issues using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.\n      </Step>\n    </Steps>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n### Test writing guidelines\n\nIn order to write effective tests, there's a few good practices to follow:\n\n* Use natural language to describe the test in docstrings\n* Use descriptive variable names\n* Be exhaustive with assertions\n\n<Tabs>\n  <Tab title=\"Unit tests\">\n    ```python  theme={null}\n    def test_document_processor_handles_empty_input():\n        \"\"\"Test processor gracefully handles empty document list.\"\"\"\n        processor = DocumentProcessor()\n\n        result = processor.process([])\n\n        assert result.success\n        assert result.processed_count == 0\n        assert len(result.errors) == 0\n    ```\n  </Tab>\n\n  <Tab title=\"Integration tests\">\n    ```python  theme={null}\n    @pytest.mark.requires(\"openai\")\n    def test_openai_chat_integration():\n        \"\"\"Test OpenAI chat integration with real API.\"\"\"\n\n        chat = ChatOpenAI()\n        response = chat.invoke(\"Hello\")\n\n        assert isinstance(response.content, str)\n        assert len(response.content) > 0\n    ```\n  </Tab>\n\n  <Tab title=\"Mock usage\">\n    ```python  theme={null}\n    def test_retry_mechanism(mocker):\n        \"\"\"Test retry mechanism handles transient failures.\"\"\"\n        mock_client = mocker.Mock()\n        mock_client.call.side_effect = [\n            ConnectionError(\"Temporary failure\"),\n            {\"result\": \"success\"}\n        ]\n\n        service = APIService(client=mock_client)\n        result = service.call_with_retry()\n\n        assert result[\"result\"] == \"success\"\n        assert mock_client.call.call_count == 2\n    ```\n  </Tab>\n</Tabs>\n\n## Getting help\n\nOur goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).\n\n<Check>\n  You're now ready to contribute high-quality code to LangChain!\n</Check>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Implement a LangChain integration\nLink: https://docs.langchain.comoss/python/contributing/implement-langchain\nContent: Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.\n\nLangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/python/integrations/chat), [tools](/oss/python/integrations/tools), [retrievers](/oss/python/integrations/retrievers), and more.\n\nYour integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.\n\n<Tabs>\n  <Tab title=\"Chat Models\">\n    Chat models are subclasses of the [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel) class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.\n\n    <Warning>\n      The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/python/langchain/models) for details on how LangChain chat models function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Tools\">\n    Tools are used in 2 main ways:\n\n    1. To define an \"input schema\" or \"args schema\" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a \"tool call\", or parameters to call the tool with.\n    2. To take a \"tool call\" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage.\n\n    The Tools class must inherit from the [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.\n\n    <Warning>\n      The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/python/langchain/tools) for details on how LangChain tools function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Retrievers\">\n    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.\n\n    <Warning>\n      The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/python/integrations/retrievers) for details on how LangChain retrievers function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Vector Stores\">\n    All vector stores must inherit from the [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.\n\n    See the [vector store integration guide](/oss/python/integrations/vectorstores) for details on implementing a vector store integration.\n\n    <Warning>\n      The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/python/integrations/vectorstores) for details on how LangChain vector stores function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Embeddings\">\n    Embedding models are subclasses of the [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) class.\n\n    <Warning>\n      The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/python/integrations/text_embedding) for details on how LangChain embedding models function.\n    </Warning>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_16328678",
    "id": "5db866f5-09c7-4df1-96d1-f4d9038fbee2",
    "content": "{\n  \"collections\": \"all\",\n  \"total\": 73,\n  \"articles\": [\n    {\n      \"id\": \"7a5d1732-c25b-4a26-9686-52ddfd17926e\",\n      \"title\": \"Merging LangSmith Runs from Remote Graph\",\n      \"url\": \"https://support.langchain.com/articles/9499391881-merging-langsmith-runs-from-remote-graph\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"796cd8bc-a7d9-419d-8141-83998b267e90\",\n      \"title\": \"Resolving common issues when upgrading LangSmith Helm chart from 0.11.x to 0.12.x?\",\n      \"url\": \"https://support.langchain.com/articles/5255204348-resolving-common-issues-when-upgrading-langsmith-helm-chart-from-0-11-x-to-0-12-x\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"adeafe65-c1b6-4620-b3aa-044873fd8c9c\",\n      \"title\": \"Why are my traces being automatically upgraded to extended retention?\",\n      \"url\": \"https://support.langchain.com/articles/3206725817-why-are-my-traces-being-automatically-upgraded-to-extended-retention\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"73778fa5-7613-4254-b687-7b0288ac9903\",\n      \"title\": \"Why am I getting 429 Rate Limit Errors during load testing on self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8364157755-why-am-i-getting-429-rate-limit-errors-during-load-testing-on-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"27a421b4-e877-4a0f-8f38-7b719b189785\",\n      \"title\": \"PagerDuty Alert Severity Not Respecting Configuration\",\n      \"url\": \"https://support.langchain.com/articles/8222693126-pagerduty-alert-severity-not-respecting-configuration\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"adf84afd-7d07-4e11-8645-a3c9a1686f08\",\n      \"title\": \"Session Timeout Configuration for LangSmith Self-Hosted\",\n      \"url\": \"https://support.langchain.com/articles/9515017706-session-timeout-configuration-for-langsmith-self-hosted\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2fcf3091-0038-4226-b56f-4559d6cb1dbc\",\n      \"title\": \"Cleaning ClickHouse System Log Tables\",\n      \"url\": \"https://support.langchain.com/articles/9482219057-cleaning-clickhouse-system-log-tables\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"065735b3-432b-41ce-8469-1494cb753727\",\n      \"title\": \"Why are my agent health checks failing with GCP Gateway API on GKE?\",\n      \"url\": \"https://support.langchain.com/articles/3603461301-why-are-my-agent-health-checks-failing-with-gcp-gateway-api-on-gke\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f9775b3-e85d-4368-ad1b-21f9a8f7552e\",\n      \"title\": \"Evaluator Creation with LiteLLM Proxy - Missing API Key Error\",\n      \"url\": \"https://support.langchain.com/articles/2378929317-evaluator-creation-with-litellm-proxy-missing-api-key-error\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3a81a902-1bca-463d-ab2d-abefd38f8721\",\n      \"title\": \"Why did my evaluator token usage and costs increase after upgrading LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/7643067789-why-did-my-evaluator-token-usage-and-costs-increase-after-upgrading-langsmith\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"61f0452e-5198-4524-9c94-a9cf476ea5bc\",\n      \"title\": \"How to Create a Locked-Down Deployment with Restricted Access\",\n      \"url\": \"https://support.langchain.com/articles/5382256129-how-to-create-a-locked-down-deployment-with-restricted-access\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"c0615bd9-c4a2-4a54-af03-0ee07470ee43\",\n      \"title\": \"How do I configure LangSmith pricing match patterns for AWS Bedrock inference profiles?\",\n      \"url\": \"https://support.langchain.com/articles/4241511543-how-do-i-configure-langsmith-pricing-match-patterns-for-aws-bedrock-inference-profiles\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"365a0d3f-d784-407d-b741-fe51fa8356d4\",\n      \"title\": \"LangSmith Deployments not working with Valkey\",\n      \"url\": \"https://support.langchain.com/articles/2265174426-why-are-my-langgraph-requests-hanging-when-using-aws-elasticache-with-valkey\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"818454c3-a656-4811-a7ca-56cb13df2355\",\n      \"title\": \"How do I enable KMS encryption for S3 storage in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9381186430-how-do-i-enable-kms-encryption-for-s3-storage-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a3c79a7d-390f-43e6-8234-8b55a7f191dd\",\n      \"title\": \"Streaming Failures When Using Shared Custom Redis Instances\",\n      \"url\": \"https://support.langchain.com/articles/3842219806-streaming-failures-when-using-shared-custom-redis-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"55a2c6fe-adf8-4b55-bcb9-969b465241b7\",\n      \"title\": \"Why are my agent threads getting deleted automatically?\",\n      \"url\": \"https://support.langchain.com/articles/7993792111-why-are-my-threads-being-marked-as-temporary-and-getting-deleted-automatically\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b4ba8c34-6d1d-4653-8123-c2eeee2d34e5\",\n      \"title\": \"How do I set up ABAC to restrict user access to specific projects?\",\n      \"url\": \"https://support.langchain.com/articles/6539560119-how-do-i-set-up-abac-to-restrict-user-access-to-specific-projects\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b42f572e-0490-4bb8-9c81-9f065f0c83ed\",\n      \"title\": \"Productionizing and Scaling Self-Hosted LangSmith - Best Practices\",\n      \"url\": \"https://support.langchain.com/articles/8478795211-productionizing-and-scaling-self-hosted-langsmith-best-practices\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"d270716f-edc0-49d9-b5d1-497b5d0e4e6d\",\n      \"title\": \"Private Network Access Blocking LangSmith Studio Connection\",\n      \"url\": \"https://support.langchain.com/articles/1897181200-private-network-access-blocking-langsmith-studio-connection\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"b20fd0bf-1ae9-445c-86ba-5a223112756f\",\n      \"title\": \"How to setup private container registry authentication for LangGraph agent deployments\",\n      \"url\": \"https://support.langchain.com/articles/8606755009-how-to-setup-private-container-registry-authentication-for-langgraph-cloud-agent-deployments\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f6f6fa0-610c-4a15-a4b9-987e932f6a3a\",\n      \"title\": \"How to setup path-based routing for self-hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/2352997668-how-to-setup-path-based-routing-for-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"cc4be529-34c6-458b-8bb3-621fab0e2ab4\",\n      \"title\": \"Best practices for scaling Self-Hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/5142502915-best-practices-for-scaling-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3cec875c-3c8d-4951-b83b-0ac5b8ba7c7c\",\n      \"title\": \"LangSmith Deployments agent is running, but the UI shows it as \\u201cUnsuccessful\\u201d\",\n      \"url\": \"https://support.langchain.com/articles/8394994213-langsmith-deployments-agent-is-running-but-the-ui-shows-it-as-unsuccessful\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"57055dee-474d-4635-93a3-79439570f6ac\",\n      \"title\": \"How do I prevent traces from mixing between projects during concurrent requests?\",\n      \"url\": \"https://support.langchain.com/articles/4784014353-how-do-i-prevent-traces-from-mixing-between-projects-during-concurrent-requests\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9619ff10-34d0-49ea-a7ee-cdbbddafc1e4\",\n      \"title\": \"How do I resolve \\\"too many connections\\\" PostgreSQL errors when scaling platform instances?\",\n      \"url\": \"https://support.langchain.com/articles/9651943867-how-do-i-resolve-too-many-connections-postgresql-errors-when-scaling-platform-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"b740be98-677d-486f-ba77-12258712f70a\",\n      \"title\": \"How do I use the LangSmith Control Plane API to manage deployments?\",\n      \"url\": \"https://support.langchain.com/articles/8184800662-how-do-i-use-the-langgraph-platform-control-plane-api-to-manage-deployments\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"52d389ff-3918-46a8-9ccf-55fe972077bf\",\n      \"title\": \"Why is my LangSmith agent queue Deployment timing out after 600 seconds?\",\n      \"url\": \"https://support.langchain.com/articles/7898522976-why-is-my-queue-deployment-timing-out-after-600-seconds\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"1caf7aa7-5382-43f5-869c-27212fe9f287\",\n      \"title\": \"How do I resolve ClickHouse migration errors after upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/6463985661-how-do-i-resolve-clickhouse-migration-errors-after-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0ff2089a-501c-45b8-b557-4947639c5072\",\n      \"title\": \"How do I disable public sharing for traces in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5345880639-how-do-i-disable-public-sharing-for-traces-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\",\n      \"title\": \"What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\",\n      \"url\": \"https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"bef0ce3c-94c2-4999-b563-ff016fa3884f\",\n      \"title\": \"How do I get LangChain to complete security questionnaires for vendor assessments?\",\n      \"url\": \"https://support.langchain.com/articles/8555158040-how-do-i-get-langchain-to-complete-security-questionnaires-for-vendor-assessments\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"fe2b3c9e-8fa1-49dd-a52d-df92d88050c4\",\n      \"title\": \"How do I update billing information on an existing invoice?\",\n      \"url\": \"https://support.langchain.com/articles/5832977338-how-do-i-update-billing-information-on-an-existing-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"6ea47321-cd15-49a7-a972-be33139a5fe3\",\n      \"title\": \"Can I use an external database cluster to host multiple self-hosted LangSmith instances?\",\n      \"url\": \"https://support.langchain.com/articles/5989664339-can-i-use-an-external-database-cluster-to-host-multiple-self-hosted-langsmith-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"4a127c79-3504-4afa-befa-ec974b85c5f8\",\n      \"title\": \"Why am I being charged for node executions and standby minutes on my development deployment?\",\n      \"url\": \"https://support.langchain.com/articles/2626278629-why-am-i-being-charged-for-node-executions-on-my-development-deployment\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"a442595a-454c-4208-8854-d51f632a3d0f\",\n      \"title\": \"Managing ClickHouse Storage\",\n      \"url\": \"https://support.langchain.com/articles/2929899751-managing-clickhouse-storage\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"c6c77d41-4376-4452-b7a0-fd42c1428242\",\n      \"title\": \"How do I deploy an MCP server on LangSmith Deployments with my agent?\",\n      \"url\": \"https://support.langchain.com/articles/8743137454-how-do-i-deploy-an-mcp-server-on-langsmith-deployments-with-my-agent\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"e5b95386-5971-4e48-be5b-9d9c7dcfe8e3\",\n      \"title\": \"How do I archive a prompt instead of deleting it?\",\n      \"url\": \"https://support.langchain.com/articles/6404244010-how-do-i-archive-a-prompt-instead-of-deleting-it\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"7444d2d0-3f90-42d8-aa4c-b36fa7665e05\",\n      \"title\": \"How do I resolve Redis disk space issues in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5370519454-how-do-i-resolve-redis-disk-space-issues-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"1076b041-020f-4738-acd0-d8ecc07f19ed\",\n      \"title\": \"How do I add existing organization members to new workspaces programmatically via API?\",\n      \"url\": \"https://support.langchain.com/articles/5797508505-how-do-i-add-existing-organization-members-to-new-workspaces-programmatically-via-api\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a91a5cf4-a8a3-4ad5-b9ef-9d0d534d0ba4\",\n      \"title\": \"ClickHouse: Drop Old Data\",\n      \"url\": \"https://support.langchain.com/articles/2379935803-clickhouse-drop-old-data\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5886208c-1f87-4672-87c4-053f2773c161\",\n      \"title\": \"How do I filter out traces that contain a specific node name in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8185473222-how-do-i-filter-out-traces-that-contain-a-specific-node-name-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b170d29c-925a-4aab-b7a0-354f47b7dc77\",\n      \"title\": \"Can I enforce Google SSO-only authentication for my organization?\",\n      \"url\": \"https://support.langchain.com/articles/9820106567-can-i-enforce-google-sso-only-authentication-for-my-organization\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b5128a24-178a-4063-9337-c273af5ac617\",\n      \"title\": \"Why is my trace exploration slow in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9719508217-why-is-my-trace-exploration-slow-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"50634017-ff08-43bb-b7ae-7518aab83623\",\n      \"title\": \"How do I get access to LangSmith Agent Builder?\",\n      \"url\": \"https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"4ad14b9c-abe8-47b0-a416-81dbe2a3a127\",\n      \"title\": \"Why are some experiment runs missing when processing large datasets in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/3506757366-why-are-some-experiment-runs-missing-when-processing-large-datasets-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"40305680-01d3-4506-8257-16db939e39bf\",\n      \"title\": \"How do I get help with billing issues and payment problems?\",\n      \"url\": \"https://support.langchain.com/articles/3779255598-how-do-i-get-help-with-billing-issues-and-payment-problems\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"9273a5a9-0a53-4d5a-917a-c19ebeec64fe\",\n      \"title\": \"How do I get a copy of my LangSmith invoice?\",\n      \"url\": \"https://support.langchain.com/articles/6317483716-how-do-i-get-a-copy-of-my-langsmith-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"ebea1a08-4488-4cac-9c65-75442f3de254\",\n      \"title\": \"Adding FastAPI dependency for custom middleware in LangGraph Platform\",\n      \"url\": \"https://support.langchain.com/articles/7593416758-adding-fastapi-dependency-for-custom-middleware-in-langgraph-platform\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"d37491ce-7f3c-4575-91d5-09c62129248e\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/8574277609-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"48541052-b69e-4e78-ae8b-c2ef4fe26af1\",\n      \"title\": \"Resolving LangGraph and LangSmith version compatibility issues in production\",\n      \"url\": \"https://support.langchain.com/articles/5102702588-resolving-langgraph-and-langsmith-version-compatibility-issues-in-production\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0b6ce386-e341-47a1-8be9-ae350de16843\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/3873263156-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"68fbf7da-954d-454e-8ad4-73e1f1f63b74\",\n      \"title\": \"LangSmith Studio vs Agent Chat UI: When to Use What\",\n      \"url\": \"https://support.langchain.com/articles/1353449293-langsmith-studio-vs-agent-chat-ui\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f7dc0a0c-fa3f-4319-b820-ae470d89877f\",\n      \"title\": \"Why is token-by-token streaming not working after upgrading LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/7150806184-why-is-token-by-token-streaming-not-working-after-upgrading-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"ada8fd1b-4800-434e-8d6f-b5186b170941\",\n      \"title\": \"LangSmith Studio URL Redirects to Error Page\",\n      \"url\": \"https://support.langchain.com/articles/4270790436-langsmith-studio-url-redirects-to-error-page\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f41e0fb9-7ed0-4d9b-85b0-fe88fd805f02\",\n      \"title\": \"Why am I getting service interruption errors when using text search filters?\",\n      \"url\": \"https://support.langchain.com/articles/6919527577-why-am-i-getting-service-interruption-errors-when-using-text-search-filters\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\",\n      \"title\": \"Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\",\n      \"url\": \"https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b1e1ee29-1124-4a92-8f9f-eaa4a679df05\",\n      \"title\": \"Why am I getting a parsing token error when upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/7019615393-why-am-i-getting-a-token-parsing-error-when-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"da8e3156-8405-4c12-a347-96d6590da06c\",\n      \"title\": \"LangSmith upgrade fails with SSL certificate verification error when connecting to Azure Cache for Redis\",\n      \"url\": \"https://support.langchain.com/articles/7558740603-langsmith-helm-upgrade-fails-with-ssl-certificate-verification-error-when-connecting-to-azure-cache-for-redis\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"af6b64a9-ed4a-4993-b54d-d4913eeb1d26\",\n      \"title\": \"How do I change my email address in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9223968956-how-do-i-change-my-email-address-in-langsmith\",\n      \"collection\": \"General\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_59279866",
    "id": "5726799d-1e16-4c16-9167-c1c1605fb0b5",
    "content": "{\n  \"collections\": \"General,LangSmith Deployment\",\n  \"total\": 21,\n  \"articles\": [\n    {\n      \"id\": \"61f0452e-5198-4524-9c94-a9cf476ea5bc\",\n      \"title\": \"How to Create a Locked-Down Deployment with Restricted Access\",\n      \"url\": \"https://support.langchain.com/articles/5382256129-how-to-create-a-locked-down-deployment-with-restricted-access\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"365a0d3f-d784-407d-b741-fe51fa8356d4\",\n      \"title\": \"LangSmith Deployments not working with Valkey\",\n      \"url\": \"https://support.langchain.com/articles/2265174426-why-are-my-langgraph-requests-hanging-when-using-aws-elasticache-with-valkey\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"55a2c6fe-adf8-4b55-bcb9-969b465241b7\",\n      \"title\": \"Why are my agent threads getting deleted automatically?\",\n      \"url\": \"https://support.langchain.com/articles/7993792111-why-are-my-threads-being-marked-as-temporary-and-getting-deleted-automatically\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b4ba8c34-6d1d-4653-8123-c2eeee2d34e5\",\n      \"title\": \"How do I set up ABAC to restrict user access to specific projects?\",\n      \"url\": \"https://support.langchain.com/articles/6539560119-how-do-i-set-up-abac-to-restrict-user-access-to-specific-projects\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"3cec875c-3c8d-4951-b83b-0ac5b8ba7c7c\",\n      \"title\": \"LangSmith Deployments agent is running, but the UI shows it as \\u201cUnsuccessful\\u201d\",\n      \"url\": \"https://support.langchain.com/articles/8394994213-langsmith-deployments-agent-is-running-but-the-ui-shows-it-as-unsuccessful\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b740be98-677d-486f-ba77-12258712f70a\",\n      \"title\": \"How do I use the LangSmith Control Plane API to manage deployments?\",\n      \"url\": \"https://support.langchain.com/articles/8184800662-how-do-i-use-the-langgraph-platform-control-plane-api-to-manage-deployments\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"52d389ff-3918-46a8-9ccf-55fe972077bf\",\n      \"title\": \"Why is my LangSmith agent queue Deployment timing out after 600 seconds?\",\n      \"url\": \"https://support.langchain.com/articles/7898522976-why-is-my-queue-deployment-timing-out-after-600-seconds\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\",\n      \"title\": \"What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\",\n      \"url\": \"https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"bef0ce3c-94c2-4999-b563-ff016fa3884f\",\n      \"title\": \"How do I get LangChain to complete security questionnaires for vendor assessments?\",\n      \"url\": \"https://support.langchain.com/articles/8555158040-how-do-i-get-langchain-to-complete-security-questionnaires-for-vendor-assessments\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"fe2b3c9e-8fa1-49dd-a52d-df92d88050c4\",\n      \"title\": \"How do I update billing information on an existing invoice?\",\n      \"url\": \"https://support.langchain.com/articles/5832977338-how-do-i-update-billing-information-on-an-existing-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"4a127c79-3504-4afa-befa-ec974b85c5f8\",\n      \"title\": \"Why am I being charged for node executions and standby minutes on my development deployment?\",\n      \"url\": \"https://support.langchain.com/articles/2626278629-why-am-i-being-charged-for-node-executions-on-my-development-deployment\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"c6c77d41-4376-4452-b7a0-fd42c1428242\",\n      \"title\": \"How do I deploy an MCP server on LangSmith Deployments with my agent?\",\n      \"url\": \"https://support.langchain.com/articles/8743137454-how-do-i-deploy-an-mcp-server-on-langsmith-deployments-with-my-agent\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b170d29c-925a-4aab-b7a0-354f47b7dc77\",\n      \"title\": \"Can I enforce Google SSO-only authentication for my organization?\",\n      \"url\": \"https://support.langchain.com/articles/9820106567-can-i-enforce-google-sso-only-authentication-for-my-organization\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"50634017-ff08-43bb-b7ae-7518aab83623\",\n      \"title\": \"How do I get access to LangSmith Agent Builder?\",\n      \"url\": \"https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"40305680-01d3-4506-8257-16db939e39bf\",\n      \"title\": \"How do I get help with billing issues and payment problems?\",\n      \"url\": \"https://support.langchain.com/articles/3779255598-how-do-i-get-help-with-billing-issues-and-payment-problems\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"9273a5a9-0a53-4d5a-917a-c19ebeec64fe\",\n      \"title\": \"How do I get a copy of my LangSmith invoice?\",\n      \"url\": \"https://support.langchain.com/articles/6317483716-how-do-i-get-a-copy-of-my-langsmith-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"ebea1a08-4488-4cac-9c65-75442f3de254\",\n      \"title\": \"Adding FastAPI dependency for custom middleware in LangGraph Platform\",\n      \"url\": \"https://support.langchain.com/articles/7593416758-adding-fastapi-dependency-for-custom-middleware-in-langgraph-platform\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"d37491ce-7f3c-4575-91d5-09c62129248e\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/8574277609-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"0b6ce386-e341-47a1-8be9-ae350de16843\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/3873263156-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"f7dc0a0c-fa3f-4319-b820-ae470d89877f\",\n      \"title\": \"Why is token-by-token streaming not working after upgrading LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/7150806184-why-is-token-by-token-streaming-not-working-after-upgrading-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"af6b64a9-ed4a-4993-b54d-d4913eeb1d26\",\n      \"title\": \"How do I change my email address in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9223968956-how-do-i-change-my-email-address-in-langsmith\",\n      \"collection\": \"General\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_46309696",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"langsmith\"}"
        }
      },
      {
        "type": "function",
        "id": "call_61849426",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"langgraph\"}"
        }
      },
      {
        "type": "function",
        "id": "call_50387987",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"langchain\"}"
        }
      },
      {
        "type": "function",
        "id": "call_98787148",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"General\"}"
        }
      },
      {
        "type": "function",
        "id": "call_25986849",
        "function": {
          "name": "get_article_content",
          "arguments": "{\"article_id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\"}"
        }
      },
      {
        "type": "function",
        "id": "call_64468550",
        "function": {
          "name": "get_article_content",
          "arguments": "{\"article_id\": \"50634017-ff08-43bb-b7ae-7518aab83623\"}"
        }
      }
    ],
    "id": "lc_run--019b3852-94f5-7a63-bd5d-50ee0546dbfd",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_46309696",
    "id": "cfaf3cef-a72c-4c76-b9a5-0aa2167f10bf",
    "content": "Result 1:\nTitle: Connect to an OpenAI compliant model provider/proxy\nLink: https://docs.langchain.comlangsmith/custom-openai-compliant-model\nContent: The LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for  in the playground.\n\n## Deploy an OpenAI compliant model\n\nMany providers offer OpenAI compliant models or proxy services. Some examples of this include:\n\n* [LiteLLM Proxy](https://github.com/BerriAI/litellm?tab=readme-ov-file#quick-start-proxy---cli)\n* [Ollama](https://ollama.com/)\n\nYou can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API.\n\nTake a look at the full [specification](https://platform.openai.com/docs/api-reference/chat) for more information.\n\n## Use the model in the LangSmith Playground\n\nOnce you have deployed a model server, you can use it in the LangSmith [Playground](/langsmith/prompt-engineering-concepts#prompt-playground).\n\nTo access the **Prompt Settings** menu:\n\n1. Under the **Prompts** heading select the gear <Icon icon=\"gear\" iconType=\"solid\" /> icon next to the model name.\n2. In the **Model Configuration** tab, select the model to edit in the dropdown.\n3. For the **Provider** dropdown, select **OpenAI Compatible Endpoint**.\n4. Add your OpenAI Compatible Endpoint to the **Base URL** input.\n\n   <div style={{ textAlign: 'center' }}>\n     <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=fdbe548e512ed40fb512578d02986b45\" alt=\"Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected.\" data-og-width=\"897\" width=\"897\" data-og-height=\"572\" height=\"572\" data-path=\"langsmith/images/openai-compatible-endpoint.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=75921e7b30edbac5263ee10178977383 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=468265c2ad0a6c1740eb18590dab27a5 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=9f9c3f68df77205264eb9221373790f2 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=ccb9820653e1e57b529bc46ac7d20e40 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c320b3f4051643b71fba4faa350daf9b 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=e9d1be7c69021b7fa575a2a466dbfe58 2500w\" />\n\n     <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=97459563da21d17228a1bb94a1b9edf3\" alt=\"Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected.\" data-og-width=\"896\" width=\"896\" data-og-height=\"552\" height=\"552\" data-path=\"langsmith/images/openai-compatible-endpoint-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c3e8e46813ec673fbc3ac4e4748a4ab6 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=381a567b022c71ed6f74abbb7e3cecbd 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=7df617e39d5bd098f4e80d523ef85778 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=06408cd348f56fcc409af8273c799a97 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=6b79b55e2868ed11e9d2c2eb396b004f 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=4e0439d889aa2175c0802e9bf5db399b 2500w\" />\n   </div>\n\nIf everything is set up correctly, you should see the model's response in the playground. You can also use this functionality to invoke downstream pipelines as well.\n\nFor information on how to store your model configuration , refer to [Configure prompt settings](/langsmith/managing-model-configurations).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-openai-compliant-model.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Evaluation quickstart\nLink: https://docs.langchain.comlangsmith/evaluation-quickstart\nContent: Prerequisites\nBefore you begin, make sure you have: A LangSmith account : Sign up or log in at smith.langchain.com. A LangSmith API key : Follow the Create an API key guide. An OpenAI API key : Generate this from the OpenAI dashboard. Select the UI or SDK filter for instructions:\n\n1. Set workspace secrets\nIn the LangSmith UI , ensure that your OpenAI API key is set as a workspace secret. Navigate to Settings and then move to the Secrets tab. Select Add secret and enter the OPENAI_API_KEY and your API key as the Value. Select Save secret. When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.\n\n6. Run and view results\nTo run the evaluation experiment, you'll call evaluate(...) , which: Pulls example from the dataset you created in Step 3. Sends each example's inputs to your target function from Step 4. Collects the outputs (the model's answers). Passes the outputs along with the reference_outputs to your evaluator from Step 5. Records all results in LangSmith as an experiment, so you can view them in the UI. Add the highlighted code to your eval file: from langsmith import Client, wrappers\nfrom openai import OpenAI\nfrom openevals.llm import create_llm_as_judge\nfrom openevals.prompts import CORRECTNESS_PROMPT\n\n# Wrap the OpenAI client for LangSmith tracing\nopenai_client = wrappers.wrap_openai(OpenAI())\n\n# Define the application logic you want to evaluate inside a target function\n# The SDK will automatically send the inputs from the dataset to your target function\ndef target(inputs: dict) -> dict:\n    response = openai_client.chat.completions.create(\n        model=\"gpt-5-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},\n            {\"role\": \"user\", \"content\": inputs[\"question\"]},\n        ],\n    )\n    return {\"answer\": response.choices[0].message.content.strip()}\n\ndef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n    evaluator = create_llm_as_judge(\n        prompt=CORRECTNESS_PROMPT,\n        model=\"openai:o3-mini\",\n        feedback_key=\"correctness\",\n    )\n    return evaluator(\n        inputs=inputs,\n        outputs=outputs,\n        reference_outputs=reference_outputs\n    )\n\n# After running the evaluation, a link will be provided to view the results in langsmith\ndef main():\n    client = Client()\n    experiment_results = client.evaluate(\n        target,\n        data=\"Sample dataset\",\n        evaluators=[\n            correctness_evaluator,\n            # can add multiple evaluators here\n        ],\n        experiment_prefix=\"first-eval-in-langsmith\",\n        max_concurrency=2,\n    )\n    print(experiment_results)\n\nif __name__ == \"__main__\":\n    main()\n\n---\n\nResult 3:\nTitle: Metadata parameters reference\nLink: https://docs.langchain.comlangsmith/ls-metadata-parameters\nContent: When you trace LLM calls with LangSmith, you often want to [track costs](/langsmith/cost-tracking), compare model configurations, and analyze performance across different providers. LangSmith's native integrations (like [LangChain](/langsmith/trace-with-langchain) or the [OpenAI](/langsmith/trace-openai)/[Anthropic](/langsmith/trace-anthropic) wrappers) handle this automatically, but custom model wrappers and self-hosted models require a standardized way to provide this information. LangSmith uses `ls_` metadata parameters for this purpose.\n\nThese metadata parameters (all prefixed with `ls_`) let you pass model configuration and identification information through the standard `metadata` field. Once set, LangSmith can automatically calculate costs, display model information in the UI, and enable filtering and analytics across your traces.\n\nUse `ls_` metadata parameters to:\n\n* **Enable automatic cost tracking** for custom or self-hosted models by identifying the provider and model name.\n* **Track model configuration** like temperature, max tokens, and other parameters for experiment comparison.\n* **Filter and analyze traces** by provider or configuration settings\n* **Improve debugging** by recording exactly which model settings were used for each run.\n\n## Basic usage example\n\nThe most common use case is enabling cost tracking for custom model wrappers. To do this, you need to provide two key pieces of information: the provider name (`ls_provider`) and the model name (`ls_model_name`). These work together to match against LangSmith's pricing database.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import traceable\n\n  @traceable(\n      run_type=\"llm\",\n      metadata={\n          \"ls_provider\": \"my_provider\",\n          \"ls_model_name\": \"my_custom_model\"\n      }\n  )\n  def my_custom_llm(prompt: str):\n      return call_custom_api(prompt)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { traceable } from \"langsmith/traceable\";\n\n  const myCustomLlm = traceable(\n    async (prompt: string) => {\n      return callCustomApi(prompt);\n    },\n    {\n      run_type: \"llm\",\n      metadata: {\n        ls_provider: \"my_provider\",\n        ls_model_name: \"my_custom_model\"\n      }\n    }\n  );\n  ```\n</CodeGroup>\n\nThis minimal setup tells LangSmith what model you're using, enabling automatic cost calculation if the model exists in the pricing database or if you've [configured custom pricing](/langsmith/cost-tracking#set-up-model-pricing).\n\nFor more comprehensive tracking, you can include additional configuration parameters. This is especially useful when [running experiments](/langsmith/evaluation-quickstart) or comparing different model settings:\n\n<CodeGroup>\n  ```python Python theme={null}\n  @traceable(\n      run_type=\"llm\",\n      metadata={\n          \"ls_provider\": \"openai\",\n          \"ls_model_name\": \"gpt-4o\",\n          \"ls_temperature\": 0.7,\n          \"ls_max_tokens\": 4096,\n          \"ls_stop\": [\"END\"],\n          \"ls_invocation_params\": {\n              \"top_p\": 0.9,\n              \"frequency_penalty\": 0.5\n          }\n      }\n  )\n  def my_configured_llm(messages: list):\n      return call_llm(messages)\n  ```\n\n  ```typescript TypeScript theme={null}\n  const myConfiguredLlm = traceable(\n    async (messages: Array<any>) => {\n      return callLlm(messages);\n    },\n    {\n      run_type: \"llm\",\n      metadata: {\n        ls_provider: \"openai\",\n        ls_model_name: \"gpt-4o\",\n        ls_temperature: 0.7,\n        ls_max_tokens: 4096,\n        ls_stop: [\"END\"],\n        ls_invocation_params: {\n          top_p: 0.9,\n          frequency_penalty: 0.5\n        }\n      }\n    }\n  );\n  ```\n</CodeGroup>\n\nWith this setup, you can later filter traces by temperature, compare runs with different max token settings, or analyze which configuration parameters produce the best results. All these parameters are optional except for the `ls_provider` and `ls_model_name` pair needed for cost tracking.\n\n## All parameters\n\n### User-configurable parameters\n\n| Parameter                                       | Type       | Required | Description                         |\n| ----------------------------------------------- | ---------- | -------- | ----------------------------------- |\n| [`ls_provider`](#ls-provider)                   | `string`   | Yes\\*    | LLM provider name for cost tracking |\n| [`ls_model_name`](#ls-model-name)               | `string`   | Yes\\*    | Model identifier for cost tracking  |\n| [`ls_temperature`](#ls-temperature)             | `number`   | No       | Temperature parameter used          |\n| [`ls_max_tokens`](#ls-max-tokens)               | `number`   | No       | Maximum tokens parameter used       |\n| [`ls_stop`](#ls-stop)                           | `string[]` | No       | Stop sequences used                 |\n| [`ls_invocation_params`](#ls-invocation-params) | `object`   | No       | Additional invocation parameters    |\n\n\\* `ls_provider` and `ls_model_name` must be provided together for cost tracking\n\n### System-generated parameters\n\n| Parameter                       | Type      | Description                                                            |\n| ------------------------------- | --------- | ---------------------------------------------------------------------- |\n| [`ls_run_depth`](#ls-run-depth) | `integer` | Depth in trace tree (0=root, 1=child, etc.) - automatically calculated |\n| [`ls_method`](#ls-method)       | `string`  | Tracing method used (e.g., \"traceable\") - set by SDK                   |\n\n### Experiment parameters\n\n| Parameter                               | Type            | Description                                                             |\n| --------------------------------------- | --------------- | ----------------------------------------------------------------------- |\n| [`ls_example_*`](#ls-example-)          | `any`           | Example metadata prefixed with `ls_example_` - added during experiments |\n| [`ls_experiment_id`](#ls-experiment-id) | `string` (UUID) | Unique experiment identifier - added during experiments                 |\n\n## Parameter details\n\n### `ls_provider`\n\n* **Type:** `string`\n* **Required:** Yes (with [`ls_model_name`](#ls-model-name))\n\n**What it does:**\nIdentifies the LLM provider. Combined with `ls_model_name`, enables automatic cost calculation by matching against [LangSmith's model pricing database](https://smith.langchain.com/settings/workspaces/models).\n\n**Common values:**\n\n* `\"openai\"`\n* `\"anthropic\"`\n* `\"azure\"`\n* `\"bedrock\"`\n* `\"google_vertexai\"`\n* `\"google_genai\"`\n* `\"fireworks\"`\n* `\"mistral\"`\n* `\"groq\"`\n* Or, any custom string\n\n**When to use:**\nWhen you want [automatic cost tracking](/langsmith/cost-tracking) for custom model wrappers or self-hosted models.\n\n**Example:**\n\n```python  theme={null}\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": \"openai\",\n        \"ls_model_name\": \"gpt-4o\"\n    }\n)\ndef my_llm_call(prompt: str):\n    return call_api(prompt)\n```\n\n**Relationships:**\n\n* **Requires** [`ls_model_name`](#ls-model-name) for cost tracking to work.\n* Works with token usage data to calculate costs.\n\n### `ls_model_name`\n\n* **Type:** `string`\n* **Required:** Yes (with `ls_provider`)\n\n**What it does:**\nIdentifies the specific model. Combined with `ls_provider`, matches against pricing database for automatic cost calculation.\n\n**Common values:**\n\n* OpenAI: `\"gpt-4o\"`, `\"gpt-4o-mini\"`, `\"gpt-3.5-turbo\"`\n* Anthropic: `\"claude-3-5-sonnet-20241022\"`, `\"claude-3-opus-20240229\"`\n* Custom: Any model identifier\n\n**When to use:**\nWhen you want automatic [cost tracking](/langsmith/cost-tracking) and model identification in the [UI](https://smith.langchain.com).\n\n**Example:**\n\n```python  theme={null}\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": \"anthropic\",\n        \"ls_model_name\": \"claude-3-5-sonnet-20241022\"\n    }\n)\ndef my_claude_call(messages: list):\n    return call_claude(messages)\n```\n\n**Relationships:**\n\n* **Requires** [`ls_provider`](#ls-provider) for cost tracking to work.\n* Works with token usage data to calculate costs.\n\n### `ls_temperature`\n\n* **Type:** `number` (nullable)\n* **Required:** No\n\n**What it does:**\nRecords the temperature setting used. This is for tracking only\u2014does not affect LangSmith behavior.\n\n**When to use:**\nWhen you want to track model configuration for experiments or debugging.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_temperature\": 0.7\n}\n```\n\n**Relationships:**\n\n* Independent; just for tracking.\n* Useful alongside other config parameters for experiment comparison.\n\n### `ls_max_tokens`\n\n* **Type:** `number` (nullable)\n* **Required:** No\n\n**What it does:**\nRecords the maximum tokens setting used. This is for tracking only\u2014does not affect LangSmith behavior.\n\n**When to use:**\nWhen you want to track model configuration for experiments or debugging.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_max_tokens\": 4096\n}\n```\n\n**Relationships:**\n\n* Independent; just for tracking.\n* Useful for cost analysis when combined with actual token usage.\n\n### `ls_stop`\n\n* **Type:** `string[]` (nullable)\n* **Required:** No\n\n**What it does:**\nRecords stop sequences used. This is for tracking only\u2014does not affect LangSmith behavior.\n\n**When to use:**\nWhen you want to track model configuration for experiments or debugging.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_stop\": [\"END\", \"STOP\", \"\\n\\n\"]\n}\n```\n\n**Relationships:**\n\n* Independent; just for tracking.\n\n### `ls_invocation_params`\n\n* **Type:** `object` (any key-value pairs)\n* **Required:** No\n\n**What it does:**\nStores additional model parameters that don't fit the specific `ls_` parameters. Can include provider-specific settings.\n\n**Common parameters:**\n`top_p`, `frequency_penalty`, `presence_penalty`, `top_k`, `seed`, or any custom parameters\n\n**When to use:**\nWhen you need to track additional configuration beyond the standard parameters.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_invocation_params\": {\n        \"top_p\": 0.9,\n        \"frequency_penalty\": 0.5,\n        \"presence_penalty\": 0.3,\n        \"seed\": 12345\n    }\n}\n```\n\n**Relationships:**\n\n* Independent; stores arbitrary configuration.\n\n### `ls_run_depth`\n\n* **Type:** `integer`\n* **Set by:** LangSmith backend (automatic)\n* **Cannot be overridden**\n\n**What it does:**\nIndicates depth in the trace tree:\n\n* `0` = Root run (top-level)\n* `1` = Direct child\n* `2` = Grandchild\n* etc.\n\n**When it's used:**\nAutomatically calculated during trace ingestion. Used for filtering (e.g., \"show only root runs\") and UI visualization.\n\n**Example query:**\n\n```\nmetadata_key = 'ls_run_depth' AND metadata_value = 0\n```\n\n**Relationships:**\n\n* Determined by trace parent-child structure.\n* Cannot be set manually.\n\n### `ls_method`\n\n* **Type:** `string`\n* **Set by:** SDK (automatic)\n\n**What it does:**\nIndicates which SDK method created the trace (commonly `\"traceable\"` for `@traceable` decorator).\n\n**When it's used:**\nAutomatically set by the tracing SDK. Used for debugging and analytics.\n\n**Relationships:**\n\n* Set by SDK based on how trace was created.\n* Cannot be set manually.\n\n### `ls_example_*`\n\n* **Type:** Any (depends on example metadata)\n* **Pattern:** `ls_example_{original_key}`\n* **Set by:** LangSmith experiments system (automatic)\n\n**What it does:**\nWhen running [experiments on datasets](/langsmith/evaluation-quickstart), metadata from the example is automatically prefixed with `ls_example_` and added to the trace.\n\n**Special parameter:**\n\n* `ls_example_dataset_split`: Dataset split (e.g., \"train\", \"test\", \"validation\")\n\n**When it's used:**\nDuring dataset experiments. Allows filtering/grouping by example characteristics.\n\n**Example:**\nIf example has metadata `{\"category\": \"technical\", \"difficulty\": \"hard\"}`, trace gets:\n\n```json  theme={null}\n{\n  \"metadata\": {\n    \"ls_example_category\": \"technical\",\n    \"ls_example_difficulty\": \"hard\",\n    \"ls_example_dataset_split\": \"test\"\n  }\n}\n```\n\n**Relationships:**\n\n* Automatically derived from example metadata.\n* Cannot be set manually on traces.\n\n### `ls_experiment_id`\n\n* **Type:** `string` (UUID)\n* **Set by:** LangSmith experiments system (automatic)\n\n**What it does:**\nUnique identifier for an experiment run.\n\n**When it's used:**\nAutomatically added when running [experiments/evaluations on datasets](/langsmith/evaluation-quickstart). Used to group all runs from the same experiment.\n\n**Relationships:**\n\n* Links runs to specific experiments.\n* Cannot be set manually.\n\n## Parameter relationships\n\n### Cost tracking dependencies\n\nFor LangSmith to automatically calculate costs, several parameters must work together. Here's what's required:\n\n**Primary requirement:** [`ls_provider`](#ls-provider) + [`ls_model_name`](#ls-model-name)\n\n* Both should be present for automatic cost calculation.\n* If [`ls_model_name`](#ls-model-name) is missing, system will fall back to checking [`ls_invocation_params`](#ls-invocation-params) for model name.\n* [`ls_provider`](#ls-provider) must match a provider in the [pricing database](https://smith.langchain.com/settings/workspaces/models) (or use custom pricing).\n\n**Additional requirements:**\n\n* Run must have `run_type=\"llm\"` (or [arbitrary cost tracking](/langsmith/cost-tracking#tracking-costs-for-arbitrary-runs) must be enabled).\n* [Token usage data](/langsmith/log-llm-trace#provide-token-and-cost-information) must be present in the trace (prompt\\_tokens, completion\\_tokens).\n* Model must exist in pricing database or have [custom pricing configured](/langsmith/cost-tracking#set-up-model-pricing).\n\n**Fallback behavior:**\nIf [`ls_model_name`](#ls-model-name) is not in metadata, the system checks [`ls_invocation_params`](#ls-invocation-params) for model identifiers like `\"model\"` before giving up on cost tracking.\n\n### Configuration tracking group\n\nThese parameters help you track model settings but don't affect LangSmith's core functionality:\n\n**Optional, work independently:** [`ls_temperature`](#ls-temperature), [`ls_max_tokens`](#ls-max-tokens), [`ls_stop`](#ls-stop)\n\n* These are for tracking/display.\n* Do not affect LangSmith behavior or cost calculation.\n* Useful for experiment comparison and debugging.\n\n### Invocation params special case\n\nThe `ls_invocation_params` parameter has a dual role as both a tracking field and a fallback mechanism:\n\n**[`ls_invocation_params`](#ls-invocation-params)**; partially independent with fallback role:\n\n* Primarily stores arbitrary configuration for tracking.\n* **Can serve as fallback** for cost tracking if [`ls_model_name`](#ls-model-name) is missing.\n* Does not directly affect cost calculation when [`ls_model_name`](#ls-model-name) is present.\n\n### System parameters\n\nThese parameters are automatically generated by LangSmith and cannot be manually set:\n\n**Cannot be user-set:** [`ls_run_depth`](#ls-run-depth), [`ls_method`](#ls-method), [`ls_example_*`](#ls-example-), [`ls_experiment_id`](#ls-experiment-id)\n\n* Automatically set by system.\n* Used for filtering, analytics, and system tracking.\n\n## Filter traces by metadata parameters\n\nOnce you've added `ls_` metadata parameters to your traces, you can use them to filter and search traces programmatically via the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or interactively in the [LangSmith UI](https://smith.langchain.com). This lets you narrow down traces by model, provider, configuration settings, or trace depth.\n\n### Use the API\n\nUse the [`Client`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client) class with the [`list_runs()`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs) method (Python) or [`listRuns()`](https://docs.smith.langchain.com/reference/js/classes/client.Client#listruns) method (TypeScript) to query traces based on metadata values. The [filter syntax](/langsmith/trace-query-syntax) supports equality checks, comparisons, and logical operators.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n\n  client = Client()\n\n  # Filter runs by provider\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_provider\" AND metadata_value = \"openai\"'\n  )\n\n  # Filter by specific model\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_model_name\" AND metadata_value = \"gpt-4o\"'\n  )\n\n  # Filter root runs only (top-level traces)\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_run_depth\" AND metadata_value = 0'\n  )\n\n  # Filter by temperature threshold\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_temperature\" AND metadata_value > 0.5'\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n\n  const client = new Client();\n\n  // Filter runs by provider\n  const runsByProvider: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_provider\" AND metadata_value = \"openai\"'\n  })) {\n    runsByProvider.push(run);\n  }\n\n  // Filter by specific model\n  const runsByModel: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_model_name\" AND metadata_value = \"gpt-4o\"'\n  })) {\n    runsByModel.push(run);\n  }\n\n  // Filter root runs only (top-level traces)\n  const rootRuns: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_run_depth\" AND metadata_value = 0'\n  })) {\n    rootRuns.push(run);\n  }\n\n  // Filter by temperature threshold\n  const highTempRuns: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_temperature\" AND metadata_value > 0.5'\n  })) {\n    highTempRuns.push(run);\n  }\n  ```\n</CodeGroup>\n\nThese examples show common filtering patterns:\n\n* **Filter by provider or model** to analyze usage patterns or costs for specific models\n* **Filter by run depth** to get only root traces (depth 0) or child runs at specific nesting levels\n* **Filter by configuration** to compare experiments with different temperature, max tokens, or other settings\n\n### Use the UI\n\nIn the [LangSmith UI](https://smith.langchain.com), use the filter/search bar with the [filter syntax](/langsmith/trace-query-syntax):\n\n```\nmetadata_key = 'ls_provider' AND metadata_value = 'openai'\nmetadata_key = 'ls_model_name' AND metadata_value = 'gpt-4o'\nmetadata_key = 'ls_run_depth' AND metadata_value = 0\n```\n\n## Related resources\n\n* [Cost tracking guide](/langsmith/cost-tracking): Learn how to track and analyze LLM costs in LangSmith.\n* [Log LLM traces](/langsmith/log-llm-trace): Format requirements for logging LLM calls with proper token tracking.\n* [Trace query syntax](/langsmith/trace-query-syntax): Complete reference for filtering and searching traces.\n* [Evaluation quickstart](/langsmith/evaluation-quickstart): Run experiments on datasets to compare model configurations.\n* [Add metadata and tags](/langsmith/add-metadata-tags): General guide to adding metadata to traces.\n* [Filter traces in application](/langsmith/filter-traces-in-application): Programmatically filter traces in your code.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/ls-metadata-parameters.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Trace a RAG application tutorial\nLink: https://docs.langchain.comlangsmith/observability-llm-tutorial\nContent: Trace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like: Notice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client ( openai_client = wrap_openai(OpenAI()) ). What happens if you call it in the following way? This will produce a trace of just the OpenAI call - it should look something like this\n\nTrace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like: Notice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client ( openai_client = wrap_openai(OpenAI()) ). What happens if you call it in the following way? rag(\"where did harrison work\")\n\nTrace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like: from openai import OpenAI\nfrom langsmith.wrappers import wrap_openai\nopenai_client = wrap_openai(OpenAI())\n\n# This is the retriever we will use in RAG\n# This is mocked out, but it could be anything we want\ndef retriever(query: str):\n    results = [\"Harrison worked at Kensho\"]\n    return results\n\n# This is the end-to-end RAG chain.\n# It does a retrieval step then calls OpenAI\ndef rag(question):\n    docs = retriever(question)\n    system_message = \"\"\"Answer the users question using only the provided information below:\n        {docs}\"\"\".format(docs=\"\\n\".join(docs))\n\n    return openai_client.chat.completions.create(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": question},\n        ],\n        model=\"gpt-4o-mini\",\n    )\n\n---\n\nResult 5:\nTitle: Tracing quickstart\nLink: https://docs.langchain.comlangsmith/observability-quickstart\nContent: 4. Trace LLM calls\nTo start, you\u2019ll trace all your OpenAI calls. LangSmith provides wrappers: Python: wrap_openai TypeScript: wrapOpenAI This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith. Include the highlighted lines in your app file: Call your application: You'll receive the following output: In the LangSmith UI , navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2 ). You'll see the OpenAI call you just instrumented.\n\n4. Trace LLM calls\nTo start, you\u2019ll trace all your OpenAI calls. LangSmith provides wrappers: Python: wrap_openai TypeScript: wrapOpenAI This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith. Include the highlighted lines in your app file: from openai import OpenAI\nfrom langsmith.wrappers import wrap_openai  # traces openai calls\n\ndef retriever(query: str):\n    return [\"Harrison worked at Kensho\"]\n\nclient = wrap_openai(OpenAI())  # log traces by wrapping the model calls\n\ndef rag(question: str) -> str:\n    docs = retriever(question)\n    system_message = (\n        \"Answer the user's question using only the provided information below:\\n\"\n        + \"\\n\".join(docs)\n    )\n    resp = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": question},\n        ],\n    )\n    return resp.choices[0].message.content\n\nif __name__ == \"__main__\":\n    print(rag(\"Where did Harrison work?\"))\n\n4. Trace LLM calls\nTo start, you\u2019ll trace all your OpenAI calls. LangSmith provides wrappers: Python: wrap_openai TypeScript: wrapOpenAI This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith. Include the highlighted lines in your app file: import \"dotenv/config\";\nimport OpenAI from \"openai\";\nimport { wrapOpenAI } from \"langsmith/wrappers\"; // traces openai calls\n\nfunction retriever(query: string): string[] {\n    return [\"Harrison worked at Kensho\"];\n}\n\nconst client = wrapOpenAI(new OpenAI()); // log traces by wrapping the model calls\n\nasync function rag(question: string) {\n    const docs = retriever(question);\n    const systemMessage =\n        \"Answer the user's question using only the provided information below:\\n\" +\n        docs.join(\"\\n\");\n\n    const resp = await client.chat.completions.create({\n        model: \"gpt-4o-mini\",\n        messages: [\n            { role: \"system\", content: systemMessage },\n            { role: \"user\", content: question },\n        ],\n    });\n\n    return resp.choices[0].message?.content;\n}\n\n(async () => {\n    console.log(await rag(\"Where did Harrison work?\"));\n})();\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_61849426",
    "id": "a613f817-514d-4e6e-9bbd-4fa1655bd855",
    "content": "Result 1:\nTitle: LangGraph CLI\nLink: https://docs.langchain.comlangsmith/cli\nContent: LangGraph CLI\nLangGraph CLI is a command-line tool for building and running the Agent Server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.\n\nLangGraph CLI\nLangGraph CLI\nLangGraph CLI is a command-line tool for building and running the Agent Server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.\n\ndev\nRun LangGraph API server in development mode with hot reloading and debugging capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory. Currently, the CLI only supports Python >= 3.11. Installation This command requires the \"inmem\" extra to be installed: Usage Options Option Default Description -c, --config FILE langgraph.json Path to configuration file declaring dependencies, graphs and environment variables --host TEXT 127.0.0.1 Host to bind the server to --port INTEGER 2024 Port to bind the server to --no-reload Disable auto-reload --n-jobs-per-worker INTEGER Number of jobs per worker. Default is 10 --debug-port INTEGER Port for debugger to listen on --wait-for-client False Wait for a debugger client to connect to the debug port before starting the server --no-browser Skip automatically opening the browser when the server starts --studio-url TEXT URL of the Studio instance to connect to. Defaults to https://smith.langchain.com --allow-blocking False Do not raise errors for synchronous I/O blocking operations in your code (added in 0.2.6) --tunnel False Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers like Safari or networks blocking localhost connections --help Display command documentation Run LangGraph API server in development mode with hot reloading capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory. Usage Options Option Default Description -c, --config FILE langgraph.json Path to configuration file declaring dependencies, graphs and environment variables --host TEXT 127.0.0.1 Host to bind the server to --port INTEGER 2024 Port to bind the server to --no-reload Disable auto-reload --n-jobs-per-worker INTEGER Number of jobs per worker. Default is 10 --debug-port INTEGER Port for debugger to listen on --wait-for-client False Wait for a debugger client to connect to the debug port before starting the server --no-browser Skip automatically opening the browser when the server starts --studio-url TEXT URL of the Studio instance to connect to. Defaults to https://smith.langchain.com --allow-blocking False Do not raise errors for synchronous I/O blocking operations in your code --tunnel False Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers or networks blocking localhost connections --help Display command documentation\n\n---\n\nResult 2:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/javascript/langgraph/overview\nContent: <Callout icon=\"bullhorn\" color=\"#DFC5FE\" iconType=\"regular\">\n  **LangGraph v1.x is now available!**\n\n  For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/javascript/releases/langgraph-v1) and [migration guide](/oss/javascript/migrate/langgraph-v1).\n\n  If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=02-langgraph.yml\\&labels=langgraph,js/ts) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langgraphjs/tree/main/docs/docs).\n</Callout>\n\nTrusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/javascript/langchain/models) and [tools](/oss/javascript/langchain/tools).\n\nWe will commonly use [LangChain](/oss/javascript/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/javascript/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph @langchain/core\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```typescript  theme={null}\nimport { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\n\nconst mockLlm = (state: typeof MessagesAnnotation.State) => {\n  return { messages: [{ role: \"ai\", content: \"hello world\" }] };\n};\n\nconst graph = new StateGraph(MessagesAnnotation)\n  .addNode(\"mock_llm\", mockLlm)\n  .addEdge(START, \"mock_llm\")\n  .addEdge(\"mock_llm\", END)\n  .compile();\n\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi!\" }] });\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/javascript/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/javascript/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/javascript/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/javascript/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: What's new in LangGraph v1\nLink: https://docs.langchain.comoss/javascript/releases/langgraph-v1\nContent: **LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.\n\nIt's designed to work hand-in-hand with [LangChain v1](/oss/javascript/releases/langchain-v1) (whose `createAgent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.\n\n<CardGroup cols={1}>\n  <Card title=\"Stable core APIs\" icon=\"diagram-project\">\n    Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\n  </Card>\n\n  <Card title=\"Reliability, by default\" icon=\"database\">\n    Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\n  </Card>\n\n  <Card title=\"Seamless with LangChain v1\" icon=\"link\">\n    LangChain's `createAgent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph @langchain/core\n  ```\n</CodeGroup>\n\nFor a complete list of changes, see the [migration guide](/oss/javascript/migrate/langgraph-v1).\n\n## Deprecation of `createReactAgent`\n\nThe LangGraph `createReactAgent` prebuilt has been deprecated in favor of LangChain's `createAgent`. It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n\n* For information on the new `createAgent` API, see the [LangChain v1 release notes](/oss/javascript/releases/langchain-v1#createagent).\n* For information on migrating from `createReactAgent` to `createAgent`, see the [LangChain v1 migration guide](/oss/javascript/migrate/langchain-v1#createagent).\n\n## Typed interrupts\n\n[`StateGraph`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html) now accepts a map of interrupt types in the constructor to more closely constrain the types of interrupts that can be used within a graph.\n\n```typescript expandable theme={null}\nimport { StateGraph, MemorySaver, interrupt } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst stateSchema = z.object({\n  foo: z.string(),\n})\n\nconst graphConfig = {\n  interrupts: {\n    // Define a simple interrupt that accepts a reason and returns messages\n    simple: interrupt<{ reason: string }, { messages: string[] }>, // [!code highlight]\n    // Define a complex interrupt with the same signature\n    complex: interrupt<{ reason: string }, { messages: string[] }>, // [!code highlight]\n  }\n}\n\nconst checkpointer = new MemorySaver();\n\nconst graph = new StateGraph(stateSchema, graphConfig)\n  .addNode(\"node\", async (state, runtime) => {\n    // Trigger the simple interrupt with a reason\n    const response = runtime.interrupt.simple({ reason: \"test\" });\n    // Return the interrupt response as the new state\n    return { foo: response };\n  })\n  // Compile the graph with the checkpointer\n  .compile({ checkpointer });\n\n// Invoke the graph with initial state\nconst result = await graph.invoke({ foo: \"test\" });\n\n// Access the interrupt data\nif (graph.isInterrupted(result)) {\n  console.log(result.__interrupt__.messages);\n}\n```\n\nFor more information on interrupts, see the [Interrupts](/oss/javascript/langgraph/interrupts) documentation.\n\n## Frontend SDK enhancements\n\nLangGraph v1 comes with a few enhancements when interacting with a LangGraph application from the frontend.\n\n### Event stream encoding\n\nThe low-level `toLangGraphEventStream` helper has been removed. Streaming responses are now handled natively by the SDK, and you can select the wire format via passing in the `encoding` format to `graph.stream`. This makes switching between SSE and normal JSON responses straightforward without changing UI logic.\n\nSee the [migration guide](/oss/javascript/migrate/langgraph-v1#stream-encoding) for more information.\n\n### Custom transports in `useStream`\n\nThe React `useStream` hook now supports pluggable transports so you can have more control over the network layer without changing UI code.\n\n```typescript  theme={null}\nconst stream = useStream({\n  transport: new FetchStreamTransport({\n    apiUrl: \"http://localhost:2024\",\n  }),\n});\n```\n\nLearn how to integrate and customize the hook: [Integrate LangGraph into your React application](/oss/javascript/langgraph/ui).\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraphjs/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraphjs/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangGraph 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Overview\" icon=\"book\" href=\"/oss/javascript/langgraph/overview\" arrow>\n    What LangGraph is and when to use it\n  </Card>\n\n  <Card title=\"Graph API\" icon=\"diagram-project\" href=\"/oss/javascript/langgraph/graph-api\" arrow>\n    Build graphs with state, nodes, and edges\n  </Card>\n\n  <Card title=\"LangChain Agents\" icon=\"robot\" href=\"/oss/javascript/langchain/agents\" arrow>\n    High-level agents built on LangGraph\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/javascript/migrate/langgraph-v1\" arrow>\n    How to migrate to LangGraph v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langgraphjs\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/javascript/versioning) \u2013 Understanding version numbers\n* [Release policy](/oss/javascript/release-policy) \u2013 Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/releases/langgraph-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/python/langgraph/overview\nContent: <Callout icon=\"bullhorn\" color=\"#DFC5FE\" iconType=\"regular\">\n  **LangGraph v1.x is now available!**\n\n  For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/python/releases/langgraph-v1) and [migration guide](/oss/python/migrate/langgraph-v1).\n\n  If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=02-langgraph.yml\\&labels=langgraph,python) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langgraph/tree/main/docs/docs).\n</Callout>\n\nTrusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools).\n\nWe will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\ndef mock_llm(state: MessagesState):\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n\ngraph = StateGraph(MessagesState)\ngraph.add_node(mock_llm)\ngraph.add_edge(START, \"mock_llm\")\ngraph.add_edge(\"mock_llm\", END)\ngraph = graph.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/python/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: What's new in LangGraph v1\nLink: https://docs.langchain.comoss/python/releases/langgraph-v1\nContent: **LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.\n\nIt's designed to work hand-in-hand with [LangChain v1](/oss/python/releases/langchain-v1) (whose `create_agent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.\n\n<CardGroup cols={1}>\n  <Card title=\"Stable core APIs\" icon=\"diagram-project\">\n    Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\n  </Card>\n\n  <Card title=\"Reliability, by default\" icon=\"database\">\n    Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\n  </Card>\n\n  <Card title=\"Seamless with LangChain v1\" icon=\"link\">\n    LangChain's `create_agent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\n## Deprecation of `create_react_agent`\n\nThe LangGraph [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt has been deprecated in favor of LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent). It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n\n* For information on the new [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) API, see the [LangChain v1 release notes](/oss/python/releases/langchain-v1#create-agent).\n* For information on migrating from [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), see the [LangChain v1 migration guide](/oss/python/migrate/langchain-v1#create-agent).\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraph/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraph/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangGraph 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Overview\" icon=\"book\" href=\"/oss/python/langgraph/overview\" arrow>\n    What LangGraph is and when to use it\n  </Card>\n\n  <Card title=\"Graph API\" icon=\"diagram-project\" href=\"/oss/python/langgraph/graph-api\" arrow>\n    Build graphs with state, nodes, and edges\n  </Card>\n\n  <Card title=\"LangChain Agents\" icon=\"robot\" href=\"/oss/python/langchain/agents\" arrow>\n    High-level agents built on LangGraph\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/python/migrate/langgraph-v1\" arrow>\n    How to migrate to LangGraph v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langgraph\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/python/versioning) \u2013 Understanding version numbers\n* [Release policy](/oss/python/release-policy) \u2013 Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langgraph-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_50387987",
    "id": "49734f20-9abc-4e40-be0d-c0b83880e69a",
    "content": "Result 1:\nTitle: Contributing to code\nLink: https://docs.langchain.comoss/javascript/contributing/code\nContent: Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.\n\n## Getting started\n\n<Note>\n  Before submitting large **new features or refactors**, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work.\n\n  This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. See the quickstart guide below.\n</Note>\n\n### Quick fix: submit a bugfix\n\nFor simple bugfixes, you can get started immediately:\n\n<Steps>\n  <Step title=\"Reproduce the issue\">\n    Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification\n  </Step>\n\n  <Step title=\"Fork the repository\">\n    Fork the [LangChain](https://github.com/langchain-ai/langchainjs) or [LangGraph](https://github.com/langchain-ai/langgraphjs) repo to your personal GitHub account\n  </Step>\n\n  <Step title=\"Clone and setup\">\n    ```bash  theme={null}\n    git clone https://github.com/your-username/name-of-forked-repo.git\n\n    # For instance, for LangChain:\n    git clone https://github.com/parrot123/langchainjs.git\n\n    # For LangGraph:\n    git clone https://github.com/parrot123/langgraphjs.git\n    ```\n\n    ```bash  theme={null}\n    # Inside your repo, install dependencies\n    pnpm install\n    # Create a build for all packages to resolve workspace dependencies\n    pnpm build\n    ```\n  </Step>\n\n  <Step title=\"Create a branch\">\n    Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later.\n\n    ```bash  theme={null}\n    git checkout -b your-username/short-bugfix-name\n    ```\n  </Step>\n\n  <Step title=\"Write failing tests\">\n    Add [unit tests](#test-writing-guidelines) that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions\n  </Step>\n\n  <Step title=\"Make your changes\">\n    Fix the bug while following our [code quality standards](#code-quality-standards). Make the **minimal change necessary** to resolve the issue\n  </Step>\n\n  <Step title=\"Run build\">\n    Run the build command to ensure the package still builds properly\n\n    ```bash  theme={null}\n    pnpm build\n    # or build a specific workspace package\n    pnpm --filter @langchain/core build\n    ```\n  </Step>\n\n  <Step title=\"Verify the fix\">\n    Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR\n\n    ```bash  theme={null}\n    pnpm lint\n    pnpm test\n\n    # For bugfixes involving integrations, also run:\n    pnpm test:int\n\n    # Or run tests in a specific workspace package\n    cd libs/langchain-core\n    pnpm test\n    pnpm lint\n\n    # Or run tests for a specific package from the root of the repo\n    pnpm --filter @langchain/core test\n    pnpm --filter @langchain/core lint\n    ```\n  </Step>\n\n  <Step title=\"Document the change\">\n    Update docstrings if behavior changes, add comments for complex logic\n  </Step>\n\n  <Step title=\"Submit a pull request\">\n    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #ISSUE_NUMBER`) so that the issue is automatically closed when your PR is merged.\n  </Step>\n</Steps>\n\n### Full development setup\n\nFor ongoing development or larger contributions:\n\n1. Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations\n2. Set up your environment following our [setup guide](#development-environment) below\n3. Understand the [repository structure](#repository-structure) and package organization\n4. Learn our [development workflow](#development-workflow) including testing and linting\n\n***\n\n## Contribution guidelines\n\nBefore you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a \"first contribution\" to your resume (or if you're just looking for a quick win) you might be better off doing a boot-camp or an online tutorial.\n\nContributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it's important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you're willing to take the time to do things well.\n\n### Backwards compatibility\n\n<Warning>\n  Breaking changes to public APIs are not allowed except for critical security fixes.\n\n  See our [versioning policy](/oss/javascript/versioning) for details on major version releases.\n</Warning>\n\nMaintain compatibility via:\n\n<AccordionGroup>\n  <Accordion title=\"Stable interfaces\">\n    **Always preserve**:\n\n    * Function signatures and parameter names\n    * Class interfaces and method names\n    * Return value structure and types\n    * Import paths for public APIs\n  </Accordion>\n\n  <Accordion title=\"Safe changes\">\n    **Acceptable modifications**:\n\n    * Adding new optional parameters/type parameters\n\n    * Adding new methods to classes\n\n    * Improving performance without changing behavior\n\n    * Adding new modules or functions\n  </Accordion>\n\n  <Accordion title=\"Before making changes\">\n    * **Would this break existing user code?**\n\n    * Check if your target is public\n\n    * Are there existing usage patterns in tests?\n  </Accordion>\n</AccordionGroup>\n\n### New features\n\nWe aim to keep the bar high for new features. We generally don't accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies.\n\nIn general, feature contribution requirements include:\n\n<Steps>\n  <Step title=\"Design discussion\">\n    Open an issue describing:\n\n    * The problem you're solving\n    * Proposed API design\n    * Expected usage patterns\n  </Step>\n\n  <Step title=\"Implementation\">\n    * Follow existing code patterns\n    * Include comprehensive tests and documentation\n    * Consider security implications\n  </Step>\n\n  <Step title=\"Integration considerations\">\n    * How does this interact with existing features?\n    * Are there performance implications?\n    * Does this introduce new dependencies?\n\n    We will reject features that are likely to lead to security vulnerabilities or reports.\n  </Step>\n</Steps>\n\n### Security guidelines\n\n<Warning>\n  Security is paramount. Never introduce vulnerabilities or unsafe patterns.\n</Warning>\n\nSecurity checklist:\n\n<AccordionGroup>\n  <Accordion title=\"Input validation\">\n    * Validate and sanitize all user inputs\n\n    * Properly escape data in templates and queries\n\n    * Never use `eval()`, as this can lead to arbitrary code execution vulnerabilities\n  </Accordion>\n\n  <Accordion title=\"Error handling\">\n    * Use specific exception types\n    * Don't expose sensitive information in error messages\n    * Implement proper resource cleanup\n  </Accordion>\n\n  <Accordion title=\"Dependencies\">\n    * Avoid adding hard dependencies\n    * Keep optional dependencies minimal\n    * Review third-party packages for security issues\n  </Accordion>\n</AccordionGroup>\n\n***\n\n## Development environment\n\n<Warning>\n  Our JS/TS projects uses [`pnpm`](https://pnpm.io/) for dependency management. Make sure you have the latest version installed, or run `corepack enable` (on Node 24+) to setup the required pnpm version.\n</Warning>\n\nOnce you've reviewed the [contribution guidelines](#contribution-guidelines), set up a development environment for the package(s) you're working on.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <AccordionGroup>\n      <Accordion title=\"Core abstractions\">\n        For changes to `langchain-core`:\n\n        ```bash  theme={null}\n        pnpm install\n        # Run tests for a package from the package directory\n        cd libs/langchain-core\n        pnpm test\n\n        # Or run tests for a package from the root of the repo\n        pnpm --filter @langchain/core test\n\n        ```\n      </Accordion>\n\n      <Accordion title=\"Main package\">\n        For changes to `langchain`:\n\n        ```bash  theme={null}\n        pnpm install\n        pnpm --filter langchain test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Provider packages\">\n        For changes to [partner integrations](/oss/javascript/integrations/providers/overview):\n\n        ```bash  theme={null}\n        pnpm install\n        pnpm --filter @langchain/{provider} test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Community packages\">\n        For changes to [community integrations](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-community):\n\n        ```bash  theme={null}\n        pnpm install\n        pnpm --filter @langchain/community test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Repository structure\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    LangChain is organized as a monorepo with multiple packages:\n\n    <AccordionGroup>\n      <Accordion title=\"Core packages\" defaultOpen>\n        * **[`langchain`](https://github.com/langchain-ai/langchainjs/tree/main/langchain#readme)** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic\n        * **[`@langchain/core`](https://github.com/langchain-ai/langchainjs/tree/main/langchain-core#readme)** (located in `libs/langchain-core/`): Base interfaces and core abstractions\n      </Accordion>\n\n      <Accordion title=\"Partner packages\">\n        Located in `libs/providers/`, these are independently versioned packages for specific integrations. For example:\n\n        * **[`@langchain/openai`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-openai#readme)**: [OpenAI](/oss/javascript/integrations/providers/openai) integrations\n        * **[`@langchain/anthropic`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-anthropic#readme)**: [Anthropic](/oss/javascript/integrations/providers/anthropic) integrations\n        * **[`@langchain/google-genai`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-google-genai#readme)**: [Google Generative AI](/oss/javascript/integrations/chat/google_generative_ai) integrations\n      </Accordion>\n\n      <Accordion title=\"Supporting packages\">\n        * **[`@langchain/textsplitters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-textsplitters#readme)**: Text splitting utilities\n        * **[`@langchain/standard-tests`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-standard-tests#readme)**: Standard test suites for integrations\n        * **[`langchain-community`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-community)**: Community maintained integrations\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Development workflow\n\n### Testing requirements\n\n<Info>\n  Directories are relative to the package you're working in.\n</Info>\n\nEvery code change must include comprehensive tests.\n\n#### Unit tests\n\n**Location**: `src/tests/FILENAME_BEING_TESTED.test.ts`\n\n**Requirements**:\n\n* No network calls allowed\n* Test all code paths including edge cases\n* Use mocks for external dependencies\n\n```bash  theme={null}\n# Run the entire test suite\npnpm test\n\n# Or run a specific test file\npnpm test src/tests/FILENAME_BEING_TESTED.test.ts\n\n# Or run a specific test function\npnpm test -t \"the test that should be run\"\n```\n\n#### Integration tests\n\n**Location**: `src/tests/FILENAME_BEING_TESTED.int.test.ts`\n\nIntegration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.\n\nNot every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.\n\n**Requirements**:\n\n* Test real integrations with external services\n* Use environment variables for API keys\n* Skip gracefully if credentials unavailable\n\n```bash  theme={null}\npnpm test:int\n```\n\n### Code quality standards\n\nContributions must adhere to the following quality requirements:\n\n<Tabs>\n  <Tab title=\"Type hints\">\n    **Required**: Complete types for all functions\n\n    ```typescript  theme={null}\n    function processDocuments(\n        docs: Document[],\n        processor: DocumentProcessor,\n        batchSize: number = 100\n    ): ProcessingResult {\n        // ...\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"Documentation\">\n    **Required**: [JSDocs](https://jsdoc.app/about-getting-started) for all exported functions and interfaces\n\n    ```typescript  theme={null}\n    /**\n     * Document processing instance.\n     */\n    interface FooDocumentProcessor {\n        /**\n         * Process documents in batches.\n         *\n         * @param docs - List of documents to process.\n         * @returns Processing results with success/failure counts.\n         */\n        process(docs: Document[]): ProcessingResult;\n    }\n\n    /**\n     * Process documents in batches.\n     *\n     * @param docs - List of documents to process.\n     * @param processor - Document processing instance.\n     * @param batchSize - Number of documents per batch.\n     * @returns Processing results with success/failure counts.\n     */\n    export function processDocuments(\n        docs: Document[],\n        processor: DocumentProcessor,\n        batchSize: number = 100\n    ): ProcessingResult {\n        // ...\n    }\n    ```\n\n    * Document all parameters and return values\n    * Include usage examples for complex functions\n    * Document raised exceptions\n    * Focus on \"why\" rather than \"what\"\n  </Tab>\n\n  <Tab title=\"Code style\">\n    **Automated**: Formatting and linting:\n\n    ```bash  theme={null}\n    pnpm lint    # Check style and types\n    pnpm format  # Apply formatting\n    ```\n\n    **Standards**:\n\n    * Descriptive variable names\n    * Break up complex functions (aim for fewer than 20 lines)\n    * Follow existing patterns in the codebase\n  </Tab>\n</Tabs>\n\n***\n\n## Testing and validation\n\n### Running tests locally\n\nBefore submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <Steps>\n      <Step title=\"Unit tests\">\n        ```bash  theme={null}\n        pnpm test\n        ```\n\n        All unit tests must pass\n      </Step>\n\n      <Step title=\"Integration tests\">\n        ```bash  theme={null}\n        pnpm test:int\n        ```\n\n        (Run if your changes affect integrations)\n      </Step>\n\n      <Step title=\"Formatting\">\n        ```bash  theme={null}\n        pnpm format\n        pnpm lint\n        ```\n\n        Code must pass all style checks\n      </Step>\n\n      <Step title=\"PR submission\">\n        Push your branch and open a pull request. Follow the provided form template. Note related issues using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.\n      </Step>\n    </Steps>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n### Test writing guidelines\n\nIn order to write effective tests, there's a few good practices to follow:\n\n* Encapsulate the test in a `describe` block that describes the component being tested\n* Use natural language to describe the test name\n* Be exhaustive with assertions\n* Only use snapshots for reasonably sized data objects\n\n<Tabs>\n  <Tab title=\"Unit tests\">\n    ```typescript  theme={null}\n    describe(\"DocumentProcessor\", () => {\n        it(\"Should handle empty document list\", () => {\n            const processor = new DocumentProcessor();\n            const result = processor.process([]);\n\n            expect(result.success).toBe(true);\n            expect(result.processedCount).toBe(0);\n            expect(result.errors).toHaveLength(0);\n        });\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"Integration tests\">\n    ```typescript  theme={null}\n    describe(\"ChatOpenAI\", () => {\n        it(\"Should test with real API\", () => {\n            const chat = new ChatOpenAI();\n            const response = chat.invoke(\"Hello\");\n        });\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"Mock usage\">\n    ```typescript  theme={null}\n    describe(\"APIService\", () => {\n        it(\"Should call with retry\", () => {\n            const mockClient = new MockClient();\n            const service = new APIService(client: mockClient);\n            const result = service.callWithRetry();\n        });\n    });\n    ```\n  </Tab>\n</Tabs>\n\n## Getting help\n\nOur goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).\n\n<Check>\n  You're now ready to contribute high-quality code to LangChain!\n</Check>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Implement a LangChain integration\nLink: https://docs.langchain.comoss/javascript/contributing/implement-langchain\nContent: Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.\n\nLangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/javascript/integrations/chat), [tools](/oss/javascript/integrations/tools), [retrievers](/oss/javascript/integrations/retrievers), and more.\n\nYour integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.\n\n<Tabs>\n  <Tab title=\"Chat Models\">\n    Chat models are subclasses of the [`BaseChatModel`](https://reference.langchain.com/javascript/classes/_langchain_core.language_models_chat_models.BaseChatModel.html) class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.\n\n    <Warning>\n      The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/javascript/langchain/models) for details on how LangChain chat models function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Tools\">\n    Tools are used in 2 main ways:\n\n    1. To define an \"input schema\" or \"args schema\" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a \"tool call\", or parameters to call the tool with.\n    2. To take a \"tool call\" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage.\n\n    The Tools class must inherit from the [`BaseTool`](https://reference.langchain.com/javascript/classes/_langchain_core.tools.StructuredTool.html) base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.\n\n    <Warning>\n      The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/javascript/langchain/tools) for details on how LangChain tools function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Retrievers\">\n    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.\n\n    <Warning>\n      The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/javascript/integrations/retrievers) for details on how LangChain retrievers function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Vector Stores\">\n    All vector stores must inherit from the [`VectorStore`](https://reference.langchain.com/javascript/classes/_langchain_core.vectorstores.VectorStore.html) base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.\n\n    See the [vector store integration guide](/oss/javascript/integrations/vectorstores) for details on implementing a vector store integration.\n\n    <Warning>\n      The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/javascript/integrations/vectorstores) for details on how LangChain vector stores function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Embeddings\">\n    Embedding models are subclasses of the [`Embeddings`](https://reference.langchain.com/javascript/classes/_langchain_core.embeddings.Embeddings.html) class.\n\n    <Warning>\n      The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/javascript/integrations/text_embedding) for details on how LangChain embedding models function.\n    </Warning>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: OpenAIEmbeddings\nLink: https://docs.langchain.comoss/javascript/integrations/text_embedding/openai\nContent: This will help you get started with OpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `OpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n| Class                                                                                           | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/openai/) |                                             Downloads                                             |                                             Version                                            |\n| :---------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |\n| [OpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   \u274c   |                                          \u2705                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\\&label=%20&) |\n\n## Setup\n\nTo access OpenAIEmbeddings embedding models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.\n\n### Credentials\n\nHead to [platform.openai.com](https://platform.openai.com) to sign up to OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:\n\n```bash  theme={null}\nexport OPENAI_API_KEY=\"your-api-key\"\n```\n\nIf you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:\n\n```bash  theme={null}\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n### Installation\n\nThe LangChain OpenAIEmbeddings integration lives in the `@langchain/openai` package:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/openai @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/openai @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/openai @langchain/core\n  ```\n</CodeGroup>\n\n## Instantiation\n\nNow we can instantiate our model object and generate chat completions:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n  batchSize: 512, // Default value if omitted is 512. Max is 2048\n  model: \"text-embedding-3-large\",\n});\n```\n\nIf you're part of an organization, you can set `process.env.OPENAI_ORGANIZATION` to your OpenAI organization id, or pass it in as `organization` when\ninitializing the model.\n\n## Indexing and Retrieval\n\nEmbedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [**Learn** tab](/oss/javascript/learn/).\n\nBelow, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/oss/javascript/integrations/vectorstores/memory).\n\n```typescript  theme={null}\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n```output  theme={null}\nLangChain is the framework for building context-aware reasoning applications\n```\n\n## Direct Usage\n\nUnder the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.\n\nYou can directly call these methods to get embeddings for your own use cases.\n\n### Embed single texts\n\nYou can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:\n\n```typescript  theme={null}\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n```output  theme={null}\n[\n    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,\n   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,\n    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,\n   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,\n   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,\n   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,\n    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,\n   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,\n  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,\n  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,\n  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,\n    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,\n    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,\n   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,\n    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,\n    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,\n     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,\n   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,\n   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,\n     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397\n]\n```\n\n### Embed multiple texts\n\nYou can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:\n\n```typescript  theme={null}\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n```output  theme={null}\n[\n    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,\n   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,\n    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,\n   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,\n   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,\n   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,\n    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,\n   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,\n  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,\n  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,\n  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,\n    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,\n    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,\n   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,\n    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,\n    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,\n     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,\n   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,\n   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,\n     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397\n]\n[\n   -0.010181213,   0.023419594,   -0.04215527, -0.0015320902,  -0.023573855,\n  -0.0091644935,  -0.014893179,   0.019016149,  -0.023475688,  0.0010219777,\n    0.009255648,    0.03996757,   -0.04366983,   -0.01640774,  -0.020194141,\n    0.019408813,  -0.027977299,  -0.022017224,   0.013539891,  -0.007769135,\n    0.032647192,  -0.015089511,  -0.022900717,   0.023798235,   0.026084099,\n   -0.024625633,   0.035003178,  -0.017978394,  -0.049615882,   0.013364594,\n    0.031132633,   0.019142363,   0.023195215,  -0.038396914,   0.005584942,\n   -0.031946007,   0.053682756, -0.0036356465,   0.011240003,  0.0056690844,\n  -0.0062791156,   0.044146635,  -0.037387207,    0.01300699,   0.018946031,\n   0.0050415234,   0.029618073,  -0.021750772,  -0.000649473, 0.00026951815,\n   -0.014710871,  -0.029814405,    0.04204308,  -0.014710871,  0.0039616977,\n   -0.021512369,   0.054608323,   0.021484323,    0.02790718,  -0.010573876,\n   -0.023952495,  -0.035143413,  -0.048802506, -0.0075798146,   0.023279356,\n   -0.022690361,  -0.016590048,  0.0060477243,   0.014100839,   0.005476258,\n   -0.017221114, -0.0100059165,  -0.017922299,  -0.021989176,    0.01830094,\n     0.05516927,   0.001033372,  0.0017310516,   -0.00960624,  -0.037864015,\n    0.013063084,   0.006591143,  -0.010160177,  0.0011394264,    0.04953174,\n    0.004806626,   0.029421741,  -0.037751824,   0.003618117,   0.007162609,\n    0.027696826, -0.0021070621,  -0.024485396, -0.0042141243,   -0.02801937,\n   -0.019605145,   0.016281527,  -0.035143413,    0.01640774,   0.042323552\n]\n```\n\n## Specifying dimensions\n\nWith the `text-embedding-3` class of models, you can specify the size of the embeddings you want returned. For example by default `text-embedding-3-large` returns embeddings of dimension 3072:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddingsDefaultDimensions = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n});\n\nconst vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments([\"some text\"]);\nconsole.log(vectorsDefaultDimensions[0].length);\n```\n\n```output  theme={null}\n3072\n```\n\nBut by passing in `dimensions: 1024` we can reduce the size of our embeddings to 1024:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings1024 = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n  dimensions: 1024,\n});\n\nconst vectors1024 = await embeddings1024.embedDocuments([\"some text\"]);\nconsole.log(vectors1024[0].length);\n```\n\n```output  theme={null}\n1024\n```\n\n## Custom URLs\n\nYou can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst model = new OpenAIEmbeddings({\n  configuration: {\n    baseURL: \"https://your_custom_url.com\",\n  },\n});\n```\n\nYou can also pass other `ClientOptions` parameters accepted by the official SDK.\n\nIf you are hosting on Azure OpenAI, see the [dedicated page instead](/oss/javascript/integrations/text_embedding/azure_openai).\n\n***\n\n## API reference\n\nFor detailed documentation of all OpenAIEmbeddings features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/openai.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Contributing to code\nLink: https://docs.langchain.comoss/python/contributing/code\nContent: Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.\n\n## Getting started\n\n<Note>\n  Before submitting large **new features or refactors**, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work.\n\n  This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. See the quickstart guide below.\n</Note>\n\n### Quick fix: submit a bugfix\n\nFor simple bugfixes, you can get started immediately:\n\n<Steps>\n  <Step title=\"Reproduce the issue\">\n    Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification\n  </Step>\n\n  <Step title=\"Fork the repository\">\n    Fork the [LangChain](https://github.com/langchain-ai/langchain) or [LangGraph](https://github.com/langchain-ai/langgraph) repo to your <Tooltip tip=\"If you fork to an organization account, maintainers will be unable to make edits.\">personal GitHub account</Tooltip>\n  </Step>\n\n  <Step title=\"Clone and setup\">\n    ```bash  theme={null}\n    git clone https://github.com/your-username/name-of-forked-repo.git\n\n    # For instance, for LangChain:\n    git clone https://github.com/parrot123/langchain.git\n\n    # For LangGraph:\n    git clone https://github.com/parrot123/langgraph.git\n    ```\n\n    ```bash  theme={null}\n    # Inside your repo, install dependencies\n    uv sync --all-groups\n    ```\n\n    You will need to install [`uv`](https://docs.astral.sh/uv/) if you haven't previously\n  </Step>\n\n  <Step title=\"Create a branch\">\n    Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later.\n\n    ```bash  theme={null}\n    git checkout -b your-username/short-bugfix-name\n    ```\n  </Step>\n\n  <Step title=\"Write failing tests\">\n    Add [unit tests](#test-writing-guidelines) that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions\n  </Step>\n\n  <Step title=\"Make your changes\">\n    Fix the bug while following our [code quality standards](#code-quality-standards). Make the **minimal change necessary** to resolve the issue\n  </Step>\n\n  <Step title=\"Verify the fix\">\n    Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR\n\n    ```bash  theme={null}\n    make lint\n    make test\n\n    # For bugfixes involving integrations, also run:\n    make integration_tests\n    # (You may need to set up API testing credentials)\n    ```\n  </Step>\n\n  <Step title=\"Document the change\">\n    Update docstrings if behavior changes, add comments for complex logic\n  </Step>\n\n  <Step title=\"Submit a pull request\">\n    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #ISSUE_NUMBER`) so that the issue is automatically closed when your PR is merged.\n  </Step>\n</Steps>\n\n### Full development setup\n\nFor ongoing development or larger contributions:\n\n1. Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations\n2. Set up your environment following our [setup guide](#development-environment) below\n3. Understand the [repository structure](#repository-structure) and package organization\n4. Learn our [development workflow](#development-workflow) including testing and linting\n\n***\n\n## Contribution guidelines\n\nBefore you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a \"first contribution\" to your resume (or if you're just looking for a quick win) you might be better off doing a boot-camp or an online tutorial.\n\nContributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it's important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you're willing to take the time to do things well.\n\n### Backwards compatibility\n\n<Warning>\n  Breaking changes to public APIs are not allowed except for critical security fixes.\n\n  See our [versioning policy](/oss/python/versioning) for details on major version releases.\n</Warning>\n\nMaintain compatibility via:\n\n<AccordionGroup>\n  <Accordion title=\"Stable interfaces\">\n    **Always preserve**:\n\n    * Function signatures and parameter names\n    * Class interfaces and method names\n    * Return value structure and types\n    * Import paths for public APIs\n  </Accordion>\n\n  <Accordion title=\"Safe changes\">\n    **Acceptable modifications**:\n\n    * Adding new optional parameters\n\n    * Adding new methods to classes\n\n    * Improving performance without changing behavior\n\n    * Adding new modules or functions\n  </Accordion>\n\n  <Accordion title=\"Before making changes\">\n    * **Would this break existing user code?**\n\n    * Check if your target is public\n\n    * If needed, is it exported in `__init__.py`?\n\n    * Are there existing usage patterns in tests?\n  </Accordion>\n</AccordionGroup>\n\n### New features\n\nWe aim to keep the bar high for new features. We generally don't accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies.\n\nIn general, feature contribution requirements include:\n\n<Steps>\n  <Step title=\"Design discussion\">\n    Open an issue describing:\n\n    * The problem you're solving\n    * Proposed API design\n    * Expected usage patterns\n  </Step>\n\n  <Step title=\"Implementation\">\n    * Follow existing code patterns\n    * Include comprehensive tests and documentation\n    * Consider security implications\n  </Step>\n\n  <Step title=\"Integration considerations\">\n    * How does this interact with existing features?\n    * Are there performance implications?\n    * Does this introduce new dependencies?\n\n    We will reject features that are likely to lead to security vulnerabilities or reports.\n  </Step>\n</Steps>\n\n### Security guidelines\n\n<Warning>\n  Security is paramount. Never introduce vulnerabilities or unsafe patterns.\n</Warning>\n\nSecurity checklist:\n\n<AccordionGroup>\n  <Accordion title=\"Input validation\">\n    * Validate and sanitize all user inputs\n    * Properly escape data in templates and queries\n    * Never use `eval()`, `exec()`, or `pickle` on user data, as this can lead to arbitrary code execution vulnerabilities\n  </Accordion>\n\n  <Accordion title=\"Error handling\">\n    * Use specific exception types\n    * Don't expose sensitive information in error messages\n    * Implement proper resource cleanup\n  </Accordion>\n\n  <Accordion title=\"Dependencies\">\n    * Avoid adding hard dependencies\n    * Keep optional dependencies minimal\n    * Review third-party packages for security issues\n  </Accordion>\n</AccordionGroup>\n\n***\n\n## Development environment\n\n<Warning>\n  Our Python projects use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management. Make sure you have the latest version installed.\n</Warning>\n\nOnce you've reviewed the [contribution guidelines](#contribution-guidelines), set up a development environment for the package(s) you're working on.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <AccordionGroup>\n      <Accordion title=\"Core abstractions\">\n        For changes to `langchain-core`:\n\n        ```bash  theme={null}\n        cd libs/core\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Main package\">\n        For changes to `langchain`:\n\n        ```bash  theme={null}\n        cd libs/langchain\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Partner packages\">\n        For changes to [partner integrations](/oss/python/integrations/providers/overview):\n\n        ```bash  theme={null}\n        cd libs/partners/langchain-{partner}\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Community packages\">\n        For changes to community integrations (located in a [separate repo](https://github.com/langchain-ai/langchain-community)):\n\n        ```bash  theme={null}\n        cd libs/community/langchain_community/path/to/integration\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Repository structure\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    LangChain is organized as a monorepo with multiple packages:\n\n    <AccordionGroup>\n      <Accordion title=\"Core packages\" defaultOpen>\n        * **[`langchain`](https://github.com/langchain-ai/langchain/tree/master/libs/langchain#readme)** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic\n        * **[`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core#readme)** (located in `libs/core/`): Base interfaces and core abstractions\n      </Accordion>\n\n      <Accordion title=\"Partner packages\">\n        Located in `libs/partners/`, these are independently versioned packages for specific integrations. For example:\n\n        * **[`langchain-openai`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai#readme)**: [OpenAI](/oss/python/integrations/providers/openai) integrations\n        * **[`langchain-anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic#readme)**: [Anthropic](/oss/python/integrations/providers/anthropic) integrations\n        * **[`langchain-google-genai`](https://github.com/langchain-ai/langchain-google/)**: [Google Generative AI](/oss/python/integrations/chat/google_generative_ai) integrations\n\n        Many partner packages are in external repositories. Please check the [list of integrations](/oss/python/integrations/providers/overview) for details.\n      </Accordion>\n\n      <Accordion title=\"Supporting packages\">\n        * **[`langchain-text-splitters`](https://github.com/langchain-ai/langchain/tree/master/libs/text-splitters#readme)**: Text splitting utilities\n        * **[`langchain-standard-tests`](https://github.com/langchain-ai/langchain/tree/master/libs/standard-tests#readme)**: Standard test suites for integrations\n        * **[`langchain-cli`](https://github.com/langchain-ai/langchain/tree/master/libs/cli#readme)**: Command line interface\n        * **[`langchain-community`](https://github.com/langchain-ai/langchain-community)**: Community maintained integrations (located in a separate repo)\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Development workflow\n\n### Testing requirements\n\n<Info>\n  Directories are relative to the package you're working in.\n</Info>\n\nEvery code change must include comprehensive tests.\n\n#### Unit tests\n\n**Location**: `tests/unit_tests/`\n\n**Requirements**:\n\n* No network calls allowed\n* Test all code paths including edge cases\n* Use mocks for external dependencies\n\n```bash  theme={null}\nmake test\n\n# Or directly:\nuv run --group test pytest tests/unit_tests\n```\n\n#### Integration tests\n\n**Location**: `tests/integration_tests/`\n\nIntegration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.\n\nNot every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.\n\n**Requirements**:\n\n* Test real integrations with external services\n* Use environment variables for API keys\n* Skip gracefully if credentials unavailable\n\n```bash  theme={null}\nmake integration_tests\n```\n\n### Code quality standards\n\nContributions must adhere to the following quality requirements:\n\n<Tabs>\n  <Tab title=\"Type hints\">\n    **Required**: Complete type annotations for all functions\n\n    ```python  theme={null}\n    def process_documents(\n        docs: list[Document],\n        processor: DocumentProcessor,\n        *,\n        batch_size: int = 100\n    ) -> ProcessingResult:\n        \"\"\"Process documents in batches.\n\n        Args:\n            docs: List of documents to process.\n            processor: Document processing instance.\n            batch_size: Number of documents per batch.\n\n        Returns:\n            Processing results with success/failure counts.\n        \"\"\"\n    ```\n  </Tab>\n\n  <Tab title=\"Documentation\">\n    **Required**: [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) for all public functions\n\n    * Document all parameters and return values\n    * Include usage examples for complex functions\n    * Document raised exceptions\n    * Focus on \"why\" rather than \"what\"\n  </Tab>\n\n  <Tab title=\"Code style\">\n    **Automated**: Formatting and linting via [`ruff`](https://docs.astral.sh/ruff/)\n\n    ```bash  theme={null}\n    make format  # Apply formatting\n    make lint    # Check style and types\n    ```\n\n    **Standards**:\n\n    * Descriptive variable names\n    * Break up complex functions (aim for fewer than 20 lines)\n    * Follow existing patterns in the codebase\n  </Tab>\n</Tabs>\n\n***\n\n## Testing and validation\n\n### Running tests locally\n\nBefore submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <Steps>\n      <Step title=\"Unit tests\">\n        ```bash  theme={null}\n        make test\n        ```\n\n        All unit tests must pass\n      </Step>\n\n      <Step title=\"Integration tests\">\n        ```bash  theme={null}\n        make integration_tests\n        ```\n\n        (Run if your changes affect integrations)\n      </Step>\n\n      <Step title=\"Formatting\">\n        ```bash  theme={null}\n        make format\n        make lint\n        ```\n\n        Code must pass all style checks\n      </Step>\n\n      <Step title=\"Type checking\">\n        ```bash  theme={null}\n        make type_check\n        ```\n\n        All type hints must be valid\n      </Step>\n\n      <Step title=\"PR submission\">\n        Push your branch and open a pull request. Follow the provided form template. Note related issues using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.\n      </Step>\n    </Steps>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n### Test writing guidelines\n\nIn order to write effective tests, there's a few good practices to follow:\n\n* Use natural language to describe the test in docstrings\n* Use descriptive variable names\n* Be exhaustive with assertions\n\n<Tabs>\n  <Tab title=\"Unit tests\">\n    ```python  theme={null}\n    def test_document_processor_handles_empty_input():\n        \"\"\"Test processor gracefully handles empty document list.\"\"\"\n        processor = DocumentProcessor()\n\n        result = processor.process([])\n\n        assert result.success\n        assert result.processed_count == 0\n        assert len(result.errors) == 0\n    ```\n  </Tab>\n\n  <Tab title=\"Integration tests\">\n    ```python  theme={null}\n    @pytest.mark.requires(\"openai\")\n    def test_openai_chat_integration():\n        \"\"\"Test OpenAI chat integration with real API.\"\"\"\n\n        chat = ChatOpenAI()\n        response = chat.invoke(\"Hello\")\n\n        assert isinstance(response.content, str)\n        assert len(response.content) > 0\n    ```\n  </Tab>\n\n  <Tab title=\"Mock usage\">\n    ```python  theme={null}\n    def test_retry_mechanism(mocker):\n        \"\"\"Test retry mechanism handles transient failures.\"\"\"\n        mock_client = mocker.Mock()\n        mock_client.call.side_effect = [\n            ConnectionError(\"Temporary failure\"),\n            {\"result\": \"success\"}\n        ]\n\n        service = APIService(client=mock_client)\n        result = service.call_with_retry()\n\n        assert result[\"result\"] == \"success\"\n        assert mock_client.call.call_count == 2\n    ```\n  </Tab>\n</Tabs>\n\n## Getting help\n\nOur goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).\n\n<Check>\n  You're now ready to contribute high-quality code to LangChain!\n</Check>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Implement a LangChain integration\nLink: https://docs.langchain.comoss/python/contributing/implement-langchain\nContent: Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.\n\nLangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/python/integrations/chat), [tools](/oss/python/integrations/tools), [retrievers](/oss/python/integrations/retrievers), and more.\n\nYour integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.\n\n<Tabs>\n  <Tab title=\"Chat Models\">\n    Chat models are subclasses of the [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel) class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.\n\n    <Warning>\n      The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/python/langchain/models) for details on how LangChain chat models function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Tools\">\n    Tools are used in 2 main ways:\n\n    1. To define an \"input schema\" or \"args schema\" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a \"tool call\", or parameters to call the tool with.\n    2. To take a \"tool call\" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage.\n\n    The Tools class must inherit from the [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.\n\n    <Warning>\n      The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/python/langchain/tools) for details on how LangChain tools function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Retrievers\">\n    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.\n\n    <Warning>\n      The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/python/integrations/retrievers) for details on how LangChain retrievers function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Vector Stores\">\n    All vector stores must inherit from the [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.\n\n    See the [vector store integration guide](/oss/python/integrations/vectorstores) for details on implementing a vector store integration.\n\n    <Warning>\n      The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/python/integrations/vectorstores) for details on how LangChain vector stores function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Embeddings\">\n    Embedding models are subclasses of the [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) class.\n\n    <Warning>\n      The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/python/integrations/text_embedding) for details on how LangChain embedding models function.\n    </Warning>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_98787148",
    "id": "4693bd5d-45e8-4ff6-b0cf-bfb62e5e215a",
    "content": "{\n  \"collections\": \"General\",\n  \"total\": 10,\n  \"articles\": [\n    {\n      \"id\": \"b4ba8c34-6d1d-4653-8123-c2eeee2d34e5\",\n      \"title\": \"How do I set up ABAC to restrict user access to specific projects?\",\n      \"url\": \"https://support.langchain.com/articles/6539560119-how-do-i-set-up-abac-to-restrict-user-access-to-specific-projects\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\",\n      \"title\": \"What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\",\n      \"url\": \"https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"bef0ce3c-94c2-4999-b563-ff016fa3884f\",\n      \"title\": \"How do I get LangChain to complete security questionnaires for vendor assessments?\",\n      \"url\": \"https://support.langchain.com/articles/8555158040-how-do-i-get-langchain-to-complete-security-questionnaires-for-vendor-assessments\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"fe2b3c9e-8fa1-49dd-a52d-df92d88050c4\",\n      \"title\": \"How do I update billing information on an existing invoice?\",\n      \"url\": \"https://support.langchain.com/articles/5832977338-how-do-i-update-billing-information-on-an-existing-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"4a127c79-3504-4afa-befa-ec974b85c5f8\",\n      \"title\": \"Why am I being charged for node executions and standby minutes on my development deployment?\",\n      \"url\": \"https://support.langchain.com/articles/2626278629-why-am-i-being-charged-for-node-executions-on-my-development-deployment\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b170d29c-925a-4aab-b7a0-354f47b7dc77\",\n      \"title\": \"Can I enforce Google SSO-only authentication for my organization?\",\n      \"url\": \"https://support.langchain.com/articles/9820106567-can-i-enforce-google-sso-only-authentication-for-my-organization\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"50634017-ff08-43bb-b7ae-7518aab83623\",\n      \"title\": \"How do I get access to LangSmith Agent Builder?\",\n      \"url\": \"https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"40305680-01d3-4506-8257-16db939e39bf\",\n      \"title\": \"How do I get help with billing issues and payment problems?\",\n      \"url\": \"https://support.langchain.com/articles/3779255598-how-do-i-get-help-with-billing-issues-and-payment-problems\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"9273a5a9-0a53-4d5a-917a-c19ebeec64fe\",\n      \"title\": \"How do I get a copy of my LangSmith invoice?\",\n      \"url\": \"https://support.langchain.com/articles/6317483716-how-do-i-get-a-copy-of-my-langsmith-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"af6b64a9-ed4a-4993-b54d-d4913eeb1d26\",\n      \"title\": \"How do I change my email address in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9223968956-how-do-i-change-my-email-address-in-langsmith\",\n      \"collection\": \"General\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "tool",
    "name": "get_article_content",
    "tool_call_id": "call_25986849",
    "id": "05e3b71d-9456-4c0e-860b-4259c722fd07",
    "content": "ID: 0ef20428-fe1b-4892-a0ce-c9c2f36677d6\nTitle: What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\nURL: https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\nCollection: LangGraph\n\nContent:\n<h1 style=\"padding-left: 0px!important;\" level=\"1\" id=\"\">Context</h1><p style=\"padding-left: 0px!important;\">Startup plan customers often have questions about LangGraph Cloud pricing and whether additional discounts are available beyond the standard Startup plan benefits. The Startup plan provides discounted LangSmith seats and additional free traces, but customers may wonder about specific pricing for LangGraph Cloud production deployments and development environments.</p><h1 style=\"padding-left: 0px!important;\" level=\"1\" id=\"\">Answer</h1><p style=\"padding-left: 0px!important;\">LangGraph Cloud pricing follows the same structure as the Plus plan, even for Startup plan customers. Here's what you need to know:</p><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">Development Deployments</h2><p style=\"padding-left: 0px!important;\">The Startup plan includes one free Developer deployment with unlimited node executions. To ensure you're using the free tier:</p><ol><li><p style=\"padding-left: 0px!important;\">Go to LangSmith &gt; Deployments</p></li><li><p style=\"padding-left: 0px!important;\">Select your deployment</p></li><li><p style=\"padding-left: 0px!important;\">Verify the \"Deployment type\" shows as \"Developer (free)\"</p></li></ol><p style=\"padding-left: 0px!important;\">If you were accidentally charged for a development deployment that should have been free, contact support for assistance.</p><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">Production Deployments</h2><p style=\"padding-left: 0px!important;\">Production deployments on LangGraph Cloud are charged at the same per-minute machine cost as the Plus plan. Currently, there are no additional discounts available for LangSmith Deployment production usage beyond what's included in the Startup plan.</p><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">Startup Plan Benefits</h2><p style=\"padding-left: 0px!important;\">The Startup plan provides:</p><ul><li><p style=\"padding-left: 0px!important;\">50% discount on seat pricing </p></li><li><p style=\"padding-left: 0px!important;\">30,000 free traces per month</p></li><li><p style=\"padding-left: 0px!important;\">One free Developer deployment with unlimited node executions</p></li><li><p style=\"padding-left: 0px!important;\">Valid for 2 years from enrollment</p></li></ul><p style=\"padding-left: 0px!important;\">For current pricing details, visit the <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://www.langchain.com/pricing\">LangChain pricing page</a>.</p>"
  },
  {
    "role": "tool",
    "name": "get_article_content",
    "tool_call_id": "call_64468550",
    "id": "8bf5a80f-7cad-4189-8c8a-e1658364644e",
    "content": "ID: 50634017-ff08-43bb-b7ae-7518aab83623\nTitle: How do I get access to LangSmith Agent Builder?\nURL: https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\nCollection: LangSmith\n\nContent:\n<h1 style=\"padding-left: 0px!important;\" level=\"1\" id=\"\">Context</h1><p style=\"padding-left: 0px!important;\">LangSmith Agent Builder is currently in private preview and allows users to create and test agents directly in LangSmith with a no-code text-to-agent experience. Many customers are interested in trying out this new feature to build agents without requiring extensive coding knowledge.</p><h1 style=\"padding-left: 0px!important;\" level=\"1\" id=\"\">Answer</h1><p style=\"padding-left: 0px!important;\">To get access to LangSmith Agent Builder, you need to join the waitlist and follow these steps:</p><ol><li><p style=\"padding-left: 0px!important;\"><strong>Sign up for a LangSmith Cloud account</strong> if you don't already have one. Agent Builder is currently only available on the cloud tier, not for self-hosted instances (though self-hosted support is coming soon).</p></li><li><p style=\"padding-left: 0px!important;\"><strong>Navigate to the Agent Builder tab</strong> in your LangSmith UI. You'll see it in the left navigation menu under the \"Agents\" section.</p></li><li><p style=\"padding-left: 0px!important;\"><strong>Fill out the waitlist form</strong> that appears when you click on the Agent Builder tab. You can also access it directly at: <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://www.langchain.com/langsmith-agent-builder-waitlist\">https://www.langchain.com/langsmith-agent-builder-waitlist</a></p></li></ol><img src=\"https://assets.usepylon.com/3343362c-fcf1-4bd8-b712-bb6b110139fa%2F1761846782172-image.png?Expires=253370764800&amp;Signature=lCYn7DltOI~nNFlf2PoLIrd2tjtKPLvCY7sp2yxw8dQ-Q48hhqFUO~Jf7arpCPwjmx9wchCSMuyCF-HGnCFeqGLSnwSr7IzgMDDY~sq6TJRZYvyN8G3nxBkeOlwpuNfqFAz3MQHT7opjkAlWI0BbjAgm0Q1PGg1RzdYFiVlNeN69J9HW455exOMwV5Mr9ZhxUAGvhh6Ger9U~9UkLeE5-bMsOLPDfnRw~yoztMTSymZcSOG9g3YLwr1rdJC6w~XN7clxu0FWkDmK0CIcGxMC7h63kJUYiCRTY4ynuncHwd24KCu-yGT85-HyMRZNdQItMCqCqvm3d3ezKfk5XowF~A__&amp;Key-Pair-Id=K3NV4LZ47N8M46\" data-width=\"100%\" data-align=\"center\" alt=\"LangSmith left-hand navigation menu showing the Agent Builder tab under the Agents section\"><p style=\"padding-left: 0px!important;\"><strong>Important notes:</strong></p><ul><li><p style=\"padding-left: 0px!important;\">Agent Builder is expected to move to public beta within approximately 2 weeks from the current date</p></li><li><p style=\"padding-left: 0px!important;\">The feature is currently only available for LangSmith Cloud accounts</p></li><li><p style=\"padding-left: 0px!important;\">You may encounter some bugs during the private preview phase</p></li><li><p style=\"padding-left: 0px!important;\">If you're part of an organization, make sure to sign up using your organization's LangSmith account rather than a personal account</p></li></ul><p style=\"padding-left: 0px!important;\">Once you're granted access, you'll receive an email notification and the Agent Builder will become available in your LangSmith interface.</p>"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3852-a3b8-7f52-a602-2a128816715e",
    "content": "**LangChain offers three main products: LangChain (open-source LLM framework), LangGraph (agent orchestration), and LangSmith (observability platform with optional LangGraph Cloud deployments).**\n\n**LangChain** and **LangGraph** are completely free open-source libraries (Python/JS) for building LLM apps and stateful agents. No costs for core usage\u2014just your LLM provider bills (OpenAI, Anthropic, etc.).\n\n**LangSmith** is the paid observability platform for tracing, debugging, evaluating, and deploying agents:\n\n| Plan | Monthly Cost | Key Limits for Small Dev Business |\n|------|--------------|-----------------------------------|\n| **Developer** | $39/seat | 10k traces/month included, then $0.50/1k extra traces |\n| **Plus** | $99/seat | 50k traces/month included, then $0.30/1k extra |\n| **Startup** (discounted) | ~$50/seat (50% off Plus) | 30k free traces/month + 1 free Developer deployment; apply via form |\n| **Enterprise** | Custom | Unlimited; contact sales |\n\n**LangGraph Cloud Deployments** (production agent hosting):\n- **Development**: Free unlimited node executions (1 per org on Startup)\n- **Production**: $0.005/agent run + machine costs (~$0.10-$1/hour depending on size); same for all plans\n\nTraces = full app/agent executions. A small dev business (1-3 devs, few thousand runs/month) typically fits Developer plan (~$40-80/month total).\n\nUpgrade via [smith.langchain.com/settings/payments](https://smith.langchain.com/settings/payments). Startup discount: [airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form).\n\n**Relevant docs:**\n- [LangSmith Pricing FAQ](https://docs.langchain.com/langsmith/pricing-faq)\n- [LangGraph Overview](https://docs.langchain.com/oss/python/langgraph/overview)\n- [LangChain Overview](https://docs.langchain.com/oss/python/langchain/overview)\n- [LangSmith Pricing Plans](https://docs.langchain.com/langsmith/pricing-plans)"
  }
]