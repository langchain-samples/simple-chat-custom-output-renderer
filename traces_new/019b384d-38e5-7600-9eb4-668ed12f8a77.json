[
  {
    "role": "user",
    "id": "7f7fac88-2541-4a2d-bcba-3997dd1ca39b",
    "content": "What is tool in langraph or langchain"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_qFSBi0VujIidhMjBEUe8KnQG",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"python\", \"page_size\": 5, \"query\": \"tools\"}"
        }
      },
      {
        "type": "function",
        "id": "call_FLbA3fadTLm4243aCxnF9duL",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"OSS (LangChain and LangGraph),LangSmith Studio,SDKs and APIs\"}"
        }
      }
    ],
    "id": "lc_run--019b384d-3db2-7340-a2f9-862a0dac60b9",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_qFSBi0VujIidhMjBEUe8KnQG",
    "id": "4be41f80-94b1-4c42-99f9-b571406f3cad",
    "content": "Result 1:\nTitle: Configure prompt settings\nLink: https://docs.langchain.comlangsmith/managing-model-configurations\nContent: The LangSmith [playground](/langsmith/prompt-engineering-concepts#prompt-playground) enables you to control various settings for your prompts. The **Prompt Settings** window contains:\n\n* [Model configuration](#model-configurations)\n* [Tool settings](#tool-settings)\n* [Prompt formatting](#prompt-formatting)\n\nTo access **Prompt Settings**:\n\n1. Navigate to the **Playground** in the left sidebar.\n2. Under the **Prompts** heading select the gear <Icon icon=\"gear\" iconType=\"solid\" /> icon next to the model name, which will launch the **Prompt Settings** window.\n\n   <div style={{ textAlign: 'center' }}>\n     <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=6c0f7d7012b1e5295fe545149f955e6b\" alt=\"Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc.\" data-og-width=\"886\" width=\"886\" data-og-height=\"689\" height=\"689\" data-path=\"langsmith/images/model-config-light.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=280&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=4e3b9ad92f6f14f4e0523bef50199318 280w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=560&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=e538eb740495a8afa8bfc552b13ae294 560w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=840&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=ebe73264e977153c869fd04d1552d09b 840w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1100&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=2eeb01882056046bc73cc019d674af7e 1100w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1650&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=8f28fe2fe8054cf0623fb9d17f91966f 1650w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=2500&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=cf9ad39be3623e73322d123699e73f19 2500w\" />\n\n     <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=2e9da272c3fc8f7ac958c6e6d1da85e3\" alt=\"Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc.\" data-og-width=\"881\" width=\"881\" data-og-height=\"732\" height=\"732\" data-path=\"langsmith/images/model-config-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=280&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=652fb75a4682cfc813743a1260764e59 280w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=560&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=02c980a8387f3d69a5870660b1668080 560w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=840&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ee633c06056fa7ad46ea58a179afa169 840w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1100&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=f62a35ed726b5f89c156a40c9ea76f2c 1100w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1650&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=18114575db8e6c7ce928763ddcb88c12 1650w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=2500&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ab24dc4975def52db55c4896ead5b77c 2500w\" />\n   </div>\n\n## Model configurations\n\nModel configurations define the parameters your prompt runs against. In the LangSmith Playground, you can save and manage these configurations, which allows you to reuse your preferred settings across prompts and sessions. For details on specific settings, refer to your model provider\u2019s documentation (for example, [Anthropic](https://platform.claude.com/docs/en/api/messages), [OpenAI](https://platform.openai.com/docs/api-reference/responses/create)).\n\n### Create saved configurations\n\n1. In the **Model Configurations** tab, adjust the model configuration as needed\u2014you can select a [saved configuration to edit](#edit-configurations).\n2. Click the **Save As** button in the top bar.\n3. Enter a name and optional description for your configuration and confirm.\n4. Now that you've saved the configuration, anyone in your organization's [workspace](/langsmith/administration-overview#workspaces) can access it. All saved configurations are available in the **Model Configuration** dropdown.\n5. Once you have created a saved configuration, you can set it as your default, so any new prompt you create will automatically use this configuration. To set a configuration as your default, click the **Set as default** <Icon icon=\"thumbtack\" iconType=\"solid\" /> icon next to the model name in the dropdown.\n\n### Edit configurations\n\n1. To rename a saved configuration, or update the description, select the configuration name or description and make the necessary changes.\n2. Update the current configuration's parameters as needed and click the **Save** button at the top.\n\n### Delete configurations\n\n1. Select the configuration you want to remove.\n2. Click the trash <Icon icon=\"trash\" iconType=\"solid\" /> icon to delete it.\n\n### Extra parameters\n\nThe **Extra Parameters** field allows you to pass additional model parameters that aren't directly supported in the LangSmith interface. This is particularly useful in two scenarios:\n\n1. When model providers release new parameters that haven't yet been integrated into the LangSmith interface. You can specify these parameters in JSON format to use them right away. For example:\n\n   ```json  theme={null}\n   {\n       \"reasoning_effort\": \"medium\"\n   }\n   ```\n\n2. When troubleshooting parameter-related errors in the playground, such as:\n\n   ```\n   TypeError: AsyncCompletions.create() got an unexpected keyword argument 'max_concurrency'\n   ```\n\n   If you receive an error about unnecessary parameters (which is more common when using [LangChain JS](/oss/python/langchain/overview) for run tracing), you can use this field to remove the extra parameters.\n\n## Tool settings\n\n[*Tools*](/langsmith/prompt-engineering-concepts#tools) enable your LLM to perform tasks like searching the web, looking up information, and so on. In the **Tools Settings** tab, you can manage the ways your LLM uses and accesses the tools you have defined in your prompt, including:\n\n* **Parallel Tool Calls**: Calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously. (Dependent on model support for parallel execution.)\n* **Tool Choice**: Select the tools that the model can access. For more details, refer to [Use tools in a prompt](/langsmith/use-tools).\n\n## Prompt formatting\n\nThe **Prompt Format** tab allows you to specify:\n\n* The **Prompt type**. For details on chat and completion prompts, refer to [Prompt engineering](/langsmith/prompt-engineering-concepts#chat-vs-completion) concepts.\n* The **Template format**. For details on prompt templating and using variables, refer to [F-string vs. mustache](/langsmith/prompt-engineering-concepts##f-string-vs-mustache).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/managing-model-configurations.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Tools and Toolkits\nLink: https://docs.langchain.comoss/javascript/integrations/tools/index\nContent: [Tools](/oss/javascript/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n\nA [toolkit](/oss/javascript/langchain/tools#toolkits) is a collection of tools meant to be used together.\n\n## Integration Platforms\n\nThe following platforms provide access to multiple tools and services through a unified interface:\n\n| Tool/Toolkit                                            | Number of Integrations | Pricing             | Key Features                                               |\n| ------------------------------------------------------- | ---------------------- | ------------------- | ---------------------------------------------------------- |\n| [Composio](/oss/javascript/integrations/tools/composio) | 500+                   | Free tier available | OAuth handling, event-driven workflows, multi-user support |\n\n## All Tools and Toolkits\n\n<Columns cols={3}>\n  <Card title=\"Azure Container Apps Dynamic Sessions\" icon=\"link\" href=\"/oss/javascript/integrations/tools/azure_dynamic_sessions\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Connery Action Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/connery\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Composio\" icon=\"link\" href=\"/oss/javascript/integrations/tools/composio\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dall-E Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/dalle\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Decodo Tools\" icon=\"link\" href=\"/oss/javascript/integrations/tools/decodo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Discord Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/discord_tool\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DuckDuckGoSearch\" icon=\"link\" href=\"/oss/javascript/integrations/tools/duckduckgo_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ExaSearchResults\" icon=\"link\" href=\"/oss/javascript/integrations/tools/exa_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FalkorDB\" icon=\"link\" href=\"/oss/javascript/integrations/tools/falkordb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gmail Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_gmail\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GOAT\" icon=\"link\" href=\"/oss/javascript/integrations/tools/goat\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Calendar Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_calendar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Places Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_places\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Routes Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_routes\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Scholar\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_scholar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Trends Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_trends\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"JigsawStack Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/jigsawstack\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Agent with AWS Lambda\" icon=\"link\" href=\"/oss/javascript/integrations/tools/lambda_agent\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Python interpreter tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/pyinterpreter\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SearchApi tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/searchapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Searxng Search tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/searxng\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SerpAPI\" icon=\"link\" href=\"/oss/javascript/integrations/tools/serpapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"StackExchange Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/stackexchange\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Stagehand AI Web Automation Toolkit\" icon=\"link\" href=\"/oss/javascript/integrations/tools/stagehand\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Crawl\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_crawl\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Extract\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_extract\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Map\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_map\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Search\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Web Browser Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/webbrowser\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wikipedia tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/wikipedia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"WolframAlpha Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/wolframalpha\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n<Info>\n  If you'd like to write your own tool, see [this how-to](/oss/javascript/langchain/tools#customize-tool-properties). If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).\n</Info>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/tools/index.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Tools\nLink: https://docs.langchain.comoss/javascript/langchain/tools\nContent: Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems\u2014such as APIs, databases, or file systems\u2014using structured input.\n\nTools are components that [agents](/oss/javascript/langchain/agents) call to perform actions. They extend model capabilities by letting them interact with the world through well-defined inputs and outputs.\n\nTools encapsulate a callable function and its input schema. These can be passed to compatible [chat models](/oss/javascript/langchain/models), allowing the model to decide whether to invoke a tool and with what arguments. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.\n\n<Note>\n  **Server-side tool use**\n\n  Some chat models (e.g., [OpenAI](/oss/javascript/integrations/chat/openai), [Anthropic](/oss/javascript/integrations/chat/anthropic), and [Gemini](/oss/javascript/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/javascript/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/javascript/integrations/providers/overview) to learn how to access these tools with your specific chat model.\n</Note>\n\n## Create tools\n\n### Basic tool definition\n\nThe simplest way to create a tool is by importing the `tool` function from the `langchain` package. You can use [zod](https://zod.dev/) to define the tool's input schema:\n\n```ts  theme={null}\nimport * as z from \"zod\"\nimport { tool } from \"langchain\"\n\nconst searchDatabase = tool(\n  ({ query, limit }) => `Found ${limit} results for '${query}'`,\n  {\n    name: \"search_database\",\n    description: \"Search the customer database for records matching the query.\",\n    schema: z.object({\n      query: z.string().describe(\"Search terms to look for\"),\n      limit: z.number().describe(\"Maximum number of results to return\"),\n    }),\n  }\n);\n```\n\n## Accessing Context\n\n<Info>\n  **Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\n  The runtime context provides a structured way to supply runtime data, such as DB connections, user IDs, or config, into your tools. This avoids global state and keeps tools testable and reusable.\n</Info>\n\n#### Context\n\nTools can access an agent's runtime context through the `config` parameter:\n\n```ts  theme={null}\nimport * as z from \"zod\"\nimport { ChatOpenAI } from \"@langchain/openai\"\nimport { createAgent } from \"langchain\"\n\nconst getUserName = tool(\n  (_, config) => {\n    return config.context.user_name\n  },\n  {\n    name: \"get_user_name\",\n    description: \"Get the user's name.\",\n    schema: z.object({}),\n  }\n);\n\nconst contextSchema = z.object({\n  user_name: z.string(),\n});\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserName],\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"What is my name?\" }]\n  },\n  {\n    context: { user_name: \"John Smith\" }\n  }\n);\n```\n\n#### Memory (Store)\n\nAccess persistent data across conversations using the store. The store is accessed via `config.store` and allows you to save and retrieve user-specific or application-specific data.\n\n```ts expandable theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst store = new InMemoryStore();\n\n// Access memory\nconst getUserInfo = tool(\n  async ({ user_id }) => {\n    const value = await store.get([\"users\"], user_id);\n    console.log(\"get_user_info\", user_id, value);\n    return value;\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({\n      user_id: z.string(),\n    }),\n  }\n);\n\n// Update memory\nconst saveUserInfo = tool(\n  async ({ user_id, name, age, email }) => {\n    console.log(\"save_user_info\", user_id, name, age, email);\n    await store.put([\"users\"], user_id, { name, age, email });\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info.\",\n    schema: z.object({\n      user_id: z.string(),\n      name: z.string(),\n      age: z.number(),\n      email: z.string(),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserInfo, saveUserInfo],\n  store,\n});\n\n// First session: save user info\nawait agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\",\n    },\n  ],\n});\n\n// Second session: get user info\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"Get user info for user with id 'abc123'\" },\n  ],\n});\n\nconsole.log(result);\n// Here is the user info for user with ID \"abc123\":\n// - Name: Foo\n// - Age: 25\n// - Email: foo@langchain.dev\n```\n\n#### Stream Writer\n\nStream custom updates from tools as they execute using `config.streamWriter`. This is useful for providing real-time feedback to users about what a tool is doing.\n\n```ts  theme={null}\nimport * as z from \"zod\";\nimport { tool } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }, config) => {\n    const writer = config.streamWriter;\n\n    // Stream custom updates as the tool executes\n    writer(`Looking up data for city: ${city}`);\n    writer(`Acquired data for city: ${city}`);\n\n    return `It's always sunny in ${city}!`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather for a given city.\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Google\nLink: https://docs.langchain.comoss/python/integrations/providers/google\nContent: Tools\nGoogle Search Perform web searches using Google Custom Search Engine (CSE). Google Drive Tools for interacting with Google Drive. Google Finance Query financial data. Google Jobs Query job listings. Google Lens Perform visual searches. Google Places Search for places information. Google Scholar Search academic papers. Google Trends Query Google Trends data.\n\nToolkits\nCollections of tools for specific Google services. Gmail Toolkit to create, get, search, and send emails using the Gmail API.\n\nYouTube\nSearch tool Search YouTube videos without the official API. Audio loader Download audio from YouTube videos. Transcripts loader Load video transcripts. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\n\n---\n\nResult 5:\nTitle: Tools\nLink: https://docs.langchain.comoss/python/langchain/tools\nContent: Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems\u2014such as APIs, databases, or file systems\u2014using structured input.\n\nTools are components that [agents](/oss/python/langchain/agents) call to perform actions. They extend model capabilities by letting them interact with the world through well-defined inputs and outputs.\n\nTools encapsulate a callable function and its input schema. These can be passed to compatible [chat models](/oss/python/langchain/models), allowing the model to decide whether to invoke a tool and with what arguments. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.\n\n<Note>\n  **Server-side tool use**\n\n  Some chat models (e.g., [OpenAI](/oss/python/integrations/chat/openai), [Anthropic](/oss/python/integrations/chat/anthropic), and [Gemini](/oss/python/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/python/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/python/integrations/providers/overview) to learn how to access these tools with your specific chat model.\n</Note>\n\n## Create tools\n\n### Basic tool definition\n\nThe simplest way to create a tool is with the [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) decorator. By default, the function's docstring becomes the tool's description that helps the model understand when to use it:\n\n```python  theme={null}\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str, limit: int = 10) -> str:\n    \"\"\"Search the customer database for records matching the query.\n\n    Args:\n        query: Search terms to look for\n        limit: Maximum number of results to return\n    \"\"\"\n    return f\"Found {limit} results for '{query}'\"\n```\n\nType hints are **required** as they define the tool's input schema. The docstring should be informative and concise to help the model understand the tool's purpose.\n\n### Customize tool properties\n\n#### Custom tool name\n\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\n\n```python  theme={null}\n@tool(\"web_search\")  # Custom name\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nprint(search.name)  # web_search\n```\n\n#### Custom tool description\n\nOverride the auto-generated tool description for clearer model guidance:\n\n```python  theme={null}\n@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\ndef calc(expression: str) -> str:\n    \"\"\"Evaluate mathematical expressions.\"\"\"\n    return str(eval(expression))\n```\n\n### Advanced schema definition\n\nDefine complex inputs with Pydantic models or JSON schemas:\n\n<CodeGroup>\n  ```python Pydantic model theme={null}\n  from pydantic import BaseModel, Field\n  from typing import Literal\n\n  class WeatherInput(BaseModel):\n      \"\"\"Input for weather queries.\"\"\"\n      location: str = Field(description=\"City name or coordinates\")\n      units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n          default=\"celsius\",\n          description=\"Temperature unit preference\"\n      )\n      include_forecast: bool = Field(\n          default=False,\n          description=\"Include 5-day forecast\"\n      )\n\n  @tool(args_schema=WeatherInput)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n\n  ```python JSON Schema theme={null}\n  weather_schema = {\n      \"type\": \"object\",\n      \"properties\": {\n          \"location\": {\"type\": \"string\"},\n          \"units\": {\"type\": \"string\"},\n          \"include_forecast\": {\"type\": \"boolean\"}\n      },\n      \"required\": [\"location\", \"units\", \"include_forecast\"]\n  }\n\n  @tool(args_schema=weather_schema)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n</CodeGroup>\n\n### Reserved argument names\n\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\n\n| Parameter name | Purpose                                                                |\n| -------------- | ---------------------------------------------------------------------- |\n| `config`       | Reserved for passing `RunnableConfig` to tools internally              |\n| `runtime`      | Reserved for `ToolRuntime` parameter (accessing state, context, store) |\n\nTo access runtime information, use the [`ToolRuntime`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.ToolRuntime) parameter instead of naming your own arguments `config` or `runtime`.\n\n## Accessing Context\n\n<Info>\n  **Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\n  Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.\n</Info>\n\nTools can access runtime information through the `ToolRuntime` parameter, which provides:\n\n* **State** - Mutable data that flows through execution (e.g., messages, counters, custom fields)\n* **Context** - Immutable configuration like user IDs, session details, or application-specific configuration\n* **Store** - Persistent long-term memory across conversations\n* **Stream Writer** - Stream custom updates as tools execute\n* **Config** - `RunnableConfig` for the execution\n* **Tool Call ID** - ID of the current tool call\n\n```mermaid  theme={null}\ngraph LR\n    %% Runtime Context\n    subgraph \"\ud83d\udd27 Tool Runtime Context\"\n        A[Tool Call] --> B[ToolRuntime]\n        B --> C[State Access]\n        B --> D[Context Access]\n        B --> E[Store Access]\n        B --> F[Stream Writer]\n    end\n\n    %% Available Resources\n    subgraph \"\ud83d\udcca Available Resources\"\n        C --> G[Messages]\n        C --> H[Custom State]\n        D --> I[User ID]\n        D --> J[Session Info]\n        E --> K[Long-term Memory]\n        E --> L[User Preferences]\n    end\n\n    %% Tool Capabilities\n    subgraph \"\u26a1 Enhanced Tool Capabilities\"\n        M[Context-Aware Tools]\n        N[Stateful Tools]\n        O[Memory-Enabled Tools]\n        P[Streaming Tools]\n    end\n\n    %% Connections\n    G --> M\n    H --> N\n    I --> M\n    J --> M\n    K --> O\n    L --> O\n    F --> P\n```\n\n### `ToolRuntime`\n\nUse `ToolRuntime` to access all runtime information in a single parameter. Simply add `runtime: ToolRuntime` to your tool signature, and it will be automatically injected without being exposed to the LLM.\n\n<Info>\n  **`ToolRuntime`**: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate [`InjectedState`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedState), [`InjectedStore`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedStore), [`get_runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.get_runtime), and [`InjectedToolCallId`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.InjectedToolCallId) annotations.\n\n  The runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.\n</Info>\n\n**Accessing state:**\n\nTools can access the current graph state using `ToolRuntime`:\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n# Access the current conversation state\n@tool\ndef summarize_conversation(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Summarize the conversation so far.\"\"\"\n    messages = runtime.state[\"messages\"]\n\n    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n\n    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\n\n# Access custom state fields\n@tool\ndef get_user_preference(\n    pref_name: str,\n    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model\n) -> str:\n    \"\"\"Get a user preference value.\"\"\"\n    preferences = runtime.state.get(\"user_preferences\", {})\n    return preferences.get(pref_name, \"Not set\")\n```\n\n<Warning>\n  The `runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `runtime` is *not* included in the request.\n</Warning>\n\n**Updating state:**\n\nUse [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) to update the agent's state or control the graph's execution flow:\n\n```python  theme={null}\nfrom langgraph.types import Command\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langchain.tools import tool, ToolRuntime\n\n# Update the conversation history by removing all messages\n@tool\ndef clear_conversation() -> Command:\n    \"\"\"Clear the conversation history.\"\"\"\n\n    return Command(\n        update={\n            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],\n        }\n    )\n\n# Update the user_name in the agent state\n@tool\ndef update_user_name(\n    new_name: str,\n    runtime: ToolRuntime\n) -> Command:\n    \"\"\"Update the user's name.\"\"\"\n    return Command(update={\"user_name\": new_name})\n```\n\n#### Context\n\nAccess immutable configuration and contextual data like user IDs, session details, or application-specific configuration through `runtime.context`.\n\nTools can access runtime context through `ToolRuntime`:\n\n```python  theme={null}\nfrom dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\nUSER_DATABASE = {\n    \"user123\": {\n        \"name\": \"Alice Johnson\",\n        \"account_type\": \"Premium\",\n        \"balance\": 5000,\n        \"email\": \"alice@example.com\"\n    },\n    \"user456\": {\n        \"name\": \"Bob Smith\",\n        \"account_type\": \"Standard\",\n        \"balance\": 1200,\n        \"email\": \"bob@example.com\"\n    }\n}\n\n@dataclass\nclass UserContext:\n    user_id: str\n\n@tool\ndef get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n    \"\"\"Get the current user's account information.\"\"\"\n    user_id = runtime.context.user_id\n\n    if user_id in USER_DATABASE:\n        user = USER_DATABASE[user_id]\n        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n    return \"User not found\"\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nagent = create_agent(\n    model,\n    tools=[get_account_info],\n    context_schema=UserContext,\n    system_prompt=\"You are a financial assistant.\"\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n    context=UserContext(user_id=\"user123\")\n)\n```\n\n#### Memory (Store)\n\nAccess persistent data across conversations using the store. The store is accessed via `runtime.store` and allows you to save and retrieve user-specific or application-specific data.\n\nTools can access and update the store through `ToolRuntime`:\n\n```python expandable theme={null}\nfrom typing import Any\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\n# Access memory\n@tool\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n    \"\"\"Look up user info.\"\"\"\n    store = runtime.store\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\n# Update memory\n@tool\ndef save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n    \"\"\"Save user info.\"\"\"\n    store = runtime.store\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nstore = InMemoryStore()\nagent = create_agent(\n    model,\n    tools=[get_user_info, save_user_info],\n    store=store\n)\n\n# First session: save user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n})\n\n# Second session: get user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email: foo@langchain.dev\n```\n\n#### Stream Writer\n\nStream custom updates from tools as they execute using `runtime.stream_writer`. This is useful for providing real-time feedback to users about what a tool is doing.\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather(city: str, runtime: ToolRuntime) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = runtime.stream_writer\n\n    # Stream custom updates as the tool executes\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n\n    return f\"It's always sunny in {city}!\"\n```\n\n<Note>\n  If you use `runtime.stream_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/python/langchain/streaming) for more details.\n</Note>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_FLbA3fadTLm4243aCxnF9duL",
    "id": "0f6d5678-d529-4c03-8841-bb02ff6f382f",
    "content": "{\n  \"collections\": \"OSS (LangChain and LangGraph),LangSmith Studio,SDKs and APIs\",\n  \"total\": 17,\n  \"articles\": [\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d270716f-edc0-49d9-b5d1-497b5d0e4e6d\",\n      \"title\": \"Private Network Access Blocking LangSmith Studio Connection\",\n      \"url\": \"https://support.langchain.com/articles/1897181200-private-network-access-blocking-langsmith-studio-connection\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"68fbf7da-954d-454e-8ad4-73e1f1f63b74\",\n      \"title\": \"LangSmith Studio vs Agent Chat UI: When to Use What\",\n      \"url\": \"https://support.langchain.com/articles/1353449293-langsmith-studio-vs-agent-chat-ui\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"ada8fd1b-4800-434e-8d6f-b5186b170941\",\n      \"title\": \"LangSmith Studio URL Redirects to Error Page\",\n      \"url\": \"https://support.langchain.com/articles/4270790436-langsmith-studio-url-redirects-to-error-page\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b384d-5fe0-73f2-9ed5-249c1e3d6a02",
    "content": "**A tool in LangChain/LangGraph is a callable component (a function) that an agent or model can invoke to perform external actions\u2014like calling APIs, querying a database, running code, or reading/writing files.**\n\nTools let the model extend its capabilities by exposing a defined input schema and returning structured outputs; in Python you typically create them with `@tool`, in JavaScript/TypeScript with `tool()`/toolkits, and tools can access runtime/context via `ToolRuntime` so they remain testable and stateful.\n\n```python\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str, limit: int = 10) -> str:\n    \"\"\"Search the customer database for records matching the query.\"\"\"\n    # Implementation: run DB query, format results\n    return f\"Found {limit} results for '{query}'\"  # returned value goes back to the agent\n```\n\n## Examples and runtime access\n\nTools can include richer schemas (Pydantic/JSON in Python, zod in JS) and can access runtime/context, store, and streaming via `ToolRuntime` (hidden from the model). This is how a tool can read user-specific context or update persistent memory:\n\n```python\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n    \"\"\"Look up user info from the runtime store.\"\"\"\n    user_info = runtime.store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n```\n\n- Tools are chosen by the model (tool calling) when the model decides a specific external action is needed.\n- Toolkits are collections of related tools (useful for grouping integrations).\n- Some providers offer server-side built-in tools (e.g., web search, code interpreters) that the model can call directly.\n\nRelevant docs:\n\n- [Tools (Python)](https://docs.langchain.com/oss/python/langchain/tools)  \n- [Tools (JavaScript)](https://docs.langchain.com/oss/javascript/langchain/tools)  \n- [Tools and Toolkits (Integrations)](https://docs.langchain.com/oss/javascript/integrations/tools/index)"
  }
]