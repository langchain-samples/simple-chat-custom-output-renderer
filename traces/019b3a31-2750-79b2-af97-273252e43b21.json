[
  {
    "role": "user",
    "id": "5cb0ca6f-c373-4f0d-9bb4-c2396c2d7f8b",
    "content": "verbose=True\u5e94\u8be5\u653e\u5728\u54ea\u91cc"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_44267808",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"python\", \"page_size\": 5, \"query\": \"verbose\"}"
        }
      },
      {
        "type": "function",
        "id": "call_73443508",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"all\"}"
        }
      }
    ],
    "id": "lc_run--019b3a31-2a9f-7550-93be-bfbfb0107ac9",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_44267808",
    "id": "d820607c-78c0-47c2-8f80-edcf8334b55d",
    "content": "Result 1:\nTitle: Tools\nLink: https://docs.langchain.comoss/javascript/integrations/tools/anthropic\nContent: Tool Search Tools\nThe tool search tools enable Claude to work with hundreds or thousands of tools by dynamically discovering and loading them on-demand. This is useful when you have a large number of tools but don't want to load them all into the context window at once. There are two variants: toolSearchRegex_20251119 - Claude constructs regex patterns (using Python's re.search() syntax) to search for tools toolSearchBM25_20251119 - Claude uses natural language queries to search for tools using the BM25 algorithm import { ChatAnthropic, tools } from \"@langchain/anthropic\";\nimport { tool } from \"langchain\";\nimport { z } from \"zod\";\n\nconst llm = new ChatAnthropic({\n  model: \"claude-sonnet-4-5-20250929\",\n});\n\n// Create tools with defer_loading to make them discoverable via search\nconst getWeather = tool(\n  async (input: { location: string }) => {\n    return `Weather in ${input.location}: Sunny, 72\u00b0F`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get the weather at a specific location\",\n    schema: z.object({\n      location: z.string(),\n    }),\n    extras: { defer_loading: true },\n  }\n);\n\nconst getNews = tool(\n  async (input: { topic: string }) => {\n    return `Latest news about ${input.topic}...`;\n  },\n  {\n    name: \"get_news\",\n    description: \"Get the latest news about a topic\",\n    schema: z.object({\n      topic: z.string(),\n    }),\n    extras: { defer_loading: true },\n  }\n);\n\n// Claude will search and discover tools as needed\nconst response = await llm.invoke(\"What is the weather in San Francisco?\", {\n  tools: [tools.toolSearchRegex_20251119(), getWeather, getNews],\n});\n\nTool Search Tools\nThe tool search tools enable Claude to work with hundreds or thousands of tools by dynamically discovering and loading them on-demand. This is useful when you have a large number of tools but don't want to load them all into the context window at once. There are two variants: toolSearchRegex_20251119 - Claude constructs regex patterns (using Python's re.search() syntax) to search for tools toolSearchBM25_20251119 - Claude uses natural language queries to search for tools using the BM25 algorithm Using the BM25 variant for natural language search: import { tools } from \"@langchain/anthropic\";\n\nconst response = await llm.invoke(\"What is the weather in San Francisco?\", {\n  tools: [tools.toolSearchBM25_20251119(), getWeather, getNews],\n});\n\nTool Search Tools\nThe tool search tools enable Claude to work with hundreds or thousands of tools by dynamically discovering and loading them on-demand. This is useful when you have a large number of tools but don't want to load them all into the context window at once. There are two variants: toolSearchRegex_20251119 - Claude constructs regex patterns (using Python's re.search() syntax) to search for tools toolSearchBM25_20251119 - Claude uses natural language queries to search for tools using the BM25 algorithm Using the BM25 variant for natural language search: For more information, see Anthropic's Tool Search documentation.\n\n---\n\nResult 2:\nTitle: Context engineering in agents\nLink: https://docs.langchain.comoss/javascript/langchain/context-engineering\nContent: Store\nConfigure output format based on user preferences in Store: import * as z from \"zod\";\nimport { createMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userId: z.string(),\n});\n\nconst verboseResponse = z.object({\n  answer: z.string().describe(\"Detailed answer\"),\n  sources: z.array(z.string()).describe(\"Sources used\"),\n});\n\nconst conciseResponse = z.object({\n  answer: z.string().describe(\"Brief answer\"),\n});\n\nconst storeBasedOutput = createMiddleware({\n  name: \"StoreBasedOutput\",\n  wrapModelCall: async (request, handler) => {\n    const userId = request.runtime.context.userId;  // [!code highlight]\n\n    // Read from Store: get user's preferred response style\n    const store = request.runtime.store;  // [!code highlight]\n    const userPrefs = await store.get([\"preferences\"], userId);  // [!code highlight]\n\n    if (userPrefs) {\n      const style = userPrefs.value?.responseStyle || \"concise\";\n      if (style === \"verbose\") {\n        request.responseFormat = verboseResponse;  // [!code highlight]\n      } else {\n        request.responseFormat = conciseResponse;  // [!code highlight]\n      }\n    }\n\n    return handler(request);\n  },\n});\n\nSelecting formats\nDynamic response format selection adapts schemas based on user preferences, conversation stage, or role\u2014returning simple formats early and detailed formats as complexity increases. Configure structured output based on conversation state: Configure output format based on user preferences in Store: import * as z from \"zod\";\nimport { createMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userId: z.string(),\n});\n\nconst verboseResponse = z.object({\n  answer: z.string().describe(\"Detailed answer\"),\n  sources: z.array(z.string()).describe(\"Sources used\"),\n});\n\nconst conciseResponse = z.object({\n  answer: z.string().describe(\"Brief answer\"),\n});\n\nconst storeBasedOutput = createMiddleware({\n  name: \"StoreBasedOutput\",\n  wrapModelCall: async (request, handler) => {\n    const userId = request.runtime.context.userId;  // [!code highlight]\n\n    // Read from Store: get user's preferred response style\n    const store = request.runtime.store;  // [!code highlight]\n    const userPrefs = await store.get([\"preferences\"], userId);  // [!code highlight]\n\n    if (userPrefs) {\n      const style = userPrefs.value?.responseStyle || \"concise\";\n      if (style === \"verbose\") {\n        request.responseFormat = verboseResponse;  // [!code highlight]\n      } else {\n        request.responseFormat = conciseResponse;  // [!code highlight]\n      }\n    }\n\n    return handler(request);\n  },\n});\n\nDefining formats\nSchema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to. import { z } from \"zod\";\n\nconst customerSupportTicket = z.object({\n  category: z.enum([\"billing\", \"technical\", \"account\", \"product\"]).describe(\n    \"Issue category\"\n  ),\n  priority: z.enum([\"low\", \"medium\", \"high\", \"critical\"]).describe(\n    \"Urgency level\"\n  ),\n  summary: z.string().describe(\n    \"One-sentence summary of the customer's issue\"\n  ),\n  customerSentiment: z.enum([\"frustrated\", \"neutral\", \"satisfied\"]).describe(\n    \"Customer's emotional tone\"\n  ),\n}).describe(\"Structured ticket information extracted from customer message\");\n\n---\n\nResult 3:\nTitle: ChatGroq\nLink: https://docs.langchain.comoss/python/integrations/chat/groq\nContent: <Warning>\n  This page makes reference to [Groq](https://console.groq.com/docs/overview), an AI hardware and software company. For information on how to use Grok models (provided by [xAI](https://docs.x.ai/docs/overview)), see the [xAI provider page](/oss/python/integrations/providers/xai).\n</Warning>\n\n<Tip>\n  **API Reference**\n\n  For detailed documentation of all features and configuration options, head to the [`ChatGroq`](https://reference.langchain.com/python/integrations/langchain_groq/#langchain_groq.ChatGroq) API reference.\n</Tip>\n\nFor a list of all Groq models, visit their [docs](https://console.groq.com/docs/models?utm_source=langchain).\n\n## Overview\n\n### Integration details\n\n| Class                                                                                                     | Package                                                                                | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/chat/groq) |                                            Downloads                                            |                                            Version                                           |\n| :-------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------- | :---: | :----------: | :----------------------------------------------------------------: | :---------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------: |\n| [`ChatGroq`](https://reference.langchain.com/python/integrations/langchain_groq/#langchain_groq.ChatGroq) | [`langchain-groq`](https://reference.langchain.com/python/integrations/langchain_groq) |   \u274c   |     beta     |                                  \u2705                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-groq?style=flat-square\\&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-groq?style=flat-square\\&label=%20) |\n\n### Model features\n\n| [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output) | JSON mode | [Image input](/oss/python/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/python/langchain/streaming#llm-tokens) | Native async | [Token usage](/oss/python/langchain/models#token-usage) | [Logprobs](/oss/python/langchain/models#log-probabilities) |\n| :-----------------------------------------: | :----------------------------------------------------------: | :-------: | :------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------------: | :----------: | :-----------------------------------------------------: | :--------------------------------------------------------: |\n|                      \u2705                      |                               \u2705                              |     \u2705     |                             \u274c                            |      \u274c      |      \u274c      |                                  \u2705                                  |       \u2705      |                            \u2705                            |                              \u2705                             |\n\n## Setup\n\nTo access Groq models you'll need to create a Groq account, get an API key, and install the `langchain-groq` integration package.\n\n### Credentials\n\nHead to the [Groq console](https://console.groq.com/login?utm_source=langchain\\&utm_content=chat_page) to sign up to Groq and generate an API key. Once you've done this set the GROQ\\_API\\_KEY environment variable:\n\n```python  theme={null}\nimport getpass\nimport os\n\nif \"GROQ_API_KEY\" not in os.environ:\n    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n```\n\nTo enable automated tracing of your model calls, set your [LangSmith](https://docs.langchain.com/langsmith/home) API key:\n\n```python  theme={null}\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\n### Installation\n\nThe LangChain Groq integration lives in the `langchain-groq` package:\n\n```python  theme={null}\npip install -qU langchain-groq\n```\n\n## Instantiation\n\nNow we can instantiate our model object and generate chat completions.\n\n<Note>\n  **Reasoning Format**\n\n  If you choose to set a `reasoning_format`, you must ensure that the model you are using supports it. You can find a list of supported models in the [Groq documentation](https://console.groq.com/docs/reasoning).\n</Note>\n\n```python  theme={null}\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(\n    model=\"deepseek-r1-distill-llama-70b\",\n    temperature=0,\n    max_tokens=None,\n    reasoning_format=\"parsed\",\n    timeout=None,\n    max_retries=2,\n    # other params...\n)\n```\n\n## Invocation\n\n```python  theme={null}\nmessages = [\n    (\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ),\n    (\"human\", \"I love programming.\"),\n]\nai_msg = llm.invoke(messages)\nai_msg\n```\n\n```text  theme={null}\nAIMessage(content=\"J'aime la programmation.\", additional_kwargs={'reasoning_content': 'Okay, so I need to translate the sentence \"I love programming.\" into French. Let me think about how to approach this. \\n\\nFirst, I know that \"I\" in French is \"Je.\" That\\'s straightforward. Now, the verb \"love\" in French is \"aime\" when referring to oneself. So, \"I love\" would be \"J\\'aime.\" \\n\\nNext, the word \"programming.\" In French, programming is \"la programmation.\" But wait, in French, when you talk about loving an activity, you often use the definite article. So, it would be \"la programmation.\" \\n\\nPutting it all together, \"I love programming\" becomes \"J\\'aime la programmation.\" That sounds right. I think that\\'s the correct translation. \\n\\nI should double-check to make sure I\\'m not missing anything. Maybe I can think of similar phrases. For example, \"I love reading\" is \"J\\'aime lire,\" but when it\\'s a noun, like \"I love music,\" it\\'s \"J\\'aime la musique.\" So, yes, using \"la programmation\" makes sense here. \\n\\nI don\\'t think I need to change anything else. The sentence structure in French is Subject-Verb-Object, just like in English, so \"J\\'aime la programmation\" should be correct. \\n\\nI guess another way to say it could be \"J\\'adore la programmation,\" using \"adore\" instead of \"aime,\" but \"aime\" is more commonly used in this context. So, sticking with \"J\\'aime la programmation\" is probably the best choice.\\n'}, response_metadata={'token_usage': {'completion_tokens': 346, 'prompt_tokens': 23, 'total_tokens': 369, 'completion_time': 1.447541218, 'prompt_time': 0.000983386, 'queue_time': 0.009673684, 'total_time': 1.448524604}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_e98d30d035', 'finish_reason': 'stop', 'logprobs': None}, id='run--5679ae4f-f4e8-4931-bcd5-7304223832c0-0', usage_metadata={'input_tokens': 23, 'output_tokens': 346, 'total_tokens': 369})\n```\n\n```python  theme={null}\nprint(ai_msg.content)\n```\n\n```text  theme={null}\nJ'aime la programmation.\n```\n\n***\n\n## API reference\n\nFor detailed documentation of all ChatGroq features and configurations head to the [API reference](https://python.langchain.com/api_reference/groq/chat_models/langchain_groq.chat_models.ChatGroq.html).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/groq.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Context engineering in agents\nLink: https://docs.langchain.comoss/python/langchain/context-engineering\nContent: Store\nConfigure output format based on user preferences in Store: from dataclasses import dataclass\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom pydantic import BaseModel, Field\nfrom typing import Callable\nfrom langgraph.store.memory import InMemoryStore\n\n@dataclass\nclass Context:\n    user_id: str\n\nclass VerboseResponse(BaseModel):\n    \"\"\"Verbose response with details.\"\"\"\n    answer: str = Field(description=\"Detailed answer\")\n    sources: list[str] = Field(description=\"Sources used\")\n\nclass ConciseResponse(BaseModel):\n    \"\"\"Concise response.\"\"\"\n    answer: str = Field(description=\"Brief answer\")\n\n@wrap_model_call\ndef store_based_output(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Select output format based on Store preferences.\"\"\"\n    user_id = request.runtime.context.user_id\n\n    # Read from Store: get user's preferred response style\n    store = request.runtime.store\n    user_prefs = store.get((\"preferences\",), user_id)\n\n    if user_prefs:\n        style = user_prefs.value.get(\"response_style\", \"concise\")\n        if style == \"verbose\":\n            request = request.override(response_format=VerboseResponse)\n        else:\n            request = request.override(response_format=ConciseResponse)\n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[store_based_output],\n    context_schema=Context,\n    store=InMemoryStore()\n)\n\nSelecting formats\nDynamic response format selection adapts schemas based on user preferences, conversation stage, or role\u2014returning simple formats early and detailed formats as complexity increases. Configure structured output based on conversation state: Configure output format based on user preferences in Store: from dataclasses import dataclass\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom pydantic import BaseModel, Field\nfrom typing import Callable\nfrom langgraph.store.memory import InMemoryStore\n\n@dataclass\nclass Context:\n    user_id: str\n\nclass VerboseResponse(BaseModel):\n    \"\"\"Verbose response with details.\"\"\"\n    answer: str = Field(description=\"Detailed answer\")\n    sources: list[str] = Field(description=\"Sources used\")\n\nclass ConciseResponse(BaseModel):\n    \"\"\"Concise response.\"\"\"\n    answer: str = Field(description=\"Brief answer\")\n\n@wrap_model_call\ndef store_based_output(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Select output format based on Store preferences.\"\"\"\n    user_id = request.runtime.context.user_id\n\n    # Read from Store: get user's preferred response style\n    store = request.runtime.store\n    user_prefs = store.get((\"preferences\",), user_id)\n\n    if user_prefs:\n        style = user_prefs.value.get(\"response_style\", \"concise\")\n        if style == \"verbose\":\n            request = request.override(response_format=VerboseResponse)\n        else:\n            request = request.override(response_format=ConciseResponse)\n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[store_based_output],\n    context_schema=Context,\n    store=InMemoryStore()\n)\n\nData sources\nThroughout this process, your agent accesses (reads / writes) different sources of data: Data Source Also Known As Scope Examples Runtime Context Static configuration Conversation-scoped User ID, API keys, database connections, permissions, environment settings State Short-term memory Conversation-scoped Current messages, uploaded files, authentication status, tool results Store Long-term memory Cross-conversation User preferences, extracted insights, memories, historical data\n\n---\n\nResult 5:\nTitle: Human-in-the-loop\nLink: https://docs.langchain.comoss/python/langchain/human-in-the-loop\nContent: The Human-in-the-Loop (HITL) [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) lets you add human oversight to agent tool calls.\nWhen a model proposes an action that might require review \u2014 for example, writing to a file or executing SQL \u2014 the middleware can pause execution and wait for a decision.\n\nIt does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) that halts execution. The graph state is saved using LangGraph's [persistence layer](/oss/python/langgraph/persistence), so execution can pause safely and resume later.\n\nA human decision then determines what happens next: the action can be approved as-is (`approve`), modified before running (`edit`), or rejected with feedback (`reject`).\n\n## Interrupt decision types\n\nThe [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) defines three built-in ways a human can respond to an interrupt:\n\n| Decision Type | Description                                                               | Example Use Case                                    |\n| ------------- | ------------------------------------------------------------------------- | --------------------------------------------------- |\n| \u2705 `approve`   | The action is approved as-is and executed without changes.                | Send an email draft exactly as written              |\n| \u270f\ufe0f `edit`     | The tool call is executed with modifications.                             | Change the recipient before sending an email        |\n| \u274c `reject`    | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it |\n\nThe available decision types for each tool depend on the policy you configure in `interrupt_on`.\nWhen multiple tool calls are paused at the same time, each action requires a separate decision.\nDecisions must be provided in the same order as the actions appear in the interrupt request.\n\n<Tip>\n  When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n</Tip>\n\n## Configuring interrupts\n\nTo use HITL, add the [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) to the agent's `middleware` list when creating the agent.\n\nYou configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver # [!code highlight]\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[write_file_tool, execute_sql_tool, read_data_tool],\n    middleware=[\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\n                \"write_file\": True,  # All decisions (approve, edit, reject) allowed\n                \"execute_sql\": {\"allowed_decisions\": [\"approve\", \"reject\"]},  # No editing allowed\n                # Safe operation, no approval needed\n                \"read_data\": False,\n            },\n            # Prefix for interrupt messages - combined with tool name and args to form the full message\n            # e.g., \"Tool execution pending approval: execute_sql with query='DELETE FROM...'\"\n            # Individual tools can override this by specifying a \"description\" in their interrupt config\n            description_prefix=\"Tool execution pending approval\",\n        ),\n    ],\n    # Human-in-the-loop requires checkpointing to handle interrupts.\n    # In production, use a persistent checkpointer like AsyncPostgresSaver.\n    checkpointer=InMemorySaver(),  # [!code highlight]\n)\n```\n\n<Info>\n  You must configure a checkpointer to persist the graph state across interrupts.\n  In production, use a persistent checkpointer like [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver). For testing or prototyping, use [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver).\n\n  When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.\n  See the [LangGraph interrupts documentation](/oss/python/langgraph/interrupts) for details.\n</Info>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"interrupt_on\" type=\"dict\" required>\n    Mapping of tool names to approval configs. Values can be `True` (interrupt with default config), `False` (auto-approve), or an `InterruptOnConfig` object.\n  </ParamField>\n\n  <ParamField body=\"description_prefix\" type=\"string\" default=\"Tool execution requires approval\">\n    Prefix for action request descriptions\n  </ParamField>\n\n  **`InterruptOnConfig` options:**\n\n  <ParamField body=\"allowed_decisions\" type=\"list[string]\">\n    List of allowed decisions: `'approve'`, `'edit'`, or `'reject'`\n  </ParamField>\n\n  <ParamField body=\"description\" type=\"string | callable\">\n    Static string or callable function for custom description\n  </ParamField>\n</Accordion>\n\n## Responding to interrupts\n\nWhen you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt_on`. In that case, the invocation result will include an `__interrupt__` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.\n\n```python  theme={null}\nfrom langgraph.types import Command\n\n# Human-in-the-loop leverages LangGraph's persistence layer.\n# You must provide a thread ID to associate the execution with a conversation thread,\n# so the conversation can be paused and resumed (as is needed for human review).\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}} # [!code highlight]\n# Run the graph until the interrupt is hit.\nresult = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Delete old records from the database\",\n            }\n        ]\n    },\n    config=config # [!code highlight]\n)\n\n# The interrupt contains the full HITL request with action_requests and review_configs\nprint(result['__interrupt__'])\n# > [\n# >    Interrupt(\n# >       value={\n# >          'action_requests': [\n# >             {\n# >                'name': 'execute_sql',\n# >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \\'30 days\\';'},\n# >                'description': 'Tool execution pending approval\\n\\nTool: execute_sql\\nArgs: {...}'\n# >             }\n# >          ],\n# >          'review_configs': [\n# >             {\n# >                'action_name': 'execute_sql',\n# >                'allowed_decisions': ['approve', 'reject']\n# >             }\n# >          ]\n# >       }\n# >    )\n# > ]\n\n\n# Resume with approval decision\nagent.invoke(\n    Command( # [!code highlight]\n        resume={\"decisions\": [{\"type\": \"approve\"}]}  # or \"reject\" [!code highlight]\n    ), # [!code highlight]\n    config=config # Same thread ID to resume the paused conversation\n)\n```\n\n### Decision types\n\n<Tabs>\n  <Tab title=\"\u2705 approve\">\n    Use `approve` to approve the tool call as-is and execute it without changes.\n\n    ```python  theme={null}\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n            resume={\n                \"decisions\": [\n                    {\n                        \"type\": \"approve\",\n                    }\n                ]\n            }\n        ),\n        config=config  # Same thread ID to resume the paused conversation\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"\u270f\ufe0f edit\">\n    Use `edit` to modify the tool call before execution.\n    Provide the edited action with the new tool name and arguments.\n\n    ```python  theme={null}\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n            resume={\n                \"decisions\": [\n                    {\n                        \"type\": \"edit\",\n                        # Edited action with tool name and args\n                        \"edited_action\": {\n                            # Tool name to call.\n                            # Will usually be the same as the original action.\n                            \"name\": \"new_tool_name\",\n                            # Arguments to pass to the tool.\n                            \"args\": {\"key1\": \"new_value\", \"key2\": \"original_value\"},\n                        }\n                    }\n                ]\n            }\n        ),\n        config=config  # Same thread ID to resume the paused conversation\n    )\n    ```\n\n    <Tip>\n      When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n    </Tip>\n  </Tab>\n\n  <Tab title=\"\u274c reject\">\n    Use `reject` to reject the tool call and provide feedback instead of execution.\n\n    ```python  theme={null}\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n            resume={\n                \"decisions\": [\n                    {\n                        \"type\": \"reject\",\n                        # An explanation about why the action was rejected\n                        \"message\": \"No, this is wrong because ..., instead do this ...\",\n                    }\n                ]\n            }\n        ),\n        config=config  # Same thread ID to resume the paused conversation\n    )\n    ```\n\n    The `message` is added to the conversation as feedback to help the agent understand why the action was rejected and what it should do instead.\n\n    ***\n\n    ### Multiple decisions\n\n    When multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:\n\n    ```python  theme={null}\n    {\n        \"decisions\": [\n            {\"type\": \"approve\"},\n            {\n                \"type\": \"edit\",\n                \"edited_action\": {\n                    \"name\": \"tool_name\",\n                    \"args\": {\"param\": \"new_value\"}\n                }\n            },\n            {\n                \"type\": \"reject\",\n                \"message\": \"This action is not allowed\"\n            }\n        ]\n    }\n    ```\n  </Tab>\n</Tabs>\n\n## Streaming with human-in-the-loop\n\nYou can use `stream()` instead of `invoke()` to get real-time updates while the agent runs and handles interrupts. Use `stream_mode=['updates', 'messages']` to stream both agent progress and LLM tokens.\n\n```python  theme={null}\nfrom langgraph.types import Command\n\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\n\n# Stream agent progress and LLM tokens until interrupt\nfor mode, chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Delete old records from the database\"}]},\n    config=config,\n    stream_mode=[\"updates\", \"messages\"],  # [!code highlight]\n):\n    if mode == \"messages\":\n        # LLM token\n        token, metadata = chunk\n        if token.content:\n            print(token.content, end=\"\", flush=True)\n    elif mode == \"updates\":\n        # Check for interrupt\n        if \"__interrupt__\" in chunk:\n            print(f\"\\n\\nInterrupt: {chunk['__interrupt__']}\")\n\n# Resume with streaming after human decision\nfor mode, chunk in agent.stream(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n    config=config,\n    stream_mode=[\"updates\", \"messages\"],\n):\n    if mode == \"messages\":\n        token, metadata = chunk\n        if token.content:\n            print(token.content, end=\"\", flush=True)\n```\n\nSee the [Streaming](/oss/python/langchain/streaming) guide for more details on stream modes.\n\n## Execution lifecycle\n\nThe middleware defines an `after_model` hook that runs after the model generates a response but before any tool calls are executed:\n\n1. The agent invokes the model to generate a response.\n2. The middleware inspects the response for tool calls.\n3. If any calls require human input, the middleware builds a `HITLRequest` with `action_requests` and `review_configs` and calls [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).\n4. The agent waits for human decisions.\n5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes [ToolMessage](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage)'s for rejected calls, and resumes execution.\n\n## Custom HITL logic\n\nFor more specialized workflows, you can build custom HITL logic directly using the [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) primitive and [middleware](/oss/python/langchain/middleware) abstraction.\n\nReview the [execution lifecycle](#execution-lifecycle) above to understand how to integrate interrupts into the agent's operation.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/human-in-the-loop.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_73443508",
    "id": "3c7acafa-8788-45d3-a73c-d952238d9657",
    "content": "{\n  \"collections\": \"all\",\n  \"total\": 73,\n  \"articles\": [\n    {\n      \"id\": \"7a5d1732-c25b-4a26-9686-52ddfd17926e\",\n      \"title\": \"Merging LangSmith Runs from Remote Graph\",\n      \"url\": \"https://support.langchain.com/articles/9499391881-merging-langsmith-runs-from-remote-graph\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"796cd8bc-a7d9-419d-8141-83998b267e90\",\n      \"title\": \"Resolving common issues when upgrading LangSmith Helm chart from 0.11.x to 0.12.x?\",\n      \"url\": \"https://support.langchain.com/articles/5255204348-resolving-common-issues-when-upgrading-langsmith-helm-chart-from-0-11-x-to-0-12-x\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"adeafe65-c1b6-4620-b3aa-044873fd8c9c\",\n      \"title\": \"Why are my traces being automatically upgraded to extended retention?\",\n      \"url\": \"https://support.langchain.com/articles/3206725817-why-are-my-traces-being-automatically-upgraded-to-extended-retention\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"73778fa5-7613-4254-b687-7b0288ac9903\",\n      \"title\": \"Why am I getting 429 Rate Limit Errors during load testing on self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8364157755-why-am-i-getting-429-rate-limit-errors-during-load-testing-on-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"27a421b4-e877-4a0f-8f38-7b719b189785\",\n      \"title\": \"PagerDuty Alert Severity Not Respecting Configuration\",\n      \"url\": \"https://support.langchain.com/articles/8222693126-pagerduty-alert-severity-not-respecting-configuration\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"adf84afd-7d07-4e11-8645-a3c9a1686f08\",\n      \"title\": \"Session Timeout Configuration for LangSmith Self-Hosted\",\n      \"url\": \"https://support.langchain.com/articles/9515017706-session-timeout-configuration-for-langsmith-self-hosted\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2fcf3091-0038-4226-b56f-4559d6cb1dbc\",\n      \"title\": \"Cleaning ClickHouse System Log Tables\",\n      \"url\": \"https://support.langchain.com/articles/9482219057-cleaning-clickhouse-system-log-tables\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"065735b3-432b-41ce-8469-1494cb753727\",\n      \"title\": \"Why are my agent health checks failing with GCP Gateway API on GKE?\",\n      \"url\": \"https://support.langchain.com/articles/3603461301-why-are-my-agent-health-checks-failing-with-gcp-gateway-api-on-gke\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f9775b3-e85d-4368-ad1b-21f9a8f7552e\",\n      \"title\": \"Evaluator Creation with LiteLLM Proxy - Missing API Key Error\",\n      \"url\": \"https://support.langchain.com/articles/2378929317-evaluator-creation-with-litellm-proxy-missing-api-key-error\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3a81a902-1bca-463d-ab2d-abefd38f8721\",\n      \"title\": \"Why did my evaluator token usage and costs increase after upgrading LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/7643067789-why-did-my-evaluator-token-usage-and-costs-increase-after-upgrading-langsmith\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"61f0452e-5198-4524-9c94-a9cf476ea5bc\",\n      \"title\": \"How to Create a Locked-Down Deployment with Restricted Access\",\n      \"url\": \"https://support.langchain.com/articles/5382256129-how-to-create-a-locked-down-deployment-with-restricted-access\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"c0615bd9-c4a2-4a54-af03-0ee07470ee43\",\n      \"title\": \"How do I configure LangSmith pricing match patterns for AWS Bedrock inference profiles?\",\n      \"url\": \"https://support.langchain.com/articles/4241511543-how-do-i-configure-langsmith-pricing-match-patterns-for-aws-bedrock-inference-profiles\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"365a0d3f-d784-407d-b741-fe51fa8356d4\",\n      \"title\": \"LangSmith Deployments not working with Valkey\",\n      \"url\": \"https://support.langchain.com/articles/2265174426-why-are-my-langgraph-requests-hanging-when-using-aws-elasticache-with-valkey\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"818454c3-a656-4811-a7ca-56cb13df2355\",\n      \"title\": \"How do I enable KMS encryption for S3 storage in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9381186430-how-do-i-enable-kms-encryption-for-s3-storage-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a3c79a7d-390f-43e6-8234-8b55a7f191dd\",\n      \"title\": \"Streaming Failures When Using Shared Custom Redis Instances\",\n      \"url\": \"https://support.langchain.com/articles/3842219806-streaming-failures-when-using-shared-custom-redis-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"55a2c6fe-adf8-4b55-bcb9-969b465241b7\",\n      \"title\": \"Why are my agent threads getting deleted automatically?\",\n      \"url\": \"https://support.langchain.com/articles/7993792111-why-are-my-threads-being-marked-as-temporary-and-getting-deleted-automatically\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b4ba8c34-6d1d-4653-8123-c2eeee2d34e5\",\n      \"title\": \"How do I set up ABAC to restrict user access to specific projects?\",\n      \"url\": \"https://support.langchain.com/articles/6539560119-how-do-i-set-up-abac-to-restrict-user-access-to-specific-projects\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b42f572e-0490-4bb8-9c81-9f065f0c83ed\",\n      \"title\": \"Productionizing and Scaling Self-Hosted LangSmith - Best Practices\",\n      \"url\": \"https://support.langchain.com/articles/8478795211-productionizing-and-scaling-self-hosted-langsmith-best-practices\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"d270716f-edc0-49d9-b5d1-497b5d0e4e6d\",\n      \"title\": \"Private Network Access Blocking LangSmith Studio Connection\",\n      \"url\": \"https://support.langchain.com/articles/1897181200-private-network-access-blocking-langsmith-studio-connection\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"b20fd0bf-1ae9-445c-86ba-5a223112756f\",\n      \"title\": \"How to setup private container registry authentication for LangGraph agent deployments\",\n      \"url\": \"https://support.langchain.com/articles/8606755009-how-to-setup-private-container-registry-authentication-for-langgraph-cloud-agent-deployments\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f6f6fa0-610c-4a15-a4b9-987e932f6a3a\",\n      \"title\": \"How to setup path-based routing for self-hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/2352997668-how-to-setup-path-based-routing-for-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"cc4be529-34c6-458b-8bb3-621fab0e2ab4\",\n      \"title\": \"Best practices for scaling Self-Hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/5142502915-best-practices-for-scaling-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3cec875c-3c8d-4951-b83b-0ac5b8ba7c7c\",\n      \"title\": \"LangSmith Deployments agent is running, but the UI shows it as \\u201cUnsuccessful\\u201d\",\n      \"url\": \"https://support.langchain.com/articles/8394994213-langsmith-deployments-agent-is-running-but-the-ui-shows-it-as-unsuccessful\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"57055dee-474d-4635-93a3-79439570f6ac\",\n      \"title\": \"How do I prevent traces from mixing between projects during concurrent requests?\",\n      \"url\": \"https://support.langchain.com/articles/4784014353-how-do-i-prevent-traces-from-mixing-between-projects-during-concurrent-requests\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9619ff10-34d0-49ea-a7ee-cdbbddafc1e4\",\n      \"title\": \"How do I resolve \\\"too many connections\\\" PostgreSQL errors when scaling platform instances?\",\n      \"url\": \"https://support.langchain.com/articles/9651943867-how-do-i-resolve-too-many-connections-postgresql-errors-when-scaling-platform-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"b740be98-677d-486f-ba77-12258712f70a\",\n      \"title\": \"How do I use the LangSmith Control Plane API to manage deployments?\",\n      \"url\": \"https://support.langchain.com/articles/8184800662-how-do-i-use-the-langgraph-platform-control-plane-api-to-manage-deployments\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"52d389ff-3918-46a8-9ccf-55fe972077bf\",\n      \"title\": \"Why is my LangSmith agent queue Deployment timing out after 600 seconds?\",\n      \"url\": \"https://support.langchain.com/articles/7898522976-why-is-my-queue-deployment-timing-out-after-600-seconds\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"1caf7aa7-5382-43f5-869c-27212fe9f287\",\n      \"title\": \"How do I resolve ClickHouse migration errors after upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/6463985661-how-do-i-resolve-clickhouse-migration-errors-after-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0ff2089a-501c-45b8-b557-4947639c5072\",\n      \"title\": \"How do I disable public sharing for traces in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5345880639-how-do-i-disable-public-sharing-for-traces-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\",\n      \"title\": \"What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\",\n      \"url\": \"https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"bef0ce3c-94c2-4999-b563-ff016fa3884f\",\n      \"title\": \"How do I get LangChain to complete security questionnaires for vendor assessments?\",\n      \"url\": \"https://support.langchain.com/articles/8555158040-how-do-i-get-langchain-to-complete-security-questionnaires-for-vendor-assessments\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"fe2b3c9e-8fa1-49dd-a52d-df92d88050c4\",\n      \"title\": \"How do I update billing information on an existing invoice?\",\n      \"url\": \"https://support.langchain.com/articles/5832977338-how-do-i-update-billing-information-on-an-existing-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"6ea47321-cd15-49a7-a972-be33139a5fe3\",\n      \"title\": \"Can I use an external database cluster to host multiple self-hosted LangSmith instances?\",\n      \"url\": \"https://support.langchain.com/articles/5989664339-can-i-use-an-external-database-cluster-to-host-multiple-self-hosted-langsmith-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"4a127c79-3504-4afa-befa-ec974b85c5f8\",\n      \"title\": \"Why am I being charged for node executions and standby minutes on my development deployment?\",\n      \"url\": \"https://support.langchain.com/articles/2626278629-why-am-i-being-charged-for-node-executions-on-my-development-deployment\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"a442595a-454c-4208-8854-d51f632a3d0f\",\n      \"title\": \"Managing ClickHouse Storage\",\n      \"url\": \"https://support.langchain.com/articles/2929899751-managing-clickhouse-storage\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"c6c77d41-4376-4452-b7a0-fd42c1428242\",\n      \"title\": \"How do I deploy an MCP server on LangSmith Deployments with my agent?\",\n      \"url\": \"https://support.langchain.com/articles/8743137454-how-do-i-deploy-an-mcp-server-on-langsmith-deployments-with-my-agent\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"e5b95386-5971-4e48-be5b-9d9c7dcfe8e3\",\n      \"title\": \"How do I archive a prompt instead of deleting it?\",\n      \"url\": \"https://support.langchain.com/articles/6404244010-how-do-i-archive-a-prompt-instead-of-deleting-it\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"7444d2d0-3f90-42d8-aa4c-b36fa7665e05\",\n      \"title\": \"How do I resolve Redis disk space issues in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5370519454-how-do-i-resolve-redis-disk-space-issues-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"1076b041-020f-4738-acd0-d8ecc07f19ed\",\n      \"title\": \"How do I add existing organization members to new workspaces programmatically via API?\",\n      \"url\": \"https://support.langchain.com/articles/5797508505-how-do-i-add-existing-organization-members-to-new-workspaces-programmatically-via-api\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a91a5cf4-a8a3-4ad5-b9ef-9d0d534d0ba4\",\n      \"title\": \"ClickHouse: Drop Old Data\",\n      \"url\": \"https://support.langchain.com/articles/2379935803-clickhouse-drop-old-data\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5886208c-1f87-4672-87c4-053f2773c161\",\n      \"title\": \"How do I filter out traces that contain a specific node name in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8185473222-how-do-i-filter-out-traces-that-contain-a-specific-node-name-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b170d29c-925a-4aab-b7a0-354f47b7dc77\",\n      \"title\": \"Can I enforce Google SSO-only authentication for my organization?\",\n      \"url\": \"https://support.langchain.com/articles/9820106567-can-i-enforce-google-sso-only-authentication-for-my-organization\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b5128a24-178a-4063-9337-c273af5ac617\",\n      \"title\": \"Why is my trace exploration slow in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9719508217-why-is-my-trace-exploration-slow-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"50634017-ff08-43bb-b7ae-7518aab83623\",\n      \"title\": \"How do I get access to LangSmith Agent Builder?\",\n      \"url\": \"https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"4ad14b9c-abe8-47b0-a416-81dbe2a3a127\",\n      \"title\": \"Why are some experiment runs missing when processing large datasets in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/3506757366-why-are-some-experiment-runs-missing-when-processing-large-datasets-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"40305680-01d3-4506-8257-16db939e39bf\",\n      \"title\": \"How do I get help with billing issues and payment problems?\",\n      \"url\": \"https://support.langchain.com/articles/3779255598-how-do-i-get-help-with-billing-issues-and-payment-problems\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"9273a5a9-0a53-4d5a-917a-c19ebeec64fe\",\n      \"title\": \"How do I get a copy of my LangSmith invoice?\",\n      \"url\": \"https://support.langchain.com/articles/6317483716-how-do-i-get-a-copy-of-my-langsmith-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"ebea1a08-4488-4cac-9c65-75442f3de254\",\n      \"title\": \"Adding FastAPI dependency for custom middleware in LangGraph Platform\",\n      \"url\": \"https://support.langchain.com/articles/7593416758-adding-fastapi-dependency-for-custom-middleware-in-langgraph-platform\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"d37491ce-7f3c-4575-91d5-09c62129248e\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/8574277609-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"48541052-b69e-4e78-ae8b-c2ef4fe26af1\",\n      \"title\": \"Resolving LangGraph and LangSmith version compatibility issues in production\",\n      \"url\": \"https://support.langchain.com/articles/5102702588-resolving-langgraph-and-langsmith-version-compatibility-issues-in-production\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0b6ce386-e341-47a1-8be9-ae350de16843\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/3873263156-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"68fbf7da-954d-454e-8ad4-73e1f1f63b74\",\n      \"title\": \"LangSmith Studio vs Agent Chat UI: When to Use What\",\n      \"url\": \"https://support.langchain.com/articles/1353449293-langsmith-studio-vs-agent-chat-ui\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f7dc0a0c-fa3f-4319-b820-ae470d89877f\",\n      \"title\": \"Why is token-by-token streaming not working after upgrading LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/7150806184-why-is-token-by-token-streaming-not-working-after-upgrading-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"ada8fd1b-4800-434e-8d6f-b5186b170941\",\n      \"title\": \"LangSmith Studio URL Redirects to Error Page\",\n      \"url\": \"https://support.langchain.com/articles/4270790436-langsmith-studio-url-redirects-to-error-page\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f41e0fb9-7ed0-4d9b-85b0-fe88fd805f02\",\n      \"title\": \"Why am I getting service interruption errors when using text search filters?\",\n      \"url\": \"https://support.langchain.com/articles/6919527577-why-am-i-getting-service-interruption-errors-when-using-text-search-filters\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\",\n      \"title\": \"Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\",\n      \"url\": \"https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b1e1ee29-1124-4a92-8f9f-eaa4a679df05\",\n      \"title\": \"Why am I getting a parsing token error when upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/7019615393-why-am-i-getting-a-token-parsing-error-when-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"da8e3156-8405-4c12-a347-96d6590da06c\",\n      \"title\": \"LangSmith upgrade fails with SSL certificate verification error when connecting to Azure Cache for Redis\",\n      \"url\": \"https://support.langchain.com/articles/7558740603-langsmith-helm-upgrade-fails-with-ssl-certificate-verification-error-when-connecting-to-azure-cache-for-redis\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"af6b64a9-ed4a-4993-b54d-d4913eeb1d26\",\n      \"title\": \"How do I change my email address in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9223968956-how-do-i-change-my-email-address-in-langsmith\",\n      \"collection\": \"General\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_35346558",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"python\", \"page_size\": 5, \"query\": \"agents\"}"
        }
      },
      {
        "type": "function",
        "id": "call_07172043",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"python\", \"page_size\": 5, \"query\": \"create_agent\"}"
        }
      },
      {
        "type": "function",
        "id": "call_42014381",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"python\", \"page_size\": 5, \"query\": \"langgraph\"}"
        }
      },
      {
        "type": "function",
        "id": "call_54442457",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"OSS (LangChain and LangGraph)\"}"
        }
      }
    ],
    "id": "lc_run--019b3a31-3064-70d3-8af6-2bd9ce12ad24",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_35346558",
    "id": "9c2be0a0-641d-4eb9-bca8-c469d028a5ce",
    "content": "Result 1:\nTitle: How to run multiple agents on the same thread\nLink: https://docs.langchain.comlangsmith/same-thread\nContent: In LangSmith Deployment, a thread is not explicitly associated with a particular agent.\nThis means that you can run multiple agents on the same thread, which allows a different agent to continue from an initial agent's progress.\n\nIn this example, we will create two agents and then call them both on the same thread.\nYou'll see that the second agent will respond using information from the [checkpoint](/oss/python/langgraph/graph-api#checkpointer-state) generated in the thread by the first agent as context.\n\n## Setup\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    from langgraph_sdk import get_client\n\n    client = get_client(url=<DEPLOYMENT_URL>)\n\n    openai_assistant = await client.assistants.create(\n        graph_id=\"agent\", config={\"configurable\": {\"model_name\": \"openai\"}}\n    )\n\n    # There should always be a default assistant with no configuration\n    assistants = await client.assistants.search()\n    default_assistant = [a for a in assistants if not a[\"config\"]][0]\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    import { Client } from \"@langchain/langgraph-sdk\";\n\n    const client = new Client({ apiUrl: <DEPLOYMENT_URL> });\n\n    const openAIAssistant = await client.assistants.create(\n      { graphId: \"agent\", config: {\"configurable\": {\"model_name\": \"openai\"}}}\n    );\n\n    const assistants = await client.assistants.search();\n    const defaultAssistant = assistants.find(a => !a.config);\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request POST \\\n        --url <DEPLOYMENT_URL>/assistants \\\n        --header 'Content-Type: application/json' \\\n        --data '{\n            \"graph_id\": \"agent\",\n            \"config\": { \"configurable\": { \"model_name\": \"openai\" } }\n        }' && \\\n    curl --request POST \\\n        --url <DEPLOYMENT_URL>/assistants/search \\\n        --header 'Content-Type: application/json' \\\n        --data '{\n            \"limit\": 10,\n            \"offset\": 0\n        }' | jq -c 'map(select(.config == null or .config == {})) | .[0]'\n    ```\n  </Tab>\n</Tabs>\n\nWe can see that these agents are different:\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    print(openai_assistant)\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    console.log(openAIAssistant);\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request GET \\\n        --url <DEPLOYMENT_URL>/assistants/<OPENAI_ASSISTANT_ID>\n    ```\n  </Tab>\n</Tabs>\n\nOutput:\n\n```\n{\n\"assistant_id\": \"db87f39d-b2b1-4da8-ac65-cf81beb3c766\",\n\"graph_id\": \"agent\",\n\"created_at\": \"2024-08-30T21:18:51.850581+00:00\",\n\"updated_at\": \"2024-08-30T21:18:51.850581+00:00\",\n\"config\": {\n\"configurable\": {\n\"model_name\": \"openai\"\n}\n},\n\"metadata\": {}\n}\n```\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    print(default_assistant)\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    console.log(defaultAssistant);\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request GET \\\n        --url <DEPLOYMENT_URL>/assistants/<DEFAULT_ASSISTANT_ID>\n    ```\n  </Tab>\n</Tabs>\n\nOutput:\n\n```\n{\n\"assistant_id\": \"fe096781-5601-53d2-b2f6-0d3403f7e9ca\",\n\"graph_id\": \"agent\",\n\"created_at\": \"2024-08-08T22:45:24.562906+00:00\",\n\"updated_at\": \"2024-08-08T22:45:24.562906+00:00\",\n\"config\": {},\n\"metadata\": {\n\"created_by\": \"system\"\n}\n}\n```\n\n## Run assistants on thread\n\n### Run OpenAI assistant\n\nWe can now run the OpenAI assistant on the thread first.\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    thread = await client.threads.create()\n    input = {\"messages\": [{\"role\": \"user\", \"content\": \"who made you?\"}]}\n    async for event in client.runs.stream(\n        thread[\"thread_id\"],\n        openai_assistant[\"assistant_id\"],\n        input=input,\n        stream_mode=\"updates\",\n    ):\n        print(f\"Receiving event of type: {event.event}\")\n        print(event.data)\n        print(\"\\n\\n\")\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    const thread = await client.threads.create();\n    let input =  {\"messages\": [{\"role\": \"user\", \"content\": \"who made you?\"}]}\n\n    const streamResponse = client.runs.stream(\n      thread[\"thread_id\"],\n      openAIAssistant[\"assistant_id\"],\n      {\n        input,\n        streamMode: \"updates\"\n      }\n    );\n    for await (const event of streamResponse) {\n      console.log(`Receiving event of type: ${event.event}`);\n      console.log(event.data);\n      console.log(\"\\n\\n\");\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    thread_id=$(curl --request POST \\\n        --url <DEPLOYMENT_URL>/threads \\\n        --header 'Content-Type: application/json' \\\n        --data '{}' | jq -r '.thread_id') && \\\n    curl --request POST \\\n        --url \"<DEPLOYMENT_URL>/threads/${thread_id}/runs/stream\" \\\n        --header 'Content-Type: application/json' \\\n        --data '{\n            \"assistant_id\": <OPENAI_ASSISTANT_ID>,\n            \"input\": {\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"who made you?\"\n                    }\n                ]\n            },\n            \"stream_mode\": [\n                \"updates\"\n            ]\n        }' | \\\n        sed 's/\\r$//' | \\\n        awk '\n        /^event:/ {\n            if (data_content != \"\") {\n                print data_content \"\\n\"\n            }\n            sub(/^event: /, \"Receiving event of type: \", $0)\n            printf \"%s...\\n\", $0\n            data_content = \"\"\n        }\n        /^data:/ {\n            sub(/^data: /, \"\", $0)\n            data_content = $0\n        }\n        END {\n            if (data_content != \"\") {\n                print data_content \"\\n\\n\"\n            }\n        }\n    '\n    ```\n  </Tab>\n</Tabs>\n\nOutput:\n\n```\nReceiving event of type: metadata\n{'run_id': '1ef671c5-fb83-6e70-b698-44dba2d9213e'}\n\nReceiving event of type: updates\n{'agent': {'messages': [{'content': 'I was created by OpenAI, a research organization focused on developing and advancing artificial intelligence technology.', 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-f5735b86-b80d-4c71-8dc3-4782b5a9c7c8', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n```\n\n### Run default assistant\n\nNow, we can run it on the default assistant and see that this second assistant is aware of the initial question, and can answer the question, \"and you?\":\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    input = {\"messages\": [{\"role\": \"user\", \"content\": \"and you?\"}]}\n    async for event in client.runs.stream(\n        thread[\"thread_id\"],\n        default_assistant[\"assistant_id\"],\n        input=input,\n        stream_mode=\"updates\",\n    ):\n        print(f\"Receiving event of type: {event.event}\")\n        print(event.data)\n        print(\"\\n\\n\")\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    let input =  {\"messages\": [{\"role\": \"user\", \"content\": \"and you?\"}]}\n\n    const streamResponse = client.runs.stream(\n      thread[\"thread_id\"],\n      defaultAssistant[\"assistant_id\"],\n      {\n        input,\n        streamMode: \"updates\"\n      }\n    );\n    for await (const event of streamResponse) {\n      console.log(`Receiving event of type: ${event.event}`);\n      console.log(event.data);\n      console.log(\"\\n\\n\");\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request POST \\\n        --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n        --header 'Content-Type: application/json' \\\n        --data '{\n            \"assistant_id\": <DEFAULT_ASSISTANT_ID>,\n            \"input\": {\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"and you?\"\n                    }\n                ]\n            },\n            \"stream_mode\": [\n                \"updates\"\n            ]\n        }' | \\\n        sed 's/\\r$//' | \\\n        awk '\n        /^event:/ {\n            if (data_content != \"\") {\n                print data_content \"\\n\"\n            }\n            sub(/^event: /, \"Receiving event of type: \", $0)\n            printf \"%s...\\n\", $0\n            data_content = \"\"\n        }\n        /^data:/ {\n            sub(/^data: /, \"\", $0)\n            data_content = $0\n        }\n        END {\n            if (data_content != \"\") {\n                print data_content \"\\n\\n\"\n            }\n        }\n    '\n    ```\n  </Tab>\n</Tabs>\n\nOutput:\n\n```\nReceiving event of type: metadata\n{'run_id': '1ef6722d-80b3-6fbb-9324-253796b1cd13'}\n\nReceiving event of type: updates\n{'agent': {'messages': [{'content': [{'text': 'I am an artificial intelligence created by Anthropic, not by OpenAI. I should not have stated that OpenAI created me, as that is incorrect. Anthropic is the company that developed and trained me using advanced language models and AI technology. I will be more careful about providing accurate information regarding my origins in the future.', 'type': 'text', 'index': 0}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'end_turn', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-ebaacf62-9dd9-4165-9535-db432e4793ec', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 302, 'output_tokens': 72, 'total_tokens': 374}}]}}\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/same-thread.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Agents\nLink: https://docs.langchain.comoss/javascript/langchain/agents\nContent: Agents combine language models with [tools](/oss/javascript/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n`createAgent()` provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid  theme={null}\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n  `createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/javascript/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n\n  Learn more about the [Graph API](/oss/javascript/langgraph/graph-api).\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/javascript/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: []\n});\n```\n\nModel identifier strings use the format `provider:model` (e.g. `\"openai:gpt-5\"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/javascript/integrations/providers/) to see available params and methods on your model.\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:\n\n```ts  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst basicModel = new ChatOpenAI({ model: \"gpt-4o-mini\" });\nconst advancedModel = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst dynamicModelSelection = createMiddleware({\n  name: \"DynamicModelSelection\",\n  wrapModelCall: (request, handler) => {\n    // Choose model based on conversation complexity\n    const messageCount = request.messages.length;\n\n    return handler({\n        ...request,\n        model: messageCount > 10 ? advancedModel : basicModel,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\", // Base model (used when messageCount \u2264 10)\n  tools,\n  middleware: [dynamicModelSelection],\n});\n```\n\nFor more details on middleware and advanced patterns, see the [middleware documentation](/oss/javascript/langchain/middleware).\n\n<Tip>\n  For model configuration details, see [Models](/oss/javascript/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/javascript/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/javascript/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({\n      query: z.string().describe(\"The query to search for\"),\n    }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [search, getWeather],\n});\n```\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\nTo customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:\n\n```ts wrap theme={null}\nimport { createAgent, createMiddleware, ToolMessage } from \"langchain\";\n\nconst handleToolErrors = createMiddleware({\n  name: \"HandleToolErrors\",\n  wrapToolCall: async (request, handler) => {\n    try {\n      return await handler(request);\n    } catch (error) {\n      // Return a custom error message to the model\n      return new ToolMessage({\n        content: `Tool error: Please check your input and try again. (${error})`,\n        tool_call_id: request.toolCall.id!,\n      });\n    }\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [handleToolErrors],\n});\n```\n\nThe agent will return a [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) with the custom error message when a tool fails.\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n  **Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n  ```\n  ================================ Human Message =================================\n\n  Find the most popular wireless headphones right now and check if they're in stock\n  ```\n\n  * **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n  * **Acting**: Call `search_products(\"wireless headphones\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    search_products (call_abc123)\n   Call ID: call_abc123\n    Args:\n      query: wireless headphones\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n  ```\n\n  * **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n  * **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    check_inventory (call_def456)\n   Call ID: call_def456\n    Args:\n      product_id: WH-1000XM5\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Product WH-1000XM5: 10 units in stock\n  ```\n\n  * **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n  * **Acting**: Produce final answer\n\n  ```\n  ================================== Ai Message ==================================\n\n  I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n  ```\n</Accordion>\n\n<Tip>\n  To learn more about tools, see [Tools](/oss/javascript/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:\n\n```ts wrap theme={null}\nconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n```\n\nWhen no `systemPrompt` is provided, the agent will infer its task from the messages directly.\n\nThe `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/javascript/integrations/chat/anthropic#prompt-caching):\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst literaryAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-5\",\n  systemPrompt: new SystemMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"You are an AI assistant tasked with analyzing literary works.\",\n      },\n      {\n        type: \"text\",\n        text: \"<the entire contents of 'Pride and Prejudice'>\",\n        cache_control: { type: \"ephemeral\" }\n      }\n    ]\n  })\n});\n\nconst result = await literaryAgent.invoke({\n  messages: [new HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]\n});\n```\n\nThe `cache_control` field with `{ type: \"ephemeral\" }` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/javascript/langchain/middleware).\n\n```typescript wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userRole: z.enum([\"expert\", \"beginner\"]),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [/* ... */],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\n      const userRole = runtime.context.userRole || \"user\";\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    }),\n  ],\n});\n\n// The system prompt will be set dynamically based on context\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Explain machine learning\" }] },\n  { context: { userRole: \"expert\" } }\n);\n```\n\n<Tip>\n  For more details on message types and formatting, see [Messages](/oss/javascript/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/javascript/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/javascript/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n```typescript  theme={null}\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"What's the weather in San Francisco?\" }],\n})\n```\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/javascript/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/javascript/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst ContactInfo = z.object({\n  name: z.string(),\n  email: z.string(),\n  phone: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  responseFormat: ContactInfo,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n    },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n// {\n//   name: 'John Doe',\n//   email: 'john@example.com',\n//   phone: '(555) 123-4567'\n// }\n```\n\n<Tip>\n  To learn about structured output, see [Structured output](/oss/javascript/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/javascript/langchain/short-term-memory) of the agent:\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport { createAgent } from \"langchain\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\n\nconst customAgentState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst CustomAgentState = createAgent({\n  model: \"gpt-4o\",\n  tools: [],\n  stateSchema: customAgentState,\n});\n```\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/javascript/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/javascript/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```ts  theme={null}\nconst stream = await agent.stream(\n  {\n    messages: [{\n      role: \"user\",\n      content: \"Search for AI news and summarize the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/javascript/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/javascript/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n<Tip>\n  For comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Workflows and agents\nLink: https://docs.langchain.comoss/javascript/langgraph/workflows-agents\nContent: Workflows and agents\nThis guide reviews common workflow and agent patterns. Workflows have predetermined code paths and are designed to operate in a certain order. Agents are dynamic and define their own processes and tool usage. LangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment.\n\nWorkflows and agents\nWorkflows and agents\nThis guide reviews common workflow and agent patterns. Workflows have predetermined code paths and are designed to operate in a certain order. Agents are dynamic and define their own processes and tool usage. LangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment.\n\nAgents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\n\n---\n\nResult 4:\nTitle: Deep Agents overview\nLink: https://docs.langchain.comoss/python/deepagents/overview\nContent: [`deepagents`](https://pypi.org/project/deepagents/) is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.\n\n## When to use deep agents\n\nUse deep agents when you need agents that can:\n\n* **Handle complex, multi-step tasks** that require planning and decomposition\n* **Manage large amounts of context** through file system tools\n* **Delegate work** to specialized subagents for context isolation\n* **Persist memory** across conversations and threads\n\nFor simpler use cases, consider using LangChain's [`create_agent`](/oss/python/langchain/agents) or building a custom [LangGraph](/oss/python/langgraph/overview) workflow.\n\n## Core capabilities\n\n<Card title=\"Planning and task decomposition\" icon=\"timeline\">\n  Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.\n</Card>\n\n<Card title=\"Context management\" icon=\"scissors\">\n  File system tools (`ls`, `read_file`, `write_file`, `edit_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.\n</Card>\n\n<Card title=\"Subagent spawning\" icon=\"people-group\">\n  A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent's context clean while still going deep on specific subtasks.\n</Card>\n\n<Card title=\"Long-term memory\" icon=\"database\">\n  Extend agents with persistent memory across threads using LangGraph's Store. Agents can save and retrieve information from previous conversations.\n</Card>\n\n## Relationship to the LangChain ecosystem\n\nDeep agents is built on top of:\n\n* [LangGraph](/oss/python/langgraph/overview) - Provides the underlying graph execution and state management\n* [LangChain](/oss/python/langchain/overview) - Tools and model integrations work seamlessly with deep agents\n* [LangSmith](/langsmith/home) - Observability, evaluation, and deployment\n\nDeep agents applications can be deployed via [LangSmith Deployment](/langsmith/deployments) and monitored with [LangSmith Observability](/langsmith/observability).\n\n## Get started\n\n<CardGroup cols={2}>\n  <Card title=\"Quickstart\" icon=\"rocket\" href=\"/oss/python/deepagents/quickstart\">\n    Build your first deep agent\n  </Card>\n\n  <Card title=\"Customization\" icon=\"sliders\" href=\"/oss/python/deepagents/customization\">\n    Learn about customization options\n  </Card>\n\n  <Card title=\"Middleware\" icon=\"layer-group\" href=\"/oss/python/deepagents/middleware\">\n    Understand the middleware architecture\n  </Card>\n\n  <Card title=\"Reference\" icon=\"arrow-up-right-from-square\" href=\"https://reference.langchain.com/python/deepagents/\">\n    See the `deepagents` API reference\n  </Card>\n</CardGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Workflows and agents\nLink: https://docs.langchain.comoss/python/langgraph/workflows-agents\nContent: Workflows and agents\nThis guide reviews common workflow and agent patterns. Workflows have predetermined code paths and are designed to operate in a certain order. Agents are dynamic and define their own processes and tool usage. LangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment.\n\nWorkflows and agents\nWorkflows and agents\nThis guide reviews common workflow and agent patterns. Workflows have predetermined code paths and are designed to operate in a certain order. Agents are dynamic and define their own processes and tool usage. LangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment.\n\nAgents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_07172043",
    "id": "ee783224-1878-4759-b28a-a0665afef2bf",
    "content": "Result 1:\nTitle: Test a ReAct agent with Pytest/Vitest and LangSmith\nLink: https://docs.langchain.comlangsmith/test-react-agent-pytest\nContent: Define agent\nNow that we have defined all of our tools, we can use create_agent to create our agent.\n\nDefine agent\nNow that we have defined all of our tools, we can use create_agent to create our agent. import { z } from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst AgentOutputFormatSchema = z.object({\n  numeric_answer: z.number().optional().describe(\"The numeric answer, if the user asked for one\"),\n  text_answer: z.string().optional().describe(\"The text answer, if the user asked for one\"),\n  reasoning: z.string().describe(\"The reasoning behind the answer\"),\n})\n\nconst tools = [codeTool, searchTool, tickerTool];\n\nconst agent = createReactAgent({\n  llm: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: tools,\n  responseFormat: AgentOutputFormatSchema,\n  stateModifier: \"You are a financial expert. Respond to the users query accurately\",\n});\n\nexport default agent;\n\nDefine agent\nNow that we have defined all of our tools, we can use create_agent to create our agent. from typing_extensions import Annotated, TypedDict\nfrom langchain.agents import create_agent\n\n\nclass AgentOutputFormat(TypedDict):\n    numeric_answer: Annotated[float | None, ..., \"The numeric answer, if the user asked for one\"]\n    text_answer: Annotated[str | None, ..., \"The text answer, if the user asked for one\"]\n    reasoning: Annotated[str, ..., \"The reasoning behind the answer\"]\n\nagent = create_agent(\n    model=\"gpt-4o-mini\",\n    tools=[code_tool, search_tool, polygon_aggregates],\n    response_format=AgentOutputFormat,\n    system_prompt=\"You are a financial expert. Respond to the users query accurately\",\n)\n\n---\n\nResult 2:\nTitle: Agents\nLink: https://docs.langchain.comoss/javascript/langchain/agents\nContent: Agents combine language models with [tools](/oss/javascript/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n`createAgent()` provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid  theme={null}\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n  `createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/javascript/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n\n  Learn more about the [Graph API](/oss/javascript/langgraph/graph-api).\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/javascript/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: []\n});\n```\n\nModel identifier strings use the format `provider:model` (e.g. `\"openai:gpt-5\"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/javascript/integrations/providers/) to see available params and methods on your model.\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:\n\n```ts  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst basicModel = new ChatOpenAI({ model: \"gpt-4o-mini\" });\nconst advancedModel = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst dynamicModelSelection = createMiddleware({\n  name: \"DynamicModelSelection\",\n  wrapModelCall: (request, handler) => {\n    // Choose model based on conversation complexity\n    const messageCount = request.messages.length;\n\n    return handler({\n        ...request,\n        model: messageCount > 10 ? advancedModel : basicModel,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\", // Base model (used when messageCount \u2264 10)\n  tools,\n  middleware: [dynamicModelSelection],\n});\n```\n\nFor more details on middleware and advanced patterns, see the [middleware documentation](/oss/javascript/langchain/middleware).\n\n<Tip>\n  For model configuration details, see [Models](/oss/javascript/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/javascript/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/javascript/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({\n      query: z.string().describe(\"The query to search for\"),\n    }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [search, getWeather],\n});\n```\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\nTo customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:\n\n```ts wrap theme={null}\nimport { createAgent, createMiddleware, ToolMessage } from \"langchain\";\n\nconst handleToolErrors = createMiddleware({\n  name: \"HandleToolErrors\",\n  wrapToolCall: async (request, handler) => {\n    try {\n      return await handler(request);\n    } catch (error) {\n      // Return a custom error message to the model\n      return new ToolMessage({\n        content: `Tool error: Please check your input and try again. (${error})`,\n        tool_call_id: request.toolCall.id!,\n      });\n    }\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [handleToolErrors],\n});\n```\n\nThe agent will return a [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) with the custom error message when a tool fails.\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n  **Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n  ```\n  ================================ Human Message =================================\n\n  Find the most popular wireless headphones right now and check if they're in stock\n  ```\n\n  * **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n  * **Acting**: Call `search_products(\"wireless headphones\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    search_products (call_abc123)\n   Call ID: call_abc123\n    Args:\n      query: wireless headphones\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n  ```\n\n  * **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n  * **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    check_inventory (call_def456)\n   Call ID: call_def456\n    Args:\n      product_id: WH-1000XM5\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Product WH-1000XM5: 10 units in stock\n  ```\n\n  * **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n  * **Acting**: Produce final answer\n\n  ```\n  ================================== Ai Message ==================================\n\n  I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n  ```\n</Accordion>\n\n<Tip>\n  To learn more about tools, see [Tools](/oss/javascript/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:\n\n```ts wrap theme={null}\nconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n```\n\nWhen no `systemPrompt` is provided, the agent will infer its task from the messages directly.\n\nThe `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/javascript/integrations/chat/anthropic#prompt-caching):\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst literaryAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-5\",\n  systemPrompt: new SystemMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"You are an AI assistant tasked with analyzing literary works.\",\n      },\n      {\n        type: \"text\",\n        text: \"<the entire contents of 'Pride and Prejudice'>\",\n        cache_control: { type: \"ephemeral\" }\n      }\n    ]\n  })\n});\n\nconst result = await literaryAgent.invoke({\n  messages: [new HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]\n});\n```\n\nThe `cache_control` field with `{ type: \"ephemeral\" }` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/javascript/langchain/middleware).\n\n```typescript wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userRole: z.enum([\"expert\", \"beginner\"]),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [/* ... */],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\n      const userRole = runtime.context.userRole || \"user\";\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    }),\n  ],\n});\n\n// The system prompt will be set dynamically based on context\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Explain machine learning\" }] },\n  { context: { userRole: \"expert\" } }\n);\n```\n\n<Tip>\n  For more details on message types and formatting, see [Messages](/oss/javascript/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/javascript/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/javascript/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n```typescript  theme={null}\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"What's the weather in San Francisco?\" }],\n})\n```\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/javascript/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/javascript/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst ContactInfo = z.object({\n  name: z.string(),\n  email: z.string(),\n  phone: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  responseFormat: ContactInfo,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n    },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n// {\n//   name: 'John Doe',\n//   email: 'john@example.com',\n//   phone: '(555) 123-4567'\n// }\n```\n\n<Tip>\n  To learn about structured output, see [Structured output](/oss/javascript/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/javascript/langchain/short-term-memory) of the agent:\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport { createAgent } from \"langchain\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\n\nconst customAgentState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst CustomAgentState = createAgent({\n  model: \"gpt-4o\",\n  tools: [],\n  stateSchema: customAgentState,\n});\n```\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/javascript/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/javascript/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```ts  theme={null}\nconst stream = await agent.stream(\n  {\n    messages: [{\n      role: \"user\",\n      content: \"Search for AI news and summarize the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/javascript/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/javascript/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n<Tip>\n  For comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Agents\nLink: https://docs.langchain.comoss/python/langchain/agents\nContent: Agents\nAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions. create_agent provides a production-ready agent implementation. An LLM Agent runs tools in a loop to achieve a goal .\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached. create_agent builds a graph -based agent runtime using LangGraph . A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware. Learn more about the Graph API.\n\nAgents\nAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions. create_agent provides a production-ready agent implementation. An LLM Agent runs tools in a loop to achieve a goal .\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached. %%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n\nAgents\nAgents\nAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions. create_agent provides a production-ready agent implementation. An LLM Agent runs tools in a loop to achieve a goal .\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached. %%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n\n---\n\nResult 4:\nTitle: Runtime\nLink: https://docs.langchain.comoss/python/langchain/runtime\nContent: ## Overview\n\nLangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) runs on LangGraph's runtime under the hood.\n\nLangGraph exposes a [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object with the following information:\n\n1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation\n2. **Store**: a [BaseStore](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) instance used for [long-term memory](/oss/python/langchain/long-term-memory)\n3. **Stream writer**: an object used for streaming information via the `\"custom\"` stream mode\n\n<Tip>\n  Runtime context provides **dependency injection** for your tools and middleware. Instead of hardcoding values or using global state, you can inject runtime dependencies (like database connections, user IDs, or configuration) when invoking your agent. This makes your tools more testable, reusable, and flexible.\n</Tip>\n\nYou can access the runtime information within [tools](#inside-tools) and [middleware](#inside-middleware).\n\n## Access\n\nWhen creating an agent with [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), you can specify a `context_schema` to define the structure of the `context` stored in the agent [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime).\n\nWhen invoking the agent, pass the `context` argument with the relevant configuration for the run:\n\n```python  theme={null}\nfrom dataclasses import dataclass\n\nfrom langchain.agents import create_agent\n\n\n@dataclass\nclass Context:\n    user_name: str\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    context_schema=Context  # [!code highlight]\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")  # [!code highlight]\n)\n```\n\n### Inside tools\n\nYou can access the runtime information inside tools to:\n\n* Access the context\n* Read or write long-term memory\n* Write to the [custom stream](/oss/python/langchain/streaming#custom-updates) (ex, tool progress / updates)\n\nUse the `ToolRuntime` parameter to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside a tool.\n\n```python  theme={null}\nfrom dataclasses import dataclass\nfrom langchain.tools import tool, ToolRuntime  # [!code highlight]\n\n@dataclass\nclass Context:\n    user_id: str\n\n@tool\ndef fetch_user_email_preferences(runtime: ToolRuntime[Context]) -> str:  # [!code highlight]\n    \"\"\"Fetch the user's email preferences from the store.\"\"\"\n    user_id = runtime.context.user_id  # [!code highlight]\n\n    preferences: str = \"The user prefers you to write a brief and polite email.\"\n    if runtime.store:  # [!code highlight]\n        if memory := runtime.store.get((\"users\",), user_id):  # [!code highlight]\n            preferences = memory.value[\"preferences\"]\n\n    return preferences\n```\n\n### Inside middleware\n\nYou can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.\n\nUse `request.runtime` to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside middleware decorators. The runtime object is available in the [`ModelRequest`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRequest) parameter passed to middleware functions.\n\n```python  theme={null}\nfrom dataclasses import dataclass\n\nfrom langchain.messages import AnyMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest, before_model, after_model\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass Context:\n    user_name: str\n\n# Dynamic prompts\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context.user_name  # [!code highlight]\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n# Before model hook\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Processing request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\n# After model hook\n@after_model\ndef log_after_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Completed request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    middleware=[dynamic_system_prompt, log_before_model, log_after_model],  # [!code highlight]\n    context_schema=Context\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")\n)\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/runtime.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: What's new in LangChain v1\nLink: https://docs.langchain.comoss/python/releases/langchain-v1\nContent: create_agent\ncreate_agent is the standard way to build agents in LangChain 1.0. It provides a simpler interface than langgraph.prebuilt.create_react_agent while offering greater customization potential by using middleware. from langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[search_web, analyze_data, send_email],\n    system_prompt=\"You are a helpful research assistant.\"\n)\n\nresult = agent.invoke({\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Research AI safety trends\"}\n    ]\n})\n\ncreate_agent\ncreate_agent is the standard way to build agents in LangChain 1.0. It provides a simpler interface than langgraph.prebuilt.create_react_agent while offering greater customization potential by using middleware. Under the hood, create_agent is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools: For more information, see Agents.\n\nBuilt on LangGraph\nBecause create_agent is built on LangGraph , you automatically get built in support for long running and reliable agents via: Persistence Conversations automatically persist across sessions with built-in checkpointing Streaming Stream tokens, tool calls, and reasoning traces in real-time Human-in-the-loop Pause agent execution for human approval before sensitive actions Time travel Rewind conversations to any point and explore alternate paths and prompts You don't need to learn LangGraph to use these features\u2014they work out of the box.\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_42014381",
    "id": "76c38451-493c-43f7-bcef-e34bd50a9532",
    "content": "Result 1:\nTitle: LangGraph CLI\nLink: https://docs.langchain.comlangsmith/cli\nContent: LangGraph CLI\nLangGraph CLI is a command-line tool for building and running the Agent Server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.\n\nLangGraph CLI\nLangGraph CLI\nLangGraph CLI is a command-line tool for building and running the Agent Server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.\n\ndev\nRun LangGraph API server in development mode with hot reloading and debugging capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory. Currently, the CLI only supports Python >= 3.11. Installation This command requires the \"inmem\" extra to be installed: Usage Options Option Default Description -c, --config FILE langgraph.json Path to configuration file declaring dependencies, graphs and environment variables --host TEXT 127.0.0.1 Host to bind the server to --port INTEGER 2024 Port to bind the server to --no-reload Disable auto-reload --n-jobs-per-worker INTEGER Number of jobs per worker. Default is 10 --debug-port INTEGER Port for debugger to listen on --wait-for-client False Wait for a debugger client to connect to the debug port before starting the server --no-browser Skip automatically opening the browser when the server starts --studio-url TEXT URL of the Studio instance to connect to. Defaults to https://smith.langchain.com --allow-blocking False Do not raise errors for synchronous I/O blocking operations in your code (added in 0.2.6) --tunnel False Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers like Safari or networks blocking localhost connections --help Display command documentation Run LangGraph API server in development mode with hot reloading capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory. Usage Options Option Default Description -c, --config FILE langgraph.json Path to configuration file declaring dependencies, graphs and environment variables --host TEXT 127.0.0.1 Host to bind the server to --port INTEGER 2024 Port to bind the server to --no-reload Disable auto-reload --n-jobs-per-worker INTEGER Number of jobs per worker. Default is 10 --debug-port INTEGER Port for debugger to listen on --wait-for-client False Wait for a debugger client to connect to the debug port before starting the server --no-browser Skip automatically opening the browser when the server starts --studio-url TEXT URL of the Studio instance to connect to. Defaults to https://smith.langchain.com --allow-blocking False Do not raise errors for synchronous I/O blocking operations in your code --tunnel False Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers or networks blocking localhost connections --help Display command documentation\n\n---\n\nResult 2:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/javascript/langgraph/overview\nContent: <Callout icon=\"bullhorn\" color=\"#DFC5FE\" iconType=\"regular\">\n  **LangGraph v1.x is now available!**\n\n  For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/javascript/releases/langgraph-v1) and [migration guide](/oss/javascript/migrate/langgraph-v1).\n\n  If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=02-langgraph.yml\\&labels=langgraph,js/ts) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langgraphjs/tree/main/docs/docs).\n</Callout>\n\nTrusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/javascript/langchain/models) and [tools](/oss/javascript/langchain/tools).\n\nWe will commonly use [LangChain](/oss/javascript/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/javascript/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph @langchain/core\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```typescript  theme={null}\nimport { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\n\nconst mockLlm = (state: typeof MessagesAnnotation.State) => {\n  return { messages: [{ role: \"ai\", content: \"hello world\" }] };\n};\n\nconst graph = new StateGraph(MessagesAnnotation)\n  .addNode(\"mock_llm\", mockLlm)\n  .addEdge(START, \"mock_llm\")\n  .addEdge(\"mock_llm\", END)\n  .compile();\n\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi!\" }] });\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/javascript/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/javascript/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/javascript/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/javascript/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: What's new in LangGraph v1\nLink: https://docs.langchain.comoss/javascript/releases/langgraph-v1\nContent: **LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.\n\nIt's designed to work hand-in-hand with [LangChain v1](/oss/javascript/releases/langchain-v1) (whose `createAgent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.\n\n<CardGroup cols={1}>\n  <Card title=\"Stable core APIs\" icon=\"diagram-project\">\n    Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\n  </Card>\n\n  <Card title=\"Reliability, by default\" icon=\"database\">\n    Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\n  </Card>\n\n  <Card title=\"Seamless with LangChain v1\" icon=\"link\">\n    LangChain's `createAgent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph @langchain/core\n  ```\n</CodeGroup>\n\nFor a complete list of changes, see the [migration guide](/oss/javascript/migrate/langgraph-v1).\n\n## Deprecation of `createReactAgent`\n\nThe LangGraph `createReactAgent` prebuilt has been deprecated in favor of LangChain's `createAgent`. It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n\n* For information on the new `createAgent` API, see the [LangChain v1 release notes](/oss/javascript/releases/langchain-v1#createagent).\n* For information on migrating from `createReactAgent` to `createAgent`, see the [LangChain v1 migration guide](/oss/javascript/migrate/langchain-v1#createagent).\n\n## Typed interrupts\n\n[`StateGraph`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html) now accepts a map of interrupt types in the constructor to more closely constrain the types of interrupts that can be used within a graph.\n\n```typescript expandable theme={null}\nimport { StateGraph, MemorySaver, interrupt } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst stateSchema = z.object({\n  foo: z.string(),\n})\n\nconst graphConfig = {\n  interrupts: {\n    // Define a simple interrupt that accepts a reason and returns messages\n    simple: interrupt<{ reason: string }, { messages: string[] }>, // [!code highlight]\n    // Define a complex interrupt with the same signature\n    complex: interrupt<{ reason: string }, { messages: string[] }>, // [!code highlight]\n  }\n}\n\nconst checkpointer = new MemorySaver();\n\nconst graph = new StateGraph(stateSchema, graphConfig)\n  .addNode(\"node\", async (state, runtime) => {\n    // Trigger the simple interrupt with a reason\n    const response = runtime.interrupt.simple({ reason: \"test\" });\n    // Return the interrupt response as the new state\n    return { foo: response };\n  })\n  // Compile the graph with the checkpointer\n  .compile({ checkpointer });\n\n// Invoke the graph with initial state\nconst result = await graph.invoke({ foo: \"test\" });\n\n// Access the interrupt data\nif (graph.isInterrupted(result)) {\n  console.log(result.__interrupt__.messages);\n}\n```\n\nFor more information on interrupts, see the [Interrupts](/oss/javascript/langgraph/interrupts) documentation.\n\n## Frontend SDK enhancements\n\nLangGraph v1 comes with a few enhancements when interacting with a LangGraph application from the frontend.\n\n### Event stream encoding\n\nThe low-level `toLangGraphEventStream` helper has been removed. Streaming responses are now handled natively by the SDK, and you can select the wire format via passing in the `encoding` format to `graph.stream`. This makes switching between SSE and normal JSON responses straightforward without changing UI logic.\n\nSee the [migration guide](/oss/javascript/migrate/langgraph-v1#stream-encoding) for more information.\n\n### Custom transports in `useStream`\n\nThe React `useStream` hook now supports pluggable transports so you can have more control over the network layer without changing UI code.\n\n```typescript  theme={null}\nconst stream = useStream({\n  transport: new FetchStreamTransport({\n    apiUrl: \"http://localhost:2024\",\n  }),\n});\n```\n\nLearn how to integrate and customize the hook: [Integrate LangGraph into your React application](/oss/javascript/langgraph/ui).\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraphjs/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraphjs/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangGraph 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Overview\" icon=\"book\" href=\"/oss/javascript/langgraph/overview\" arrow>\n    What LangGraph is and when to use it\n  </Card>\n\n  <Card title=\"Graph API\" icon=\"diagram-project\" href=\"/oss/javascript/langgraph/graph-api\" arrow>\n    Build graphs with state, nodes, and edges\n  </Card>\n\n  <Card title=\"LangChain Agents\" icon=\"robot\" href=\"/oss/javascript/langchain/agents\" arrow>\n    High-level agents built on LangGraph\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/javascript/migrate/langgraph-v1\" arrow>\n    How to migrate to LangGraph v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langgraphjs\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/javascript/versioning) \u2013 Understanding version numbers\n* [Release policy](/oss/javascript/release-policy) \u2013 Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/releases/langgraph-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/python/langgraph/overview\nContent: <Callout icon=\"bullhorn\" color=\"#DFC5FE\" iconType=\"regular\">\n  **LangGraph v1.x is now available!**\n\n  For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/python/releases/langgraph-v1) and [migration guide](/oss/python/migrate/langgraph-v1).\n\n  If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=02-langgraph.yml\\&labels=langgraph,python) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langgraph/tree/main/docs/docs).\n</Callout>\n\nTrusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools).\n\nWe will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\ndef mock_llm(state: MessagesState):\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n\ngraph = StateGraph(MessagesState)\ngraph.add_node(mock_llm)\ngraph.add_edge(START, \"mock_llm\")\ngraph.add_edge(\"mock_llm\", END)\ngraph = graph.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/python/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: What's new in LangGraph v1\nLink: https://docs.langchain.comoss/python/releases/langgraph-v1\nContent: **LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.\n\nIt's designed to work hand-in-hand with [LangChain v1](/oss/python/releases/langchain-v1) (whose `create_agent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.\n\n<CardGroup cols={1}>\n  <Card title=\"Stable core APIs\" icon=\"diagram-project\">\n    Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\n  </Card>\n\n  <Card title=\"Reliability, by default\" icon=\"database\">\n    Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\n  </Card>\n\n  <Card title=\"Seamless with LangChain v1\" icon=\"link\">\n    LangChain's `create_agent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\n## Deprecation of `create_react_agent`\n\nThe LangGraph [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt has been deprecated in favor of LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent). It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n\n* For information on the new [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) API, see the [LangChain v1 release notes](/oss/python/releases/langchain-v1#create-agent).\n* For information on migrating from [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), see the [LangChain v1 migration guide](/oss/python/migrate/langchain-v1#create-agent).\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraph/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraph/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangGraph 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Overview\" icon=\"book\" href=\"/oss/python/langgraph/overview\" arrow>\n    What LangGraph is and when to use it\n  </Card>\n\n  <Card title=\"Graph API\" icon=\"diagram-project\" href=\"/oss/python/langgraph/graph-api\" arrow>\n    Build graphs with state, nodes, and edges\n  </Card>\n\n  <Card title=\"LangChain Agents\" icon=\"robot\" href=\"/oss/python/langchain/agents\" arrow>\n    High-level agents built on LangGraph\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/python/migrate/langgraph-v1\" arrow>\n    How to migrate to LangGraph v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langgraph\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/python/versioning) \u2013 Understanding version numbers\n* [Release policy](/oss/python/release-policy) \u2013 Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langgraph-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_54442457",
    "id": "f33eaa03-fa26-4457-8795-6110a55152d7",
    "content": "{\n  \"collections\": \"OSS (LangChain and LangGraph)\",\n  \"total\": 4,\n  \"articles\": [\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3a31-3a13-7363-a791-d57b85472eec",
    "content": "**`verbose=True` \u901a\u5e38\u7528\u4e8e LangChain v0 \u7684\u65e7\u7248 AgentExecutor\uff0c\u73b0\u5728 LangChain v1 \u7684 `create_agent()` \u4e2d\u5df2**\u65e0\u6b64\u53c2\u6570**\uff0c\u6539\u7528 LangSmith \u8ffd\u8e2a\u6216\u81ea\u5b9a\u4e49 middleware \u5b9e\u73b0\u8be6\u7ec6\u65e5\u5fd7\u3002**\n\n\u65b0\u7248 `create_agent()` \u6784\u5efa\u5728 LangGraph \u4e0a\uff0c\u79fb\u9664\u4e86 `verbose` \u53c2\u6570\u3002\u542f\u7528 LangSmith \u8ffd\u8e2a\u662f\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u6765\u67e5\u770b\u8be6\u7ec6\u6267\u884c\u65e5\u5fd7\uff1a\n\n```python\n# \u542f\u7528 LangSmith \u8ffd\u8e2a\uff08\u63a8\u8350\uff09\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"  # \u663e\u793a\u8be6\u7ec6\u6267\u884c\u6b65\u9aa4\nos.environ[\"LANGSMITH_API_KEY\"] = \"your-api-key\"\n\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4o-mini\",\n    tools=[your_tools],\n    # \u65e0 verbose \u53c2\u6570\n)\n```\n\n## \u81ea\u5b9a\u4e49\u8be6\u7ec6\u65e5\u5fd7\uff08\u66ff\u4ee3 verbose\uff09\n\n\u4f7f\u7528 middleware \u6dfb\u52a0\u8be6\u7ec6\u65e5\u5fd7\u8f93\u51fa\uff1a\n\n```python\nfrom langchain.agents.middleware import before_model, after_model\n\n@before_model\ndef log_before_model(state, runtime):\n    print(f\"\ud83d\udd04 \u8c03\u7528\u6a21\u578b\uff0c\u6d88\u606f\u6570: {len(state['messages'])}\")\n    return None\n\n@after_model\ndef log_after_model(state, runtime):\n    last_msg = state['messages'][-1]\n    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n        print(f\"\u26a1 \u8c03\u7528\u5de5\u5177: {[tc.name for tc in last_msg.tool_calls]}\")\n    elif last_msg.content:\n        print(f\"\u2705 \u6700\u7ec8\u56de\u7b54: {last_msg.content[:100]}...\")\n    return None\n\nagent = create_agent(\n    model=\"gpt-4o-mini\",\n    tools=[your_tools],\n    middleware=[log_before_model, log_after_model]  # \u8be6\u7ec6\u65e5\u5fd7\n)\n```\n\n## LangGraph \u8c03\u8bd5\u6a21\u5f0f\n\n\u5982\u679c\u76f4\u63a5\u4f7f\u7528 LangGraph\uff1a\n\n```python\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\n\ngraph = StateGraph(state_schema)\n# ... \u6dfb\u52a0\u8282\u70b9\u548c\u8fb9 ...\napp = graph.compile(checkpointer=MemorySaver())  # \u542f\u7528\u68c0\u67e5\u70b9\u8ffd\u8e2a\n\n# \u8be6\u7ec6\u6d41\u5f0f\u8f93\u51fa\nfor chunk in app.stream(input, stream_mode=[\"values\", \"debug\"]):\n    print(chunk)\n```\n\n**Relevant docs:**\n- [Agents](https://docs.langchain.com/oss/python/langchain/agents)\n- [create_agent API](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)\n- [Middleware](https://docs.langchain.com/oss/python/langchain/middleware)"
  }
]