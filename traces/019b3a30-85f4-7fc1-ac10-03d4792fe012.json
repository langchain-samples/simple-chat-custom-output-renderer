[
  {
    "role": "user",
    "id": "862ced41-d844-4531-a03f-d79f2e2a82d4",
    "content": "What's the payload limit for a langsmith run"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_78087127",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"payload limit\"}"
        }
      },
      {
        "type": "function",
        "id": "call_41790433",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"run limits\"}"
        }
      },
      {
        "type": "function",
        "id": "call_31055125",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"langsmith limits\"}"
        }
      },
      {
        "type": "function",
        "id": "call_29447684",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"LangSmith Observability,all\"}"
        }
      }
    ],
    "id": "lc_run--019b3a30-898c-7951-9f7b-ad7b13a56536",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_78087127",
    "id": "a3cd4fbf-efa0-4553-9857-77b171cf8fe6",
    "content": "Result 1:\nTitle: LangSmith data plane\nLink: https://docs.langchain.comlangsmith/data-plane\nContent: The *data plane* consists of your [Agent Servers](/langsmith/agent-server) (deployments), their supporting infrastructure, and the \"listener\" application that continuously polls for updates from the [LangSmith control plane](/langsmith/control-plane).\n\n## Server infrastructure\n\nIn addition to the [Agent Server](/langsmith/agent-server) itself, the following infrastructure components for each server are also included in the broad definition of \"data plane\":\n\n* **PostgreSQL**: persistence layer for user, run, and memory data.\n* **Redis**: communication and ephemeral metadata for workers.\n* **Secrets store**: secure management of environment secrets.\n* **Autoscalers**: scale server containers based on load.\n\n## \"Listener\" application\n\nThe data plane \"listener\" application periodically calls [control plane APIs](/langsmith/control-plane#control-plane-api) to:\n\n* Determine if new deployments should be created.\n* Determine if existing deployments should be updated (i.e. new revisions).\n* Determine if existing deployments should be deleted.\n\nIn other words, the data plane \"listener\" reads the latest state of the control plane (desired state) and takes action to reconcile outstanding deployments (current state) to match the latest state.\n\n## PostgreSQL\n\nPostgreSQL is the persistence layer for all user, run, and long-term memory data in a Agent Server. This stores both checkpoints (see more info [here](/oss/python/langgraph/persistence)), server resources (threads, runs, assistants and crons), as well as items saved in the long-term memory store (see more info [here](/oss/python/langgraph/persistence#memory-store)).\n\n## Redis\n\nRedis is used in each Agent Server as a way for server and queue workers to communicate, and to store ephemeral metadata. No user or run data is stored in Redis.\n\n### Communication\n\nAll runs in an Agent Server are executed by a pool of background workers that are part of each deployment. In order to enable some features for those runs (such as cancellation and output streaming) we need a channel for two-way communication between the server and the worker handling a particular run. We use Redis to organize that communication.\n\n1. A Redis list is used as a mechanism to wake up a worker as soon as a new run is created. Only a sentinel value is stored in this list, no actual run information. The run information is then retrieved from PostgreSQL by the worker.\n2. A combination of a Redis string and Redis PubSub channel is used for the server to communicate a run cancellation request to the appropriate worker.\n3. A Redis PubSub channel is used by the worker to broadcast streaming output from an agent while the run is being handled. Any open `/stream` request in the server will subscribe to that channel and forward any events to the response as they arrive. No events are stored in Redis at any time.\n\n### Ephemeral metadata\n\nRuns in an Agent Server may be retried for specific failures (currently only for transient PostgreSQL errors encountered during the run). In order to limit the number of retries (currently limited to 3 attempts per run) we record the attempt number in a Redis string when it is picked up. This contains no run-specific info other than its ID, and expires after a short delay.\n\n## Data plane features\n\nThis section describes various features of the data plane.\n\n### Data region\n\n<Info>\n  **Only for Cloud**\n  Data regions are only applicable for [Cloud](/langsmith/cloud) deployments.\n</Info>\n\nDeployments can be created in 2 data regions: US and EU\n\nThe data region for a deployment is implied by the data region of the LangSmith organization where the deployment is created. Deployments and the underlying database for the deployments cannot be migrated between data regions.\n\n### Autoscaling\n\n[`Production` type](/langsmith/control-plane#deployment-types) deployments automatically scale up to 10 containers. Scaling is based on 3 metrics:\n\n1. CPU utilization\n2. Memory utilization\n3. Number of pending (in progress) [runs](/langsmith/assistants#execution)\n\nFor CPU utilization, the autoscaler targets 75% utilization. This means the autoscaler will scale the number of containers up or down to ensure that CPU utilization is at or near 75%. For memory utilization, the autoscaler targets 75% utilization as well.\n\nFor number of pending runs, the autoscaler targets 10 pending runs. For example, if the current number of containers is 1, but the number of pending runs is 20, the autoscaler will scale up the deployment to 2 containers (20 pending runs / 2 containers = 10 pending runs per container).\n\nEach metric is computed independently and the autoscaler will determine the scaling action based on the metric that results in the largest number of containers.\n\nScale down actions are delayed for 30 minutes before any action is taken. In other words, if the autoscaler decides to scale down a deployment, it will first wait for 30 minutes before scaling down. After 30 minutes, the metrics are recomputed and the deployment will scale down if the recomputed metrics result in a lower number of containers than the current number. Otherwise, the deployment remains scaled up. This \"cool down\" period ensures that deployments do not scale up and down too frequently.\n\n### Static IP addresses\n\n<Info>\n  **Only for Cloud**\n  Static IP addresses are only available for [Cloud](/langsmith/cloud) deployments.\n</Info>\n\nAll traffic from deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static IP addresses depending on the data region. Refer to the table below for the list of static IP addresses:\n\n| US             | EU             |\n| -------------- | -------------- |\n| 35.197.29.146  | 34.13.192.67   |\n| 34.145.102.123 | 34.147.105.64  |\n| 34.169.45.153  | 34.90.22.166   |\n| 34.82.222.17   | 34.147.36.213  |\n| 35.227.171.135 | 34.32.137.113  |\n| 34.169.88.30   | 34.91.238.184  |\n| 34.19.93.202   | 35.204.101.241 |\n| 34.19.34.50    | 35.204.48.32   |\n| 34.59.244.194  |                |\n| 34.9.99.224    |                |\n| 34.68.27.146   |                |\n| 34.41.178.137  |                |\n| 34.123.151.210 |                |\n| 34.135.61.140  |                |\n| 34.121.166.52  |                |\n| 34.31.121.70   |                |\n\n### Payload size\n\n<Info>\n  **Only for Cloud**\n  Payload size restrictions are only applicable to [Cloud](/langsmith/cloud) deployments.\n</Info>\n\nThe maximum payload size for all requests sent to [Cloud](/langsmith/cloud) deployments is 25 MB. Attempting to send a request with a payload larger than 25 MB will result in a `413 Payload Too Large` error.\n\n### Custom PostgreSQL\n\n<Info>\n  Custom PostgreSQL instances are only available for [hybrid](/langsmith/hybrid) and [self-hosted](/langsmith/self-hosted) deployments.\n</Info>\n\nA custom PostgreSQL instance can be used instead of the [one automatically created by the control plane](/langsmith/control-plane#database-provisioning). Specify the [`POSTGRES_URI_CUSTOM`](/langsmith/env-var#postgres-uri-custom) environment variable to use a custom PostgreSQL instance.\n\nMultiple deployments can share the same PostgreSQL instance. For example, for `Deployment A`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_1>?host=<hostname_1>` and for `Deployment B`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>`. `<database_name_1>` and `database_name_2` are different databases within the same instance, but `<hostname_1>` is shared. **The same database cannot be used for separate deployments**.\n\n### Custom Redis\n\n<Info>\n  Custom Redis instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.\n</Info>\n\nA custom Redis instance can be used instead of the one automatically created by the control plane. Specify the [REDIS\\_URI\\_CUSTOM](/langsmith/env-var#redis-uri-custom) environment variable to use a custom Redis instance.\n\nMultiple deployments can share the same Redis instance. For example, for `Deployment A`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/1` and for `Deployment B`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/2`. `1` and `2` are different database numbers within the same instance, but `<hostname_1>` is shared. **The same database number cannot be used for separate deployments**.\n\n### LangSmith tracing\n\nAgent Server is automatically configured to send traces to LangSmith. See the table below for details with respect to each deployment option.\n\n| Cloud                                  | Hybrid                                                    | Self-Hosted                                                                                |\n| -------------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------ |\n| Required<br />Trace to LangSmith SaaS. | Optional<br />Disable tracing or trace to LangSmith SaaS. | Optional<br />Disable tracing, trace to LangSmith SaaS, or trace to Self-Hosted LangSmith. |\n\n### Telemetry\n\nAgent Server is automatically configured to report telemetry metadata for billing purposes. See the table below for details with respect to each deployment option.\n\n| Cloud                             | Hybrid                            | Self-Hosted                                                                                                              |\n| --------------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n| Telemetry sent to LangSmith SaaS. | Telemetry sent to LangSmith SaaS. | Self-reported usage (audit) for air-gapped license key.<br />Telemetry sent to LangSmith SaaS for LangSmith License Key. |\n\n### Licensing\n\nAgent Server is automatically configured to perform license key validation. See the table below for details with respect to each deployment option.\n\n| Cloud                                               | Hybrid                                              | Self-Hosted                                                                      |\n| --------------------------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------------- |\n| LangSmith API Key validated against LangSmith SaaS. | LangSmith API Key validated against LangSmith SaaS. | Air-gapped license key or Platform License Key validated against LangSmith SaaS. |\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-plane.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Filter traces\nLink: https://docs.langchain.comlangsmith/filter-traces-in-application\nContent: Filter based on input / output key-value pairs\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won't be indexed. This helps us ensure fast, reliable performance. To filter based on key-value pairs, select the Input Key or Output Key filter from the filters dropdown. For example, to match the following input: {\n  \"input\": \"What is the capital of France?\"\n}\n\nFilter based on input / output key-value pairs\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won't be indexed. This helps us ensure fast, reliable performance. To filter based on key-value pairs, select the Input Key or Output Key filter from the filters dropdown. For example, to match the following input: Select Filters , Add Filter to bring up the filtering options. Then select Input Key , enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output: {\n  \"documents\": [\n    {\n      \"page_content\": \"The capital of France is Paris\",\n      \"metadata\": {},\n      \"type\": \"Document\"\n    }\n  ]\n}\n\nFilter based on input / output key-value pairs\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won't be indexed. This helps us ensure fast, reliable performance. To filter based on key-value pairs, select the Input Key or Output Key filter from the filters dropdown. For example, to match the following input: Select Filters , Add Filter to bring up the filtering options. Then select Input Key , enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output: Select Output Key , enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key-value pairs as shown below:\n\n---\n\nResult 3:\nTitle: Egress for subscription metrics and operational metadata\nLink: https://docs.langchain.comlangsmith/self-host-egress\nContent: <Info>\n  This section only applies to customers who are not running in offline mode and assumes you are using a self-hosted LangSmith instance serving version 0.9.0 or later. Previous versions of LangSmith did not have this feature.\n</Info>\n\nSelf-Hosted LangSmith instances store all information locally and will never send sensitive information outside of your network. We currently only track platform usage for billing purposes according to the entitlements in your order. In order to better remotely support our customers, we do require egress to `https://beacon.langchain.com`.\n\nIn the future, we will be introducing support diagnostics to help us ensure that LangSmith is running at an optimal level within your environment.\n\n<Warning>\n  **This will require egress to `https://beacon.langchain.com` from your network. Refer to the [allowlisting IP section](/langsmith/cloud#allowlisting-ip-addresses) for static IP addresses, if needed.**\n</Warning>\n\nGenerally, data that we send to Beacon can be categorized as follows:\n\n* Subscription Metrics\n\n  * Subscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:\n\n    * Number of traces\n    * Seats allocated per contract\n    * Seats in currently use\n\n* Operational Metadata\n  * This metadata will contain and collect the above subscription metrics to assist with remote support, allowing the LangChain team to diagnose and troubleshoot performance issues more effectively and proactively.\n\n## LangSmith Telemetry\n\nAs of version ***0.11***, LangSmith deployments will by default send telemetry data back to our backend. All telemetry data is associated with an organization and deployment, but never identified with individual users. We ***do not collect PII*** (personally identifiable information) in any form.\n\n### What we use it for\n\n* To provide more proactive support and faster troubleshooting of self-hosted instances.\n* Assisting with performance tuning.\n* Understanding real-world usage to prioritize improvements.\n\n### What we collect\n\n* **Request metadata**: anonymized request counts, sizes, and durations.\n* **Database metrics**: query durations, error rates, and performance counters.\n* **Operational LangSmith traces**: traces with timing and error information for high-latency or failed requests. These are **not** customer traces, these are operational traces about the functioning of the LangSmith instance.\n\n<Info>\n  We do not collect actual payload contents, database records, or any data that can identify your end users or customers.\n</Info>\n\n### How to disable\n\nSet the following values in your `langsmith_config.yaml` file:\n\n```yaml  theme={null}\nconfig:\n  telemetry:\n    metrics: false\n    traces: false\n```\n\n## Example payloads\n\nIn an effort to maximize transparency, we provide sample payloads here:\n\n### License Verification\n\n**Endpoint:**\n\n`POST beacon.langchain.com/v1/beacon/verify`\n\n**Request:**\n\n```json  theme={null}\n{\n  \"license\": \"<YOUR_LICENSE_KEY>\"\n}\n```\n\n**Response:**\n\n```json  theme={null}\n{\n  \"token\": \"Valid JWT\" //Short-lived JWT token to avoid repeated license checks\n}\n```\n\n### Usage Reporting\n\n**Endpoint:**\n\n`POST beacon.langchain.com/v1/beacon/ingest-traces`\n\n**Request:**\n\n```json  theme={null}\n{\n  \"license\": \"<YOUR_LICENSE_KEY>\",\n  \"trace_transactions\": [\n    {\n      \"id\": \"af28dfea-5358-463d-a2dc-37df1da72498\",\n      \"tenant_id\": \"3a1c2b6f-4430-4b92-8a5b-79b8b567bbc1\",\n      \"session_id\": \"b26ae531-cdb3-42a5-8bcf-05355199fe27\",\n      \"trace_count\": 5,\n      \"start_insertion_time\": \"2025-01-06T10:00:00Z\",\n      \"end_insertion_time\": \"2025-01-06T11:00:00Z\",\n      \"start_interval_time\": \"2025-01-06T09:00:00Z\",\n      \"end_interval_time\": \"2025-01-06T10:00:00Z\",\n      \"status\": \"completed\",\n      \"num_failed_send_attempts\": 0,\n      \"transaction_type\": \"type1\",\n      \"organization_id\": \"c5b5f53a-4716-4326-8967-d4f7f7799735\"\n    }\n  ]\n}\n```\n\n**Response:**\n\n```json  theme={null}\n{\n  \"inserted_count\": 1 //Number of transactions successfully ingested\n}\n```\n\n### Telemetry: Operational LangSmith Metrics\n\n**Endpoint:**\n\n`POST beacon.langchain.com/v1/beacon/v1/metrics`\n\n**Request:**\n\n```json  theme={null}\n{\n  \"resourceMetrics\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"resource.name\",\n            \"value\": { \"stringValue\": \"langsmith-metrics\" }\n          },\n          {\n            \"key\": \"env\",\n            \"value\": { \"stringValue\": \"ls_self_hosted\" }\n          }\n        ]\n      },\n      \"scopeMetrics\": [\n        {\n          \"scope\": {\n            \"name\": \"langsmith.metrics\",\n            \"version\": \"0.1.0\"\n          },\n          \"metrics\": [\n            {\n              \"name\": \"langsmith_http_requests_latency\",\n              \"unit\": \"seconds\",\n              \"description\": \"Request latency of LangSmith services\",\n              \"gauge\": {\n                \"dataPoints\": [\n                  {\n                    \"asDouble\": 12.34,\n                    \"startTimeUnixNano\": 1678886400000000000,\n                    \"timeUnixNano\": 1678886400000000000,\n                    \"attributes\": [\n                      {\n                        \"key\": \"endpoint\",\n                        \"value\": { \"stringValue\": \"/sessions\" }\n                      },\n                      { \"key\": \"method\", \"value\": { \"stringValue\": \"GET\" } },\n                      {\n                        \"key\": \"service_name\",\n                        \"value\": { \"stringValue\": \"langsmith_backend\" }\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            {\n              \"name\": \"langsmith_http_requests_failed\",\n              \"unit\": \"1\",\n              \"description\": \"Counter of failed requests for LangSmith services\",\n              \"sum\": {\n                \"dataPoints\": [\n                  {\n                    \"asInt\": 456,\n                    \"startTimeUnixNano\": 1678886400000000000,\n                    \"timeUnixNano\": 1678886400000000000,\n                    \"attributes\": [\n                      {\n                        \"key\": \"endpoint\",\n                        \"value\": { \"stringValue\": \"/info\" }\n                      },\n                      { \"key\": \"method\", \"value\": { \"stringValue\": \"POST\" } },\n                      {\n                        \"key\": \"service_name\",\n                        \"value\": { \"stringValue\": \"langsmith_platform_backend\" }\n                      }\n                    ],\n                    \"aggregationTemporality\": 2,\n                    \"isMonotonic\": true\n                  }\n                ]\n              }\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Telemetry: Operational LangSmith Traces\n\n**Endpoint:**\n\n`POST beacon.langchain.com/v1/beacon/v1/traces`\n\n**Request:**\n\n```json  theme={null}\n{\n  \"resourceSpans\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"env\",\n            \"value\": {\n              \"stringValue\": \"ls_self_hosted\"\n            }\n          },\n          {\n            \"key\": \"service.name\",\n            \"value\": {\n              \"stringValue\": \"langsmith_backend\"\n            }\n          }\n        ]\n      },\n      \"scopeSpans\": [\n        {\n          \"scope\": {},\n          \"spans\": [\n            {\n              \"traceId\": \"71699b6fe85982c7c8995ea3d9c95df2\",\n              \"spanId\": \"3c191d03fa8be0\",\n              \"parentSpanId\": \"\",\n              \"name\": \"receive_request\",\n              \"startTimeUnixNano\": \"1581452772000000321\",\n              \"endTimeUnixNano\": \"1581452773000000789\",\n              \"droppedAttributesCount\": 1,\n              \"events\": [\n                {\n                  \"timeUnixNano\": \"1581452773000000123\",\n                  \"name\": \"parse_request\",\n                  \"attributes\": [\n                    {\n                      \"key\": \"request_size\",\n                      \"value\": {\n                        \"stringValue\": \"100\"\n                      }\n                    }\n                  ],\n                  \"droppedAttributesCount\": 2\n                },\n                {\n                  \"timeUnixNano\": \"1581452773000000123\",\n                  \"name\": \"event\",\n                  \"droppedAttributesCount\": 2\n                }\n              ],\n              \"droppedEventsCount\": 1,\n              \"status\": {\n                \"message\": \"status-cancelled\",\n                \"code\": 2\n              }\n            },\n            {\n              \"traceId\": \"71699b6fe85982c7c8995ea3d9c95df2\",\n              \"spanId\": \"0932ksdka12345\",\n              \"parentSpanId\": \"3c191d03fa8be0\",\n              \"name\": \"process_request\",\n              \"startTimeUnixNano\": \"1581452772000000321\",\n              \"endTimeUnixNano\": \"1581452773000000789\",\n              \"links\": [],\n              \"droppedLinksCount\": 3,\n              \"status\": {}\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n## Our Commitment\n\nLangChain will not store any sensitive information in the Subscription Metrics or Operational Metadata. Any data collected will not be shared with a third party. If you have any concerns about the data being sent, please reach out to your account team.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-egress.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Use webhooks\nLink: https://docs.langchain.comlangsmith/use-webhooks\nContent: Webhooks enable event-driven communication from your LangSmith application to external services. For example, you may want to issue an update to a separate service once an API call to LangSmith has finished running.\n\nMany LangSmith endpoints accept a `webhook` parameter. If this parameter is specified by an endpoint that can accept POST requests, LangSmith will send a request at the completion of a run.\n\nWhen working with LangSmith, you may want to use webhooks to receive updates after an API call completes. Webhooks are useful for triggering actions in your service once a run has finished processing. To implement this, you need to expose an endpoint that can accept `POST` requests and pass this endpoint as a `webhook` parameter in your API request.\n\nCurrently, the SDK does not provide built-in support for defining webhook endpoints, but you can specify them manually using API requests.\n\n## Supported endpoints\n\nThe following API endpoints accept a `webhook` parameter:\n\n| Operation            | HTTP Method | Endpoint                          |\n| -------------------- | ----------- | --------------------------------- |\n| Create Run           | `POST`      | `/thread/{thread_id}/runs`        |\n| Create Thread Cron   | `POST`      | `/thread/{thread_id}/runs/crons`  |\n| Stream Run           | `POST`      | `/thread/{thread_id}/runs/stream` |\n| Wait Run             | `POST`      | `/thread/{thread_id}/runs/wait`   |\n| Create Cron          | `POST`      | `/runs/crons`                     |\n| Stream Run Stateless | `POST`      | `/runs/stream`                    |\n| Wait Run Stateless   | `POST`      | `/runs/wait`                      |\n\nIn this guide, we\u2019ll show how to trigger a webhook after streaming a run.\n\n## Set up your assistant and thread\n\nBefore making API calls, set up your assistant and thread.\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    from langgraph_sdk import get_client\n\n    client = get_client(url=<DEPLOYMENT_URL>)\n    assistant_id = \"agent\"\n    thread = await client.threads.create()\n    print(thread)\n    ```\n  </Tab>\n\n  <Tab title=\"JavaScript\">\n    ```js  theme={null}\n    import { Client } from \"@langchain/langgraph-sdk\";\n\n    const client = new Client({ apiUrl: <DEPLOYMENT_URL> });\n    const assistantID = \"agent\";\n    const thread = await client.threads.create();\n    console.log(thread);\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request POST \\\n        --url <DEPLOYMENT_URL>/assistants/search \\\n        --header 'Content-Type: application/json' \\\n        --data '{ \"limit\": 10, \"offset\": 0 }' | jq -c 'map(select(.config == null or .config == {})) | .[0]' && \\\n    curl --request POST \\\n        --url <DEPLOYMENT_URL>/threads \\\n        --header 'Content-Type: application/json' \\\n        --data '{}'\n    ```\n  </Tab>\n</Tabs>\n\nExample response:\n\n```json  theme={null}\n{\n    \"thread_id\": \"9dde5490-2b67-47c8-aa14-4bfec88af217\",\n    \"created_at\": \"2024-08-30T23:07:38.242730+00:00\",\n    \"updated_at\": \"2024-08-30T23:07:38.242730+00:00\",\n    \"metadata\": {},\n    \"status\": \"idle\",\n    \"config\": {},\n    \"values\": null\n}\n```\n\n## Use a webhook with a graph run\n\nTo use a webhook, specify the `webhook` parameter in your API request. When the run completes, LangSmith sends a `POST` request to the specified webhook URL.\n\nFor example, if your server listens for webhook events at `https://my-server.app/my-webhook-endpoint`, include this in your request:\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    input = { \"messages\": [{ \"role\": \"user\", \"content\": \"Hello!\" }] }\n\n    async for chunk in client.runs.stream(\n        thread_id=thread[\"thread_id\"],\n        assistant_id=assistant_id,\n        input=input,\n        stream_mode=\"events\",\n        webhook=\"https://my-server.app/my-webhook-endpoint\"\n    ):\n        pass\n    ```\n  </Tab>\n\n  <Tab title=\"JavaScript\">\n    ```js  theme={null}\n    const input = { messages: [{ role: \"human\", content: \"Hello!\" }] };\n\n    const streamResponse = client.runs.stream(\n      thread[\"thread_id\"],\n      assistantID,\n      {\n        input: input,\n        webhook: \"https://my-server.app/my-webhook-endpoint\"\n      }\n    );\n\n    for await (const chunk of streamResponse) {\n      // Handle stream output\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request POST \\\n        --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n        --header 'Content-Type: application/json' \\\n        --data '{\n            \"assistant_id\": <ASSISTANT_ID>,\n            \"input\": {\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]},\n            \"webhook\": \"https://my-server.app/my-webhook-endpoint\"\n        }'\n    ```\n  </Tab>\n</Tabs>\n\n## Webhook payload\n\nLangSmith sends webhook notifications in the format of a [Run](/langsmith/assistants#execution). See the [API Reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/assistants) for details. The request payload includes run input, configuration, and other metadata in the `kwargs` field.\n\n## Secure webhooks\n\nTo ensure only authorized requests hit your webhook endpoint, consider adding a security token as a query parameter:\n\n```\nhttps://my-server.app/my-webhook-endpoint?token=YOUR_SECRET_TOKEN\n```\n\nYour server should extract and validate this token before processing requests.\n\n## Disable webhooks\n\nAs of `langgraph-api>=0.2.78`, developers can disable webhooks in the `langgraph.json` file:\n\n```json  theme={null}\n{\n  \"http\": {\n    \"disable_webhooks\": true\n  }\n}\n```\n\nThis feature is primarily intended for self-hosted deployments, where platform administrators or developers may prefer to disable webhooks to simplify their security posture\u2014especially if they are not configuring firewall rules or other network controls. Disabling webhooks helps prevent untrusted payloads from being sent to internal endpoints.\n\nFor full configuration details, refer to the [configuration file reference](/langsmith/cli?h=disable_webhooks#configuration-file).\n\n## Test webhooks\n\nYou can test your webhook using online services like:\n\n* **[Beeceptor](https://beeceptor.com/)** \u2013 Quickly create a test endpoint and inspect incoming webhook payloads.\n* **[Webhook.site](https://webhook.site/)** \u2013 View, debug, and log incoming webhook requests in real time.\n\nThese tools help you verify that LangSmith is correctly triggering and sending webhooks to your service.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-webhooks.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Configure webhook notifications for rules\nLink: https://docs.langchain.comlangsmith/webhooks\nContent: When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=da310e976aa8824071d65b8fb44b9123\" alt=\"Webhook\" data-og-width=\"872\" width=\"872\" data-og-height=\"991\" height=\"991\" data-path=\"langsmith/images/webhook.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6f30f7cd2de82b0ccb826d257b933f12 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0fce81ff2661e8944ebfb781a07017fe 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5799cf7458a15ac99579ba273d0b875e 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aa958b944f848f3a64ce068a64bc8433 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a3e94463ac9d8fa498accb27124785ab 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a5db18db5cf7097fdba6d6b7162ba6d 2500w\" />\n\n## Webhook payload\n\nThe payload we send to your webhook endpoint contains:\n\n* `\"rule_id\"`: this is the ID of the automation that sent this payload\n* `\"start_time\"` and `\"end_time\"`: these are the time boundaries where we found matching runs\n* `\"runs\"`: this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.\n* `\"feedback_stats\"`: this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below.\n\n```json  theme={null}\n\"feedback_stats\": {\n    \"about_langchain\": {\n        \"n\": 1,\n        \"avg\": 0.0,\n        \"show_feedback_arrow\": true,\n        \"values\": {}\n    },\n    \"category\": {\n        \"n\": 0,\n        \"avg\": null,\n        \"show_feedback_arrow\": true,\n        \"values\": {\n            \"CONCEPTUAL\": 1\n        }\n    },\n    \"user_score\": {\n        \"n\": 2,\n        \"avg\": 0.0,\n        \"show_feedback_arrow\": false,\n        \"values\": {}\n    },\n    \"vagueness\": {\n        \"n\": 1,\n        \"avg\": 0.0,\n        \"show_feedback_arrow\": true,\n        \"values\": {}\n    }\n}\n```\n\n<Note>\n  **fetching from S3 URLs**\n\n  Depending on how recent your runs are, the `inputs_s3_urls` and `outputs_s3_urls` fields may contain S3 URLs to the actual data instead of the data itself.\n\n  The `inputs` and `outputs` can be fetched by the `ROOT.presigned_url` provided in `inputs_s3_urls` and `outputs_s3_urls` respectively.\n</Note>\n\nThis is an example of the entire payload we send to your webhook endpoint:\n\n```json  theme={null}\n{\n  \"rule_id\": \"d75d7417-0c57-4655-88fe-1db3cda3a47a\",\n  \"start_time\": \"2024-04-05T01:28:54.734491+00:00\",\n  \"end_time\": \"2024-04-05T01:28:56.492563+00:00\",\n  \"runs\": [\n    {\n      \"status\": \"success\",\n      \"is_root\": true,\n      \"trace_id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",\n      \"dotted_order\": \"20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630\",\n      \"run_type\": \"tool\",\n      \"modified_at\": \"2024-04-05T01:28:54.145062\",\n      \"tenant_id\": \"2ebda79f-2946-4491-a9ad-d642f49e0815\",\n      \"end_time\": \"2024-04-05T01:28:54.085649\",\n      \"name\": \"Search\",\n      \"start_time\": \"2024-04-05T01:28:54.085646\",\n      \"id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",\n      \"session_id\": \"6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5\",\n      \"parent_run_ids\": [],\n      \"child_run_ids\": null,\n      \"direct_child_run_ids\": null,\n      \"total_tokens\": 0,\n      \"completion_tokens\": 0,\n      \"prompt_tokens\": 0,\n      \"total_cost\": null,\n      \"completion_cost\": null,\n      \"prompt_cost\": null,\n      \"first_token_time\": null,\n      \"app_path\": \"/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809\",\n      \"in_dataset\": false,\n      \"last_queued_at\": null,\n      \"inputs\": null,\n      \"inputs_s3_urls\": null,\n      \"outputs\": null,\n      \"outputs_s3_urls\": null,\n      \"extra\": null,\n      \"events\": null,\n      \"feedback_stats\": null,\n      \"serialized\": null,\n      \"share_token\": null\n    }\n  ]\n}\n```\n\n## Security\n\nWe strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.\n\nAn example would be\n\n```\nhttps://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d\n```\n\n### Webhook custom HTTP headers\n\nIf you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the `Headers` option next to the URL field and add your headers.\n\n<Note>\n  Headers are stored in encrypted format.\n</Note>\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8d6fde711d74784b803c13aba4b38837\" alt=\"Webhook headers\" data-og-width=\"848\" width=\"848\" data-og-height=\"1004\" height=\"1004\" data-path=\"langsmith/images/webhook-headers.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1d6c9f67920f0de5bc4b440593b87116 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a62d07be3f38e9c659faadb091f8a23e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5a6c128eb91f899f9213fd0f8999a11f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a9dd4e7f434f2f2806df60232f34ac3 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=315d0e900bd0c65dd5264869bd545351 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=31ce3f7628c5c3ba3916b77d906ff0c5 2500w\" />\n\n### Webhook Delivery\n\nWhen delivering events to your webhook endpoint we follow these guidelines\n\n* If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.\n* If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .\n* If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.\n* If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.\n* Anything your endpoint returns in the body will be ignored\n\n## Example with Modal\n\n### Setup\n\nFor an example of how to set this up, we will use [Modal](https://modal.com/). Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here.\n\nFirst, create a Modal account. Then, locally install the Modal SDK:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install modal\n  ```\n\n  ```bash uv theme={null}\n  uv add modal\n  ```\n</CodeGroup>\n\nTo finish setting up your account, run the command:\n\n```shell  theme={null}\nmodal setup\n```\n\nand follow the instructions\n\n### Secrets\n\nNext, you will need to set up some secrets in Modal.\n\nFirst, LangSmith will need to authenticate to Modal by passing in a secret.\nThe easiest way to do this is to pass in a secret in the query parameters.\nTo validate this secret, we will need to add a secret in *Modal* to validate it.\nWe will do that by creating a Modal secret.\nYou can see instructions for secrets [here](https://modal.com/docs/guide/secrets).\nFor this purpose, let's call our secret `ls-webhook` and have it set an environment variable with the name `LS_WEBHOOK`.\n\nWe can also set up a LangSmith secret - luckily there is already an integration template for this!\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0c3209b59cb36273d82fb44383efa1d5\" alt=\"LangSmith Modal Template\" data-og-width=\"1229\" width=\"1229\" data-og-height=\"779\" height=\"779\" data-path=\"langsmith/images/modal-langsmith-secret.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c2ff70b647c04bb6a45a08de537b4d22 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=02181b882935f45339d31f48adeed1e9 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=52119f475cca739369cebb71bfefafae 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=aa8a1fc73b2e0b7f27c3186732e3bde9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=91d36950d22b86f7ad790a61957cbad7 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=efcdfe7d54ca30079b147e00a0d9e934 2500w\" />\n\n### Service\n\nAfter that, you can create a Python file that will serve as your endpoint.\nAn example is below, with comments explaining what is going on:\n\n```python  theme={null}\nfrom fastapi import HTTPException, status, Request, Query\nfrom modal import Secret, Stub, web_endpoint, Image\n\nstub = Stub(\"auth-example\", image=Image.debian_slim().pip_install(\"langsmith\"))\n\n\n@stub.function(\n    secrets=[Secret.from_name(\"ls-webhook\"), Secret.from_name(\"my-langsmith-secret\")]\n)\n# We want this to be a `POST` endpoint since we will post data here\n@web_endpoint(method=\"POST\")\n# We set up a `secret` query parameter\ndef f(data: dict, secret: str = Query(...)):\n    # You can import dependencies you don't have locally inside Modal functions\n    from langsmith import Client\n\n    # First, we validate the secret key we pass\n    import os\n\n    if secret != os.environ[\"LS_WEBHOOK\"]:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect bearer token\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n\n    # This is where we put the logic for what should happen inside this webhook\n    ls_client = Client()\n    runs = data[\"runs\"]\n    ids = [r[\"id\"] for r in runs]\n    feedback = list(ls_client.list_feedback(run_ids=ids))\n    for r, f in zip(runs, feedback):\n        try:\n            ls_client.create_example(\n                inputs=r[\"inputs\"],\n                outputs={\"output\": f.correction},\n                dataset_name=\"classifier-github-issues\",\n            )\n        except Exception:\n            raise ValueError(f\"{r} and {f}\")\n    # Function body\n    return \"success!\"\n```\n\nWe can now deploy this easily with `modal deploy ...` (see docs [here](https://modal.com/docs/guide/managing-deployments)).\n\nYou should now get something like:\n\n```\n\u2713 Created objects.\n\u251c\u2500\u2500 \ud83d\udd28 Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py\n\u251c\u2500\u2500 \ud83d\udd28 Created mount PythonPackage:langsmith\n\u2514\u2500\u2500 \ud83d\udd28 Created f => https://hwchase17--auth-example-f.modal.run\n\u2713 App deployed! \ud83c\udf89\n\nView Deployment: https://modal.com/apps/hwchase17/auth-example\n```\n\nThe important thing to remember is `https://hwchase17--auth-example-f.modal.run` - the function we created to run.\nNOTE: this is NOT the final deployment URL, make sure not to accidentally use that.\n\n### Hooking it up\n\nWe can now take the function URL we create above and add it as a webhook.\nWe have to remember to also pass in the secret key as a query parameter.\nPutting it all together, it should look something like:\n\n```\nhttps://hwchase17--auth-example-f-dev.modal.run?secret={SECRET}\n```\n\nReplace `{SECRET}` with the secret key you created to access the Modal service.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/webhooks.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_41790433",
    "id": "5b92ebd6-5d80-44bf-80ba-de78e577156c",
    "content": "Result 1:\nTitle: Cloud\nLink: https://docs.langchain.comlangsmith/cloud\nContent: <Callout icon=\"rocket\" color=\"#4F46E5\" iconType=\"regular\">\n  If you're ready to deploy your app to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or the [full setup guide](/langsmith/deploy-to-cloud). This page explains the Cloud managed architecture for reference.\n</Callout>\n\nThe **Cloud** option is a fully managed model where LangChain hosts and operates all LangSmith infrastructure and services:\n\n* **Fully managed infrastructure**: LangChain handles all infrastructure, updates, scaling, and maintenance.\n* **Deploy from GitHub**: Connect your repositories and deploy with a few clicks.\n* **Automated CI/CD**: Build process is handled automatically by the platform.\n* **LangSmith UI**: Full access to [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), [deployment management](/langsmith/deployments), and [Studio](/langsmith/studio).\n\n|                                               | **Who manages it** | **Where it runs** |\n| --------------------------------------------- | ------------------ | ----------------- |\n| **LangSmith platform (UI, APIs, datastores)** | LangChain          | LangChain's cloud |\n| **Your Agent Servers**                        | LangChain          | LangChain's cloud |\n| **CI/CD for your apps**                       | LangChain          | LangChain's cloud |\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3f0316122425895270d0ecd47b12e139\" alt=\"Cloud deployment: LangChain hosts and manages all components including the UI, APIs, and your Agent Servers.\" data-og-width=\"1425\" width=\"1425\" data-og-height=\"1063\" height=\"1063\" data-path=\"langsmith/images/langgraph-cloud-architecture.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b34a10fb40ca5188dddfb6be69696c75 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d310ce86e8421878575cf4d4fa72bf78 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=7cc3fcee9c36a8d1e8da4cc4abbde3b9 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a184c950c8585e2fdb5e75ac7f0f5642 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=cb778e4a6707b00eaf703886d32569bd 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=00f9d9df44512dded307613502d03299 2500w\" />\n\n## Get started\n\nTo deploy your first application to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or refer to the [comprehensive setup guide](/langsmith/deploy-to-cloud).\n\n## Cloud architecture and scalability\n\n<Note>\n  This section is only relevant for the cloud-managed LangSmith services available at [https://smith.langchain.com](https://smith.langchain.com) and [https://eu.smith.langchain.com](https://eu.smith.langchain.com).\n\n  For information on the self-hosted LangSmith solution, please refer to the [self-hosted documentation](/langsmith/self-hosted).\n</Note>\n\nLangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for LLM application observability, evaluation, and agent deployment\n\n### Architecture\n\nThe US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.\n\n<Note>\n  The [EU-based LangSmith service](https://eu.smith.langchain.com) is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, [contact our sales team](https://www.langchain.com/contact-sales).\n</Note>\n\n#### Regional storage\n\nThe resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses [Supabase](https://supabase.com) for authentication/authorization and [ClickHouse Cloud](https://clickhouse.com/cloud) for data warehouse.\n\n|                                                | US                                                                 | EU                                                                       |\n| ---------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------ |\n| URL                                            | [https://smith.langchain.com](https://smith.langchain.com)         | [https://eu.smith.langchain.com](https://eu.smith.langchain.com)         |\n| API URL                                        | [https://api.smith.langchain.com](https://api.smith.langchain.com) | [https://eu.api.smith.langchain.com](https://eu.api.smith.langchain.com) |\n| GCP                                            | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| Supabase                                       | AWS us-east-1 (N. Virginia)                                        | AWS eu-central-1 (Germany)                                               |\n| ClickHouse Cloud                               | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| [LangSmith deployment](/langsmith/deployments) | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n\nSee the [Regions FAQ](/langsmith/regions-faq) for more information.\n\n#### Region-independent storage\n\nData listed here is stored exclusively in the US:\n\n* Payment and billing information with Stripe and Metronome\n\n#### GCP services\n\nLangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):\n\n* LangSmith Frontend: serves the LangSmith UI.\n* LangSmith Backend: serves the LangSmith API.\n* LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)\n* LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.\n* LangSmith Queue: handles processing of asynchronous tasks. (Internal service)\n\nLangSmith uses the following GCP storage services:\n\n* Google Cloud Storage (GCS) for runs inputs and outputs.\n* Google Cloud SQL PostgreSQL for transactional workloads.\n* Google Cloud Memorystore for Redis for queuing and caching.\n* Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.\n\nSome additional GCP services we use include:\n\n* Google Cloud Load Balancer for routing traffic to the LangSmith services.\n* Google Cloud CDN for caching static assets.\n* Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to [this guide](/langsmith/administration-overview#rate-limits).\n\n<div style={{ textAlign: 'center' }}>\n  <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3\" alt=\"Light mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-light.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w\" />\n\n  <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b\" alt=\"Dark mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w\" />\n</div>\n\n### Allowlisting IP addresses\n\n#### Egress from LangChain SaaS\n\nAll traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:\n\n| US             | EU             |\n| -------------- | -------------- |\n| 34.59.65.97    | 34.13.192.67   |\n| 34.67.51.221   | 34.147.105.64  |\n| 34.46.212.37   | 34.90.22.166   |\n| 34.132.150.88  | 34.147.36.213  |\n| 35.188.222.201 | 34.32.137.113  |\n| 34.58.194.127  | 34.91.238.184  |\n| 34.59.97.173   | 35.204.101.241 |\n| 104.198.162.55 | 35.204.48.32   |\n\nIt may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.\n\n#### Ingress into LangChain SaaS\n\nThe langchain endpoints map to the following static IP addresses:\n\n| US             | EU           |\n| -------------- | ------------ |\n| 34.8.121.39    | 34.95.92.214 |\n| 34.107.251.234 | 34.13.73.122 |\n\nYou may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (`api.smith.langchain.com`, `smith.langchain.com`, `beacon.langchain.com`, `eu.api.smith.langchain.com`, `eu.smith.langchain.com`, `eu.beacon.langchain.com`).\n\n### API rate limits\n\nLangSmith enforces rate limits on API endpoints to ensure service stability and fair usage. The following table shows the rate limits for different endpoints in both US and EU regions. Note that:\n\n* Rate limits are expressed as `count / interval` where count is the number of requests allowed within the interval (in seconds). For example, `2000 / 10` means 2000 requests per 10 seconds.\n* When no HTTP method is specified in the endpoint column, the rate limit applies to all HTTP methods for that endpoint.\n* When a specific method is listed (e.g., `POST`, `GET`), the rate limit applies only to that method.\n\n| Match / Endpoint (method)                   | Identity key     | US prod limit | EU prod limit | Category                                     |\n| ------------------------------------------- | ---------------- | ------------- | ------------- | -------------------------------------------- |\n| OPTIONS, `/info`, `*/v1/metadata/submit`    | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-user-id` + IP | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/v1/beacon`                                | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/repos`                                    | `x-api-key`      | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `/repos`                                    | `x-user-id` + IP | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `POST /runs/batch`                          | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `POST /otel/v1/traces`                      | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST` containing `/charts`                 | `x-api-key`      | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST` containing `/charts`                 | `x-user-id` + IP | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST /runs/multipart`                      | `x-api-key`      | 6000 / 10     | 6000 / 10     | [Multipart ingest](#rate-limit-categories)   |\n| `POST /runs/query`                          | `x-api-key`      | 15 / 10       | 15 / 10       | [Run query (API)](#rate-limit-categories)    |\n| `POST /runs/query`                          | `x-user-id` + IP | 300 / 10      | 300 / 10      | [Run query (User)](#rate-limit-categories)   |\n| `/generate`                                 | `x-api-key`      | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/generate`                                 | `x-user-id` + IP | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/commits`                                  | `x-api-key`      | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `/commits`                                  | `x-user-id` + IP | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `DELETE /sessions` or `*/trigger`           | `x-api-key`      | 10 / 60       | 10 / 60       | [Deletion](#rate-limit-categories)           |\n| `DELETE /sessions` or `*/trigger`           | `x-user-id` + IP | 30 / 60       | 30 / 60       | [Deletion](#rate-limit-categories)           |\n| `POST /runs` (single run ingest)            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `PATCH` containing `/runs`                  | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST /feedback`                            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `GET /runs/{uuid}` or `/api/v1/runs/{uuid}` | `x-api-key`      | 30 / 60       | 30 / 60       | [Run lookup](#rate-limit-categories)         |\n| `GET` containing `/examples`                | `x-api-key`      | 5000 / 60     | 5000 / 60     | [Examples](#rate-limit-categories)           |\n| Any request with `x-api-key`                | `x-api-key`      | 1000 / 10     | 1000 / 10     | [Default (API key)](#rate-limit-categories)  |\n| Any request with `x-user-id`                | `x-user-id` + IP | 1000 / 10     | 1000 / 10     | [Default (User)](#rate-limit-categories)     |\n| `/public/download`                          | IP               | 5000 / 60     | 5000 / 60     | [Public download](#rate-limit-categories)    |\n| `/runs/stats`                               | `x-api-key`      | 1 / 10        | 20 / 10       | [Stats](#rate-limit-categories)              |\n| All other IPs (catch-all)                   | IP               | 100 / 60      | 100 / 60      | [Public (catch-all)](#rate-limit-categories) |\n\n#### Rate limit categories\n\n* **High throughput**: General high-volume endpoints for core operations like authentication, metadata, and feedback.\n* **Repository**: Repository and prompt management operations.\n* **Run ingest**: Individual trace/run ingestion endpoints for observability.\n* **Charts**: Chart generation and visualization endpoints.\n* **Multipart ingest**: Bulk run ingestion via multipart upload for high-volume tracing.\n* **Run query (API)**: API key-based run query operations with stricter limits for complex queries.\n* **Run query (User)**: User-based run query operations with higher limits for interactive use.\n* **Generation**: AI-powered code and content generation endpoints (limited to prevent abuse).\n* **Commits**: Prompt versioning and commit operations.\n* **Deletion**: Session deletion and workflow trigger operations.\n* **Run lookup**: Retrieving specific runs by UUID.\n* **Examples**: Fetching dataset examples for few-shot prompting.\n* **Default (API key)**: Fallback rate limit for authenticated API requests not matching specific patterns.\n* **Default (User)**: Fallback rate limit for authenticated user requests not matching specific patterns.\n* **Public download**: High-volume public download endpoints for shared resources.\n* **Stats**: Run statistics and analytics endpoints (region-specific limits apply).\n* **Public (catch-all)**: Default rate limit for unauthenticated public access.\n\nFor more information on rate limits and other service limits, refer to the [Administration overview](/langsmith/administration-overview#rate-limits).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cloud.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: How to handle model rate limits\nLink: https://docs.langchain.comlangsmith/rate-limiting\nContent: A common issue when running large evaluation jobs is running into third-party API rate limits, usually from model providers. There are a few ways to deal with rate limits.\n\n## Using `langchain` RateLimiters (Python only)\n\nIf you're using `langchain` Python chat models in your application or evaluators, you can add rate limiters to your model(s) that will add client-side control of the frequency with which requests are sent to the model provider API to avoid rate limit errors.\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.rate_limiters import InMemoryRateLimiter\n\nrate_limiter = InMemoryRateLimiter(\n    requests_per_second=0.1,  # <-- Super slow! We can only make a request once every 10 seconds!!\n    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n    max_bucket_size=10,  # Controls the maximum burst size.\n)\n\nmodel = init_chat_model(\"gpt-4o\", rate_limiter=rate_limiter)\n\ndef app(inputs: dict) -> dict:\n    response = model.invoke(...)\n    ...\n\ndef evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n    response = model.invoke(...)\n    ...\n```\n\nSee the [`langchain`](/oss/python/langchain/models#rate-limiting) documentation for more on how to configure rate limiters.\n\n## Retrying with exponential backoff\n\nA very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.\n\n#### With `langchain`\n\nIf you're using `langchain` components you can add retries to all model calls with the `.with_retry(...)` / `.withRetry()` method:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain import init_chat_model\n\n  model_with_retry = init_chat_model(\"gpt-4o-mini\").with_retry(stop_after_attempt=6)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { initChatModel } from \"langchain\";\n\n  const model = await initChatModel(\"gpt-4o\", {\n      modelProvider: \"openai\",\n  });\n\n  const modelWithRetry = model.withRetry({ stopAfterAttept: 2 });\n  ```\n</CodeGroup>\n\nSee the `langchain` [Python](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.BaseChatModel.with_retry) and [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API references for more.\n\n#### Without `langchain`\n\nIf you're not using `langchain` you can use other libraries like `tenacity` (Python) or `backoff` (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the [OpenAI docs](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff).\n\n## Limiting `max_concurrency`\n\nLimiting the number of concurrent calls you're making to your application and evaluators is another way to decrease the frequency of model calls you're making, and in that way avoid rate limit errors. `max_concurrency` can be set directly on the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) functions. This parallelizes evaluation by effectively splitting the dataset across threads.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import aevaluate\n\n  results = await aevaluate(\n      ...\n      max_concurrency=4,\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { evaluate } from \"langsmith/evaluation\";\n\n  await evaluate(..., {\n    ...,\n    maxConcurrency: 4,\n  });\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rate-limiting.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Built-in middleware\nLink: https://docs.langchain.comoss/javascript/langchain/middleware/built-in\nContent: Tool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. Watch this video guide demonstrating Tool Call Limit middleware behavior. toolName Name of specific tool to limit. If not provided, limits apply to all tools globally. threadLimit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. undefined means no thread limit. runLimit Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. undefined means no run limit. Note: At least one of threadLimit or runLimit must be specified. exitBehavior {\"body\":\"exitBehavior\",\"type\":\"string\",\"default\":\"continue\"}\n\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. Watch this video guide demonstrating Tool Call Limit middleware behavior. toolName Name of specific tool to limit. If not provided, limits apply to all tools globally. threadLimit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. undefined means no thread limit. runLimit {\"body\":\"runLimit\",\"type\":\"number\"}\n\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. Watch this video guide demonstrating Tool Call Limit middleware behavior. toolName Name of specific tool to limit. If not provided, limits apply to all tools globally. threadLimit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. undefined means no thread limit. runLimit Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. undefined means no run limit. Note: At least one of threadLimit or runLimit must be specified. exitBehavior Behavior when limit is reached: 'continue' (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages. 'error' - Throw a ToolCallLimitExceededError exception, stopping execution immediately 'end' - Stop execution immediately with a ToolMessage and AI message for the exceeded tool call. Only works when limiting a single tool; throws error if other tools have pending calls. Specify limits with: Thread limit - Max calls across all runs in a conversation (requires checkpointer) Run limit - Max calls per single invocation (resets each turn) Exit behaviors: 'continue' (default) - Block exceeded calls with error messages, agent continues 'error' - Raise exception immediately 'end' - Stop with ToolMessage + AI message (single-tool scenarios only)\n\n---\n\nResult 4:\nTitle: Graph API overview\nLink: https://docs.langchain.comoss/javascript/langgraph/graph-api\nContent: Recursion limit\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError . By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/ stream via the config object. Importantly, recursionLimit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below: await graph.invoke(inputs, {\n  recursionLimit: 5,\n  context: { llm: \"anthropic\" },\n});\n\nRecursion limit\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError . By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/ stream via the config object. Importantly, recursionLimit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\n\nProactive vs reactive approaches\nThere are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally). The reactive approach catches GraphRecursionError after the limit is exceeded. Design your graph with explicit termination conditions to avoid hitting the limit in the first place. Approach Detection Handling Control Flow Reactive (catching GraphRecursionError) After limit exceeded Outside graph in try/catch Graph execution terminated Reactive advantages: Simple implementation No need to modify graph logic Centralized error handling\n\n---\n\nResult 5:\nTitle: Built-in middleware\nLink: https://docs.langchain.comoss/python/langchain/middleware/built-in\nContent: Model call limit\nLimit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following: Preventing runaway agents from making too many API calls. Enforcing cost controls on production deployments. Testing agent behavior within specific call budgets. API reference: ModelCallLimitMiddleware Watch this video guide demonstrating Model Call Limit middleware behavior. thread_limit Maximum model calls across all runs in a thread. Defaults to no limit. run_limit Maximum model calls per single invocation. Defaults to no limit. exit_behavior {\"body\":\"exit_behavior\",\"type\":\"string\",\"default\":\"end\"}\n\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. API reference: ToolCallLimitMiddleware Watch this video guide demonstrating Tool Call Limit middleware behavior. tool_name Name of specific tool to limit. If not provided, limits apply to all tools globally. thread_limit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. None means no thread limit. run_limit Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. None means no run limit. Note: At least one of thread_limit or run_limit must be specified. exit_behavior {\"body\":\"exit_behavior\",\"type\":\"string\",\"default\":\"continue\"}\n\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. API reference: ToolCallLimitMiddleware Watch this video guide demonstrating Tool Call Limit middleware behavior. tool_name Name of specific tool to limit. If not provided, limits apply to all tools globally. thread_limit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. None means no thread limit. run_limit {\"body\":\"run_limit\",\"type\":\"number\"}\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_31055125",
    "id": "9e0d6e13-b52e-4008-8e13-7dfe77d1519f",
    "content": "Result 1:\nTitle: Connect to an OpenAI compliant model provider/proxy\nLink: https://docs.langchain.comlangsmith/custom-openai-compliant-model\nContent: The LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for  in the playground.\n\n## Deploy an OpenAI compliant model\n\nMany providers offer OpenAI compliant models or proxy services. Some examples of this include:\n\n* [LiteLLM Proxy](https://github.com/BerriAI/litellm?tab=readme-ov-file#quick-start-proxy---cli)\n* [Ollama](https://ollama.com/)\n\nYou can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API.\n\nTake a look at the full [specification](https://platform.openai.com/docs/api-reference/chat) for more information.\n\n## Use the model in the LangSmith Playground\n\nOnce you have deployed a model server, you can use it in the LangSmith [Playground](/langsmith/prompt-engineering-concepts#prompt-playground).\n\nTo access the **Prompt Settings** menu:\n\n1. Under the **Prompts** heading select the gear <Icon icon=\"gear\" iconType=\"solid\" /> icon next to the model name.\n2. In the **Model Configuration** tab, select the model to edit in the dropdown.\n3. For the **Provider** dropdown, select **OpenAI Compatible Endpoint**.\n4. Add your OpenAI Compatible Endpoint to the **Base URL** input.\n\n   <div style={{ textAlign: 'center' }}>\n     <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=fdbe548e512ed40fb512578d02986b45\" alt=\"Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected.\" data-og-width=\"897\" width=\"897\" data-og-height=\"572\" height=\"572\" data-path=\"langsmith/images/openai-compatible-endpoint.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=75921e7b30edbac5263ee10178977383 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=468265c2ad0a6c1740eb18590dab27a5 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=9f9c3f68df77205264eb9221373790f2 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=ccb9820653e1e57b529bc46ac7d20e40 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c320b3f4051643b71fba4faa350daf9b 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=e9d1be7c69021b7fa575a2a466dbfe58 2500w\" />\n\n     <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=97459563da21d17228a1bb94a1b9edf3\" alt=\"Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected.\" data-og-width=\"896\" width=\"896\" data-og-height=\"552\" height=\"552\" data-path=\"langsmith/images/openai-compatible-endpoint-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c3e8e46813ec673fbc3ac4e4748a4ab6 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=381a567b022c71ed6f74abbb7e3cecbd 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=7df617e39d5bd098f4e80d523ef85778 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=06408cd348f56fcc409af8273c799a97 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=6b79b55e2868ed11e9d2c2eb396b004f 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=4e0439d889aa2175c0802e9bf5db399b 2500w\" />\n   </div>\n\nIf everything is set up correctly, you should see the model's response in the playground. You can also use this functionality to invoke downstream pipelines as well.\n\nFor information on how to store your model configuration , refer to [Configure prompt settings](/langsmith/managing-model-configurations).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-openai-compliant-model.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Evaluation quickstart\nLink: https://docs.langchain.comlangsmith/evaluation-quickstart\nContent: Prerequisites\nBefore you begin, make sure you have: A LangSmith account : Sign up or log in at smith.langchain.com. A LangSmith API key : Follow the Create an API key guide. An OpenAI API key : Generate this from the OpenAI dashboard. Select the UI or SDK filter for instructions:\n\n1. Set workspace secrets\nIn the LangSmith UI , ensure that your OpenAI API key is set as a workspace secret. Navigate to Settings and then move to the Secrets tab. Select Add secret and enter the OPENAI_API_KEY and your API key as the Value. Select Save secret. When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.\n\n6. Run and view results\nTo run the evaluation experiment, you'll call evaluate(...) , which: Pulls example from the dataset you created in Step 3. Sends each example's inputs to your target function from Step 4. Collects the outputs (the model's answers). Passes the outputs along with the reference_outputs to your evaluator from Step 5. Records all results in LangSmith as an experiment, so you can view them in the UI. Add the highlighted code to your eval file: from langsmith import Client, wrappers\nfrom openai import OpenAI\nfrom openevals.llm import create_llm_as_judge\nfrom openevals.prompts import CORRECTNESS_PROMPT\n\n# Wrap the OpenAI client for LangSmith tracing\nopenai_client = wrappers.wrap_openai(OpenAI())\n\n# Define the application logic you want to evaluate inside a target function\n# The SDK will automatically send the inputs from the dataset to your target function\ndef target(inputs: dict) -> dict:\n    response = openai_client.chat.completions.create(\n        model=\"gpt-5-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},\n            {\"role\": \"user\", \"content\": inputs[\"question\"]},\n        ],\n    )\n    return {\"answer\": response.choices[0].message.content.strip()}\n\ndef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n    evaluator = create_llm_as_judge(\n        prompt=CORRECTNESS_PROMPT,\n        model=\"openai:o3-mini\",\n        feedback_key=\"correctness\",\n    )\n    return evaluator(\n        inputs=inputs,\n        outputs=outputs,\n        reference_outputs=reference_outputs\n    )\n\n# After running the evaluation, a link will be provided to view the results in langsmith\ndef main():\n    client = Client()\n    experiment_results = client.evaluate(\n        target,\n        data=\"Sample dataset\",\n        evaluators=[\n            correctness_evaluator,\n            # can add multiple evaluators here\n        ],\n        experiment_prefix=\"first-eval-in-langsmith\",\n        max_concurrency=2,\n    )\n    print(experiment_results)\n\nif __name__ == \"__main__\":\n    main()\n\n---\n\nResult 3:\nTitle: Metadata parameters reference\nLink: https://docs.langchain.comlangsmith/ls-metadata-parameters\nContent: When you trace LLM calls with LangSmith, you often want to [track costs](/langsmith/cost-tracking), compare model configurations, and analyze performance across different providers. LangSmith's native integrations (like [LangChain](/langsmith/trace-with-langchain) or the [OpenAI](/langsmith/trace-openai)/[Anthropic](/langsmith/trace-anthropic) wrappers) handle this automatically, but custom model wrappers and self-hosted models require a standardized way to provide this information. LangSmith uses `ls_` metadata parameters for this purpose.\n\nThese metadata parameters (all prefixed with `ls_`) let you pass model configuration and identification information through the standard `metadata` field. Once set, LangSmith can automatically calculate costs, display model information in the UI, and enable filtering and analytics across your traces.\n\nUse `ls_` metadata parameters to:\n\n* **Enable automatic cost tracking** for custom or self-hosted models by identifying the provider and model name.\n* **Track model configuration** like temperature, max tokens, and other parameters for experiment comparison.\n* **Filter and analyze traces** by provider or configuration settings\n* **Improve debugging** by recording exactly which model settings were used for each run.\n\n## Basic usage example\n\nThe most common use case is enabling cost tracking for custom model wrappers. To do this, you need to provide two key pieces of information: the provider name (`ls_provider`) and the model name (`ls_model_name`). These work together to match against LangSmith's pricing database.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import traceable\n\n  @traceable(\n      run_type=\"llm\",\n      metadata={\n          \"ls_provider\": \"my_provider\",\n          \"ls_model_name\": \"my_custom_model\"\n      }\n  )\n  def my_custom_llm(prompt: str):\n      return call_custom_api(prompt)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { traceable } from \"langsmith/traceable\";\n\n  const myCustomLlm = traceable(\n    async (prompt: string) => {\n      return callCustomApi(prompt);\n    },\n    {\n      run_type: \"llm\",\n      metadata: {\n        ls_provider: \"my_provider\",\n        ls_model_name: \"my_custom_model\"\n      }\n    }\n  );\n  ```\n</CodeGroup>\n\nThis minimal setup tells LangSmith what model you're using, enabling automatic cost calculation if the model exists in the pricing database or if you've [configured custom pricing](/langsmith/cost-tracking#set-up-model-pricing).\n\nFor more comprehensive tracking, you can include additional configuration parameters. This is especially useful when [running experiments](/langsmith/evaluation-quickstart) or comparing different model settings:\n\n<CodeGroup>\n  ```python Python theme={null}\n  @traceable(\n      run_type=\"llm\",\n      metadata={\n          \"ls_provider\": \"openai\",\n          \"ls_model_name\": \"gpt-4o\",\n          \"ls_temperature\": 0.7,\n          \"ls_max_tokens\": 4096,\n          \"ls_stop\": [\"END\"],\n          \"ls_invocation_params\": {\n              \"top_p\": 0.9,\n              \"frequency_penalty\": 0.5\n          }\n      }\n  )\n  def my_configured_llm(messages: list):\n      return call_llm(messages)\n  ```\n\n  ```typescript TypeScript theme={null}\n  const myConfiguredLlm = traceable(\n    async (messages: Array<any>) => {\n      return callLlm(messages);\n    },\n    {\n      run_type: \"llm\",\n      metadata: {\n        ls_provider: \"openai\",\n        ls_model_name: \"gpt-4o\",\n        ls_temperature: 0.7,\n        ls_max_tokens: 4096,\n        ls_stop: [\"END\"],\n        ls_invocation_params: {\n          top_p: 0.9,\n          frequency_penalty: 0.5\n        }\n      }\n    }\n  );\n  ```\n</CodeGroup>\n\nWith this setup, you can later filter traces by temperature, compare runs with different max token settings, or analyze which configuration parameters produce the best results. All these parameters are optional except for the `ls_provider` and `ls_model_name` pair needed for cost tracking.\n\n## All parameters\n\n### User-configurable parameters\n\n| Parameter                                       | Type       | Required | Description                         |\n| ----------------------------------------------- | ---------- | -------- | ----------------------------------- |\n| [`ls_provider`](#ls-provider)                   | `string`   | Yes\\*    | LLM provider name for cost tracking |\n| [`ls_model_name`](#ls-model-name)               | `string`   | Yes\\*    | Model identifier for cost tracking  |\n| [`ls_temperature`](#ls-temperature)             | `number`   | No       | Temperature parameter used          |\n| [`ls_max_tokens`](#ls-max-tokens)               | `number`   | No       | Maximum tokens parameter used       |\n| [`ls_stop`](#ls-stop)                           | `string[]` | No       | Stop sequences used                 |\n| [`ls_invocation_params`](#ls-invocation-params) | `object`   | No       | Additional invocation parameters    |\n\n\\* `ls_provider` and `ls_model_name` must be provided together for cost tracking\n\n### System-generated parameters\n\n| Parameter                       | Type      | Description                                                            |\n| ------------------------------- | --------- | ---------------------------------------------------------------------- |\n| [`ls_run_depth`](#ls-run-depth) | `integer` | Depth in trace tree (0=root, 1=child, etc.) - automatically calculated |\n| [`ls_method`](#ls-method)       | `string`  | Tracing method used (e.g., \"traceable\") - set by SDK                   |\n\n### Experiment parameters\n\n| Parameter                               | Type            | Description                                                             |\n| --------------------------------------- | --------------- | ----------------------------------------------------------------------- |\n| [`ls_example_*`](#ls-example-)          | `any`           | Example metadata prefixed with `ls_example_` - added during experiments |\n| [`ls_experiment_id`](#ls-experiment-id) | `string` (UUID) | Unique experiment identifier - added during experiments                 |\n\n## Parameter details\n\n### `ls_provider`\n\n* **Type:** `string`\n* **Required:** Yes (with [`ls_model_name`](#ls-model-name))\n\n**What it does:**\nIdentifies the LLM provider. Combined with `ls_model_name`, enables automatic cost calculation by matching against [LangSmith's model pricing database](https://smith.langchain.com/settings/workspaces/models).\n\n**Common values:**\n\n* `\"openai\"`\n* `\"anthropic\"`\n* `\"azure\"`\n* `\"bedrock\"`\n* `\"google_vertexai\"`\n* `\"google_genai\"`\n* `\"fireworks\"`\n* `\"mistral\"`\n* `\"groq\"`\n* Or, any custom string\n\n**When to use:**\nWhen you want [automatic cost tracking](/langsmith/cost-tracking) for custom model wrappers or self-hosted models.\n\n**Example:**\n\n```python  theme={null}\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": \"openai\",\n        \"ls_model_name\": \"gpt-4o\"\n    }\n)\ndef my_llm_call(prompt: str):\n    return call_api(prompt)\n```\n\n**Relationships:**\n\n* **Requires** [`ls_model_name`](#ls-model-name) for cost tracking to work.\n* Works with token usage data to calculate costs.\n\n### `ls_model_name`\n\n* **Type:** `string`\n* **Required:** Yes (with `ls_provider`)\n\n**What it does:**\nIdentifies the specific model. Combined with `ls_provider`, matches against pricing database for automatic cost calculation.\n\n**Common values:**\n\n* OpenAI: `\"gpt-4o\"`, `\"gpt-4o-mini\"`, `\"gpt-3.5-turbo\"`\n* Anthropic: `\"claude-3-5-sonnet-20241022\"`, `\"claude-3-opus-20240229\"`\n* Custom: Any model identifier\n\n**When to use:**\nWhen you want automatic [cost tracking](/langsmith/cost-tracking) and model identification in the [UI](https://smith.langchain.com).\n\n**Example:**\n\n```python  theme={null}\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": \"anthropic\",\n        \"ls_model_name\": \"claude-3-5-sonnet-20241022\"\n    }\n)\ndef my_claude_call(messages: list):\n    return call_claude(messages)\n```\n\n**Relationships:**\n\n* **Requires** [`ls_provider`](#ls-provider) for cost tracking to work.\n* Works with token usage data to calculate costs.\n\n### `ls_temperature`\n\n* **Type:** `number` (nullable)\n* **Required:** No\n\n**What it does:**\nRecords the temperature setting used. This is for tracking only\u2014does not affect LangSmith behavior.\n\n**When to use:**\nWhen you want to track model configuration for experiments or debugging.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_temperature\": 0.7\n}\n```\n\n**Relationships:**\n\n* Independent; just for tracking.\n* Useful alongside other config parameters for experiment comparison.\n\n### `ls_max_tokens`\n\n* **Type:** `number` (nullable)\n* **Required:** No\n\n**What it does:**\nRecords the maximum tokens setting used. This is for tracking only\u2014does not affect LangSmith behavior.\n\n**When to use:**\nWhen you want to track model configuration for experiments or debugging.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_max_tokens\": 4096\n}\n```\n\n**Relationships:**\n\n* Independent; just for tracking.\n* Useful for cost analysis when combined with actual token usage.\n\n### `ls_stop`\n\n* **Type:** `string[]` (nullable)\n* **Required:** No\n\n**What it does:**\nRecords stop sequences used. This is for tracking only\u2014does not affect LangSmith behavior.\n\n**When to use:**\nWhen you want to track model configuration for experiments or debugging.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_stop\": [\"END\", \"STOP\", \"\\n\\n\"]\n}\n```\n\n**Relationships:**\n\n* Independent; just for tracking.\n\n### `ls_invocation_params`\n\n* **Type:** `object` (any key-value pairs)\n* **Required:** No\n\n**What it does:**\nStores additional model parameters that don't fit the specific `ls_` parameters. Can include provider-specific settings.\n\n**Common parameters:**\n`top_p`, `frequency_penalty`, `presence_penalty`, `top_k`, `seed`, or any custom parameters\n\n**When to use:**\nWhen you need to track additional configuration beyond the standard parameters.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_invocation_params\": {\n        \"top_p\": 0.9,\n        \"frequency_penalty\": 0.5,\n        \"presence_penalty\": 0.3,\n        \"seed\": 12345\n    }\n}\n```\n\n**Relationships:**\n\n* Independent; stores arbitrary configuration.\n\n### `ls_run_depth`\n\n* **Type:** `integer`\n* **Set by:** LangSmith backend (automatic)\n* **Cannot be overridden**\n\n**What it does:**\nIndicates depth in the trace tree:\n\n* `0` = Root run (top-level)\n* `1` = Direct child\n* `2` = Grandchild\n* etc.\n\n**When it's used:**\nAutomatically calculated during trace ingestion. Used for filtering (e.g., \"show only root runs\") and UI visualization.\n\n**Example query:**\n\n```\nmetadata_key = 'ls_run_depth' AND metadata_value = 0\n```\n\n**Relationships:**\n\n* Determined by trace parent-child structure.\n* Cannot be set manually.\n\n### `ls_method`\n\n* **Type:** `string`\n* **Set by:** SDK (automatic)\n\n**What it does:**\nIndicates which SDK method created the trace (commonly `\"traceable\"` for `@traceable` decorator).\n\n**When it's used:**\nAutomatically set by the tracing SDK. Used for debugging and analytics.\n\n**Relationships:**\n\n* Set by SDK based on how trace was created.\n* Cannot be set manually.\n\n### `ls_example_*`\n\n* **Type:** Any (depends on example metadata)\n* **Pattern:** `ls_example_{original_key}`\n* **Set by:** LangSmith experiments system (automatic)\n\n**What it does:**\nWhen running [experiments on datasets](/langsmith/evaluation-quickstart), metadata from the example is automatically prefixed with `ls_example_` and added to the trace.\n\n**Special parameter:**\n\n* `ls_example_dataset_split`: Dataset split (e.g., \"train\", \"test\", \"validation\")\n\n**When it's used:**\nDuring dataset experiments. Allows filtering/grouping by example characteristics.\n\n**Example:**\nIf example has metadata `{\"category\": \"technical\", \"difficulty\": \"hard\"}`, trace gets:\n\n```json  theme={null}\n{\n  \"metadata\": {\n    \"ls_example_category\": \"technical\",\n    \"ls_example_difficulty\": \"hard\",\n    \"ls_example_dataset_split\": \"test\"\n  }\n}\n```\n\n**Relationships:**\n\n* Automatically derived from example metadata.\n* Cannot be set manually on traces.\n\n### `ls_experiment_id`\n\n* **Type:** `string` (UUID)\n* **Set by:** LangSmith experiments system (automatic)\n\n**What it does:**\nUnique identifier for an experiment run.\n\n**When it's used:**\nAutomatically added when running [experiments/evaluations on datasets](/langsmith/evaluation-quickstart). Used to group all runs from the same experiment.\n\n**Relationships:**\n\n* Links runs to specific experiments.\n* Cannot be set manually.\n\n## Parameter relationships\n\n### Cost tracking dependencies\n\nFor LangSmith to automatically calculate costs, several parameters must work together. Here's what's required:\n\n**Primary requirement:** [`ls_provider`](#ls-provider) + [`ls_model_name`](#ls-model-name)\n\n* Both should be present for automatic cost calculation.\n* If [`ls_model_name`](#ls-model-name) is missing, system will fall back to checking [`ls_invocation_params`](#ls-invocation-params) for model name.\n* [`ls_provider`](#ls-provider) must match a provider in the [pricing database](https://smith.langchain.com/settings/workspaces/models) (or use custom pricing).\n\n**Additional requirements:**\n\n* Run must have `run_type=\"llm\"` (or [arbitrary cost tracking](/langsmith/cost-tracking#tracking-costs-for-arbitrary-runs) must be enabled).\n* [Token usage data](/langsmith/log-llm-trace#provide-token-and-cost-information) must be present in the trace (prompt\\_tokens, completion\\_tokens).\n* Model must exist in pricing database or have [custom pricing configured](/langsmith/cost-tracking#set-up-model-pricing).\n\n**Fallback behavior:**\nIf [`ls_model_name`](#ls-model-name) is not in metadata, the system checks [`ls_invocation_params`](#ls-invocation-params) for model identifiers like `\"model\"` before giving up on cost tracking.\n\n### Configuration tracking group\n\nThese parameters help you track model settings but don't affect LangSmith's core functionality:\n\n**Optional, work independently:** [`ls_temperature`](#ls-temperature), [`ls_max_tokens`](#ls-max-tokens), [`ls_stop`](#ls-stop)\n\n* These are for tracking/display.\n* Do not affect LangSmith behavior or cost calculation.\n* Useful for experiment comparison and debugging.\n\n### Invocation params special case\n\nThe `ls_invocation_params` parameter has a dual role as both a tracking field and a fallback mechanism:\n\n**[`ls_invocation_params`](#ls-invocation-params)**; partially independent with fallback role:\n\n* Primarily stores arbitrary configuration for tracking.\n* **Can serve as fallback** for cost tracking if [`ls_model_name`](#ls-model-name) is missing.\n* Does not directly affect cost calculation when [`ls_model_name`](#ls-model-name) is present.\n\n### System parameters\n\nThese parameters are automatically generated by LangSmith and cannot be manually set:\n\n**Cannot be user-set:** [`ls_run_depth`](#ls-run-depth), [`ls_method`](#ls-method), [`ls_example_*`](#ls-example-), [`ls_experiment_id`](#ls-experiment-id)\n\n* Automatically set by system.\n* Used for filtering, analytics, and system tracking.\n\n## Filter traces by metadata parameters\n\nOnce you've added `ls_` metadata parameters to your traces, you can use them to filter and search traces programmatically via the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or interactively in the [LangSmith UI](https://smith.langchain.com). This lets you narrow down traces by model, provider, configuration settings, or trace depth.\n\n### Use the API\n\nUse the [`Client`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client) class with the [`list_runs()`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs) method (Python) or [`listRuns()`](https://docs.smith.langchain.com/reference/js/classes/client.Client#listruns) method (TypeScript) to query traces based on metadata values. The [filter syntax](/langsmith/trace-query-syntax) supports equality checks, comparisons, and logical operators.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n\n  client = Client()\n\n  # Filter runs by provider\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_provider\" AND metadata_value = \"openai\"'\n  )\n\n  # Filter by specific model\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_model_name\" AND metadata_value = \"gpt-4o\"'\n  )\n\n  # Filter root runs only (top-level traces)\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_run_depth\" AND metadata_value = 0'\n  )\n\n  # Filter by temperature threshold\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_temperature\" AND metadata_value > 0.5'\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n\n  const client = new Client();\n\n  // Filter runs by provider\n  const runsByProvider: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_provider\" AND metadata_value = \"openai\"'\n  })) {\n    runsByProvider.push(run);\n  }\n\n  // Filter by specific model\n  const runsByModel: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_model_name\" AND metadata_value = \"gpt-4o\"'\n  })) {\n    runsByModel.push(run);\n  }\n\n  // Filter root runs only (top-level traces)\n  const rootRuns: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_run_depth\" AND metadata_value = 0'\n  })) {\n    rootRuns.push(run);\n  }\n\n  // Filter by temperature threshold\n  const highTempRuns: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_temperature\" AND metadata_value > 0.5'\n  })) {\n    highTempRuns.push(run);\n  }\n  ```\n</CodeGroup>\n\nThese examples show common filtering patterns:\n\n* **Filter by provider or model** to analyze usage patterns or costs for specific models\n* **Filter by run depth** to get only root traces (depth 0) or child runs at specific nesting levels\n* **Filter by configuration** to compare experiments with different temperature, max tokens, or other settings\n\n### Use the UI\n\nIn the [LangSmith UI](https://smith.langchain.com), use the filter/search bar with the [filter syntax](/langsmith/trace-query-syntax):\n\n```\nmetadata_key = 'ls_provider' AND metadata_value = 'openai'\nmetadata_key = 'ls_model_name' AND metadata_value = 'gpt-4o'\nmetadata_key = 'ls_run_depth' AND metadata_value = 0\n```\n\n## Related resources\n\n* [Cost tracking guide](/langsmith/cost-tracking): Learn how to track and analyze LLM costs in LangSmith.\n* [Log LLM traces](/langsmith/log-llm-trace): Format requirements for logging LLM calls with proper token tracking.\n* [Trace query syntax](/langsmith/trace-query-syntax): Complete reference for filtering and searching traces.\n* [Evaluation quickstart](/langsmith/evaluation-quickstart): Run experiments on datasets to compare model configurations.\n* [Add metadata and tags](/langsmith/add-metadata-tags): General guide to adding metadata to traces.\n* [Filter traces in application](/langsmith/filter-traces-in-application): Programmatically filter traces in your code.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/ls-metadata-parameters.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Trace a RAG application tutorial\nLink: https://docs.langchain.comlangsmith/observability-llm-tutorial\nContent: Trace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like: Notice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client ( openai_client = wrap_openai(OpenAI()) ). What happens if you call it in the following way? This will produce a trace of just the OpenAI call - it should look something like this\n\nTrace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like: Notice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client ( openai_client = wrap_openai(OpenAI()) ). What happens if you call it in the following way? rag(\"where did harrison work\")\n\nTrace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like: from openai import OpenAI\nfrom langsmith.wrappers import wrap_openai\nopenai_client = wrap_openai(OpenAI())\n\n# This is the retriever we will use in RAG\n# This is mocked out, but it could be anything we want\ndef retriever(query: str):\n    results = [\"Harrison worked at Kensho\"]\n    return results\n\n# This is the end-to-end RAG chain.\n# It does a retrieval step then calls OpenAI\ndef rag(question):\n    docs = retriever(question)\n    system_message = \"\"\"Answer the users question using only the provided information below:\n        {docs}\"\"\".format(docs=\"\\n\".join(docs))\n\n    return openai_client.chat.completions.create(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": question},\n        ],\n        model=\"gpt-4o-mini\",\n    )\n\n---\n\nResult 5:\nTitle: Tracing quickstart\nLink: https://docs.langchain.comlangsmith/observability-quickstart\nContent: 4. Trace LLM calls\nTo start, you\u2019ll trace all your OpenAI calls. LangSmith provides wrappers: Python: wrap_openai TypeScript: wrapOpenAI This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith. Include the highlighted lines in your app file: Call your application: You'll receive the following output: In the LangSmith UI , navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2 ). You'll see the OpenAI call you just instrumented.\n\n4. Trace LLM calls\nTo start, you\u2019ll trace all your OpenAI calls. LangSmith provides wrappers: Python: wrap_openai TypeScript: wrapOpenAI This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith. Include the highlighted lines in your app file: from openai import OpenAI\nfrom langsmith.wrappers import wrap_openai  # traces openai calls\n\ndef retriever(query: str):\n    return [\"Harrison worked at Kensho\"]\n\nclient = wrap_openai(OpenAI())  # log traces by wrapping the model calls\n\ndef rag(question: str) -> str:\n    docs = retriever(question)\n    system_message = (\n        \"Answer the user's question using only the provided information below:\\n\"\n        + \"\\n\".join(docs)\n    )\n    resp = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": question},\n        ],\n    )\n    return resp.choices[0].message.content\n\nif __name__ == \"__main__\":\n    print(rag(\"Where did Harrison work?\"))\n\n4. Trace LLM calls\nTo start, you\u2019ll trace all your OpenAI calls. LangSmith provides wrappers: Python: wrap_openai TypeScript: wrapOpenAI This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith. Include the highlighted lines in your app file: import \"dotenv/config\";\nimport OpenAI from \"openai\";\nimport { wrapOpenAI } from \"langsmith/wrappers\"; // traces openai calls\n\nfunction retriever(query: string): string[] {\n    return [\"Harrison worked at Kensho\"];\n}\n\nconst client = wrapOpenAI(new OpenAI()); // log traces by wrapping the model calls\n\nasync function rag(question: string) {\n    const docs = retriever(question);\n    const systemMessage =\n        \"Answer the user's question using only the provided information below:\\n\" +\n        docs.join(\"\\n\");\n\n    const resp = await client.chat.completions.create({\n        model: \"gpt-4o-mini\",\n        messages: [\n            { role: \"system\", content: systemMessage },\n            { role: \"user\", content: question },\n        ],\n    });\n\n    return resp.choices[0].message?.content;\n}\n\n(async () => {\n    console.log(await rag(\"Where did Harrison work?\"));\n})();\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_29447684",
    "id": "8039e883-01d5-4040-9ad1-d977fd9631f0",
    "content": "{\n  \"error\": \"Collection 'all' not found. Available collections: General, LangSmith Studio, LangSmith Deployment, LangSmith Evaluation, LangSmith Observability, OSS (LangChain and LangGraph), SDKs and APIs, Self Hosted, Troubleshooting\"\n}"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_43406472",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"langsmith run payload limit\"}"
        }
      },
      {
        "type": "function",
        "id": "call_36457895",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"trace size limit\"}"
        }
      },
      {
        "type": "function",
        "id": "call_11568068",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"LangSmith Observability\"}"
        }
      },
      {
        "type": "function",
        "id": "call_14987747",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"Troubleshooting\"}"
        }
      }
    ],
    "id": "lc_run--019b3a30-94d7-7542-90ff-105e04303a79",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_43406472",
    "id": "87ae4a28-aef4-47df-b6a7-cc7f57adbae6",
    "content": "Result 1:\nTitle: Connect to an OpenAI compliant model provider/proxy\nLink: https://docs.langchain.comlangsmith/custom-openai-compliant-model\nContent: The LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for  in the playground.\n\n## Deploy an OpenAI compliant model\n\nMany providers offer OpenAI compliant models or proxy services. Some examples of this include:\n\n* [LiteLLM Proxy](https://github.com/BerriAI/litellm?tab=readme-ov-file#quick-start-proxy---cli)\n* [Ollama](https://ollama.com/)\n\nYou can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API.\n\nTake a look at the full [specification](https://platform.openai.com/docs/api-reference/chat) for more information.\n\n## Use the model in the LangSmith Playground\n\nOnce you have deployed a model server, you can use it in the LangSmith [Playground](/langsmith/prompt-engineering-concepts#prompt-playground).\n\nTo access the **Prompt Settings** menu:\n\n1. Under the **Prompts** heading select the gear <Icon icon=\"gear\" iconType=\"solid\" /> icon next to the model name.\n2. In the **Model Configuration** tab, select the model to edit in the dropdown.\n3. For the **Provider** dropdown, select **OpenAI Compatible Endpoint**.\n4. Add your OpenAI Compatible Endpoint to the **Base URL** input.\n\n   <div style={{ textAlign: 'center' }}>\n     <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=fdbe548e512ed40fb512578d02986b45\" alt=\"Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected.\" data-og-width=\"897\" width=\"897\" data-og-height=\"572\" height=\"572\" data-path=\"langsmith/images/openai-compatible-endpoint.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=75921e7b30edbac5263ee10178977383 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=468265c2ad0a6c1740eb18590dab27a5 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=9f9c3f68df77205264eb9221373790f2 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=ccb9820653e1e57b529bc46ac7d20e40 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c320b3f4051643b71fba4faa350daf9b 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=e9d1be7c69021b7fa575a2a466dbfe58 2500w\" />\n\n     <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=97459563da21d17228a1bb94a1b9edf3\" alt=\"Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected.\" data-og-width=\"896\" width=\"896\" data-og-height=\"552\" height=\"552\" data-path=\"langsmith/images/openai-compatible-endpoint-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c3e8e46813ec673fbc3ac4e4748a4ab6 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=381a567b022c71ed6f74abbb7e3cecbd 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=7df617e39d5bd098f4e80d523ef85778 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=06408cd348f56fcc409af8273c799a97 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=6b79b55e2868ed11e9d2c2eb396b004f 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=4e0439d889aa2175c0802e9bf5db399b 2500w\" />\n   </div>\n\nIf everything is set up correctly, you should see the model's response in the playground. You can also use this functionality to invoke downstream pipelines as well.\n\nFor information on how to store your model configuration , refer to [Configure prompt settings](/langsmith/managing-model-configurations).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-openai-compliant-model.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Evaluation quickstart\nLink: https://docs.langchain.comlangsmith/evaluation-quickstart\nContent: Prerequisites\nBefore you begin, make sure you have: A LangSmith account : Sign up or log in at smith.langchain.com. A LangSmith API key : Follow the Create an API key guide. An OpenAI API key : Generate this from the OpenAI dashboard. Select the UI or SDK filter for instructions:\n\n1. Set workspace secrets\nIn the LangSmith UI , ensure that your OpenAI API key is set as a workspace secret. Navigate to Settings and then move to the Secrets tab. Select Add secret and enter the OPENAI_API_KEY and your API key as the Value. Select Save secret. When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.\n\n6. Run and view results\nTo run the evaluation experiment, you'll call evaluate(...) , which: Pulls example from the dataset you created in Step 3. Sends each example's inputs to your target function from Step 4. Collects the outputs (the model's answers). Passes the outputs along with the reference_outputs to your evaluator from Step 5. Records all results in LangSmith as an experiment, so you can view them in the UI. Add the highlighted code to your eval file: from langsmith import Client, wrappers\nfrom openai import OpenAI\nfrom openevals.llm import create_llm_as_judge\nfrom openevals.prompts import CORRECTNESS_PROMPT\n\n# Wrap the OpenAI client for LangSmith tracing\nopenai_client = wrappers.wrap_openai(OpenAI())\n\n# Define the application logic you want to evaluate inside a target function\n# The SDK will automatically send the inputs from the dataset to your target function\ndef target(inputs: dict) -> dict:\n    response = openai_client.chat.completions.create(\n        model=\"gpt-5-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},\n            {\"role\": \"user\", \"content\": inputs[\"question\"]},\n        ],\n    )\n    return {\"answer\": response.choices[0].message.content.strip()}\n\ndef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n    evaluator = create_llm_as_judge(\n        prompt=CORRECTNESS_PROMPT,\n        model=\"openai:o3-mini\",\n        feedback_key=\"correctness\",\n    )\n    return evaluator(\n        inputs=inputs,\n        outputs=outputs,\n        reference_outputs=reference_outputs\n    )\n\n# After running the evaluation, a link will be provided to view the results in langsmith\ndef main():\n    client = Client()\n    experiment_results = client.evaluate(\n        target,\n        data=\"Sample dataset\",\n        evaluators=[\n            correctness_evaluator,\n            # can add multiple evaluators here\n        ],\n        experiment_prefix=\"first-eval-in-langsmith\",\n        max_concurrency=2,\n    )\n    print(experiment_results)\n\nif __name__ == \"__main__\":\n    main()\n\n---\n\nResult 3:\nTitle: Metadata parameters reference\nLink: https://docs.langchain.comlangsmith/ls-metadata-parameters\nContent: When you trace LLM calls with LangSmith, you often want to [track costs](/langsmith/cost-tracking), compare model configurations, and analyze performance across different providers. LangSmith's native integrations (like [LangChain](/langsmith/trace-with-langchain) or the [OpenAI](/langsmith/trace-openai)/[Anthropic](/langsmith/trace-anthropic) wrappers) handle this automatically, but custom model wrappers and self-hosted models require a standardized way to provide this information. LangSmith uses `ls_` metadata parameters for this purpose.\n\nThese metadata parameters (all prefixed with `ls_`) let you pass model configuration and identification information through the standard `metadata` field. Once set, LangSmith can automatically calculate costs, display model information in the UI, and enable filtering and analytics across your traces.\n\nUse `ls_` metadata parameters to:\n\n* **Enable automatic cost tracking** for custom or self-hosted models by identifying the provider and model name.\n* **Track model configuration** like temperature, max tokens, and other parameters for experiment comparison.\n* **Filter and analyze traces** by provider or configuration settings\n* **Improve debugging** by recording exactly which model settings were used for each run.\n\n## Basic usage example\n\nThe most common use case is enabling cost tracking for custom model wrappers. To do this, you need to provide two key pieces of information: the provider name (`ls_provider`) and the model name (`ls_model_name`). These work together to match against LangSmith's pricing database.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import traceable\n\n  @traceable(\n      run_type=\"llm\",\n      metadata={\n          \"ls_provider\": \"my_provider\",\n          \"ls_model_name\": \"my_custom_model\"\n      }\n  )\n  def my_custom_llm(prompt: str):\n      return call_custom_api(prompt)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { traceable } from \"langsmith/traceable\";\n\n  const myCustomLlm = traceable(\n    async (prompt: string) => {\n      return callCustomApi(prompt);\n    },\n    {\n      run_type: \"llm\",\n      metadata: {\n        ls_provider: \"my_provider\",\n        ls_model_name: \"my_custom_model\"\n      }\n    }\n  );\n  ```\n</CodeGroup>\n\nThis minimal setup tells LangSmith what model you're using, enabling automatic cost calculation if the model exists in the pricing database or if you've [configured custom pricing](/langsmith/cost-tracking#set-up-model-pricing).\n\nFor more comprehensive tracking, you can include additional configuration parameters. This is especially useful when [running experiments](/langsmith/evaluation-quickstart) or comparing different model settings:\n\n<CodeGroup>\n  ```python Python theme={null}\n  @traceable(\n      run_type=\"llm\",\n      metadata={\n          \"ls_provider\": \"openai\",\n          \"ls_model_name\": \"gpt-4o\",\n          \"ls_temperature\": 0.7,\n          \"ls_max_tokens\": 4096,\n          \"ls_stop\": [\"END\"],\n          \"ls_invocation_params\": {\n              \"top_p\": 0.9,\n              \"frequency_penalty\": 0.5\n          }\n      }\n  )\n  def my_configured_llm(messages: list):\n      return call_llm(messages)\n  ```\n\n  ```typescript TypeScript theme={null}\n  const myConfiguredLlm = traceable(\n    async (messages: Array<any>) => {\n      return callLlm(messages);\n    },\n    {\n      run_type: \"llm\",\n      metadata: {\n        ls_provider: \"openai\",\n        ls_model_name: \"gpt-4o\",\n        ls_temperature: 0.7,\n        ls_max_tokens: 4096,\n        ls_stop: [\"END\"],\n        ls_invocation_params: {\n          top_p: 0.9,\n          frequency_penalty: 0.5\n        }\n      }\n    }\n  );\n  ```\n</CodeGroup>\n\nWith this setup, you can later filter traces by temperature, compare runs with different max token settings, or analyze which configuration parameters produce the best results. All these parameters are optional except for the `ls_provider` and `ls_model_name` pair needed for cost tracking.\n\n## All parameters\n\n### User-configurable parameters\n\n| Parameter                                       | Type       | Required | Description                         |\n| ----------------------------------------------- | ---------- | -------- | ----------------------------------- |\n| [`ls_provider`](#ls-provider)                   | `string`   | Yes\\*    | LLM provider name for cost tracking |\n| [`ls_model_name`](#ls-model-name)               | `string`   | Yes\\*    | Model identifier for cost tracking  |\n| [`ls_temperature`](#ls-temperature)             | `number`   | No       | Temperature parameter used          |\n| [`ls_max_tokens`](#ls-max-tokens)               | `number`   | No       | Maximum tokens parameter used       |\n| [`ls_stop`](#ls-stop)                           | `string[]` | No       | Stop sequences used                 |\n| [`ls_invocation_params`](#ls-invocation-params) | `object`   | No       | Additional invocation parameters    |\n\n\\* `ls_provider` and `ls_model_name` must be provided together for cost tracking\n\n### System-generated parameters\n\n| Parameter                       | Type      | Description                                                            |\n| ------------------------------- | --------- | ---------------------------------------------------------------------- |\n| [`ls_run_depth`](#ls-run-depth) | `integer` | Depth in trace tree (0=root, 1=child, etc.) - automatically calculated |\n| [`ls_method`](#ls-method)       | `string`  | Tracing method used (e.g., \"traceable\") - set by SDK                   |\n\n### Experiment parameters\n\n| Parameter                               | Type            | Description                                                             |\n| --------------------------------------- | --------------- | ----------------------------------------------------------------------- |\n| [`ls_example_*`](#ls-example-)          | `any`           | Example metadata prefixed with `ls_example_` - added during experiments |\n| [`ls_experiment_id`](#ls-experiment-id) | `string` (UUID) | Unique experiment identifier - added during experiments                 |\n\n## Parameter details\n\n### `ls_provider`\n\n* **Type:** `string`\n* **Required:** Yes (with [`ls_model_name`](#ls-model-name))\n\n**What it does:**\nIdentifies the LLM provider. Combined with `ls_model_name`, enables automatic cost calculation by matching against [LangSmith's model pricing database](https://smith.langchain.com/settings/workspaces/models).\n\n**Common values:**\n\n* `\"openai\"`\n* `\"anthropic\"`\n* `\"azure\"`\n* `\"bedrock\"`\n* `\"google_vertexai\"`\n* `\"google_genai\"`\n* `\"fireworks\"`\n* `\"mistral\"`\n* `\"groq\"`\n* Or, any custom string\n\n**When to use:**\nWhen you want [automatic cost tracking](/langsmith/cost-tracking) for custom model wrappers or self-hosted models.\n\n**Example:**\n\n```python  theme={null}\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": \"openai\",\n        \"ls_model_name\": \"gpt-4o\"\n    }\n)\ndef my_llm_call(prompt: str):\n    return call_api(prompt)\n```\n\n**Relationships:**\n\n* **Requires** [`ls_model_name`](#ls-model-name) for cost tracking to work.\n* Works with token usage data to calculate costs.\n\n### `ls_model_name`\n\n* **Type:** `string`\n* **Required:** Yes (with `ls_provider`)\n\n**What it does:**\nIdentifies the specific model. Combined with `ls_provider`, matches against pricing database for automatic cost calculation.\n\n**Common values:**\n\n* OpenAI: `\"gpt-4o\"`, `\"gpt-4o-mini\"`, `\"gpt-3.5-turbo\"`\n* Anthropic: `\"claude-3-5-sonnet-20241022\"`, `\"claude-3-opus-20240229\"`\n* Custom: Any model identifier\n\n**When to use:**\nWhen you want automatic [cost tracking](/langsmith/cost-tracking) and model identification in the [UI](https://smith.langchain.com).\n\n**Example:**\n\n```python  theme={null}\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": \"anthropic\",\n        \"ls_model_name\": \"claude-3-5-sonnet-20241022\"\n    }\n)\ndef my_claude_call(messages: list):\n    return call_claude(messages)\n```\n\n**Relationships:**\n\n* **Requires** [`ls_provider`](#ls-provider) for cost tracking to work.\n* Works with token usage data to calculate costs.\n\n### `ls_temperature`\n\n* **Type:** `number` (nullable)\n* **Required:** No\n\n**What it does:**\nRecords the temperature setting used. This is for tracking only\u2014does not affect LangSmith behavior.\n\n**When to use:**\nWhen you want to track model configuration for experiments or debugging.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_temperature\": 0.7\n}\n```\n\n**Relationships:**\n\n* Independent; just for tracking.\n* Useful alongside other config parameters for experiment comparison.\n\n### `ls_max_tokens`\n\n* **Type:** `number` (nullable)\n* **Required:** No\n\n**What it does:**\nRecords the maximum tokens setting used. This is for tracking only\u2014does not affect LangSmith behavior.\n\n**When to use:**\nWhen you want to track model configuration for experiments or debugging.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_max_tokens\": 4096\n}\n```\n\n**Relationships:**\n\n* Independent; just for tracking.\n* Useful for cost analysis when combined with actual token usage.\n\n### `ls_stop`\n\n* **Type:** `string[]` (nullable)\n* **Required:** No\n\n**What it does:**\nRecords stop sequences used. This is for tracking only\u2014does not affect LangSmith behavior.\n\n**When to use:**\nWhen you want to track model configuration for experiments or debugging.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_stop\": [\"END\", \"STOP\", \"\\n\\n\"]\n}\n```\n\n**Relationships:**\n\n* Independent; just for tracking.\n\n### `ls_invocation_params`\n\n* **Type:** `object` (any key-value pairs)\n* **Required:** No\n\n**What it does:**\nStores additional model parameters that don't fit the specific `ls_` parameters. Can include provider-specific settings.\n\n**Common parameters:**\n`top_p`, `frequency_penalty`, `presence_penalty`, `top_k`, `seed`, or any custom parameters\n\n**When to use:**\nWhen you need to track additional configuration beyond the standard parameters.\n\n**Example:**\n\n```python  theme={null}\nmetadata={\n    \"ls_provider\": \"openai\",\n    \"ls_model_name\": \"gpt-4o\",\n    \"ls_invocation_params\": {\n        \"top_p\": 0.9,\n        \"frequency_penalty\": 0.5,\n        \"presence_penalty\": 0.3,\n        \"seed\": 12345\n    }\n}\n```\n\n**Relationships:**\n\n* Independent; stores arbitrary configuration.\n\n### `ls_run_depth`\n\n* **Type:** `integer`\n* **Set by:** LangSmith backend (automatic)\n* **Cannot be overridden**\n\n**What it does:**\nIndicates depth in the trace tree:\n\n* `0` = Root run (top-level)\n* `1` = Direct child\n* `2` = Grandchild\n* etc.\n\n**When it's used:**\nAutomatically calculated during trace ingestion. Used for filtering (e.g., \"show only root runs\") and UI visualization.\n\n**Example query:**\n\n```\nmetadata_key = 'ls_run_depth' AND metadata_value = 0\n```\n\n**Relationships:**\n\n* Determined by trace parent-child structure.\n* Cannot be set manually.\n\n### `ls_method`\n\n* **Type:** `string`\n* **Set by:** SDK (automatic)\n\n**What it does:**\nIndicates which SDK method created the trace (commonly `\"traceable\"` for `@traceable` decorator).\n\n**When it's used:**\nAutomatically set by the tracing SDK. Used for debugging and analytics.\n\n**Relationships:**\n\n* Set by SDK based on how trace was created.\n* Cannot be set manually.\n\n### `ls_example_*`\n\n* **Type:** Any (depends on example metadata)\n* **Pattern:** `ls_example_{original_key}`\n* **Set by:** LangSmith experiments system (automatic)\n\n**What it does:**\nWhen running [experiments on datasets](/langsmith/evaluation-quickstart), metadata from the example is automatically prefixed with `ls_example_` and added to the trace.\n\n**Special parameter:**\n\n* `ls_example_dataset_split`: Dataset split (e.g., \"train\", \"test\", \"validation\")\n\n**When it's used:**\nDuring dataset experiments. Allows filtering/grouping by example characteristics.\n\n**Example:**\nIf example has metadata `{\"category\": \"technical\", \"difficulty\": \"hard\"}`, trace gets:\n\n```json  theme={null}\n{\n  \"metadata\": {\n    \"ls_example_category\": \"technical\",\n    \"ls_example_difficulty\": \"hard\",\n    \"ls_example_dataset_split\": \"test\"\n  }\n}\n```\n\n**Relationships:**\n\n* Automatically derived from example metadata.\n* Cannot be set manually on traces.\n\n### `ls_experiment_id`\n\n* **Type:** `string` (UUID)\n* **Set by:** LangSmith experiments system (automatic)\n\n**What it does:**\nUnique identifier for an experiment run.\n\n**When it's used:**\nAutomatically added when running [experiments/evaluations on datasets](/langsmith/evaluation-quickstart). Used to group all runs from the same experiment.\n\n**Relationships:**\n\n* Links runs to specific experiments.\n* Cannot be set manually.\n\n## Parameter relationships\n\n### Cost tracking dependencies\n\nFor LangSmith to automatically calculate costs, several parameters must work together. Here's what's required:\n\n**Primary requirement:** [`ls_provider`](#ls-provider) + [`ls_model_name`](#ls-model-name)\n\n* Both should be present for automatic cost calculation.\n* If [`ls_model_name`](#ls-model-name) is missing, system will fall back to checking [`ls_invocation_params`](#ls-invocation-params) for model name.\n* [`ls_provider`](#ls-provider) must match a provider in the [pricing database](https://smith.langchain.com/settings/workspaces/models) (or use custom pricing).\n\n**Additional requirements:**\n\n* Run must have `run_type=\"llm\"` (or [arbitrary cost tracking](/langsmith/cost-tracking#tracking-costs-for-arbitrary-runs) must be enabled).\n* [Token usage data](/langsmith/log-llm-trace#provide-token-and-cost-information) must be present in the trace (prompt\\_tokens, completion\\_tokens).\n* Model must exist in pricing database or have [custom pricing configured](/langsmith/cost-tracking#set-up-model-pricing).\n\n**Fallback behavior:**\nIf [`ls_model_name`](#ls-model-name) is not in metadata, the system checks [`ls_invocation_params`](#ls-invocation-params) for model identifiers like `\"model\"` before giving up on cost tracking.\n\n### Configuration tracking group\n\nThese parameters help you track model settings but don't affect LangSmith's core functionality:\n\n**Optional, work independently:** [`ls_temperature`](#ls-temperature), [`ls_max_tokens`](#ls-max-tokens), [`ls_stop`](#ls-stop)\n\n* These are for tracking/display.\n* Do not affect LangSmith behavior or cost calculation.\n* Useful for experiment comparison and debugging.\n\n### Invocation params special case\n\nThe `ls_invocation_params` parameter has a dual role as both a tracking field and a fallback mechanism:\n\n**[`ls_invocation_params`](#ls-invocation-params)**; partially independent with fallback role:\n\n* Primarily stores arbitrary configuration for tracking.\n* **Can serve as fallback** for cost tracking if [`ls_model_name`](#ls-model-name) is missing.\n* Does not directly affect cost calculation when [`ls_model_name`](#ls-model-name) is present.\n\n### System parameters\n\nThese parameters are automatically generated by LangSmith and cannot be manually set:\n\n**Cannot be user-set:** [`ls_run_depth`](#ls-run-depth), [`ls_method`](#ls-method), [`ls_example_*`](#ls-example-), [`ls_experiment_id`](#ls-experiment-id)\n\n* Automatically set by system.\n* Used for filtering, analytics, and system tracking.\n\n## Filter traces by metadata parameters\n\nOnce you've added `ls_` metadata parameters to your traces, you can use them to filter and search traces programmatically via the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or interactively in the [LangSmith UI](https://smith.langchain.com). This lets you narrow down traces by model, provider, configuration settings, or trace depth.\n\n### Use the API\n\nUse the [`Client`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client) class with the [`list_runs()`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs) method (Python) or [`listRuns()`](https://docs.smith.langchain.com/reference/js/classes/client.Client#listruns) method (TypeScript) to query traces based on metadata values. The [filter syntax](/langsmith/trace-query-syntax) supports equality checks, comparisons, and logical operators.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n\n  client = Client()\n\n  # Filter runs by provider\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_provider\" AND metadata_value = \"openai\"'\n  )\n\n  # Filter by specific model\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_model_name\" AND metadata_value = \"gpt-4o\"'\n  )\n\n  # Filter root runs only (top-level traces)\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_run_depth\" AND metadata_value = 0'\n  )\n\n  # Filter by temperature threshold\n  runs = client.list_runs(\n      project_name=\"my-app\",\n      filter='metadata_key = \"ls_temperature\" AND metadata_value > 0.5'\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n\n  const client = new Client();\n\n  // Filter runs by provider\n  const runsByProvider: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_provider\" AND metadata_value = \"openai\"'\n  })) {\n    runsByProvider.push(run);\n  }\n\n  // Filter by specific model\n  const runsByModel: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_model_name\" AND metadata_value = \"gpt-4o\"'\n  })) {\n    runsByModel.push(run);\n  }\n\n  // Filter root runs only (top-level traces)\n  const rootRuns: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_run_depth\" AND metadata_value = 0'\n  })) {\n    rootRuns.push(run);\n  }\n\n  // Filter by temperature threshold\n  const highTempRuns: any[] = [];\n  for await (const run of client.listRuns({\n    projectName: \"my-app\",\n    filter: 'metadata_key = \"ls_temperature\" AND metadata_value > 0.5'\n  })) {\n    highTempRuns.push(run);\n  }\n  ```\n</CodeGroup>\n\nThese examples show common filtering patterns:\n\n* **Filter by provider or model** to analyze usage patterns or costs for specific models\n* **Filter by run depth** to get only root traces (depth 0) or child runs at specific nesting levels\n* **Filter by configuration** to compare experiments with different temperature, max tokens, or other settings\n\n### Use the UI\n\nIn the [LangSmith UI](https://smith.langchain.com), use the filter/search bar with the [filter syntax](/langsmith/trace-query-syntax):\n\n```\nmetadata_key = 'ls_provider' AND metadata_value = 'openai'\nmetadata_key = 'ls_model_name' AND metadata_value = 'gpt-4o'\nmetadata_key = 'ls_run_depth' AND metadata_value = 0\n```\n\n## Related resources\n\n* [Cost tracking guide](/langsmith/cost-tracking): Learn how to track and analyze LLM costs in LangSmith.\n* [Log LLM traces](/langsmith/log-llm-trace): Format requirements for logging LLM calls with proper token tracking.\n* [Trace query syntax](/langsmith/trace-query-syntax): Complete reference for filtering and searching traces.\n* [Evaluation quickstart](/langsmith/evaluation-quickstart): Run experiments on datasets to compare model configurations.\n* [Add metadata and tags](/langsmith/add-metadata-tags): General guide to adding metadata to traces.\n* [Filter traces in application](/langsmith/filter-traces-in-application): Programmatically filter traces in your code.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/ls-metadata-parameters.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Trace a RAG application tutorial\nLink: https://docs.langchain.comlangsmith/observability-llm-tutorial\nContent: Trace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like: Notice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client ( openai_client = wrap_openai(OpenAI()) ). What happens if you call it in the following way? This will produce a trace of just the OpenAI call - it should look something like this\n\nTrace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like: Notice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client ( openai_client = wrap_openai(OpenAI()) ). What happens if you call it in the following way? rag(\"where did harrison work\")\n\nTrace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like: from openai import OpenAI\nfrom langsmith.wrappers import wrap_openai\nopenai_client = wrap_openai(OpenAI())\n\n# This is the retriever we will use in RAG\n# This is mocked out, but it could be anything we want\ndef retriever(query: str):\n    results = [\"Harrison worked at Kensho\"]\n    return results\n\n# This is the end-to-end RAG chain.\n# It does a retrieval step then calls OpenAI\ndef rag(question):\n    docs = retriever(question)\n    system_message = \"\"\"Answer the users question using only the provided information below:\n        {docs}\"\"\".format(docs=\"\\n\".join(docs))\n\n    return openai_client.chat.completions.create(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": question},\n        ],\n        model=\"gpt-4o-mini\",\n    )\n\n---\n\nResult 5:\nTitle: Tracing quickstart\nLink: https://docs.langchain.comlangsmith/observability-quickstart\nContent: 4. Trace LLM calls\nTo start, you\u2019ll trace all your OpenAI calls. LangSmith provides wrappers: Python: wrap_openai TypeScript: wrapOpenAI This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith. Include the highlighted lines in your app file: Call your application: You'll receive the following output: In the LangSmith UI , navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2 ). You'll see the OpenAI call you just instrumented.\n\n4. Trace LLM calls\nTo start, you\u2019ll trace all your OpenAI calls. LangSmith provides wrappers: Python: wrap_openai TypeScript: wrapOpenAI This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith. Include the highlighted lines in your app file: from openai import OpenAI\nfrom langsmith.wrappers import wrap_openai  # traces openai calls\n\ndef retriever(query: str):\n    return [\"Harrison worked at Kensho\"]\n\nclient = wrap_openai(OpenAI())  # log traces by wrapping the model calls\n\ndef rag(question: str) -> str:\n    docs = retriever(question)\n    system_message = (\n        \"Answer the user's question using only the provided information below:\\n\"\n        + \"\\n\".join(docs)\n    )\n    resp = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": question},\n        ],\n    )\n    return resp.choices[0].message.content\n\nif __name__ == \"__main__\":\n    print(rag(\"Where did Harrison work?\"))\n\n4. Trace LLM calls\nTo start, you\u2019ll trace all your OpenAI calls. LangSmith provides wrappers: Python: wrap_openai TypeScript: wrapOpenAI This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith. Include the highlighted lines in your app file: import \"dotenv/config\";\nimport OpenAI from \"openai\";\nimport { wrapOpenAI } from \"langsmith/wrappers\"; // traces openai calls\n\nfunction retriever(query: string): string[] {\n    return [\"Harrison worked at Kensho\"];\n}\n\nconst client = wrapOpenAI(new OpenAI()); // log traces by wrapping the model calls\n\nasync function rag(question: string) {\n    const docs = retriever(question);\n    const systemMessage =\n        \"Answer the user's question using only the provided information below:\\n\" +\n        docs.join(\"\\n\");\n\n    const resp = await client.chat.completions.create({\n        model: \"gpt-4o-mini\",\n        messages: [\n            { role: \"system\", content: systemMessage },\n            { role: \"user\", content: question },\n        ],\n    });\n\n    return resp.choices[0].message?.content;\n}\n\n(async () => {\n    console.log(await rag(\"Where did Harrison work?\"));\n})();\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_36457895",
    "id": "9e7cd515-d5e4-4699-8d22-3d6650df66e0",
    "content": "Result 1:\nTitle: Trace with LangChain (Python and JS/TS)\nLink: https://docs.langchain.comlangsmith/trace-with-langchain\nContent: Tracing LangChain objects inside traceable (JS only)\nStarting with langchain@0.2.x , LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x , you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable.\n\nTrace with LangChain (Python and JS/TS)\nLangSmith integrates seamlessly with LangChain (Python and JavaScript), the popular open-source framework for building LLM applications.\n\nTrace with LangChain (Python and JS/TS)\nTrace with LangChain (Python and JS/TS)\nLangSmith integrates seamlessly with LangChain (Python and JavaScript), the popular open-source framework for building LLM applications.\n\n---\n\nResult 2:\nTitle: Trace with LangGraph\nLink: https://docs.langchain.comlangsmith/trace-with-langgraph\nContent: LangSmith smoothly integrates with LangGraph (Python and JS) to help you trace agents, whether you're using LangChain modules or other SDKs.\n\n## With LangChain\n\nIf you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.\n\nThis guide will walk through a basic example. For more detailed information on configuration, see the [Trace With LangChain](/langsmith/trace-with-langchain) guide.\n\n### 1. Installation\n\nInstall the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).\n\nFor a full list of packages available, see the [LangChain Python docs](https://python.langchain.com/docs/integrations/platforms/) and [LangChain JS docs](https://js.langchain.com/docs/integrations/platforms/).\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain_openai langgraph\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/openai @langchain/langgraph\n  ```\n\n  ```bash npm theme={null}\n  npm install @langchain/openai @langchain/langgraph\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/openai @langchain/langgraph\n  ```\n</CodeGroup>\n\n### 2. Configure your environment\n\n```bash wrap theme={null}\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n# This example uses OpenAI, but you can use any LLM provider of choice\nexport OPENAI_API_KEY=<your-openai-api-key>\n# For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.\nexport LANGSMITH_WORKSPACE_ID=<your-workspace-id>\n```\n\n<Info>\n  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`\n\n  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`\n\n  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.\n</Info>\n\n### 3. Log a trace\n\nOnce you've set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from typing import Literal\n  from langchain.messages import HumanMessage\n  from langchain_openai import ChatOpenAI\n  from langchain.tools import tool\n  from langgraph.prebuilt import ToolNode\n  from langgraph.graph import StateGraph, MessagesState\n\n  @tool\n  def search(query: str):\n      \"\"\"Call to surf the web.\"\"\"\n      if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n          return \"It's 60 degrees and foggy.\"\n      return \"It's 90 degrees and sunny.\"\n\n  tools = [search]\n  tool_node = ToolNode(tools)\n\n  model = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\n  def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n      messages = state['messages']\n      last_message = messages[-1]\n      if last_message.tool_calls:\n          return \"tools\"\n      return \"__end__\"\n\n  def call_model(state: MessagesState):\n      messages = state['messages']\n      # Invoking `model` will automatically infer the correct tracing context\n      response = model.invoke(messages)\n      return {\"messages\": [response]}\n\n  workflow = StateGraph(MessagesState)\n  workflow.add_node(\"agent\", call_model)\n  workflow.add_node(\"tools\", tool_node)\n  workflow.add_edge(\"__start__\", \"agent\")\n  workflow.add_conditional_edges(\n      \"agent\",\n      should_continue,\n  )\n  workflow.add_edge(\"tools\", 'agent')\n\n  app = workflow.compile()\n\n  final_state = app.invoke(\n      {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n      config={\"configurable\": {\"thread_id\": 42}}\n  )\n\n  final_state[\"messages\"][-1].content\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n  import { tool } from \"@langchain/core/tools\";\n  import { z } from \"zod\";\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { StateGraph, StateGraphArgs } from \"@langchain/langgraph\";\n  import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n\n  interface AgentState {\n    messages: HumanMessage[];\n  }\n\n  const graphState: StateGraphArgs<AgentState>[\"channels\"] = {\n    messages: {\n      reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),\n    },\n  };\n\n  const searchTool = tool(async ({ query }: { query: string }) => {\n    if (query.toLowerCase().includes(\"sf\") || query.toLowerCase().includes(\"san francisco\")) {\n      return \"It's 60 degrees and foggy.\"\n    }\n    return \"It's 90 degrees and sunny.\"\n  }, {\n    name: \"search\",\n    description:\n      \"Call to surf the web.\",\n    schema: z.object({\n      query: z.string().describe(\"The query to use in your search.\"),\n    }),\n  });\n\n  const tools = [searchTool];\n  const toolNode = new ToolNode<AgentState>(tools);\n\n  const model = new ChatOpenAI({\n    model: \"gpt-4o\",\n    temperature: 0,\n  }).bindTools(tools);\n\n  function shouldContinue(state: AgentState) {\n    const messages = state.messages;\n    const lastMessage = messages[messages.length - 1] as AIMessage;\n    if (lastMessage.tool_calls?.length) {\n      return \"tools\";\n    }\n    return \"__end__\";\n  }\n\n  async function callModel(state: AgentState) {\n    const messages = state.messages;\n    // Invoking `model` will automatically infer the correct tracing context\n    const response = await model.invoke(messages);\n    return { messages: [response] };\n  }\n\n  const workflow = new StateGraph<AgentState>({ channels: graphState })\n    .addNode(\"agent\", callModel)\n    .addNode(\"tools\", toolNode)\n    .addEdge(\"__start__\", \"agent\")\n    .addConditionalEdges(\"agent\", shouldContinue)\n    .addEdge(\"tools\", \"agent\");\n\n  const app = workflow.compile();\n\n  const finalState = await app.invoke(\n    { messages: [new HumanMessage(\"what is the weather in sf\")] },\n    { configurable: { thread_id: \"42\" } }\n  );\n\n  finalState.messages[finalState.messages.length - 1].content;\n  ```\n</CodeGroup>\n\nAn example trace from running the above code [looks like this](https://smith.langchain.com/public/10863294-ee79-484a-927f-0558230f1547/r):\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a589f14351fb48e721205d1e363753ea\" alt=\"Trace tree for a LangGraph run with LangChain\" data-og-width=\"3314\" width=\"3314\" data-og-height=\"1766\" height=\"1766\" data-path=\"langsmith/images/langgraph-with-langchain-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=01650d2547ba6e440a66ceb0bdeb566a 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c0d7d3f04d58edd25e9b99d2a61890ce 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf074976fbc12b0baad0e8320fd7fa47 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=627f66d7b9e2b8a054e66e8dbc7afa73 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7946baa3ad54fa851cd8a34eb31b47b0 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6b8e6a216b312f5ae3c74503fabba63 2500w\" />\n\n## Without LangChain\n\nIf you are using other SDKs or custom functions within LangGraph, you will need to [wrap or decorate them appropriately](/langsmith/annotate-code#use-traceable--traceable) (with the `@traceable` decorator in Python or the `traceable` function in JS, or something like e.g. `wrap_openai` for SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.\n\nHere's an example. You can also see this page for more information.\n\n### 1. Installation\n\nInstall the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install openai langsmith langgraph\n  ```\n\n  ```bash yarn theme={null}\n  yarn add openai langsmith @langchain/langgraph\n  ```\n\n  ```bash npm theme={null}\n  npm install openai langsmith @langchain/langgraph\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add openai langsmith @langchain/langgraph\n  ```\n</CodeGroup>\n\n### 2. Configure your environment\n\n```bash wrap theme={null}\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n# This example uses OpenAI, but you can use any LLM provider of choice\nexport OPENAI_API_KEY=<your-openai-api-key>\n```\n\n<Info>\n  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`\n\n  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`\n\n  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.\n</Info>\n\n### 3. Log a trace\n\nOnce you've set up your environment, [wrap or decorate the custom functions/SDKs](/langsmith/annotate-code#use-traceable--traceable) you want to trace. LangSmith will then infer the proper tracing config:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import json\n  import openai\n  import operator\n  from langsmith import traceable\n  from langsmith.wrappers import wrap_openai\n  from typing import Annotated, Literal, TypedDict\n  from langgraph.graph import StateGraph\n\n  class State(TypedDict):\n      messages: Annotated[list, operator.add]\n\n  tool_schema = {\n      \"type\": \"function\",\n      \"function\": {\n          \"name\": \"search\",\n          \"description\": \"Call to surf the web.\",\n          \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\"query\": {\"type\": \"string\"}},\n              \"required\": [\"query\"],\n          },\n      },\n  }\n\n  # Decorating the tool function will automatically trace it with the correct context\n  @traceable(run_type=\"tool\", name=\"Search Tool\")\n  def search(query: str):\n      \"\"\"Call to surf the web.\"\"\"\n      if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n          return \"It's 60 degrees and foggy.\"\n      return \"It's 90 degrees and sunny.\"\n\n  tools = [search]\n\n  def call_tools(state):\n      function_name_to_function = {\"search\": search}\n      messages = state[\"messages\"]\n      tool_call = messages[-1][\"tool_calls\"][0]\n      function_name = tool_call[\"function\"][\"name\"]\n      function_arguments = tool_call[\"function\"][\"arguments\"]\n      arguments = json.loads(function_arguments)\n      function_response = function_name_to_function[function_name](**arguments)\n      tool_message = {\n          \"tool_call_id\": tool_call[\"id\"],\n          \"role\": \"tool\",\n          \"name\": function_name,\n          \"content\": function_response,\n      }\n      return {\"messages\": [tool_message]}\n\n  wrapped_client = wrap_openai(openai.Client())\n\n  def should_continue(state: State) -> Literal[\"tools\", \"__end__\"]:\n      messages = state[\"messages\"]\n      last_message = messages[-1]\n      if last_message[\"tool_calls\"]:\n          return \"tools\"\n      return \"__end__\"\n\n  def call_model(state: State):\n      messages = state[\"messages\"]\n      # Calling the wrapped client will automatically infer the correct tracing context\n      response = wrapped_client.chat.completions.create(\n          messages=messages, model=\"gpt-4o-mini\", tools=[tool_schema]\n      )\n      raw_tool_calls = response.choices[0].message.tool_calls\n      tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []\n      response_message = {\n          \"role\": \"assistant\",\n          \"content\": response.choices[0].message.content,\n          \"tool_calls\": tool_calls,\n      }\n      return {\"messages\": [response_message]}\n\n  workflow = StateGraph(State)\n  workflow.add_node(\"agent\", call_model)\n  workflow.add_node(\"tools\", call_tools)\n  workflow.add_edge(\"__start__\", \"agent\")\n  workflow.add_conditional_edges(\n      \"agent\",\n      should_continue,\n  )\n  workflow.add_edge(\"tools\", 'agent')\n\n  app = workflow.compile()\n\n  final_state = app.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n  )\n\n  final_state[\"messages\"][-1][\"content\"]\n  ```\n\n  ```typescript TypeScript theme={null}\n  **Note:** The below example requires `langsmith>=0.1.39` and `@langchain/langgraph>=0.0.31`\n\n  import OpenAI from \"openai\";\n  import { StateGraph } from \"@langchain/langgraph\";\n  import { wrapOpenAI } from \"langsmith/wrappers/openai\";\n  import { traceable } from \"langsmith/traceable\";\n\n  type GraphState = {\n    messages: OpenAI.ChatCompletionMessageParam[];\n  };\n\n  const wrappedClient = wrapOpenAI(new OpenAI({}));\n\n  const toolSchema: OpenAI.ChatCompletionTool = {\n    type: \"function\",\n    function: {\n      name: \"search\",\n      description: \"Use this tool to query the web.\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          query: {\n            type: \"string\",\n          },\n        },\n        required: [\"query\"],\n      }\n    }\n  };\n\n  // Wrapping the tool function will automatically trace it with the correct context\n  const search = traceable(async ({ query }: { query: string }) => {\n    if (\n      query.toLowerCase().includes(\"sf\") ||\n      query.toLowerCase().includes(\"san francisco\")\n    ) {\n      return \"It's 60 degrees and foggy.\";\n    }\n    return \"It's 90 degrees and sunny.\";\n  }, { run_type: \"tool\", name: \"Search Tool\" });\n\n  const callTools = async ({ messages }: GraphState) => {\n    const mostRecentMessage = messages[messages.length - 1];\n    const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;\n    if (toolCalls === undefined || toolCalls.length === 0) {\n      throw new Error(\"No tool calls passed to node.\");\n    }\n    const toolNameMap = {\n      search,\n    };\n    const functionName = toolCalls[0].function.name;\n    const functionArguments = JSON.parse(toolCalls[0].function.arguments);\n    const response = await toolNameMap[functionName](functionArguments);\n    const toolMessage = {\n      tool_call_id: toolCalls[0].id,\n      role: \"tool\",\n      name: functionName,\n      content: response,\n    }\n    return { messages: [toolMessage] };\n  };\n\n  const callModel = async ({ messages }: GraphState) => {\n    // Calling the wrapped client will automatically infer the correct tracing context\n    const response = await wrappedClient.chat.completions.create({\n      messages,\n      model: \"gpt-4o-mini\",\n      tools: [toolSchema],\n    });\n    const responseMessage = {\n      role: \"assistant\",\n      content: response.choices[0].message.content,\n      tool_calls: response.choices[0].message.tool_calls ?? [],\n    };\n    return { messages: [responseMessage] };\n  };\n\n  const shouldContinue = ({ messages }: GraphState) => {\n    const lastMessage =\n      messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;\n    if (\n      lastMessage?.tool_calls !== undefined &&\n      lastMessage?.tool_calls.length > 0\n    ) {\n      return \"tools\";\n    }\n    return \"__end__\";\n  }\n\n  const workflow = new StateGraph<GraphState>({\n    channels: {\n      messages: {\n        reducer: (a: any, b: any) => a.concat(b),\n      }\n    }\n  });\n\n  const graph = workflow\n    .addNode(\"model\", callModel)\n    .addNode(\"tools\", callTools)\n    .addEdge(\"__start__\", \"model\")\n    .addConditionalEdges(\"model\", shouldContinue, {\n      tools: \"tools\",\n      __end__: \"__end__\",\n    })\n    .addEdge(\"tools\", \"model\")\n    .compile();\n\n  await graph.invoke({\n    messages: [{ role: \"user\", content: \"what is the weather in sf\" }]\n  });\n  ```\n</CodeGroup>\n\nAn example trace from running the above code [looks like this](https://smith.langchain.com/public/353f27da-c221-4b67-b9ec-ede3777f3271/r):\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=abe0ae173d182563c343f6596e0ce4e2\" alt=\"Trace tree for a LangGraph run without LangChain\" data-og-width=\"3296\" width=\"3296\" data-og-height=\"1774\" height=\"1774\" data-path=\"langsmith/images/langgraph-without-langchain-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=794e9ce04677bbf721880ebb07ada7c6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6208cbec91ba187ba8f75f6cc916b3f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=418509190558d6a87363d3ba146b7722 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a3c3e2e11cbdac8c32e80e7a895b1eb0 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ea893862eb3f9bc5910649cf0ccd2abe 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2b244ca0ec1296552991428d1efffde6 2500w\" />\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langgraph.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Trace with OpenTelemetry\nLink: https://docs.langchain.comlangsmith/trace-with-opentelemetry\nContent: Trace with OpenTelemetry\nTrace with OpenTelemetry\nLangSmith supports OpenTelemetry-based tracing, allowing you to send traces from any OpenTelemetry-compatible application. This guide covers both automatic instrumentation for LangChain applications and manual instrumentation for other frameworks. Learn how to trace your LLM applications using OpenTelemetry with LangSmith. Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below. For the EU region, use eu.api.smith.langchain.com.\n\nTrace with OpenTelemetry\nLangSmith supports OpenTelemetry-based tracing, allowing you to send traces from any OpenTelemetry-compatible application. This guide covers both automatic instrumentation for LangChain applications and manual instrumentation for other frameworks. Learn how to trace your LLM applications using OpenTelemetry with LangSmith. Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below. For the EU region, use eu.api.smith.langchain.com.\n\nTrace a LangChain application\nIf you're using LangChain or LangGraph, use the built-in integration to trace your application: Install the LangSmith package with OpenTelemetry support: Requires Python SDK version langsmith>=0.3.18 . We recommend langsmith>=0.4.25 to benefit from important OpenTelemetry fixes. In your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the LANGSMITH_OTEL_ENABLED environment variable: Create a LangChain application with tracing. For example: View the traces in your LangSmith dashboard ( example ) once your application runs.\n\n---\n\nResult 4:\nTitle: LangChain overview\nLink: https://docs.langchain.comoss/javascript/langchain/overview\nContent: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/javascript/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/javascript/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain [agents](/oss/javascript/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\n## <Icon icon=\"wand-magic-sparkles\" /> Create an agent\n\n```ts  theme={null}\nimport * as z from \"zod\";\n// npm install @langchain/anthropic to call the model\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }) => `It's always sunny in ${city}!`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  },\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n\nSee the [Installation instructions](/oss/javascript/langchain/install) and [Quickstart guide](/oss/javascript/langchain/quickstart) to get started building your own agents and applications with LangChain.\n\n## <Icon icon=\"star\" size={20} /> Core benefits\n\n<Columns cols={2}>\n  <Card title=\"Standard model interface\" icon=\"arrows-rotate\" href=\"/oss/javascript/langchain/models\" arrow cta=\"Learn more\">\n    Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\n  </Card>\n\n  <Card title=\"Easy to use, highly flexible agent\" icon=\"wand-magic-sparkles\" href=\"/oss/javascript/langchain/agents\" arrow cta=\"Learn more\">\n    LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\n  </Card>\n\n  <Card title=\"Built on top of LangGraph\" icon=\"circle-nodes\" href=\"/oss/javascript/langgraph/overview\" arrow cta=\"Learn more\">\n    LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.\n  </Card>\n\n  <Card title=\"Debug with LangSmith\" icon=\"eye\" href=\"/langsmith/home\" arrow cta=\"Learn more\">\n    Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: LangChain overview\nLink: https://docs.langchain.comoss/python/langchain/overview\nContent: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/python/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/python/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain [agents](/oss/python/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\n## <Icon icon=\"wand-magic-sparkles\" /> Create an agent\n\n```python  theme={null}\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n```\n\nSee the [Installation instructions](/oss/python/langchain/install) and [Quickstart guide](/oss/python/langchain/quickstart) to get started building your own agents and applications with LangChain.\n\n## <Icon icon=\"star\" size={20} /> Core benefits\n\n<Columns cols={2}>\n  <Card title=\"Standard model interface\" icon=\"arrows-rotate\" href=\"/oss/python/langchain/models\" arrow cta=\"Learn more\">\n    Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\n  </Card>\n\n  <Card title=\"Easy to use, highly flexible agent\" icon=\"wand-magic-sparkles\" href=\"/oss/python/langchain/agents\" arrow cta=\"Learn more\">\n    LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\n  </Card>\n\n  <Card title=\"Built on top of LangGraph\" icon=\"circle-nodes\" href=\"/oss/python/langgraph/overview\" arrow cta=\"Learn more\">\n    LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.\n  </Card>\n\n  <Card title=\"Debug with LangSmith\" icon=\"eye\" href=\"/langsmith/home\" arrow cta=\"Learn more\">\n    Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_11568068",
    "id": "f7c0bae6-1467-4e4a-b3c6-5702ea7257f0",
    "content": "{\n  \"collections\": \"LangSmith Observability\",\n  \"total\": 11,\n  \"articles\": [\n    {\n      \"id\": \"7a5d1732-c25b-4a26-9686-52ddfd17926e\",\n      \"title\": \"Merging LangSmith Runs from Remote Graph\",\n      \"url\": \"https://support.langchain.com/articles/9499391881-merging-langsmith-runs-from-remote-graph\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"adeafe65-c1b6-4620-b3aa-044873fd8c9c\",\n      \"title\": \"Why are my traces being automatically upgraded to extended retention?\",\n      \"url\": \"https://support.langchain.com/articles/3206725817-why-are-my-traces-being-automatically-upgraded-to-extended-retention\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"27a421b4-e877-4a0f-8f38-7b719b189785\",\n      \"title\": \"PagerDuty Alert Severity Not Respecting Configuration\",\n      \"url\": \"https://support.langchain.com/articles/8222693126-pagerduty-alert-severity-not-respecting-configuration\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"c0615bd9-c4a2-4a54-af03-0ee07470ee43\",\n      \"title\": \"How do I configure LangSmith pricing match patterns for AWS Bedrock inference profiles?\",\n      \"url\": \"https://support.langchain.com/articles/4241511543-how-do-i-configure-langsmith-pricing-match-patterns-for-aws-bedrock-inference-profiles\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"57055dee-474d-4635-93a3-79439570f6ac\",\n      \"title\": \"How do I prevent traces from mixing between projects during concurrent requests?\",\n      \"url\": \"https://support.langchain.com/articles/4784014353-how-do-i-prevent-traces-from-mixing-between-projects-during-concurrent-requests\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"0ff2089a-501c-45b8-b557-4947639c5072\",\n      \"title\": \"How do I disable public sharing for traces in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5345880639-how-do-i-disable-public-sharing-for-traces-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"e5b95386-5971-4e48-be5b-9d9c7dcfe8e3\",\n      \"title\": \"How do I archive a prompt instead of deleting it?\",\n      \"url\": \"https://support.langchain.com/articles/6404244010-how-do-i-archive-a-prompt-instead-of-deleting-it\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"5886208c-1f87-4672-87c4-053f2773c161\",\n      \"title\": \"How do I filter out traces that contain a specific node name in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8185473222-how-do-i-filter-out-traces-that-contain-a-specific-node-name-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b5128a24-178a-4063-9337-c273af5ac617\",\n      \"title\": \"Why is my trace exploration slow in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9719508217-why-is-my-trace-exploration-slow-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"f41e0fb9-7ed0-4d9b-85b0-fe88fd805f02\",\n      \"title\": \"Why am I getting service interruption errors when using text search filters?\",\n      \"url\": \"https://support.langchain.com/articles/6919527577-why-am-i-getting-service-interruption-errors-when-using-text-search-filters\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\",\n      \"title\": \"Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\",\n      \"url\": \"https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\",\n      \"collection\": \"LangSmith Observability\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_14987747",
    "id": "3bb8cc9e-302a-4c2e-8f97-4dd0bd29afc8",
    "content": "{\n  \"collections\": \"Troubleshooting\",\n  \"total\": 0,\n  \"articles\": [],\n  \"note\": \"No articles found\"\n}"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_69111855",
        "function": {
          "name": "get_article_content",
          "arguments": "{\"article_id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\"}"
        }
      },
      {
        "type": "function",
        "id": "call_37882978",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"langsmith trace size limit\"}"
        }
      },
      {
        "type": "function",
        "id": "call_63383087",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"run size limits\"}"
        }
      },
      {
        "type": "function",
        "id": "call_34685791",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"input output limits\"}"
        }
      }
    ],
    "id": "lc_run--019b3a30-a165-7df1-a945-a8bcbb2415b9",
    "content": ""
  },
  {
    "role": "tool",
    "name": "get_article_content",
    "tool_call_id": "call_69111855",
    "id": "70564b5f-5ca6-4b69-b122-45b0c69811ec",
    "content": "ID: 992b5c54-45de-4dd3-9e10-b5020ca21e64\nTitle: Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\nURL: https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\nCollection: LangSmith\n\nContent:\n<h1 style=\"padding-left: 0px!important;\" level=\"1\" id=\"\">Context</h1><p style=\"padding-left: 0px!important;\">Customers may need to configure specific data retention periods for their traces on LangSmith Cloud to comply with their organization's data retention policies. By default, trace data is stored for either 14 days or 400 days depending on the plan.</p><h1 style=\"padding-left: 0px!important;\" level=\"1\" id=\"\">Answer</h1><p style=\"padding-left: 0px!important;\">Currently, data retention periods are not configurable within the platform. The system offers two fixed retention periods:</p><ul><li><p style=\"padding-left: 0px!important;\">14 days retention</p></li><li><p style=\"padding-left: 0px!important;\">400 days retention</p></li></ul><p style=\"padding-left: 0px!important;\">While direct configuration of retention periods is not available, there are two workaround options for customers who need custom retention periods:</p><ol><li><p style=\"padding-left: 0px!important;\">Use the programmatic deletion API to remove traces after your desired retention period</p></li><li><p style=\"padding-left: 0px!important;\">Set up an automated job to handle trace deletion according to your retention requirements</p></li></ol><div data-color=\"gray\" data-icon=\"lightBulb\" data-type=\"callout\"><p style=\"padding-left: 0px!important;\">Note: The ability to configure custom retention periods is expected to be developed in the future as part of platform improvements to the data layer.</p></div><p style=\"padding-left: 0px!important;\"></p>"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_37882978",
    "id": "8ec29717-41da-4f4d-bc4d-d765c6d92d15",
    "content": "Result 1:\nTitle: LangSmith docs\nLink: https://docs.langchain.comlangsmith/home\nContent: **LangSmith provides tools for developing, debugging, and deploying LLM applications.**\nIt helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.\nLangSmith is framework agnostic, so you can use it with or without LangChain's open-source libraries\n[`langchain`](/oss/python/langchain/overview) and [`langgraph`](/oss/python/langgraph/overview).\nPrototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.\n\n<Callout icon=\"bullhorn\" color=\"#DFC5FE\" iconType=\"regular\">\n  LangGraph Platform is now [LangSmith Deployment](/langsmith/deployments). For more information, check out the [Changelog](https://changelog.langchain.com/announcements/product-naming-changes-langsmith-deployment-and-langsmith-studio).\n</Callout>\n\n## Get started\n\n<Steps>\n  <Step title=\"Create an account\" icon=\"user-plus\">\n    Sign up at [smith.langchain.com](https://smith.langchain.com) (no credit card required).\n    You can log in with **Google**, **GitHub**, or **email**.\n  </Step>\n\n  <Step title=\"Create an API key\" icon=\"key\">\n    Go to your [Settings page](https://smith.langchain.com/settings) \u2192 **API Keys** \u2192 **Create API Key**.\n    Copy the key and save it securely.\n  </Step>\n</Steps>\n\nOnce your account and API key are ready, choose a quickstart to begin building with LangSmith:\n\n<Columns cols={3}>\n  <Card title=\"Observability\" icon=\"magnifying-glass\" href=\"/langsmith/observability-quickstart\" arrow=\"true\" cta=\"Start tracing\">\n    Gain visibility into every step your application takes to debug faster and improve reliability.\n  </Card>\n\n  <Card title=\"Evaluation\" icon=\"chart-line\" href=\"/langsmith/evaluation-quickstart\" arrow=\"true\" cta=\"Evaluate your app\">\n    Measure and track quality over time to ensure your AI applications are consistent and trustworthy.\n  </Card>\n\n  <Card title=\"Deployment\" icon=\"cloud-arrow-up\" iconType=\"solid\" href=\"/langsmith/deployments\" arrow=\"true\" cta=\"Deploy your agents\">\n    Deploy your agents as Agent Servers, ready to scale in production.\n  </Card>\n</Columns>\n\n### More ways to build\n\n<Columns cols={2}>\n  <Card title=\"Platform setup\" icon=\"server\" iconType=\"solid\" href=\"/langsmith/platform-setup\" arrow=\"true\" cta=\"Choose how to set up LangSmith\">\n    Use LangSmith in managed cloud, in a self-hosted environment, or hybrid to match your infrastructure and compliance needs.\n  </Card>\n\n  <Card title=\"Agent Builder (Beta)\" icon=\"sparkles\" href=\"/langsmith/agent-builder\" arrow=\"true\" cta=\"Build an agent\">\n    Design and deploy AI agents visually with a no-code interface\u2014perfect for rapid prototyping and getting started without writing code.\n  </Card>\n\n  <Card title=\"Studio\" icon=\"window\" href=\"/langsmith/quick-start-studio\" arrow=\"true\" cta=\"Develop with Studio\">\n    Use a visual interface to design, test, and refine applications end-to-end.\n  </Card>\n\n  <Card title=\"Prompt testing\" icon=\"flask\" href=\"/langsmith/prompt-engineering-quickstart\" arrow=\"true\" cta=\"Test your prompts\">\n    Iterate on prompts with built-in versioning and collaboration to ship improvements faster.\n  </Card>\n</Columns>\n\n<Callout icon=\"lock\" color=\"#DFC5FE\" iconType=\"regular\">\n  LangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the [Trust Center](https://trust.langchain.com/).\n</Callout>\n\n## Workflow\n\nLangSmith combines observability, evaluation, deployment, and platform setup in one integrated workflow\u2014from local development to production.\n\n<img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=84312d9c72c44e44ac513eaa78abefc6\" alt=\"Diagram showing how LangSmith integrates observability, evaluation, deployment, and platform setup in a single workflow from development to production.\" data-og-width=\"1138\" width=\"1138\" data-og-height=\"549\" height=\"549\" data-path=\"langsmith/images/overview-light.svg\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=280&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=32fa495af106d1eb30c9d512b59c3926 280w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=560&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=f30ac54ecdc55dd22b5762800f33a5bb 560w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=840&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=aee1643d7b4ab4cf74ffb54b555e3788 840w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=1100&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=65946469f91ff9cfbd26d154d31c6caf 1100w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=1650&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=a310e56592be23e233075ebffa1b124d 1650w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=2500&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=f9e2edcac083797d0498806cb512d5de 2500w\" />\n\n<img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=f930d2f475ab68e1cdfe7105b5f1abe6\" alt=\"Diagram showing how LangSmith integrates observability, evaluation, deployment, and platform setup in a single workflow from development to production.\" data-og-width=\"1157\" width=\"1157\" data-og-height=\"549\" height=\"549\" data-path=\"langsmith/images/overview-dark.svg\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=280&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=cab90851ba7c49d855f89c86f6bfdd0b 280w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=560&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=d04a2b423106d1f64fa0a3a05948a19a 560w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=840&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=84c821c5464a20cf4a3d397e5baf7357 840w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=1100&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=202f8ed1e7c38c5a6184b9e1b2ea377f 1100w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=1650&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=f8d03af520cd2f0363f9b41f86428396 1650w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=2500&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=576885f21fd2e6bc4ecb4e4176515a3d 2500w\" />\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/home.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Self-host LangSmith on Kubernetes\nLink: https://docs.langchain.comlangsmith/kubernetes\nContent: <Info>\n  Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and [contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.\n</Info>\n\nThis page describes how to set up **LangSmith** (observability, tracing, and evaluation) in a Kubernetes cluster. You'll use Helm to install LangSmith and its dependencies.\n\nAfter completing this page, you'll have:\n\n* **LangSmith UI and APIs**: for [observability](/langsmith/observability), tracing, and [evaluation](/langsmith/evaluation).\n* **Backend services**: (queue, playground, ACE).\n* **Datastores**: (PostgreSQL, Redis, ClickHouse, optional blob storage).\n\nFor [agent deployment](/langsmith/deployments): To add deployment capabilities, complete this guide first, then follow [Enable LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform).\n\nLangChain has successfully tested LangSmith on the following Kubernetes distributions:\n\n* Google Kubernetes Engine (GKE)\n* Amazon Elastic Kubernetes Service (EKS): For architecture patterns and best practices, refer to [self-hosting on AWS](/langsmith/aws-self-hosted).\n* Azure Kubernetes Service (AKS): For architecture patterns and best practices, refer to [self-hosting on AWS](/langsmith/azure-self-hosted).\n* OpenShift (4.14+)\n* Minikube and Kind (for development purposes)\n\n<Note>\n  LangChain provides Terraform modules to help provision infrastructure for LangSmith. These modules can quickly set up Kubernetes clusters, storage, and networking for your deployment.\n\n  Available modules:\n\n  * [AWS Terraform modules](https://github.com/langchain-ai/terraform/tree/main/modules/aws)\n  * [Azure Terraform modules](https://github.com/langchain-ai/terraform/tree/main/modules/azure)\n\n  View the [full Terraform repository](https://github.com/langchain-ai/terraform) for documentation and additional resources.\n</Note>\n\n## Prerequisites\n\nEnsure you have the following tools/items ready. Some items are marked optional:\n\n1. LangSmith License Key\n\n   1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.\n\n2. Api Key Salt\n\n   1. This is a secret key that you can generate. It should be a random string of characters.\n   2. You can generate this using the following command:\n\n   ```bash  theme={null}\n   openssl rand -base64 32\n   ```\n\n3. JWT Secret (Optional but used for basic auth)\n\n   1. This is a secret key that you can generate. It should be a random string of characters.\n   2. You can generate this using the following command:\n\n   ```bash  theme={null}\n   openssl rand -base64 32\n   ```\n\n### Databases\n\nLangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud provider\u2019s managed services.\n\nFor more information, refer to the following setup guides for external services:\n\n* [PostgreSQL](/langsmith/self-host-external-postgres)\n* [Redis](/langsmith/self-host-external-redis)\n* [ClickHouse](/langsmith/self-host-external-clickhouse)\n\n### Kubernetes cluster requirements\n\n1. You will need a working Kubernetes cluster that you can access via `kubectl`. Your cluster should have the following minimum requirements:\n\n   1. Recommended: At least 16 vCPUs, 64GB Memory available\n\n      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found [here](/langsmith/self-host-scale).\n      * We recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage.\n      * We recommend setting up the metrics server so that autoscaling can be turned on.\n      * If you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory **allocatable** as ClickHouse will request this amount of resources by default.\n\n   2. Valid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)\n\n      * To enable persistence, we will try to provision volumes for any database running in-cluster.\n      * If using PVs in your cluster, we highly recommend setting up backups in a production environment.\n      * **We strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.**\n      * On EKS, you may need to ensure you have the `ebs-csi-driver` installed and configured for dynamic provisioning. Refer to the [EBS CSI Driver documentation](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) for more information.\n\n      You can verify this by running:\n\n      ```bash  theme={null}\n      kubectl get storageclass\n      ```\n\n      The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:\n\n      ```bash  theme={null}\n      NAME            PROVISIONER                 RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\n      gp2 (default)   ebs.csi.eks.amazonaws.com   Delete          WaitForFirstConsumer   true                   161d\n      ```\n\n      <Note>\n        We highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.\n      </Note>\n\n      Refer to the [Kubernetes documentation](https://kubernetes.io/do/langsmith/observability-concepts/storage/storage-classes/) for more information on storage classes.\n\n2. Helm\n\n   1. To install `helm` refer to the [Helm documentation](https://helm.sh/docs/intro/install/)\n\n3. Egress to `https://beacon.langchain.com` (if not running in offline mode)\n\n   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.\n\n## Configure your Helm Charts:\n\n1. Create a new file called `langsmith_config.yaml` with the configuration options from the previous step.\n   1. There are several configuration options that you can set in the `langsmith_config.yaml` file. You can find more information on specific configuration options in the [Configuration](/langsmith/self-hosted) section.\n   2. If you are new to Kubernetes or Helm, we\u2019d recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: [LangSmith helm chart examples](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/examples).\n   3. You can see a full list of configuration options in the `values.yaml` file in the Helm Chart repository here: [LangSmith Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/values.yaml)\n\n<Warning>\n  Only override the settings you need in `langsmith_config.yaml`; don\u2019t copy the entire `values.yaml`.\n  Keeping your config minimal ensures you continue to inherit new defaults and upgrades from the Helm chart.\n</Warning>\n\n2. At a minimum, you will need to set the following configuration options (using basic auth):\n\n   ```yaml  theme={null}\n   config:\n     langsmithLicenseKey: \"<your license key>\"\n     apiKeySalt: \"<your api key salt>\"\n     authType: mixed\n     basicAuth:\n       enabled: true\n       initialOrgAdminEmail: \"admin@langchain.dev\" # Change this to your admin email address\n       initialOrgAdminPassword: \"secure-password\" # Must be at least 12 characters long and have at least one lowercase, uppercase, and symbol\n       jwtSecret: <your jwt salt> # A random string of characters used to sign JWT tokens for basic auth.\n   ```\n\nYou will also need to specify connection details for any external databases you are using.\n\n## Deploying to Kubernetes:\n\n1. Verify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)\n\n   1. Run `kubectl get pods`\n\n      Output should look something like:\n\n      ```bash  theme={null}\n      kubectl get pods                                                                                                                                                                     \u2388 langsmith-eks-2vauP7wf 21:07:46No resources found in default namespace.\n      ```\n\n   <Note>\n     If you are using a namespace other than the default namespace, you will need to specify the namespace in the `helm` and `kubectl` commands by using the `-n <namespace>` flag.\n   </Note>\n\n2. Ensure you have the LangChain Helm repo added. (skip this step if you are using local charts)\n\n   ```bash  theme={null}\n   helm repo add langchain https://langchain-ai.github.io/helm/\"langchain\" has been added to your repositories\n   ```\n\n3. Find the latest version of the chart. You can find the available versions in the [Helm Chart repository](https://github.com/langchain-ai/helm/releases).\n\n   * We generally recommend using the latest version.\n   * You can also run `helm search repo langchain/langsmith --versions` to see the available versions. The output will look something like this:\n\n   ```\n   langchain/langsmith     0.10.14         0.10.32         Helm chart to deploy the langsmith application ...\n   langchain/langsmith     0.10.13         0.10.32         Helm chart to deploy the langsmith application ...\n   langchain/langsmith     0.10.12         0.10.32         Helm chart to deploy the langsmith application ...\n   langchain/langsmith     0.10.11         0.10.29         Helm chart to deploy the langsmith application ...\n   langchain/langsmith     0.10.10         0.10.29         Helm chart to deploy the langsmith application ...\n   ```\n\n4. Run `helm upgrade -i langsmith langchain/langsmith --values langsmith_config.yaml --version <version> -n <namespace> --wait --debug`\n\n   * Replace `<namespace>` with the namespace you want to deploy LangSmith to.\n   * Replace `<version>` with the version of LangSmith you want to install from the previous step. Most users should install the latest version available.\n\n   Once the `helm install` command runs and finishes successfully, you should see output similar to this:\n\n   ```\n   NAME: langsmith\n   LAST DEPLOYED: Fri Sep 17 21:08:47 2021\n   NAMESPACE: langsmith\n   STATUS: deployed\n   REVISION: 1\n   TEST SUITE: None\n   ```\n\n   This may take a few minutes to complete as it will create several Kubernetes resources and run several jobs to initialize the database and other services.\n\n5. Run `kubectl get pods` Output should now look something like this (note the exact pod names may vary based on the version and configuration you used):\n\n   ```\n   langsmith-backend-6ff46c99c4-wz22d       1/1     Running   0          3h2m\n   langsmith-frontend-6bbb94c5df-8xrlr      1/1     Running   0          3h2m\n   langsmith-hub-backend-5cc68c888c-vppjj   1/1     Running   0          3h2m\n   langsmith-playground-6d95fd8dc6-x2d9b    1/1     Running   0          3h2m\n   langsmith-postgres-0                     1/1     Running   0          9h\n   langsmith-queue-5898b9d566-tv6q8         1/1     Running   0          3h2m\n   langsmith-redis-0                        1/1     Running   0          9h\n   ```\n\n## Validate your deployment:\n\n1. Run `kubectl get services`\n\n   Output should look something like:\n\n   ```\n   NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)        AGE\n   langsmith-backend       ClusterIP      172.20.140.77    <none>                                                                    1984/TCP       35h\n   langsmith-frontend      LoadBalancer   172.20.253.251   <external ip>                                                             80:31591/TCP   35h\n   langsmith-hub-backend   ClusterIP      172.20.112.234   <none>                                                                    1985/TCP       35h\n   langsmith-playground    ClusterIP      172.20.153.194   <none>                                                                    3001/TCP       9h\n   langsmith-postgres      ClusterIP      172.20.244.82    <none>                                                                    5432/TCP       35h\n   langsmith-redis         ClusterIP      172.20.81.217    <none>                                                                    6379/TCP       35h\n   ```\n\n2. Curl the external ip of the `langsmith-frontend` service:\n\n   ```bash  theme={null}\n   curl <external ip>/api/tenants\n   ```\n\n   Expected output:\n\n   ```json  theme={null}\n   [{\"id\":\"00000000-0000-0000-0000-000000000000\",\"has_waitlist_access\":true,\"created_at\":\"2023-09-13T18:25:10.488407\",\"display_name\":\"Personal\",\"config\":{\"is_personal\":true,\"max_identities\":1},\"tenant_handle\":\"default\"}]\n   ```\n\n3. Visit the external ip for the `langsmith-frontend` service on your browser\n\n   The LangSmith UI should be visible/operational\n\n   <img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5310f686e7b9eebaaee4fe2a152a8675\" alt=\"Langsmith ui\" data-og-width=\"2886\" width=\"2886\" data-og-height=\"1698\" height=\"1698\" data-path=\"langsmith/images/langsmith-ui.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f155ce778ca848f89fefff237b69bcb 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d55d4068a9f53387c129b4688b0971e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=feb20198d67249ece559e5fd0e6d8e98 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3e5eba764d911e567d5aaa9e5702327b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d45af56632578a8d1b05e546dfc8d01d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16a49517a6c224930fdb81c9ccde5527 2500w\" />\n\n## Using LangSmith\n\nNow that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [self-hosted usage guide](/langsmith/self-hosted).\n\nYour LangSmith instance is now running but may not be fully setup yet.\n\nIf you used one of the basic configs, you will have a default admin user account created for you. You can log in with the email address and password you specified in the `langsmith_config.yaml` file.\n\nAs a next step, it is strongly recommended you work with your infrastructure administrators to:\n\n* Setup DNS for your LangSmith instance to enable easier access\n* Configure SSL to ensure in-transit encryption of traces submitted to LangSmith\n* Configure LangSmith with [Single Sign-On](/langsmith/self-host-sso) to secure your LangSmith instance\n* Connect LangSmith to external Postgres and Redis instances\n* Set up [Blob Storage](/langsmith/self-host-blob-storage) for storing large files\n\nReview our [configuration section](/langsmith/self-hosted) for more information on how to configure these options.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/kubernetes.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Document API authentication in OpenAPI\nLink: https://docs.langchain.comlangsmith/openapi-security\nContent: This guide shows how to customize the OpenAPI security schema for your LangSmith API documentation. A well-documented security schema helps API consumers understand how to authenticate with your API and even enables automatic client generation. See the [Authentication & Access Control conceptual guide](/langsmith/auth) for more details about LangGraph's authentication system.\n\n<Note>\n  **Implementation vs Documentation**\n  This guide only covers how to document your security requirements in OpenAPI. To implement the actual authentication logic, see [How to add custom authentication](/langsmith/custom-auth).\n</Note>\n\nThis guide applies to all LangSmith deployments (Cloud and self-hosted). It does not apply to usage of the LangGraph open source library if you are not using LangSmith.\n\n## Default Schema\n\nThe default security scheme varies by deployment type:\n\n<Tabs>\n  <Tab title=\"LangSmith\" />\n</Tabs>\n\nBy default, LangSmith requires a LangSmith API key in the `x-api-key` header:\n\n```yaml  theme={null}\ncomponents:\n  securitySchemes:\n    apiKeyAuth:\n      type: apiKey\n      in: header\n      name: x-api-key\nsecurity:\n  - apiKeyAuth: []\n```\n\nWhen using one of the LangGraph SDK's, this can be inferred from environment variables.\n\n<Tabs>\n  <Tab title=\"Self-hosted\" />\n</Tabs>\n\nBy default, self-hosted deployments have no security scheme. This means they are to be deployed only on a secured network or with authentication. To add custom authentication, see [How to add custom authentication](/langsmith/custom-auth).\n\n## Custom Security Schema\n\nTo customize the security schema in your OpenAPI documentation, add an `openapi` field to your `auth` configuration in `langgraph.json`. Remember that this only updates the API documentation - you must also implement the corresponding authentication logic as shown in [How to add custom authentication](/langsmith/custom-auth).\n\nNote that LangSmith does not provide authentication endpoints - you'll need to handle user authentication in your client application and pass the resulting credentials to the LangGraph API.\n\n<Tabs>\n  <Tab title=\"OAuth2 with Bearer Token\">\n    ```json  theme={null}\n    {\n      \"auth\": {\n        \"path\": \"./auth.py:my_auth\",  // Implement auth logic here\n        \"openapi\": {\n          \"securitySchemes\": {\n            \"OAuth2\": {\n              \"type\": \"oauth2\",\n              \"flows\": {\n                \"implicit\": {\n                  \"authorizationUrl\": \"https://your-auth-server.com/oauth/authorize\",\n                  \"scopes\": {\n                    \"me\": \"Read information about the current user\",\n                    \"threads\": \"Access to create and manage threads\"\n                  }\n                }\n              }\n            }\n          },\n          \"security\": [\n            {\"OAuth2\": [\"me\", \"threads\"]}\n          ]\n        }\n      }\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"API Key\">\n    ```json  theme={null}\n    {\n      \"auth\": {\n        \"path\": \"./auth.py:my_auth\",  // Implement auth logic here\n        \"openapi\": {\n          \"securitySchemes\": {\n            \"apiKeyAuth\": {\n              \"type\": \"apiKey\",\n              \"in\": \"header\",\n              \"name\": \"X-API-Key\"\n            }\n          },\n          \"security\": [\n            {\"apiKeyAuth\": []}\n          ]\n        }\n      }\n    }\n    ```\n  </Tab>\n</Tabs>\n\n## Testing\n\nAfter updating your configuration:\n\n1. Deploy your application\n2. Visit `/docs` to see the updated OpenAPI documentation\n3. Try out the endpoints using credentials from your authentication server (make sure you've implemented the authentication logic first)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/openapi-security.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: LangSmith Observability\nLink: https://docs.langchain.comoss/javascript/langgraph/observability\nContent: Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use [LangSmith](https://smith.langchain.com/) to visualize these execution steps. To use it, [enable tracing for your application](/langsmith/trace-with-langgraph). This enables you to do the following:\n\n* [Debug a locally running application](/langsmith/observability-studio#debug-langsmith-traces).\n* [Evaluate the application performance](/oss/javascript/langchain/evals).\n* [Monitor the application](/langsmith/dashboards).\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n* **A LangSmith account**: Sign up (for free) or log in at [smith.langchain.com](https://smith.langchain.com).\n* **A LangSmith API key**: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide.\n\n## Enable tracing\n\nTo enable tracing for your application, set the following environment variables:\n\n```python  theme={null}\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n```\n\nBy default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).\n\nFor more information, see [Trace with LangGraph](/langsmith/trace-with-langgraph).\n\n## Trace selectively\n\nYou may opt to trace specific invocations or parts of your application using LangSmith's `tracing_context` context manager:\n\n```ts  theme={null}\nimport { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";\n\n// This WILL be traced\nconst tracer = new LangChainTracer();\nawait agent.invoke(\n  {\n    messages: [{role: \"user\", content: \"Send a test email to alice@example.com\"}]\n  },\n  { callbacks: [tracer] }\n);\n\n// This will NOT be traced (if LANGSMITH_TRACING is not set)\nawait agent.invoke(\n  {\n    messages: [{role: \"user\", content: \"Send another email\"}]\n  }\n);\n```\n\n## Log to a project\n\n<Accordion title=\"Statically\">\n  You can set a custom project name for your entire application by setting the `LANGSMITH_PROJECT` environment variable:\n\n  ```bash  theme={null}\n  export LANGSMITH_PROJECT=my-agent-project\n  ```\n</Accordion>\n\n<Accordion title=\"Dynamically\">\n  You can set the project name programmatically for specific operations:\n\n  ```ts  theme={null}\n  import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";\n\n  const tracer = new LangChainTracer({ projectName: \"email-agent-test\" });\n  await agent.invoke(\n    {\n      messages: [{role: \"user\", content: \"Send a test email to alice@example.com\"}]\n    },\n    { callbacks: [tracer] }\n  );\n  ```\n</Accordion>\n\n## Add metadata to traces\n\nYou can annotate your traces with custom metadata and tags:\n\n```ts  theme={null}\nimport { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";\n\nconst tracer = new LangChainTracer({ projectName: \"email-agent-test\" });\nawait agent.invoke(\n  {\n    messages: [{role: \"user\", content: \"Send a test email to alice@example.com\"}]\n  },\n  config: {\n    tags: [\"production\", \"email-assistant\", \"v1.0\"],\n    metadata: {\n      userId: \"user123\",\n      sessionId: \"session456\",\n      environment: \"production\"\n    }\n  },\n);\n\n```\n\nThis custom metadata and tags will be attached to the trace in LangSmith.\n\n<Tip>\n  To learn more about how to use traces to debug, evaluate, and monitor your agents, see the [LangSmith documentation](/langsmith/home).\n</Tip>\n\n## Use anonymizers to prevent logging of sensitive data in traces\n\nYou may want to mask sensitive data to prevent it from being logged to LangSmith. You can create [anonymizers](/langsmith/mask-inputs-outputs#rule-based-masking-of-inputs-and-outputs) and apply them to\nyour graph using configuration. This example will redact anything matching the Social Security Number format XXX-XX-XXXX from traces sent to LangSmith.\n\n```typescript TypeScript theme={null}\nimport { StateGraph } from \"@langchain/langgraph\";\nimport { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";\nimport { StateAnnotation } from \"./state.js\";\nimport { createAnonymizer } from \"langsmith/anonymizer\"\nimport { Client } from \"langsmith\"\n\n\nconst anonymizer = createAnonymizer([\n    # Matches SSNs\n    { pattern: /\\b\\d{3}-?\\d{2}-?\\d{4}\\b/, replace: \"<ssn>\" }\n])\n\nconst langsmithClient = new Client({ anonymizer })\nconst tracer = new LangChainTracer({\n  client: langsmithClient,\n});\n\nexport const graph = new StateGraph(StateAnnotation)\n  .compile()\n  .withConfig({\n    callbacks: [tracer],\n});\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/observability.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: LangSmith Deployment\nLink: https://docs.langchain.comoss/python/langchain/deploy\nContent: When you're ready to deploy your LangChain agent to production, LangSmith provides a managed hosting platform designed for agent workloads. Traditional hosting platforms are built for stateless, short-lived web applications, while LangGraph is **purpose-built for stateful, long-running agents** that require persistent state and background execution. LangSmith handles the infrastructure, scaling, and operational concerns so you can deploy directly from your repository.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n* A [GitHub account](https://github.com/)\n* A [LangSmith account](https://smith.langchain.com/) (free to sign up)\n\n## Deploy your agent\n\n### 1. Create a repository on GitHub\n\nYour application's code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the [local server setup guide](/oss/python/langchain/studio#setup-local-agent-server). Then, push your code to the repository.\n\n### 2. Deploy to LangSmith\n\n<Steps>\n  <Step title=\"Navigate to LangSmith Deployment\">\n    Log in to [LangSmith](https://smith.langchain.com/). In the left sidebar, select **Deployments**.\n  </Step>\n\n  <Step title=\"Create new deployment\">\n    Click the **+ New Deployment** button. A pane will open where you can fill in the required fields.\n  </Step>\n\n  <Step title=\"Link repository\">\n    If you are a first time user or adding a private repository that has not been previously connected, click the **Add new account** button and follow the instructions to connect your GitHub account.\n  </Step>\n\n  <Step title=\"Deploy repository\">\n    Select your application's repository. Click **Submit** to deploy. This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.\n  </Step>\n</Steps>\n\n### 3. Test your application in Studio\n\nOnce your application is deployed:\n\n1. Select the deployment you just created to view more details.\n2. Click the **Studio** button in the top right corner. Studio will open to display your graph.\n\n### 4. Get the API URL for your deployment\n\n1. In the **Deployment details** view in LangGraph, click the **API URL** to copy it to your clipboard.\n2. Click the `URL` to copy it to the clipboard.\n\n### 5. Test the API\n\nYou can now test the API:\n\n<Tabs>\n  <Tab title=\"Python\">\n    1. Install LangGraph Python:\n\n    ```shell  theme={null}\n    pip install langgraph-sdk\n    ```\n\n    2. Send a message to the agent:\n\n    ```python  theme={null}\n    from langgraph_sdk import get_sync_client # or get_client for async\n\n    client = get_sync_client(url=\"your-deployment-url\", api_key=\"your-langsmith-api-key\")\n\n    for chunk in client.runs.stream(\n        None,    # Threadless run\n        \"agent\", # Name of agent. Defined in langgraph.json.\n        input={\n            \"messages\": [{\n                \"role\": \"human\",\n                \"content\": \"What is LangGraph?\",\n            }],\n        },\n        stream_mode=\"updates\",\n    ):\n        print(f\"Receiving new event of type: {chunk.event}...\")\n        print(chunk.data)\n        print(\"\\n\\n\")\n    ```\n  </Tab>\n\n  <Tab title=\"Rest API\">\n    ```bash  theme={null}\n    curl -s --request POST \\\n        --url <DEPLOYMENT_URL>/runs/stream \\\n        --header 'Content-Type: application/json' \\\n        --header \"X-Api-Key: <LANGSMITH API KEY> \\\n        --data \"{\n            \\\"assistant_id\\\": \\\"agent\\\", `# Name of agent. Defined in langgraph.json.`\n            \\\"input\\\": {\n                \\\"messages\\\": [\n                    {\n                        \\\"role\\\": \\\"human\\\",\n                        \\\"content\\\": \\\"What is LangGraph?\\\"\n                    }\n                ]\n            },\n            \\\"stream_mode\\\": \\\"updates\\\"\n        }\"\n    ```\n  </Tab>\n</Tabs>\n\n<Tip>\n  LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the [Platform setup overview](/langsmith/platform-setup).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/deploy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_63383087",
    "id": "f555751e-885e-425b-b344-df6415107c0b",
    "content": "Result 1:\nTitle: Cloud\nLink: https://docs.langchain.comlangsmith/cloud\nContent: <Callout icon=\"rocket\" color=\"#4F46E5\" iconType=\"regular\">\n  If you're ready to deploy your app to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or the [full setup guide](/langsmith/deploy-to-cloud). This page explains the Cloud managed architecture for reference.\n</Callout>\n\nThe **Cloud** option is a fully managed model where LangChain hosts and operates all LangSmith infrastructure and services:\n\n* **Fully managed infrastructure**: LangChain handles all infrastructure, updates, scaling, and maintenance.\n* **Deploy from GitHub**: Connect your repositories and deploy with a few clicks.\n* **Automated CI/CD**: Build process is handled automatically by the platform.\n* **LangSmith UI**: Full access to [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), [deployment management](/langsmith/deployments), and [Studio](/langsmith/studio).\n\n|                                               | **Who manages it** | **Where it runs** |\n| --------------------------------------------- | ------------------ | ----------------- |\n| **LangSmith platform (UI, APIs, datastores)** | LangChain          | LangChain's cloud |\n| **Your Agent Servers**                        | LangChain          | LangChain's cloud |\n| **CI/CD for your apps**                       | LangChain          | LangChain's cloud |\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3f0316122425895270d0ecd47b12e139\" alt=\"Cloud deployment: LangChain hosts and manages all components including the UI, APIs, and your Agent Servers.\" data-og-width=\"1425\" width=\"1425\" data-og-height=\"1063\" height=\"1063\" data-path=\"langsmith/images/langgraph-cloud-architecture.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b34a10fb40ca5188dddfb6be69696c75 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d310ce86e8421878575cf4d4fa72bf78 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=7cc3fcee9c36a8d1e8da4cc4abbde3b9 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a184c950c8585e2fdb5e75ac7f0f5642 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=cb778e4a6707b00eaf703886d32569bd 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=00f9d9df44512dded307613502d03299 2500w\" />\n\n## Get started\n\nTo deploy your first application to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or refer to the [comprehensive setup guide](/langsmith/deploy-to-cloud).\n\n## Cloud architecture and scalability\n\n<Note>\n  This section is only relevant for the cloud-managed LangSmith services available at [https://smith.langchain.com](https://smith.langchain.com) and [https://eu.smith.langchain.com](https://eu.smith.langchain.com).\n\n  For information on the self-hosted LangSmith solution, please refer to the [self-hosted documentation](/langsmith/self-hosted).\n</Note>\n\nLangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for LLM application observability, evaluation, and agent deployment\n\n### Architecture\n\nThe US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.\n\n<Note>\n  The [EU-based LangSmith service](https://eu.smith.langchain.com) is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, [contact our sales team](https://www.langchain.com/contact-sales).\n</Note>\n\n#### Regional storage\n\nThe resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses [Supabase](https://supabase.com) for authentication/authorization and [ClickHouse Cloud](https://clickhouse.com/cloud) for data warehouse.\n\n|                                                | US                                                                 | EU                                                                       |\n| ---------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------ |\n| URL                                            | [https://smith.langchain.com](https://smith.langchain.com)         | [https://eu.smith.langchain.com](https://eu.smith.langchain.com)         |\n| API URL                                        | [https://api.smith.langchain.com](https://api.smith.langchain.com) | [https://eu.api.smith.langchain.com](https://eu.api.smith.langchain.com) |\n| GCP                                            | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| Supabase                                       | AWS us-east-1 (N. Virginia)                                        | AWS eu-central-1 (Germany)                                               |\n| ClickHouse Cloud                               | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| [LangSmith deployment](/langsmith/deployments) | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n\nSee the [Regions FAQ](/langsmith/regions-faq) for more information.\n\n#### Region-independent storage\n\nData listed here is stored exclusively in the US:\n\n* Payment and billing information with Stripe and Metronome\n\n#### GCP services\n\nLangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):\n\n* LangSmith Frontend: serves the LangSmith UI.\n* LangSmith Backend: serves the LangSmith API.\n* LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)\n* LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.\n* LangSmith Queue: handles processing of asynchronous tasks. (Internal service)\n\nLangSmith uses the following GCP storage services:\n\n* Google Cloud Storage (GCS) for runs inputs and outputs.\n* Google Cloud SQL PostgreSQL for transactional workloads.\n* Google Cloud Memorystore for Redis for queuing and caching.\n* Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.\n\nSome additional GCP services we use include:\n\n* Google Cloud Load Balancer for routing traffic to the LangSmith services.\n* Google Cloud CDN for caching static assets.\n* Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to [this guide](/langsmith/administration-overview#rate-limits).\n\n<div style={{ textAlign: 'center' }}>\n  <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3\" alt=\"Light mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-light.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w\" />\n\n  <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b\" alt=\"Dark mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w\" />\n</div>\n\n### Allowlisting IP addresses\n\n#### Egress from LangChain SaaS\n\nAll traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:\n\n| US             | EU             |\n| -------------- | -------------- |\n| 34.59.65.97    | 34.13.192.67   |\n| 34.67.51.221   | 34.147.105.64  |\n| 34.46.212.37   | 34.90.22.166   |\n| 34.132.150.88  | 34.147.36.213  |\n| 35.188.222.201 | 34.32.137.113  |\n| 34.58.194.127  | 34.91.238.184  |\n| 34.59.97.173   | 35.204.101.241 |\n| 104.198.162.55 | 35.204.48.32   |\n\nIt may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.\n\n#### Ingress into LangChain SaaS\n\nThe langchain endpoints map to the following static IP addresses:\n\n| US             | EU           |\n| -------------- | ------------ |\n| 34.8.121.39    | 34.95.92.214 |\n| 34.107.251.234 | 34.13.73.122 |\n\nYou may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (`api.smith.langchain.com`, `smith.langchain.com`, `beacon.langchain.com`, `eu.api.smith.langchain.com`, `eu.smith.langchain.com`, `eu.beacon.langchain.com`).\n\n### API rate limits\n\nLangSmith enforces rate limits on API endpoints to ensure service stability and fair usage. The following table shows the rate limits for different endpoints in both US and EU regions. Note that:\n\n* Rate limits are expressed as `count / interval` where count is the number of requests allowed within the interval (in seconds). For example, `2000 / 10` means 2000 requests per 10 seconds.\n* When no HTTP method is specified in the endpoint column, the rate limit applies to all HTTP methods for that endpoint.\n* When a specific method is listed (e.g., `POST`, `GET`), the rate limit applies only to that method.\n\n| Match / Endpoint (method)                   | Identity key     | US prod limit | EU prod limit | Category                                     |\n| ------------------------------------------- | ---------------- | ------------- | ------------- | -------------------------------------------- |\n| OPTIONS, `/info`, `*/v1/metadata/submit`    | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-user-id` + IP | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/v1/beacon`                                | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/repos`                                    | `x-api-key`      | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `/repos`                                    | `x-user-id` + IP | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `POST /runs/batch`                          | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `POST /otel/v1/traces`                      | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST` containing `/charts`                 | `x-api-key`      | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST` containing `/charts`                 | `x-user-id` + IP | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST /runs/multipart`                      | `x-api-key`      | 6000 / 10     | 6000 / 10     | [Multipart ingest](#rate-limit-categories)   |\n| `POST /runs/query`                          | `x-api-key`      | 15 / 10       | 15 / 10       | [Run query (API)](#rate-limit-categories)    |\n| `POST /runs/query`                          | `x-user-id` + IP | 300 / 10      | 300 / 10      | [Run query (User)](#rate-limit-categories)   |\n| `/generate`                                 | `x-api-key`      | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/generate`                                 | `x-user-id` + IP | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/commits`                                  | `x-api-key`      | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `/commits`                                  | `x-user-id` + IP | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `DELETE /sessions` or `*/trigger`           | `x-api-key`      | 10 / 60       | 10 / 60       | [Deletion](#rate-limit-categories)           |\n| `DELETE /sessions` or `*/trigger`           | `x-user-id` + IP | 30 / 60       | 30 / 60       | [Deletion](#rate-limit-categories)           |\n| `POST /runs` (single run ingest)            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `PATCH` containing `/runs`                  | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST /feedback`                            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `GET /runs/{uuid}` or `/api/v1/runs/{uuid}` | `x-api-key`      | 30 / 60       | 30 / 60       | [Run lookup](#rate-limit-categories)         |\n| `GET` containing `/examples`                | `x-api-key`      | 5000 / 60     | 5000 / 60     | [Examples](#rate-limit-categories)           |\n| Any request with `x-api-key`                | `x-api-key`      | 1000 / 10     | 1000 / 10     | [Default (API key)](#rate-limit-categories)  |\n| Any request with `x-user-id`                | `x-user-id` + IP | 1000 / 10     | 1000 / 10     | [Default (User)](#rate-limit-categories)     |\n| `/public/download`                          | IP               | 5000 / 60     | 5000 / 60     | [Public download](#rate-limit-categories)    |\n| `/runs/stats`                               | `x-api-key`      | 1 / 10        | 20 / 10       | [Stats](#rate-limit-categories)              |\n| All other IPs (catch-all)                   | IP               | 100 / 60      | 100 / 60      | [Public (catch-all)](#rate-limit-categories) |\n\n#### Rate limit categories\n\n* **High throughput**: General high-volume endpoints for core operations like authentication, metadata, and feedback.\n* **Repository**: Repository and prompt management operations.\n* **Run ingest**: Individual trace/run ingestion endpoints for observability.\n* **Charts**: Chart generation and visualization endpoints.\n* **Multipart ingest**: Bulk run ingestion via multipart upload for high-volume tracing.\n* **Run query (API)**: API key-based run query operations with stricter limits for complex queries.\n* **Run query (User)**: User-based run query operations with higher limits for interactive use.\n* **Generation**: AI-powered code and content generation endpoints (limited to prevent abuse).\n* **Commits**: Prompt versioning and commit operations.\n* **Deletion**: Session deletion and workflow trigger operations.\n* **Run lookup**: Retrieving specific runs by UUID.\n* **Examples**: Fetching dataset examples for few-shot prompting.\n* **Default (API key)**: Fallback rate limit for authenticated API requests not matching specific patterns.\n* **Default (User)**: Fallback rate limit for authenticated user requests not matching specific patterns.\n* **Public download**: High-volume public download endpoints for shared resources.\n* **Stats**: Run statistics and analytics endpoints (region-specific limits apply).\n* **Public (catch-all)**: Default rate limit for unauthenticated public access.\n\nFor more information on rate limits and other service limits, refer to the [Administration overview](/langsmith/administration-overview#rate-limits).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cloud.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: How to handle model rate limits\nLink: https://docs.langchain.comlangsmith/rate-limiting\nContent: A common issue when running large evaluation jobs is running into third-party API rate limits, usually from model providers. There are a few ways to deal with rate limits.\n\n## Using `langchain` RateLimiters (Python only)\n\nIf you're using `langchain` Python chat models in your application or evaluators, you can add rate limiters to your model(s) that will add client-side control of the frequency with which requests are sent to the model provider API to avoid rate limit errors.\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.rate_limiters import InMemoryRateLimiter\n\nrate_limiter = InMemoryRateLimiter(\n    requests_per_second=0.1,  # <-- Super slow! We can only make a request once every 10 seconds!!\n    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n    max_bucket_size=10,  # Controls the maximum burst size.\n)\n\nmodel = init_chat_model(\"gpt-4o\", rate_limiter=rate_limiter)\n\ndef app(inputs: dict) -> dict:\n    response = model.invoke(...)\n    ...\n\ndef evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n    response = model.invoke(...)\n    ...\n```\n\nSee the [`langchain`](/oss/python/langchain/models#rate-limiting) documentation for more on how to configure rate limiters.\n\n## Retrying with exponential backoff\n\nA very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.\n\n#### With `langchain`\n\nIf you're using `langchain` components you can add retries to all model calls with the `.with_retry(...)` / `.withRetry()` method:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain import init_chat_model\n\n  model_with_retry = init_chat_model(\"gpt-4o-mini\").with_retry(stop_after_attempt=6)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { initChatModel } from \"langchain\";\n\n  const model = await initChatModel(\"gpt-4o\", {\n      modelProvider: \"openai\",\n  });\n\n  const modelWithRetry = model.withRetry({ stopAfterAttept: 2 });\n  ```\n</CodeGroup>\n\nSee the `langchain` [Python](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.BaseChatModel.with_retry) and [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API references for more.\n\n#### Without `langchain`\n\nIf you're not using `langchain` you can use other libraries like `tenacity` (Python) or `backoff` (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the [OpenAI docs](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff).\n\n## Limiting `max_concurrency`\n\nLimiting the number of concurrent calls you're making to your application and evaluators is another way to decrease the frequency of model calls you're making, and in that way avoid rate limit errors. `max_concurrency` can be set directly on the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) functions. This parallelizes evaluation by effectively splitting the dataset across threads.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import aevaluate\n\n  results = await aevaluate(\n      ...\n      max_concurrency=4,\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { evaluate } from \"langsmith/evaluation\";\n\n  await evaluate(..., {\n    ...,\n    maxConcurrency: 4,\n  });\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rate-limiting.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Built-in middleware\nLink: https://docs.langchain.comoss/javascript/langchain/middleware/built-in\nContent: Tool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. Watch this video guide demonstrating Tool Call Limit middleware behavior. toolName Name of specific tool to limit. If not provided, limits apply to all tools globally. threadLimit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. undefined means no thread limit. runLimit Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. undefined means no run limit. Note: At least one of threadLimit or runLimit must be specified. exitBehavior {\"body\":\"exitBehavior\",\"type\":\"string\",\"default\":\"continue\"}\n\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. Watch this video guide demonstrating Tool Call Limit middleware behavior. toolName Name of specific tool to limit. If not provided, limits apply to all tools globally. threadLimit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. undefined means no thread limit. runLimit {\"body\":\"runLimit\",\"type\":\"number\"}\n\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. Watch this video guide demonstrating Tool Call Limit middleware behavior. toolName Name of specific tool to limit. If not provided, limits apply to all tools globally. threadLimit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. undefined means no thread limit. runLimit Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. undefined means no run limit. Note: At least one of threadLimit or runLimit must be specified. exitBehavior Behavior when limit is reached: 'continue' (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages. 'error' - Throw a ToolCallLimitExceededError exception, stopping execution immediately 'end' - Stop execution immediately with a ToolMessage and AI message for the exceeded tool call. Only works when limiting a single tool; throws error if other tools have pending calls. Specify limits with: Thread limit - Max calls across all runs in a conversation (requires checkpointer) Run limit - Max calls per single invocation (resets each turn) Exit behaviors: 'continue' (default) - Block exceeded calls with error messages, agent continues 'error' - Raise exception immediately 'end' - Stop with ToolMessage + AI message (single-tool scenarios only)\n\n---\n\nResult 4:\nTitle: Graph API overview\nLink: https://docs.langchain.comoss/javascript/langgraph/graph-api\nContent: Recursion limit\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError . By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/ stream via the config object. Importantly, recursionLimit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below: await graph.invoke(inputs, {\n  recursionLimit: 5,\n  context: { llm: \"anthropic\" },\n});\n\nRecursion limit\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError . By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/ stream via the config object. Importantly, recursionLimit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\n\nProactive vs reactive approaches\nThere are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally). The reactive approach catches GraphRecursionError after the limit is exceeded. Design your graph with explicit termination conditions to avoid hitting the limit in the first place. Approach Detection Handling Control Flow Reactive (catching GraphRecursionError) After limit exceeded Outside graph in try/catch Graph execution terminated Reactive advantages: Simple implementation No need to modify graph logic Centralized error handling\n\n---\n\nResult 5:\nTitle: Built-in middleware\nLink: https://docs.langchain.comoss/python/langchain/middleware/built-in\nContent: Model call limit\nLimit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following: Preventing runaway agents from making too many API calls. Enforcing cost controls on production deployments. Testing agent behavior within specific call budgets. API reference: ModelCallLimitMiddleware Watch this video guide demonstrating Model Call Limit middleware behavior. thread_limit Maximum model calls across all runs in a thread. Defaults to no limit. run_limit Maximum model calls per single invocation. Defaults to no limit. exit_behavior {\"body\":\"exit_behavior\",\"type\":\"string\",\"default\":\"end\"}\n\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. API reference: ToolCallLimitMiddleware Watch this video guide demonstrating Tool Call Limit middleware behavior. tool_name Name of specific tool to limit. If not provided, limits apply to all tools globally. thread_limit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. None means no thread limit. run_limit Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. None means no run limit. Note: At least one of thread_limit or run_limit must be specified. exit_behavior {\"body\":\"exit_behavior\",\"type\":\"string\",\"default\":\"continue\"}\n\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. API reference: ToolCallLimitMiddleware Watch this video guide demonstrating Tool Call Limit middleware behavior. tool_name Name of specific tool to limit. If not provided, limits apply to all tools globally. thread_limit Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. None means no thread limit. run_limit {\"body\":\"run_limit\",\"type\":\"number\"}\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_34685791",
    "id": "8ee02679-1dd7-4183-84ce-e2b4b36839c9",
    "content": "Result 1:\nTitle: Cloud\nLink: https://docs.langchain.comlangsmith/cloud\nContent: <Callout icon=\"rocket\" color=\"#4F46E5\" iconType=\"regular\">\n  If you're ready to deploy your app to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or the [full setup guide](/langsmith/deploy-to-cloud). This page explains the Cloud managed architecture for reference.\n</Callout>\n\nThe **Cloud** option is a fully managed model where LangChain hosts and operates all LangSmith infrastructure and services:\n\n* **Fully managed infrastructure**: LangChain handles all infrastructure, updates, scaling, and maintenance.\n* **Deploy from GitHub**: Connect your repositories and deploy with a few clicks.\n* **Automated CI/CD**: Build process is handled automatically by the platform.\n* **LangSmith UI**: Full access to [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), [deployment management](/langsmith/deployments), and [Studio](/langsmith/studio).\n\n|                                               | **Who manages it** | **Where it runs** |\n| --------------------------------------------- | ------------------ | ----------------- |\n| **LangSmith platform (UI, APIs, datastores)** | LangChain          | LangChain's cloud |\n| **Your Agent Servers**                        | LangChain          | LangChain's cloud |\n| **CI/CD for your apps**                       | LangChain          | LangChain's cloud |\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3f0316122425895270d0ecd47b12e139\" alt=\"Cloud deployment: LangChain hosts and manages all components including the UI, APIs, and your Agent Servers.\" data-og-width=\"1425\" width=\"1425\" data-og-height=\"1063\" height=\"1063\" data-path=\"langsmith/images/langgraph-cloud-architecture.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b34a10fb40ca5188dddfb6be69696c75 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d310ce86e8421878575cf4d4fa72bf78 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=7cc3fcee9c36a8d1e8da4cc4abbde3b9 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a184c950c8585e2fdb5e75ac7f0f5642 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=cb778e4a6707b00eaf703886d32569bd 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=00f9d9df44512dded307613502d03299 2500w\" />\n\n## Get started\n\nTo deploy your first application to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or refer to the [comprehensive setup guide](/langsmith/deploy-to-cloud).\n\n## Cloud architecture and scalability\n\n<Note>\n  This section is only relevant for the cloud-managed LangSmith services available at [https://smith.langchain.com](https://smith.langchain.com) and [https://eu.smith.langchain.com](https://eu.smith.langchain.com).\n\n  For information on the self-hosted LangSmith solution, please refer to the [self-hosted documentation](/langsmith/self-hosted).\n</Note>\n\nLangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for LLM application observability, evaluation, and agent deployment\n\n### Architecture\n\nThe US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.\n\n<Note>\n  The [EU-based LangSmith service](https://eu.smith.langchain.com) is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, [contact our sales team](https://www.langchain.com/contact-sales).\n</Note>\n\n#### Regional storage\n\nThe resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses [Supabase](https://supabase.com) for authentication/authorization and [ClickHouse Cloud](https://clickhouse.com/cloud) for data warehouse.\n\n|                                                | US                                                                 | EU                                                                       |\n| ---------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------ |\n| URL                                            | [https://smith.langchain.com](https://smith.langchain.com)         | [https://eu.smith.langchain.com](https://eu.smith.langchain.com)         |\n| API URL                                        | [https://api.smith.langchain.com](https://api.smith.langchain.com) | [https://eu.api.smith.langchain.com](https://eu.api.smith.langchain.com) |\n| GCP                                            | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| Supabase                                       | AWS us-east-1 (N. Virginia)                                        | AWS eu-central-1 (Germany)                                               |\n| ClickHouse Cloud                               | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| [LangSmith deployment](/langsmith/deployments) | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n\nSee the [Regions FAQ](/langsmith/regions-faq) for more information.\n\n#### Region-independent storage\n\nData listed here is stored exclusively in the US:\n\n* Payment and billing information with Stripe and Metronome\n\n#### GCP services\n\nLangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):\n\n* LangSmith Frontend: serves the LangSmith UI.\n* LangSmith Backend: serves the LangSmith API.\n* LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)\n* LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.\n* LangSmith Queue: handles processing of asynchronous tasks. (Internal service)\n\nLangSmith uses the following GCP storage services:\n\n* Google Cloud Storage (GCS) for runs inputs and outputs.\n* Google Cloud SQL PostgreSQL for transactional workloads.\n* Google Cloud Memorystore for Redis for queuing and caching.\n* Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.\n\nSome additional GCP services we use include:\n\n* Google Cloud Load Balancer for routing traffic to the LangSmith services.\n* Google Cloud CDN for caching static assets.\n* Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to [this guide](/langsmith/administration-overview#rate-limits).\n\n<div style={{ textAlign: 'center' }}>\n  <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3\" alt=\"Light mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-light.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w\" />\n\n  <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b\" alt=\"Dark mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w\" />\n</div>\n\n### Allowlisting IP addresses\n\n#### Egress from LangChain SaaS\n\nAll traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:\n\n| US             | EU             |\n| -------------- | -------------- |\n| 34.59.65.97    | 34.13.192.67   |\n| 34.67.51.221   | 34.147.105.64  |\n| 34.46.212.37   | 34.90.22.166   |\n| 34.132.150.88  | 34.147.36.213  |\n| 35.188.222.201 | 34.32.137.113  |\n| 34.58.194.127  | 34.91.238.184  |\n| 34.59.97.173   | 35.204.101.241 |\n| 104.198.162.55 | 35.204.48.32   |\n\nIt may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.\n\n#### Ingress into LangChain SaaS\n\nThe langchain endpoints map to the following static IP addresses:\n\n| US             | EU           |\n| -------------- | ------------ |\n| 34.8.121.39    | 34.95.92.214 |\n| 34.107.251.234 | 34.13.73.122 |\n\nYou may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (`api.smith.langchain.com`, `smith.langchain.com`, `beacon.langchain.com`, `eu.api.smith.langchain.com`, `eu.smith.langchain.com`, `eu.beacon.langchain.com`).\n\n### API rate limits\n\nLangSmith enforces rate limits on API endpoints to ensure service stability and fair usage. The following table shows the rate limits for different endpoints in both US and EU regions. Note that:\n\n* Rate limits are expressed as `count / interval` where count is the number of requests allowed within the interval (in seconds). For example, `2000 / 10` means 2000 requests per 10 seconds.\n* When no HTTP method is specified in the endpoint column, the rate limit applies to all HTTP methods for that endpoint.\n* When a specific method is listed (e.g., `POST`, `GET`), the rate limit applies only to that method.\n\n| Match / Endpoint (method)                   | Identity key     | US prod limit | EU prod limit | Category                                     |\n| ------------------------------------------- | ---------------- | ------------- | ------------- | -------------------------------------------- |\n| OPTIONS, `/info`, `*/v1/metadata/submit`    | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-user-id` + IP | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/v1/beacon`                                | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/repos`                                    | `x-api-key`      | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `/repos`                                    | `x-user-id` + IP | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `POST /runs/batch`                          | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `POST /otel/v1/traces`                      | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST` containing `/charts`                 | `x-api-key`      | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST` containing `/charts`                 | `x-user-id` + IP | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST /runs/multipart`                      | `x-api-key`      | 6000 / 10     | 6000 / 10     | [Multipart ingest](#rate-limit-categories)   |\n| `POST /runs/query`                          | `x-api-key`      | 15 / 10       | 15 / 10       | [Run query (API)](#rate-limit-categories)    |\n| `POST /runs/query`                          | `x-user-id` + IP | 300 / 10      | 300 / 10      | [Run query (User)](#rate-limit-categories)   |\n| `/generate`                                 | `x-api-key`      | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/generate`                                 | `x-user-id` + IP | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/commits`                                  | `x-api-key`      | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `/commits`                                  | `x-user-id` + IP | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `DELETE /sessions` or `*/trigger`           | `x-api-key`      | 10 / 60       | 10 / 60       | [Deletion](#rate-limit-categories)           |\n| `DELETE /sessions` or `*/trigger`           | `x-user-id` + IP | 30 / 60       | 30 / 60       | [Deletion](#rate-limit-categories)           |\n| `POST /runs` (single run ingest)            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `PATCH` containing `/runs`                  | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST /feedback`                            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `GET /runs/{uuid}` or `/api/v1/runs/{uuid}` | `x-api-key`      | 30 / 60       | 30 / 60       | [Run lookup](#rate-limit-categories)         |\n| `GET` containing `/examples`                | `x-api-key`      | 5000 / 60     | 5000 / 60     | [Examples](#rate-limit-categories)           |\n| Any request with `x-api-key`                | `x-api-key`      | 1000 / 10     | 1000 / 10     | [Default (API key)](#rate-limit-categories)  |\n| Any request with `x-user-id`                | `x-user-id` + IP | 1000 / 10     | 1000 / 10     | [Default (User)](#rate-limit-categories)     |\n| `/public/download`                          | IP               | 5000 / 60     | 5000 / 60     | [Public download](#rate-limit-categories)    |\n| `/runs/stats`                               | `x-api-key`      | 1 / 10        | 20 / 10       | [Stats](#rate-limit-categories)              |\n| All other IPs (catch-all)                   | IP               | 100 / 60      | 100 / 60      | [Public (catch-all)](#rate-limit-categories) |\n\n#### Rate limit categories\n\n* **High throughput**: General high-volume endpoints for core operations like authentication, metadata, and feedback.\n* **Repository**: Repository and prompt management operations.\n* **Run ingest**: Individual trace/run ingestion endpoints for observability.\n* **Charts**: Chart generation and visualization endpoints.\n* **Multipart ingest**: Bulk run ingestion via multipart upload for high-volume tracing.\n* **Run query (API)**: API key-based run query operations with stricter limits for complex queries.\n* **Run query (User)**: User-based run query operations with higher limits for interactive use.\n* **Generation**: AI-powered code and content generation endpoints (limited to prevent abuse).\n* **Commits**: Prompt versioning and commit operations.\n* **Deletion**: Session deletion and workflow trigger operations.\n* **Run lookup**: Retrieving specific runs by UUID.\n* **Examples**: Fetching dataset examples for few-shot prompting.\n* **Default (API key)**: Fallback rate limit for authenticated API requests not matching specific patterns.\n* **Default (User)**: Fallback rate limit for authenticated user requests not matching specific patterns.\n* **Public download**: High-volume public download endpoints for shared resources.\n* **Stats**: Run statistics and analytics endpoints (region-specific limits apply).\n* **Public (catch-all)**: Default rate limit for unauthenticated public access.\n\nFor more information on rate limits and other service limits, refer to the [Administration overview](/langsmith/administration-overview#rate-limits).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cloud.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: How to compare experiment results\nLink: https://docs.langchain.comlangsmith/compare-experiment-results\nContent: When you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different [*experiments*](/langsmith/evaluation-concepts#experiment).\n\nLangSmith supports a comparison view that lets you hone in on key differences, regressions, and improvements between different experiments.\n\n## Open the comparison view\n\n1. To access the experiment comaprison view, navigate to the **Datasets & Experiments** page.\n2. Select a dataset, which will open the **Experiments** tab.\n3. Select two or more experiments abd then click **Compare**.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67d4d6068012e92b101f900595734977\" alt=\"The Experiments view in the UI with 3 experiments selected and the Compare button highlighted.\" data-og-width=\"1626\" width=\"1626\" data-og-height=\"966\" height=\"966\" data-path=\"langsmith/images/compare-select.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8e520c4ec316531d45a9f538c3f36f78 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0aed7d0b1cb5d70321ea536ce1decea9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=71ed0d1faed09fe9bd845e8049327948 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b8ccc3f4aa28630b633021b18eb64dd0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6e8b473e44e88e18aa743d1f75b9f6b5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c0784778d3b730d9e8b34400c852462b 2500w\" />\n\n## Adjust the table display\n\nYou can toggle between different views by clicking **Full** or **Compact** at the top of the **Comparing Experiments** page.\n\nToggling **Full** will show the full text of the input, output, and reference output for each run. If the reference output is too long to display in the table, you can click on **Expand detailed view** to view the full content.\n\nYou can also select and hide individual feedback keys or individual metrics in the **Display** settings dropdown to isolate the information you need in the comparison view.\n\n## View regressions and improvements\n\nIn the comparison view, runs that *regressed* on your specified feedback key against your baseline experiment will be highlighted in red, while runs that *improved* will be highlighted in green. At the top of each column, you can find how many runs in that experiment did better and how many did worse than your baseline experiment.\n\nClick on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=14f3a9b65dec55c9e4a0f5688d9e8f43\" alt=\"The comparison view comparing 2 experiments with the regressions and improvements highlighted in red and green respectively.\" data-og-width=\"1632\" width=\"1632\" data-og-height=\"739\" height=\"739\" data-path=\"langsmith/images/regression-view.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c7256046a4a6c5d28f350dd8d26bb7e3 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=78c6a2cb4baa784abeb3336d5bea94c4 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9772371f026939748db446fff60fe19b 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d5639c88a1a0214a0955ffbcda23faf0 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e4f232fbef17ff53192d8ac6db3913e 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d55c8e280732ba5d9e3866564af29de9 2500w\" />\n\n## Update baseline experiment and metric\n\nIn order to track regressions, you need to:\n\n1. In the **Baseline** dropdown at the top of the comparison view, select a **Baseline experiment** against which to compare. By default, the newest experiment is selected as the baseline.\n2. Select a  **Feedback key** (evaluation metric) you want to focus compare against. One will be assigned by default, but you can adjust as needed.\n3. Configure whether a higher score is better for the selected feedback key. This preference will be stored.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3df57789664fd92aba18ea2f438934bd\" alt=\"The Baseline dropdown highlighted with a selected experiment and feedback key of &#x22;hallucination&#x22;.\" data-og-width=\"1627\" width=\"1627\" data-og-height=\"898\" height=\"898\" data-path=\"langsmith/images/select-baseline.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=173b5ead77441998fc19669be3290aff 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=46f3bd52bad1fe8fc11ea269b37b15f9 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7cf328e494a3b674ad60f8a082d6b44 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae7ec9831c8c36855d085f48a8d8e6b3 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4ef21b8740df8f02f7579239e9a9792a 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c87682dbb3438f9c59a837a740d56979 2500w\" />\n\n## Open a trace\n\nIf the example you're evaluating is from an ingested [run](/langsmith/observability-concepts#runs), you can hover over the output cell and click on the trace icon to open the trace view for that run. This will open up a trace in the side panel.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b5feaa0894a645f4642c7422de937c7d\" alt=\"The View trace icon highlighted from an ingested run.\" data-og-width=\"1632\" width=\"1632\" data-og-height=\"662\" height=\"662\" data-path=\"langsmith/images/open-source-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=49a42505f2992ab6caeab6f1c52c8e81 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c05cf153d430c062d42d1bf18ef73d6a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=734ad1f9a66a03615b259305298480f0 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3ad016ed31dc9c43eaa74cdffcc66a5e 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=991be6dc57ad88ef21aaba9c90ea24fa 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c4110db6d5e60b4e8df840c409cb5c38 2500w\" />\n\n## Expand detailed view\n\nFrom any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1ff7f02d5ba6ea89902b4de0e37967e7\" alt=\"An example in the Comparing Experiments view of a expanded view of the repetitions.\" data-og-width=\"1643\" width=\"1643\" data-og-height=\"926\" height=\"926\" data-path=\"langsmith/images/expanded-view.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4803efab7ae4a4363145857941d50055 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=98f67feac8ad53ddb18be06955ab2895 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=628927173d27260eb7f76d82e66ff2d4 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6de1f01362d8658d9f673e91deec77b6 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8f68caed39717af5314beb187c3f542f 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=00fa65a0580a10973a91d506b7cdbc09 2500w\" />\n\n## View summary charts\n\nView summary charts by clicking on the **Charts** tab at the top of the page.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ada485c324964c8df96caa566ad11b1d\" alt=\"The Charts summary page with 8 summary charts for the comparison.\" data-og-width=\"1639\" width=\"1639\" data-og-height=\"1147\" height=\"1147\" data-path=\"langsmith/images/charts-tab.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e21f174643b26892cae4280962653263 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=40f98d83faf7c13a05a9b410787c9a75 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a49ddad4c2c02bc14a0d765dc7dc261 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8aec566c2fd07b7d6e12a3e639cf6660 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=716edd3ad3626831a88c6db07f0fa566 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=08e8ea763e825413a3aff09c061593ca 2500w\" />\n\n## Use experiment metadata as chart labels\n\nYou can configure the x-axis labels for the charts based on [experiment metadata](/langsmith/filter-experiments-ui#background-add-metadata-to-your-experiments).\n\nSelect a metadata key in the **x-axis** dropdown to change the chart labels.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb175478369f9a7a2314e44f6becc9e4\" alt=\"x-axis dropdown highlighted with a list of the metadata attached to the experiment.\" data-og-width=\"1637\" width=\"1637\" data-og-height=\"1141\" height=\"1141\" data-path=\"langsmith/images/metadata-in-charts.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b34f45a5f0736b6b83a73a00f94212f6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7084f0126711a63d6f539904f4e9091f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dee53003f1c958d09f839d24cb54eb63 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7a0323fef549e2aef3fcf22295d40d19 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=81c37649a07ffa594bad4fd57cc8fb32 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=70d700760e7ba9dd0edce54429d11f95 2500w\" />\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/compare-experiment-results.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Filter traces\nLink: https://docs.langchain.comlangsmith/filter-traces-in-application\nContent: Filter based on input / output key-value pairs\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won't be indexed. This helps us ensure fast, reliable performance. To filter based on key-value pairs, select the Input Key or Output Key filter from the filters dropdown. For example, to match the following input: {\n  \"input\": \"What is the capital of France?\"\n}\n\nFilter based on inputs and outputs\nYou can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use the Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. For performance, we index up to 250 characters of data for full-text search. If your search query exceeds this limit, we recommend using Input/Output key-value search instead. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done by splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common JSON keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\n\nFilter based on input / output key-value pairs\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won't be indexed. This helps us ensure fast, reliable performance. To filter based on key-value pairs, select the Input Key or Output Key filter from the filters dropdown. For example, to match the following input: Select Filters , Add Filter to bring up the filtering options. Then select Input Key , enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output: {\n  \"documents\": [\n    {\n      \"page_content\": \"The capital of France is Paris\",\n      \"metadata\": {},\n      \"type\": \"Document\"\n    }\n  ]\n}\n\n---\n\nResult 4:\nTitle: Use the graph API\nLink: https://docs.langchain.comoss/javascript/langgraph/use-graph-api\nContent: Define input and output schemas\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph. When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema. Below, we'll see how to define distinct input and output schema. Notice that the output of invoke only includes the output schema.\n\nDefine input and output schemas\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph. When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema. Below, we'll see how to define distinct input and output schema. { answer: 'bye' }\n\nDefine input and output schemas\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph. When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema. Below, we'll see how to define distinct input and output schema. import { StateGraph, START, END } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Define the schema for the input\nconst InputState = z.object({\n  question: z.string(),\n});\n\n// Define the schema for the output\nconst OutputState = z.object({\n  answer: z.string(),\n});\n\n// Define the overall schema, combining both input and output\nconst OverallState = InputState.merge(OutputState);\n\n// Build the graph with input and output schemas specified\nconst graph = new StateGraph({\n  input: InputState,\n  output: OutputState,\n  state: OverallState,\n})\n  .addNode(\"answerNode\", (state) => {\n    // Example answer and an extra key\n    return { answer: \"bye\", question: state.question };\n  })\n  .addEdge(START, \"answerNode\")\n  .addEdge(\"answerNode\", END)\n  .compile();\n\n// Invoke the graph with an input and print the result\nconsole.log(await graph.invoke({ question: \"hi\" }));\n\n---\n\nResult 5:\nTitle: Use the graph API\nLink: https://docs.langchain.comoss/python/langgraph/use-graph-api\nContent: Define input and output schemas\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph. When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema. Below, we'll see how to define distinct input and output schema. Notice that the output of invoke only includes the output schema.\n\nDefine input and output schemas\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph. When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema. Below, we'll see how to define distinct input and output schema. {'answer': 'bye'}\n\nDefine input and output schemas\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph. When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema. Below, we'll see how to define distinct input and output schema. from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# Define the schema for the input\nclass InputState(TypedDict):\n    question: str\n\n# Define the schema for the output\nclass OutputState(TypedDict):\n    answer: str\n\n# Define the overall schema, combining both input and output\nclass OverallState(InputState, OutputState):\n    pass\n\n# Define the node that processes the input and generates an answer\ndef answer_node(state: InputState):\n    # Example answer and an extra key\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\n\n# Build the graph with input and output schemas specified\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\nbuilder.add_node(answer_node)  # Add the answer node\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\ngraph = builder.compile()  # Compile the graph\n\n# Invoke the graph with an input and print the result\nprint(graph.invoke({\"question\": \"hi\"}))\n"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3a30-afaa-7323-96cf-0fa72c68b119",
    "content": "**For LangSmith Cloud deployments, the maximum payload size for requests (including run creation) is 25 MB.**\n\nExceeding this limit returns a `413 Payload Too Large` error. This applies to all API requests sent to Cloud deployments.\n\nFor trace inputs/outputs specifically:\n- Full-text search indexes up to **250 characters** per value\n- Up to **100 unique keys** are indexed per run for key-value filtering\n\nLarge payloads may be stored in external storage (S3 URLs) rather than inline.\n\n**Relevant docs:**\n- [LangSmith Data Plane#Payload size](https://docs.langchain.com/langsmith/data-plane#payload-size)\n- [Filter traces#input-output-limits](https://docs.langchain.com/langsmith/filter-traces-in-application#filter-based-on-input-output-key-value-pairs)"
  }
]