[
  {
    "role": "user",
    "id": "3bd33d3b-912f-4eb8-b55f-336a02a4ff3d",
    "content": "\u6ce8\u610f\uff1a\u4ee5\u4e0b\u5bf9\u8bdd\uff0c\u5168\u7a0b\u5fc5\u987b\u4f7f\u7528\u4e2d\u6587\u4ea4\u6d41\n\u6ce8\u610f\uff1a\u4ee5\u4e0b\u6211\u95ee\u7684\u95ee\u9898\uff0c\u5747\u4e3anodejs\u8bed\u8a00\u4e0b\u7684langchain\n\n\u6211\u60f3\u95ee\u7684\u662f\uff0ccreateAgent\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u9ed8\u8ba4\u662f\u53ef\u4ee5\u8c03\u7528\u591a\u4e2a\u5de5\u5177\uff0c\u8fd8\u662f\u53ea\u8c03\u7528\u4e00\u4e2a\u5de5\u5177\uff1f"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_83842834",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"javascript\", \"page_size\": 5, \"query\": \"agents\"}"
        }
      },
      {
        "type": "function",
        "id": "call_37028349",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"javascript\", \"page_size\": 5, \"query\": \"createAgent\"}"
        }
      },
      {
        "type": "function",
        "id": "call_07294050",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"javascript\", \"page_size\": 5, \"query\": \"tools\"}"
        }
      },
      {
        "type": "function",
        "id": "call_71569374",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"OSS (LangChain and LangGraph),SDKs and APIs\"}"
        }
      }
    ],
    "id": "lc_run--019b3a32-255b-74a3-b3f5-3b10d390de60",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_83842834",
    "id": "c7216084-ef89-4011-b7c8-1ed0eea385ad",
    "content": "Result 1:\nTitle: Deep Agents overview\nLink: https://docs.langchain.comoss/javascript/deepagents/overview\nContent: [`deepagents`](https://www.npmjs.com/package/deepagents) is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.\n\n## When to use deep agents\n\nUse deep agents when you need agents that can:\n\n* **Handle complex, multi-step tasks** that require planning and decomposition\n* **Manage large amounts of context** through file system tools\n* **Delegate work** to specialized subagents for context isolation\n* **Persist memory** across conversations and threads\n\nFor simpler use cases, consider using LangChain's [`createAgent`](/oss/javascript/langchain/agents) or building a custom [LangGraph](/oss/javascript/langgraph/overview) workflow.\n\n## Core capabilities\n\n<Card title=\"Planning and task decomposition\" icon=\"timeline\">\n  Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.\n</Card>\n\n<Card title=\"Context management\" icon=\"scissors\">\n  File system tools (`ls`, `read_file`, `write_file`, `edit_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.\n</Card>\n\n<Card title=\"Subagent spawning\" icon=\"people-group\">\n  A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent's context clean while still going deep on specific subtasks.\n</Card>\n\n<Card title=\"Long-term memory\" icon=\"database\">\n  Extend agents with persistent memory across threads using LangGraph's Store. Agents can save and retrieve information from previous conversations.\n</Card>\n\n## Relationship to the LangChain ecosystem\n\nDeep agents is built on top of:\n\n* [LangGraph](/oss/javascript/langgraph/overview) - Provides the underlying graph execution and state management\n* [LangChain](/oss/javascript/langchain/overview) - Tools and model integrations work seamlessly with deep agents\n* [LangSmith](/langsmith/home) - Observability, evaluation, and deployment\n\nDeep agents applications can be deployed via [LangSmith Deployment](/langsmith/deployments) and monitored with [LangSmith Observability](/langsmith/observability).\n\n## Get started\n\n<CardGroup cols={2}>\n  <Card title=\"Quickstart\" icon=\"rocket\" href=\"/oss/javascript/deepagents/quickstart\">\n    Build your first deep agent\n  </Card>\n\n  <Card title=\"Customization\" icon=\"sliders\" href=\"/oss/javascript/deepagents/customization\">\n    Learn about customization options\n  </Card>\n\n  <Card title=\"Middleware\" icon=\"layer-group\" href=\"/oss/javascript/deepagents/middleware\">\n    Understand the middleware architecture\n  </Card>\n</CardGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Agents\nLink: https://docs.langchain.comoss/javascript/langchain/agents\nContent: Agents combine language models with [tools](/oss/javascript/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n`createAgent()` provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid  theme={null}\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n  `createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/javascript/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n\n  Learn more about the [Graph API](/oss/javascript/langgraph/graph-api).\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/javascript/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: []\n});\n```\n\nModel identifier strings use the format `provider:model` (e.g. `\"openai:gpt-5\"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/javascript/integrations/providers/) to see available params and methods on your model.\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:\n\n```ts  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst basicModel = new ChatOpenAI({ model: \"gpt-4o-mini\" });\nconst advancedModel = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst dynamicModelSelection = createMiddleware({\n  name: \"DynamicModelSelection\",\n  wrapModelCall: (request, handler) => {\n    // Choose model based on conversation complexity\n    const messageCount = request.messages.length;\n\n    return handler({\n        ...request,\n        model: messageCount > 10 ? advancedModel : basicModel,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\", // Base model (used when messageCount \u2264 10)\n  tools,\n  middleware: [dynamicModelSelection],\n});\n```\n\nFor more details on middleware and advanced patterns, see the [middleware documentation](/oss/javascript/langchain/middleware).\n\n<Tip>\n  For model configuration details, see [Models](/oss/javascript/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/javascript/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/javascript/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({\n      query: z.string().describe(\"The query to search for\"),\n    }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [search, getWeather],\n});\n```\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\nTo customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:\n\n```ts wrap theme={null}\nimport { createAgent, createMiddleware, ToolMessage } from \"langchain\";\n\nconst handleToolErrors = createMiddleware({\n  name: \"HandleToolErrors\",\n  wrapToolCall: async (request, handler) => {\n    try {\n      return await handler(request);\n    } catch (error) {\n      // Return a custom error message to the model\n      return new ToolMessage({\n        content: `Tool error: Please check your input and try again. (${error})`,\n        tool_call_id: request.toolCall.id!,\n      });\n    }\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [handleToolErrors],\n});\n```\n\nThe agent will return a [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) with the custom error message when a tool fails.\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n  **Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n  ```\n  ================================ Human Message =================================\n\n  Find the most popular wireless headphones right now and check if they're in stock\n  ```\n\n  * **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n  * **Acting**: Call `search_products(\"wireless headphones\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    search_products (call_abc123)\n   Call ID: call_abc123\n    Args:\n      query: wireless headphones\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n  ```\n\n  * **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n  * **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    check_inventory (call_def456)\n   Call ID: call_def456\n    Args:\n      product_id: WH-1000XM5\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Product WH-1000XM5: 10 units in stock\n  ```\n\n  * **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n  * **Acting**: Produce final answer\n\n  ```\n  ================================== Ai Message ==================================\n\n  I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n  ```\n</Accordion>\n\n<Tip>\n  To learn more about tools, see [Tools](/oss/javascript/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:\n\n```ts wrap theme={null}\nconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n```\n\nWhen no `systemPrompt` is provided, the agent will infer its task from the messages directly.\n\nThe `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/javascript/integrations/chat/anthropic#prompt-caching):\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst literaryAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-5\",\n  systemPrompt: new SystemMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"You are an AI assistant tasked with analyzing literary works.\",\n      },\n      {\n        type: \"text\",\n        text: \"<the entire contents of 'Pride and Prejudice'>\",\n        cache_control: { type: \"ephemeral\" }\n      }\n    ]\n  })\n});\n\nconst result = await literaryAgent.invoke({\n  messages: [new HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]\n});\n```\n\nThe `cache_control` field with `{ type: \"ephemeral\" }` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/javascript/langchain/middleware).\n\n```typescript wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userRole: z.enum([\"expert\", \"beginner\"]),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [/* ... */],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\n      const userRole = runtime.context.userRole || \"user\";\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    }),\n  ],\n});\n\n// The system prompt will be set dynamically based on context\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Explain machine learning\" }] },\n  { context: { userRole: \"expert\" } }\n);\n```\n\n<Tip>\n  For more details on message types and formatting, see [Messages](/oss/javascript/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/javascript/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/javascript/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n```typescript  theme={null}\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"What's the weather in San Francisco?\" }],\n})\n```\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/javascript/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/javascript/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst ContactInfo = z.object({\n  name: z.string(),\n  email: z.string(),\n  phone: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  responseFormat: ContactInfo,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n    },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n// {\n//   name: 'John Doe',\n//   email: 'john@example.com',\n//   phone: '(555) 123-4567'\n// }\n```\n\n<Tip>\n  To learn about structured output, see [Structured output](/oss/javascript/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/javascript/langchain/short-term-memory) of the agent:\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport { createAgent } from \"langchain\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\n\nconst customAgentState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst CustomAgentState = createAgent({\n  model: \"gpt-4o\",\n  tools: [],\n  stateSchema: customAgentState,\n});\n```\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/javascript/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/javascript/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```ts  theme={null}\nconst stream = await agent.stream(\n  {\n    messages: [{\n      role: \"user\",\n      content: \"Search for AI news and summarize the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/javascript/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/javascript/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n<Tip>\n  For comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Workflows and agents\nLink: https://docs.langchain.comoss/javascript/langgraph/workflows-agents\nContent: Agents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nAgents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. import { tool } from \"@langchain/core/tools\";\nimport * as z from \"zod\";\n\n// Define tools\nconst multiply = tool(\n  ({ a, b }) => {\n    return a * b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers together\",\n    schema: z.object({\n      a: z.number().describe(\"first number\"),\n      b: z.number().describe(\"second number\"),\n    }),\n  }\n);\n\nconst add = tool(\n  ({ a, b }) => {\n    return a + b;\n  },\n  {\n    name: \"add\",\n    description: \"Add two numbers together\",\n    schema: z.object({\n      a: z.number().describe(\"first number\"),\n      b: z.number().describe(\"second number\"),\n    }),\n  }\n);\n\nconst divide = tool(\n  ({ a, b }) => {\n    return a / b;\n  },\n  {\n    name: \"divide\",\n    description: \"Divide two numbers\",\n    schema: z.object({\n      a: z.number().describe(\"first number\"),\n      b: z.number().describe(\"second number\"),\n    }),\n  }\n);\n\n// Augment the LLM with tools\nconst tools = [add, multiply, divide];\nconst toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool]));\nconst llmWithTools = llm.bindTools(tools);\n\nAgents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. import { task, entrypoint, addMessages } from \"@langchain/langgraph\";\nimport { BaseMessageLike, ToolCall } from \"@langchain/core/messages\";\n\nconst callLlm = task(\"llmCall\", async (messages: BaseMessageLike[]) => {\n  // LLM decides whether to call a tool or not\n  return llmWithTools.invoke([\n    {\n      role: \"system\",\n      content: \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n    },\n    ...messages\n  ]);\n});\n\nconst callTool = task(\"toolCall\", async (toolCall: ToolCall) => {\n  // Performs the tool call\n  const tool = toolsByName[toolCall.name];\n  return tool.invoke(toolCall.args);\n});\n\nconst agent = entrypoint(\n  \"agent\",\n  async (messages) => {\n    let llmResponse = await callLlm(messages);\n\n    while (true) {\n      if (!llmResponse.tool_calls?.length) {\n        break;\n      }\n\n      // Execute tools\n      const toolResults = await Promise.all(\n        llmResponse.tool_calls.map((toolCall) => callTool(toolCall))\n      );\n\n      messages = addMessages(messages, [llmResponse, ...toolResults]);\n      llmResponse = await callLlm(messages);\n    }\n\n    messages = addMessages(messages, [llmResponse]);\n    return messages;\n  }\n);\n\n// Invoke\nconst messages = [{\n  role: \"user\",\n  content: \"Add 3 and 4.\"\n}];\n\nconst stream = await agent.stream([messages], {\n  streamMode: \"updates\",\n});\n\nfor await (const step of stream) {\n  console.log(step);\n}\n\n---\n\nResult 4:\nTitle: Deep Agents overview\nLink: https://docs.langchain.comoss/python/deepagents/overview\nContent: [`deepagents`](https://pypi.org/project/deepagents/) is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.\n\n## When to use deep agents\n\nUse deep agents when you need agents that can:\n\n* **Handle complex, multi-step tasks** that require planning and decomposition\n* **Manage large amounts of context** through file system tools\n* **Delegate work** to specialized subagents for context isolation\n* **Persist memory** across conversations and threads\n\nFor simpler use cases, consider using LangChain's [`create_agent`](/oss/python/langchain/agents) or building a custom [LangGraph](/oss/python/langgraph/overview) workflow.\n\n## Core capabilities\n\n<Card title=\"Planning and task decomposition\" icon=\"timeline\">\n  Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.\n</Card>\n\n<Card title=\"Context management\" icon=\"scissors\">\n  File system tools (`ls`, `read_file`, `write_file`, `edit_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.\n</Card>\n\n<Card title=\"Subagent spawning\" icon=\"people-group\">\n  A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent's context clean while still going deep on specific subtasks.\n</Card>\n\n<Card title=\"Long-term memory\" icon=\"database\">\n  Extend agents with persistent memory across threads using LangGraph's Store. Agents can save and retrieve information from previous conversations.\n</Card>\n\n## Relationship to the LangChain ecosystem\n\nDeep agents is built on top of:\n\n* [LangGraph](/oss/python/langgraph/overview) - Provides the underlying graph execution and state management\n* [LangChain](/oss/python/langchain/overview) - Tools and model integrations work seamlessly with deep agents\n* [LangSmith](/langsmith/home) - Observability, evaluation, and deployment\n\nDeep agents applications can be deployed via [LangSmith Deployment](/langsmith/deployments) and monitored with [LangSmith Observability](/langsmith/observability).\n\n## Get started\n\n<CardGroup cols={2}>\n  <Card title=\"Quickstart\" icon=\"rocket\" href=\"/oss/python/deepagents/quickstart\">\n    Build your first deep agent\n  </Card>\n\n  <Card title=\"Customization\" icon=\"sliders\" href=\"/oss/python/deepagents/customization\">\n    Learn about customization options\n  </Card>\n\n  <Card title=\"Middleware\" icon=\"layer-group\" href=\"/oss/python/deepagents/middleware\">\n    Understand the middleware architecture\n  </Card>\n\n  <Card title=\"Reference\" icon=\"arrow-up-right-from-square\" href=\"https://reference.langchain.com/python/deepagents/\">\n    See the `deepagents` API reference\n  </Card>\n</CardGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Workflows and agents\nLink: https://docs.langchain.comoss/python/langgraph/workflows-agents\nContent: Agents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nAgents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. from langchain.tools import tool\n\n\n# Define tools\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a * b\n\n\n@tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a + b\n\n\n@tool\ndef divide(a: int, b: int) -> float:\n    \"\"\"Divide `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a / b\n\n\n# Augment the LLM with tools\ntools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nllm_with_tools = llm.bind_tools(tools)\n\nAgents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. from langgraph.graph import add_messages\nfrom langchain.messages import (\n    SystemMessage,\n    HumanMessage,\n    ToolCall,\n)\nfrom langchain_core.messages import BaseMessage\n\n\n@task\ndef call_llm(messages: list[BaseMessage]):\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\n    return llm_with_tools.invoke(\n        [\n            SystemMessage(\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n            )\n        ]\n        + messages\n    )\n\n\n@task\ndef call_tool(tool_call: ToolCall):\n    \"\"\"Performs the tool call\"\"\"\n    tool = tools_by_name[tool_call[\"name\"]]\n    return tool.invoke(tool_call)\n\n\n@entrypoint()\ndef agent(messages: list[BaseMessage]):\n    llm_response = call_llm(messages).result()\n\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # Execute tools\n        tool_result_futures = [\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\n        ]\n        tool_results = [fut.result() for fut in tool_result_futures]\n        messages = add_messages(messages, [llm_response, *tool_results])\n        llm_response = call_llm(messages).result()\n\n    messages = add_messages(messages, llm_response)\n    return messages\n\n# Invoke\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\n    print(chunk)\n    print(\"\\n\")\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_37028349",
    "id": "adaa302a-7081-4814-a23e-2c5fdd990a3a",
    "content": "Result 1:\nTitle: Agents\nLink: https://docs.langchain.comoss/javascript/langchain/agents\nContent: Agents combine language models with [tools](/oss/javascript/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n`createAgent()` provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid  theme={null}\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n  `createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/javascript/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n\n  Learn more about the [Graph API](/oss/javascript/langgraph/graph-api).\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/javascript/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: []\n});\n```\n\nModel identifier strings use the format `provider:model` (e.g. `\"openai:gpt-5\"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/javascript/integrations/providers/) to see available params and methods on your model.\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:\n\n```ts  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst basicModel = new ChatOpenAI({ model: \"gpt-4o-mini\" });\nconst advancedModel = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst dynamicModelSelection = createMiddleware({\n  name: \"DynamicModelSelection\",\n  wrapModelCall: (request, handler) => {\n    // Choose model based on conversation complexity\n    const messageCount = request.messages.length;\n\n    return handler({\n        ...request,\n        model: messageCount > 10 ? advancedModel : basicModel,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\", // Base model (used when messageCount \u2264 10)\n  tools,\n  middleware: [dynamicModelSelection],\n});\n```\n\nFor more details on middleware and advanced patterns, see the [middleware documentation](/oss/javascript/langchain/middleware).\n\n<Tip>\n  For model configuration details, see [Models](/oss/javascript/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/javascript/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/javascript/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({\n      query: z.string().describe(\"The query to search for\"),\n    }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [search, getWeather],\n});\n```\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\nTo customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:\n\n```ts wrap theme={null}\nimport { createAgent, createMiddleware, ToolMessage } from \"langchain\";\n\nconst handleToolErrors = createMiddleware({\n  name: \"HandleToolErrors\",\n  wrapToolCall: async (request, handler) => {\n    try {\n      return await handler(request);\n    } catch (error) {\n      // Return a custom error message to the model\n      return new ToolMessage({\n        content: `Tool error: Please check your input and try again. (${error})`,\n        tool_call_id: request.toolCall.id!,\n      });\n    }\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [handleToolErrors],\n});\n```\n\nThe agent will return a [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) with the custom error message when a tool fails.\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n  **Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n  ```\n  ================================ Human Message =================================\n\n  Find the most popular wireless headphones right now and check if they're in stock\n  ```\n\n  * **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n  * **Acting**: Call `search_products(\"wireless headphones\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    search_products (call_abc123)\n   Call ID: call_abc123\n    Args:\n      query: wireless headphones\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n  ```\n\n  * **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n  * **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    check_inventory (call_def456)\n   Call ID: call_def456\n    Args:\n      product_id: WH-1000XM5\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Product WH-1000XM5: 10 units in stock\n  ```\n\n  * **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n  * **Acting**: Produce final answer\n\n  ```\n  ================================== Ai Message ==================================\n\n  I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n  ```\n</Accordion>\n\n<Tip>\n  To learn more about tools, see [Tools](/oss/javascript/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:\n\n```ts wrap theme={null}\nconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n```\n\nWhen no `systemPrompt` is provided, the agent will infer its task from the messages directly.\n\nThe `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/javascript/integrations/chat/anthropic#prompt-caching):\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst literaryAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-5\",\n  systemPrompt: new SystemMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"You are an AI assistant tasked with analyzing literary works.\",\n      },\n      {\n        type: \"text\",\n        text: \"<the entire contents of 'Pride and Prejudice'>\",\n        cache_control: { type: \"ephemeral\" }\n      }\n    ]\n  })\n});\n\nconst result = await literaryAgent.invoke({\n  messages: [new HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]\n});\n```\n\nThe `cache_control` field with `{ type: \"ephemeral\" }` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/javascript/langchain/middleware).\n\n```typescript wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userRole: z.enum([\"expert\", \"beginner\"]),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [/* ... */],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\n      const userRole = runtime.context.userRole || \"user\";\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    }),\n  ],\n});\n\n// The system prompt will be set dynamically based on context\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Explain machine learning\" }] },\n  { context: { userRole: \"expert\" } }\n);\n```\n\n<Tip>\n  For more details on message types and formatting, see [Messages](/oss/javascript/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/javascript/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/javascript/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n```typescript  theme={null}\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"What's the weather in San Francisco?\" }],\n})\n```\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/javascript/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/javascript/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst ContactInfo = z.object({\n  name: z.string(),\n  email: z.string(),\n  phone: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  responseFormat: ContactInfo,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n    },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n// {\n//   name: 'John Doe',\n//   email: 'john@example.com',\n//   phone: '(555) 123-4567'\n// }\n```\n\n<Tip>\n  To learn about structured output, see [Structured output](/oss/javascript/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/javascript/langchain/short-term-memory) of the agent:\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport { createAgent } from \"langchain\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\n\nconst customAgentState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst CustomAgentState = createAgent({\n  model: \"gpt-4o\",\n  tools: [],\n  stateSchema: customAgentState,\n});\n```\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/javascript/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/javascript/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```ts  theme={null}\nconst stream = await agent.stream(\n  {\n    messages: [{\n      role: \"user\",\n      content: \"Search for AI news and summarize the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/javascript/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/javascript/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n<Tip>\n  For comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: LangChain overview\nLink: https://docs.langchain.comoss/javascript/langchain/overview\nContent: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/javascript/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/javascript/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain [agents](/oss/javascript/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\n## <Icon icon=\"wand-magic-sparkles\" /> Create an agent\n\n```ts  theme={null}\nimport * as z from \"zod\";\n// npm install @langchain/anthropic to call the model\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }) => `It's always sunny in ${city}!`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  },\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n\nSee the [Installation instructions](/oss/javascript/langchain/install) and [Quickstart guide](/oss/javascript/langchain/quickstart) to get started building your own agents and applications with LangChain.\n\n## <Icon icon=\"star\" size={20} /> Core benefits\n\n<Columns cols={2}>\n  <Card title=\"Standard model interface\" icon=\"arrows-rotate\" href=\"/oss/javascript/langchain/models\" arrow cta=\"Learn more\">\n    Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\n  </Card>\n\n  <Card title=\"Easy to use, highly flexible agent\" icon=\"wand-magic-sparkles\" href=\"/oss/javascript/langchain/agents\" arrow cta=\"Learn more\">\n    LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\n  </Card>\n\n  <Card title=\"Built on top of LangGraph\" icon=\"circle-nodes\" href=\"/oss/javascript/langgraph/overview\" arrow cta=\"Learn more\">\n    LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.\n  </Card>\n\n  <Card title=\"Debug with LangSmith\" icon=\"eye\" href=\"/langsmith/home\" arrow cta=\"Learn more\">\n    Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Build a SQL agent\nLink: https://docs.langchain.comoss/javascript/langchain/sql-agent\nContent: 5. Use createAgent\nUse createAgent to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful. Initialize the agent with a descriptive system prompt to customize its behavior: Now, create an agent with the model, tools, and prompt:\n\n5. Use createAgent\nUse createAgent to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful. Initialize the agent with a descriptive system prompt to customize its behavior: Now, create an agent with the model, tools, and prompt: import { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: [executeSql],\n  systemPrompt: getSystemPrompt,\n});\n\n\n5. Use createAgent\nUse createAgent to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful. Initialize the agent with a descriptive system prompt to customize its behavior: import { SystemMessage } from \"langchain\";\n\nconst getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.\n\nAuthoritative schema (do not invent columns/tables):\n${await getSchema()}\n\nRules:\n- Think step-by-step.\n- When you need data, call the tool \\`execute_sql\\` with ONE SELECT query.\n- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.\n- Limit to 5 rows unless user explicitly asks otherwise.\n- If the tool returns 'Error:', revise the SQL and try again.\n- Limit the number of attempts to 5.\n- If you are not successful after 5 attempts, return a note to the user.\n- Prefer explicit column lists; avoid SELECT *.\n`);\n\n---\n\nResult 4:\nTitle: LangGraph v1 migration guide\nLink: https://docs.langchain.comoss/javascript/migrate/langgraph-v1\nContent: This guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of what's new, see the [release notes](/oss/javascript/releases/langgraph-v1).\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph@latest @langchain/core@latest\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph@latest @langchain/core@latest\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph@latest @langchain/core@latest\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph@latest @langchain/core@latest\n  ```\n</CodeGroup>\n\n## Summary of changes\n\n| Area                             | What changed                                               |\n| -------------------------------- | ---------------------------------------------------------- |\n| React prebuilt                   | `createReactAgent` deprecated; use LangChain `createAgent` |\n| Interrupts                       | Typed interrupts supported via `interrupts` config         |\n| `toLangGraphEventStream` removed | Use `graph.stream` with the desired `encoding` format      |\n| `useStream`                      | Supports custom transports                                 |\n\n***\n\n## Deprecation: `createReactAgent` \u2192 `createAgent`\n\nLangGraph v1 deprecates the `createReactAgent` prebuilt. Use LangChain's `createAgent`, which runs on LangGraph and adds a flexible middleware system.\n\nSee the LangChain v1 docs for details:\n\n* [Release notes](/oss/javascript/releases/langchain-v1#createagent)\n* [Migration guide](/oss/javascript/migrate/langchain-v1#createagent)\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent } from \"langchain\";\n\n  const agent = createAgent({\n    model,\n    tools,\n    systemPrompt: \"You are a helpful assistant.\", // [!code highlight]\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n\n  const agent = createReactAgent({\n    model,\n    tools,\n    prompt: \"You are a helpful assistant.\", // [!code highlight]\n  });\n  ```\n</CodeGroup>\n\n***\n\n## Typed interrupts\n\nYou can now define interrupt types at graph construction to strictly type the values passed to and received from interrupts.\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { StateGraph, interrupt } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  const State = z.object({ foo: z.string() });\n\n  const graphConfig = {\n    interrupts: {\n      approve: interrupt<{ reason: string }, { messages: string[] }>(),\n    },\n  }\n\n  const graph = new StateGraph(State, graphConfig)\n    .addNode(\"node\", async (state, runtime) => {\n      const value = runtime.interrupt.approve({ reason: \"review\" }); // [!code highlight]\n      return { foo: value };\n    })\n    .compile();\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { StateGraph } from \"@langchain/langgraph\";\n\n  const graph = new StateGraph(State)\n    .addNode(\"node\", async (state, runtime) => {\n      const value = runtime.interrupt.approve({ reason: \"review\" }); // [!code highlight]\n      return state;\n    })\n    .compile();\n  ```\n</CodeGroup>\n\nSee [Interrupts](/oss/javascript/langgraph/interrupts) to learn more.\n\n***\n\n## Event stream encoding\n\nThe low-level `toLangGraphEventStream` helper is removed. Streaming responses are handled by the SDK; when using low-level clients, select the wire format via an `encoding` option passed to `graph.stream`.\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  const stream = await graph.stream(input, {\n    encoding: \"text/event-stream\",\n    streamMode: [\"values\", \"messages\"],\n  });\n\n  return new Response(stream, {\n    headers: { \"Content-Type\": \"text/event-stream\" }, // [!code highlight]\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  return toLangGraphEventStreamResponse({\n    stream: graph.streamEvents(input, {\n      version: \"v2\",\n      streamMode: [\"values\", \"messages\"],\n    }),\n  });\n  ```\n</CodeGroup>\n\n***\n\n## Breaking changes\n\n### Dropped Node 18 support\n\nAll LangGraph packages now require **Node.js 20 or higher**. Node.js 18 reached [end of life](https://nodejs.org/en/about/releases/) in March 2025.\n\n### New build outputs\n\nBuilds for all langgraph packages now use a bundler based approach instead of using raw typescript outputs. If you were importing files from the `dist/` directory (which is not recommended), you will need to update your imports to use the new module system.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/migrate/langgraph-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: What's new in LangChain v1\nLink: https://docs.langchain.comoss/javascript/releases/langchain-v1\nContent: **LangChain v1 is a focused, production-ready foundation for building agents.** We've streamlined the framework around three core improvements:\n\n<CardGroup cols={1}>\n  <Card title=\"createAgent\" icon=\"robot\" href=\"#create-agent\" arrow>\n    A new standard way to build agents in LangChain, replacing `createReactAgent` from LangGraph with a cleaner, more powerful API.\n  </Card>\n\n  <Card title=\"Standard content blocks\" icon=\"cube\" href=\"#standard-content-blocks\" arrow>\n    A new `contentBlocks` property that provides unified access to modern LLM features across all providers.\n  </Card>\n\n  <Card title=\"Simplified package\" icon=\"sitemap\" href=\"#simplified-package\" arrow>\n    The `langchain` package has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to `@langchain/classic`.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install langchain @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm install langchain @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add langchain @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add langchain @langchain/core\n  ```\n</CodeGroup>\n\nFor a complete list of changes, see the [migration guide](/oss/javascript/migrate/langchain-v1).\n\n## `createAgent`\n\n`createAgent` is the standard way to build agents in LangChain 1.0. It provides a simpler interface than the prebuilt `createReactAgent` exported from LangGraph while offering greater customization potential by using middleware.\n\n```ts  theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n  systemPrompt: \"You are a helpful assistant.\",\n});\n\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"What is the weather in Tokyo?\" },\n  ],\n});\n\nconsole.log(result.content);\n```\n\nUnder the hood, `createAgent` is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063\" alt=\"Core agent loop diagram\" className=\"rounded-lg\" data-og-width=\"300\" width=\"300\" data-og-height=\"268\" height=\"268\" data-path=\"oss/images/core_agent_loop.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=280&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=a4c4b766b6678ef52a6ed556b1a0b032 280w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=560&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=111869e6e99a52c0eff60a1ef7ddc49c 560w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=840&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=6c1e21de7b53bd0a29683aca09c6f86e 840w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1100&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=88bef556edba9869b759551c610c60f4 1100w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1650&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=9b0bdd138e9548eeb5056dc0ed2d4a4b 1650w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=2500&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=41eb4f053ed5e6b0ba5bad2badf6d755 2500w\" />\n</div>\n\nFor more information, see [Agents](/oss/javascript/langchain/agents).\n\n### Middleware\n\nMiddleware is the defining feature of `createAgent`. It makes `createAgent` highly customizable, raising the ceiling for what you can build.\n\nGreat agents require [context engineering](/oss/javascript/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n\n#### Prebuilt middleware\n\nLangChain provides a few [prebuilt middlewares](/oss/javascript/langchain/middleware#built-in-middleware) for common patterns, including:\n\n* `summarizationMiddleware`: Condense conversation history when it gets too long\n* `humanInTheLoopMiddleware`: Require approval for sensitive tool calls\n* `piiRedactionMiddleware`: Redact sensitive information before sending to the model\n\n```ts  theme={null}\nimport {\n  createAgent,\n  summarizationMiddleware,\n  humanInTheLoopMiddleware,\n  piiRedactionMiddleware,\n} from \"langchain\";\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [readEmail, sendEmail],\n  middleware: [\n    piiRedactionMiddleware({ patterns: [\"email\", \"phone\", \"ssn\"] }),\n    summarizationMiddleware({\n      model: \"claude-sonnet-4-5-20250929\",\n      trigger: { tokens: 500 },\n    }),\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        sendEmail: {\n          allowedDecisions: [\"approve\", \"edit\", \"reject\"],\n        },\n      },\n    }),\n  ],\n});\n```\n\n#### Custom middleware\n\nYou can also build custom middleware to fit your specific needs.\n\nBuild custom middleware by implementing any of these hooks using the `createMiddleware` function:\n\n| Hook            | When it runs             | Use cases                               |\n| --------------- | ------------------------ | --------------------------------------- |\n| `beforeAgent`   | Before calling the agent | Load memory, validate input             |\n| `beforeModel`   | Before each LLM call     | Update prompts, trim messages           |\n| `wrapModelCall` | Around each LLM call     | Intercept and modify requests/responses |\n| `wrapToolCall`  | Around each tool call    | Intercept and modify tool execution     |\n| `afterModel`    | After each LLM response  | Validate output, apply guardrails       |\n| `afterAgent`    | After agent completes    | Save results, cleanup                   |\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1\" alt=\"Middleware flow diagram\" className=\"rounded-lg\" data-og-width=\"500\" width=\"500\" data-og-height=\"560\" height=\"560\" data-path=\"oss/images/middleware_final.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w\" />\n</div>\n\nExample custom middleware:\n\n```ts  theme={null}\nimport { createMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userExpertise: z.enum([\"beginner\", \"expert\"]).default(\"beginner\"),\n})\n\nconst expertiseBasedToolMiddleware = createMiddleware({\n  wrapModelCall: async (request, handler) => {\n    const userLevel = request.runtime.context.userExpertise;\n    if (userLevel === \"expert\") {\n      const tools = [advancedSearch, dataAnalysis];\n      return handler(\n        request.replace(\"openai:gpt-5\", tools)\n      );\n    }\n    const tools = [simpleSearch, basicCalculator];\n    return handler(\n      request.replace(\"openai:gpt-5-nano\", tools)\n    );\n  },\n});\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [simpleSearch, advancedSearch, basicCalculator, dataAnalysis],\n  middleware: [expertiseBasedToolMiddleware],\n  contextSchema,\n});\n```\n\nFor more information, see [the complete middleware guide](/oss/javascript/langchain/middleware).\n\n### Built on LangGraph\n\nBecause `createAgent` is built on LangGraph, you automatically get built in support for long running and reliable agents via:\n\n<CardGroup cols={2}>\n  <Card title=\"Persistence\" icon=\"database\">\n    Conversations automatically persist across sessions with built-in checkpointing\n  </Card>\n\n  <Card title=\"Streaming\" icon=\"water\">\n    Stream tokens, tool calls, and reasoning traces in real-time\n  </Card>\n\n  <Card title=\"Human-in-the-loop\" icon=\"hand\">\n    Pause agent execution for human approval before sensitive actions\n  </Card>\n\n  <Card title=\"Time travel\" icon=\"clock-rotate-left\">\n    Rewind conversations to any point and explore alternate paths and prompts\n  </Card>\n</CardGroup>\n\nYou don't need to learn LangGraph to use these features\u2014they work out of the box.\n\n### Structured output\n\n`createAgent` has improved structured output generation:\n\n* **Main loop integration**: Structured output is now generated in the main loop instead of requiring an additional LLM call\n* **Structured output strategy**: Models can choose between calling tools or using provider-side structured output generation\n* **Cost reduction**: Eliminates extra expense from additional LLM calls\n\n```ts  theme={null}\nimport { createAgent } from \"langchain\";\nimport * as z from \"zod\";\n\nconst weatherSchema = z.object({\n  temperature: z.number(),\n  condition: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\",\n  tools: [getWeather],\n  responseFormat: weatherSchema,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"What is the weather in Tokyo?\" },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n```\n\n**Error handling**: Control error handling via the `handleErrors` parameter to `ToolStrategy`:\n\n* **Parsing errors**: Model generates data that doesn't match desired structure\n* **Multiple tool calls**: Model generates 2+ tool calls for structured output schemas\n\n***\n\n## Standard content blocks\n\n<Note>\n  1.0 releases are available for most packages. Only the following currently support new content blocks:\n\n  * `langchain`\n  * `@langchain/core`\n  * `@langchain/anthropic`\n  * `@langchain/openai`\n\n  Broader support for content blocks is planned.\n</Note>\n\n### Benefits\n\n* **Provider agnostic**: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider\n* **Type safe**: Full type hints for all content block types\n* **Backward compatible**: Standard content can be [loaded lazily](/oss/javascript/langchain/messages#standard-content-blocks), so there are no associated breaking changes\n\nFor more information, see our guide on [content blocks](/oss/javascript/langchain/messages#message-content)\n\n***\n\n## Simplified package\n\nLangChain v1 streamlines the `langchain` package namespace to focus on essential building blocks for agents. The package exposes only the most useful and relevant functionality:\n\nMost of these are re-exported from `@langchain/core` for convenience, which gives you a focused API surface for building agents.\n\n### `@langchain/classic`\n\nLegacy functionality has moved to [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic) to keep the core package lean and focused.\n\n#### What's in `@langchain/classic`\n\n* Legacy chains and chain implementations\n* Retrievers\n* The indexing API\n* [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) exports\n* Other deprecated functionality\n\nIf you use any of this functionality, install [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic):\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/classic\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm install @langchain/classic\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/classic\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/classic\n  ```\n</CodeGroup>\n\nThen update your imports:\n\n```typescript  theme={null}\nimport { ... } from \"langchain\"; // [!code --]\nimport { ... } from \"@langchain/classic\"; // [!code ++]\n\nimport { ... } from \"langchain/chains\"; // [!code --]\nimport { ... } from \"@langchain/classic/chains\"; // [!code ++]\n```\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langchainjs/issues) using the [`'v1'` label](https://github.com/langchain-ai/langchainjs/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangChain 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Middleware guide\" icon=\"puzzle-piece\" href=\"https://blog.langchain.com/agent-middleware/\">\n    Deep dive into middleware\n  </Card>\n\n  <Card title=\"Agents Documentation\" icon=\"book\" href=\"/oss/javascript/langchain/agents\" arrow>\n    Full agent documentation\n  </Card>\n\n  <Card title=\"Message Content\" icon=\"message\" href=\"/oss/javascript/langchain/messages#message-content\" arrow>\n    New content blocks API\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/javascript/migrate/langchain-v1\" arrow>\n    How to migrate to LangChain v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langchainjs\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/javascript/versioning) \u2013 Understanding version numbers\n* [Release policy](/oss/javascript/release-policy) \u2013 Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/releases/langchain-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_07294050",
    "id": "c28a93f1-583d-476c-9350-b581830ab2f9",
    "content": "Result 1:\nTitle: Tools\nLink: https://docs.langchain.comoss/javascript/integrations/tools/anthropic\nContent: Tool Search Tools\nThe tool search tools enable Claude to work with hundreds or thousands of tools by dynamically discovering and loading them on-demand. This is useful when you have a large number of tools but don't want to load them all into the context window at once. There are two variants: toolSearchRegex_20251119 - Claude constructs regex patterns (using Python's re.search() syntax) to search for tools toolSearchBM25_20251119 - Claude uses natural language queries to search for tools using the BM25 algorithm Using the BM25 variant for natural language search: For more information, see Anthropic's Tool Search documentation.\n\nTool Search Tools\nThe tool search tools enable Claude to work with hundreds or thousands of tools by dynamically discovering and loading them on-demand. This is useful when you have a large number of tools but don't want to load them all into the context window at once. There are two variants: toolSearchRegex_20251119 - Claude constructs regex patterns (using Python's re.search() syntax) to search for tools toolSearchBM25_20251119 - Claude uses natural language queries to search for tools using the BM25 algorithm Using the BM25 variant for natural language search: import { tools } from \"@langchain/anthropic\";\n\nconst response = await llm.invoke(\"What is the weather in San Francisco?\", {\n  tools: [tools.toolSearchBM25_20251119(), getWeather, getNews],\n});\n\nTool Search Tools\nThe tool search tools enable Claude to work with hundreds or thousands of tools by dynamically discovering and loading them on-demand. This is useful when you have a large number of tools but don't want to load them all into the context window at once. There are two variants: toolSearchRegex_20251119 - Claude constructs regex patterns (using Python's re.search() syntax) to search for tools toolSearchBM25_20251119 - Claude uses natural language queries to search for tools using the BM25 algorithm import { ChatAnthropic, tools } from \"@langchain/anthropic\";\nimport { tool } from \"langchain\";\nimport { z } from \"zod\";\n\nconst llm = new ChatAnthropic({\n  model: \"claude-sonnet-4-5-20250929\",\n});\n\n// Create tools with defer_loading to make them discoverable via search\nconst getWeather = tool(\n  async (input: { location: string }) => {\n    return `Weather in ${input.location}: Sunny, 72\u00b0F`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get the weather at a specific location\",\n    schema: z.object({\n      location: z.string(),\n    }),\n    extras: { defer_loading: true },\n  }\n);\n\nconst getNews = tool(\n  async (input: { topic: string }) => {\n    return `Latest news about ${input.topic}...`;\n  },\n  {\n    name: \"get_news\",\n    description: \"Get the latest news about a topic\",\n    schema: z.object({\n      topic: z.string(),\n    }),\n    extras: { defer_loading: true },\n  }\n);\n\n// Claude will search and discover tools as needed\nconst response = await llm.invoke(\"What is the weather in San Francisco?\", {\n  tools: [tools.toolSearchRegex_20251119(), getWeather, getNews],\n});\n\n---\n\nResult 2:\nTitle: Tools and Toolkits\nLink: https://docs.langchain.comoss/javascript/integrations/tools/index\nContent: [Tools](/oss/javascript/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n\nA [toolkit](/oss/javascript/langchain/tools#toolkits) is a collection of tools meant to be used together.\n\n## Integration Platforms\n\nThe following platforms provide access to multiple tools and services through a unified interface:\n\n| Tool/Toolkit                                            | Number of Integrations | Pricing             | Key Features                                               |\n| ------------------------------------------------------- | ---------------------- | ------------------- | ---------------------------------------------------------- |\n| [Composio](/oss/javascript/integrations/tools/composio) | 500+                   | Free tier available | OAuth handling, event-driven workflows, multi-user support |\n\n## All Tools and Toolkits\n\n<Columns cols={3}>\n  <Card title=\"Azure Container Apps Dynamic Sessions\" icon=\"link\" href=\"/oss/javascript/integrations/tools/azure_dynamic_sessions\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Connery Action Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/connery\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Composio\" icon=\"link\" href=\"/oss/javascript/integrations/tools/composio\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Dall-E Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/dalle\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Decodo Tools\" icon=\"link\" href=\"/oss/javascript/integrations/tools/decodo\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Discord Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/discord_tool\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"DuckDuckGoSearch\" icon=\"link\" href=\"/oss/javascript/integrations/tools/duckduckgo_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"ExaSearchResults\" icon=\"link\" href=\"/oss/javascript/integrations/tools/exa_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"FalkorDB\" icon=\"link\" href=\"/oss/javascript/integrations/tools/falkordb\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Gmail Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_gmail\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"GOAT\" icon=\"link\" href=\"/oss/javascript/integrations/tools/goat\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Calendar Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_calendar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Places Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_places\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Routes Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_routes\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Scholar\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_scholar\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Google Trends Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/google_trends\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"JigsawStack Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/jigsawstack\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Agent with AWS Lambda\" icon=\"link\" href=\"/oss/javascript/integrations/tools/lambda_agent\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Python interpreter tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/pyinterpreter\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SearchApi tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/searchapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Searxng Search tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/searxng\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"SerpAPI\" icon=\"link\" href=\"/oss/javascript/integrations/tools/serpapi\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"StackExchange Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/stackexchange\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Stagehand AI Web Automation Toolkit\" icon=\"link\" href=\"/oss/javascript/integrations/tools/stagehand\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Crawl\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_crawl\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Extract\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_extract\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Map\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_map\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Tavily Search\" icon=\"link\" href=\"/oss/javascript/integrations/tools/tavily_search\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Web Browser Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/webbrowser\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"Wikipedia tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/wikipedia\" arrow=\"true\" cta=\"View guide\" />\n\n  <Card title=\"WolframAlpha Tool\" icon=\"link\" href=\"/oss/javascript/integrations/tools/wolframalpha\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n<Info>\n  If you'd like to write your own tool, see [this how-to](/oss/javascript/langchain/tools#customize-tool-properties). If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).\n</Info>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/tools/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Tools\nLink: https://docs.langchain.comoss/javascript/langchain/tools\nContent: Tools extend what [agents](/oss/javascript/langchain/agents) can do\u2014letting them fetch real-time data, execute code, query external databases, and take actions in the world.\n\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a [chat model](/oss/javascript/langchain/models). The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\n\n<Tip>\n  For details on how models handle tool calls, see [Tool calling](/oss/javascript/langchain/models#tool-calling).\n</Tip>\n\n## Create tools\n\n### Basic tool definition\n\nThe simplest way to create a tool is by importing the `tool` function from the `langchain` package. You can use [zod](https://zod.dev/) to define the tool's input schema:\n\n```ts  theme={null}\nimport * as z from \"zod\"\nimport { tool } from \"langchain\"\n\nconst searchDatabase = tool(\n  ({ query, limit }) => `Found ${limit} results for '${query}'`,\n  {\n    name: \"search_database\",\n    description: \"Search the customer database for records matching the query.\",\n    schema: z.object({\n      query: z.string().describe(\"Search terms to look for\"),\n      limit: z.number().describe(\"Maximum number of results to return\"),\n    }),\n  }\n);\n```\n\n<Note>\n  **Server-side tool use**\n\n  Some chat models (e.g., [OpenAI](/oss/javascript/integrations/chat/openai), [Anthropic](/oss/javascript/integrations/chat/anthropic), and [Gemini](/oss/javascript/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/javascript/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/javascript/integrations/providers/overview) to learn how to access these tools with your specific chat model.\n</Note>\n\n## Accessing Context\n\n<Info>\n  **Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\n  The runtime context provides a structured way to supply runtime data, such as DB connections, user IDs, or config, into your tools. This avoids global state and keeps tools testable and reusable.\n</Info>\n\n#### Context\n\nTools can access an agent's runtime context through the `config` parameter:\n\n```ts  theme={null}\nimport * as z from \"zod\"\nimport { ChatOpenAI } from \"@langchain/openai\"\nimport { createAgent } from \"langchain\"\n\nconst getUserName = tool(\n  (_, config) => {\n    return config.context.user_name\n  },\n  {\n    name: \"get_user_name\",\n    description: \"Get the user's name.\",\n    schema: z.object({}),\n  }\n);\n\nconst contextSchema = z.object({\n  user_name: z.string(),\n});\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserName],\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"What is my name?\" }]\n  },\n  {\n    context: { user_name: \"John Smith\" }\n  }\n);\n```\n\n#### Memory (Store)\n\nAccess persistent data across conversations using the store. The store is accessed via `config.store` and allows you to save and retrieve user-specific or application-specific data.\n\n```ts expandable theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst store = new InMemoryStore();\n\n// Access memory\nconst getUserInfo = tool(\n  async ({ user_id }) => {\n    const value = await store.get([\"users\"], user_id);\n    console.log(\"get_user_info\", user_id, value);\n    return value;\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({\n      user_id: z.string(),\n    }),\n  }\n);\n\n// Update memory\nconst saveUserInfo = tool(\n  async ({ user_id, name, age, email }) => {\n    console.log(\"save_user_info\", user_id, name, age, email);\n    await store.put([\"users\"], user_id, { name, age, email });\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info.\",\n    schema: z.object({\n      user_id: z.string(),\n      name: z.string(),\n      age: z.number(),\n      email: z.string(),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4o\" }),\n  tools: [getUserInfo, saveUserInfo],\n  store,\n});\n\n// First session: save user info\nawait agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\",\n    },\n  ],\n});\n\n// Second session: get user info\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"Get user info for user with id 'abc123'\" },\n  ],\n});\n\nconsole.log(result);\n// Here is the user info for user with ID \"abc123\":\n// - Name: Foo\n// - Age: 25\n// - Email: foo@langchain.dev\n```\n\n#### Stream Writer\n\nStream custom updates from tools as they execute using `config.streamWriter`. This is useful for providing real-time feedback to users about what a tool is doing.\n\n```ts  theme={null}\nimport * as z from \"zod\";\nimport { tool, ToolRuntime } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }, config: ToolRuntime) => {\n    const writer = config.writer;\n\n    // Stream custom updates as the tool executes\n    if (writer) {\n      writer(`Looking up data for city: ${city}`);\n      writer(`Acquired data for city: ${city}`);\n    }\n\n    return `It's always sunny in ${city}!`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather for a given city.\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Google\nLink: https://docs.langchain.comoss/python/integrations/providers/google\nContent: Tools\nGoogle Search Perform web searches using Google Custom Search Engine (CSE). Google Drive Tools for interacting with Google Drive. Google Finance Query financial data. Google Jobs Query job listings. Google Lens Perform visual searches. Google Places Search for places information. Google Scholar Search academic papers. Google Trends Query Google Trends data.\n\nToolkits\nCollections of tools for specific Google services. Gmail Toolkit to create, get, search, and send emails using the Gmail API.\n\nRetrievers\nRetrieve information using Google Cloud services. Vertex AI Search Build generative AI powered search engines using Vertex AI Search Document AI Warehouse Search, store, and manage documents using Document AI Warehouse. from langchain_google_community import VertexAIMultiTurnSearchRetriever\nfrom langchain_google_community import VertexAISearchRetriever\nfrom langchain_google_community import VertexAISearchSummaryTool\n\n---\n\nResult 5:\nTitle: Tools\nLink: https://docs.langchain.comoss/python/langchain/tools\nContent: Tools extend what [agents](/oss/python/langchain/agents) can do\u2014letting them fetch real-time data, execute code, query external databases, and take actions in the world.\n\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a [chat model](/oss/python/langchain/models). The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\n\n<Tip>\n  For details on how models handle tool calls, see [Tool calling](/oss/python/langchain/models#tool-calling).\n</Tip>\n\n## Create tools\n\n### Basic tool definition\n\nThe simplest way to create a tool is with the [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) decorator. By default, the function's docstring becomes the tool's description that helps the model understand when to use it:\n\n```python  theme={null}\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str, limit: int = 10) -> str:\n    \"\"\"Search the customer database for records matching the query.\n\n    Args:\n        query: Search terms to look for\n        limit: Maximum number of results to return\n    \"\"\"\n    return f\"Found {limit} results for '{query}'\"\n```\n\nType hints are **required** as they define the tool's input schema. The docstring should be informative and concise to help the model understand the tool's purpose.\n\n<Note>\n  **Server-side tool use**\n\n  Some chat models (e.g., [OpenAI](/oss/python/integrations/chat/openai), [Anthropic](/oss/python/integrations/chat/anthropic), and [Gemini](/oss/python/integrations/chat/google_generative_ai)) feature [built-in tools](/oss/python/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/python/integrations/providers/overview) to learn how to access these tools with your specific chat model.\n</Note>\n\n### Customize tool properties\n\n#### Custom tool name\n\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\n\n```python  theme={null}\n@tool(\"web_search\")  # Custom name\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nprint(search.name)  # web_search\n```\n\n#### Custom tool description\n\nOverride the auto-generated tool description for clearer model guidance:\n\n```python  theme={null}\n@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\ndef calc(expression: str) -> str:\n    \"\"\"Evaluate mathematical expressions.\"\"\"\n    return str(eval(expression))\n```\n\n### Advanced schema definition\n\nDefine complex inputs with Pydantic models or JSON schemas:\n\n<CodeGroup>\n  ```python Pydantic model theme={null}\n  from pydantic import BaseModel, Field\n  from typing import Literal\n\n  class WeatherInput(BaseModel):\n      \"\"\"Input for weather queries.\"\"\"\n      location: str = Field(description=\"City name or coordinates\")\n      units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n          default=\"celsius\",\n          description=\"Temperature unit preference\"\n      )\n      include_forecast: bool = Field(\n          default=False,\n          description=\"Include 5-day forecast\"\n      )\n\n  @tool(args_schema=WeatherInput)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n\n  ```python JSON Schema theme={null}\n  weather_schema = {\n      \"type\": \"object\",\n      \"properties\": {\n          \"location\": {\"type\": \"string\"},\n          \"units\": {\"type\": \"string\"},\n          \"include_forecast\": {\"type\": \"boolean\"}\n      },\n      \"required\": [\"location\", \"units\", \"include_forecast\"]\n  }\n\n  @tool(args_schema=weather_schema)\n  def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n      \"\"\"Get current weather and optional forecast.\"\"\"\n      temp = 22 if units == \"celsius\" else 72\n      result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n      if include_forecast:\n          result += \"\\nNext 5 days: Sunny\"\n      return result\n  ```\n</CodeGroup>\n\n### Reserved argument names\n\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\n\n| Parameter name | Purpose                                                                |\n| -------------- | ---------------------------------------------------------------------- |\n| `config`       | Reserved for passing `RunnableConfig` to tools internally              |\n| `runtime`      | Reserved for `ToolRuntime` parameter (accessing state, context, store) |\n\nTo access runtime information, use the [`ToolRuntime`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.ToolRuntime) parameter instead of naming your own arguments `config` or `runtime`.\n\n## Accessing Context\n\n<Info>\n  **Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\n  Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.\n</Info>\n\nTools can access runtime information through the `ToolRuntime` parameter, which provides:\n\n* **State** - Mutable data that flows through execution (e.g., messages, counters, custom fields)\n* **Context** - Immutable configuration like user IDs, session details, or application-specific configuration\n* **Store** - Persistent long-term memory across conversations\n* **Stream Writer** - Stream custom updates as tools execute\n* **Config** - `RunnableConfig` for the execution\n* **Tool Call ID** - ID of the current tool call\n\n```mermaid  theme={null}\ngraph LR\n    %% Runtime Context\n    subgraph \"\ud83d\udd27 Tool Runtime Context\"\n        A[Tool Call] --> B[ToolRuntime]\n        B --> C[State Access]\n        B --> D[Context Access]\n        B --> E[Store Access]\n        B --> F[Stream Writer]\n    end\n\n    %% Available Resources\n    subgraph \"\ud83d\udcca Available Resources\"\n        C --> G[Messages]\n        C --> H[Custom State]\n        D --> I[User ID]\n        D --> J[Session Info]\n        E --> K[Long-term Memory]\n        E --> L[User Preferences]\n    end\n\n    %% Tool Capabilities\n    subgraph \"\u26a1 Enhanced Tool Capabilities\"\n        M[Context-Aware Tools]\n        N[Stateful Tools]\n        O[Memory-Enabled Tools]\n        P[Streaming Tools]\n    end\n\n    %% Connections\n    G --> M\n    H --> N\n    I --> M\n    J --> M\n    K --> O\n    L --> O\n    F --> P\n```\n\n### `ToolRuntime`\n\nUse `ToolRuntime` to access all runtime information in a single parameter. Simply add `runtime: ToolRuntime` to your tool signature, and it will be automatically injected without being exposed to the LLM.\n\n<Info>\n  **`ToolRuntime`**: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate [`InjectedState`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedState), [`InjectedStore`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.InjectedStore), [`get_runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.get_runtime), and [`InjectedToolCallId`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.InjectedToolCallId) annotations.\n\n  The runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.\n</Info>\n\n**Accessing state:**\n\nTools can access the current graph state using `ToolRuntime`:\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n# Access the current conversation state\n@tool\ndef summarize_conversation(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Summarize the conversation so far.\"\"\"\n    messages = runtime.state[\"messages\"]\n\n    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n\n    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\n\n# Access custom state fields\n@tool\ndef get_user_preference(\n    pref_name: str,\n    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model\n) -> str:\n    \"\"\"Get a user preference value.\"\"\"\n    preferences = runtime.state.get(\"user_preferences\", {})\n    return preferences.get(pref_name, \"Not set\")\n```\n\n<Warning>\n  The `runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `runtime` is *not* included in the request.\n</Warning>\n\n**Updating state:**\n\nUse [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) to update the agent's state or control the graph's execution flow:\n\n```python  theme={null}\nfrom langgraph.types import Command\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langchain.tools import tool, ToolRuntime\n\n# Update the conversation history by removing all messages\n@tool\ndef clear_conversation() -> Command:\n    \"\"\"Clear the conversation history.\"\"\"\n\n    return Command(\n        update={\n            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],\n        }\n    )\n\n# Update the user_name in the agent state\n@tool\ndef update_user_name(\n    new_name: str,\n    runtime: ToolRuntime\n) -> Command:\n    \"\"\"Update the user's name.\"\"\"\n    return Command(update={\"user_name\": new_name})\n```\n\n#### Context\n\nAccess immutable configuration and contextual data like user IDs, session details, or application-specific configuration through `runtime.context`.\n\nTools can access runtime context through `ToolRuntime`:\n\n```python  theme={null}\nfrom dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\nUSER_DATABASE = {\n    \"user123\": {\n        \"name\": \"Alice Johnson\",\n        \"account_type\": \"Premium\",\n        \"balance\": 5000,\n        \"email\": \"alice@example.com\"\n    },\n    \"user456\": {\n        \"name\": \"Bob Smith\",\n        \"account_type\": \"Standard\",\n        \"balance\": 1200,\n        \"email\": \"bob@example.com\"\n    }\n}\n\n@dataclass\nclass UserContext:\n    user_id: str\n\n@tool\ndef get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n    \"\"\"Get the current user's account information.\"\"\"\n    user_id = runtime.context.user_id\n\n    if user_id in USER_DATABASE:\n        user = USER_DATABASE[user_id]\n        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n    return \"User not found\"\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nagent = create_agent(\n    model,\n    tools=[get_account_info],\n    context_schema=UserContext,\n    system_prompt=\"You are a financial assistant.\"\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n    context=UserContext(user_id=\"user123\")\n)\n```\n\n#### Memory (Store)\n\nAccess persistent data across conversations using the store. The store is accessed via `runtime.store` and allows you to save and retrieve user-specific or application-specific data.\n\nTools can access and update the store through `ToolRuntime`:\n\n```python expandable theme={null}\nfrom typing import Any\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\n# Access memory\n@tool\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n    \"\"\"Look up user info.\"\"\"\n    store = runtime.store\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\n# Update memory\n@tool\ndef save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n    \"\"\"Save user info.\"\"\"\n    store = runtime.store\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nstore = InMemoryStore()\nagent = create_agent(\n    model,\n    tools=[get_user_info, save_user_info],\n    store=store\n)\n\n# First session: save user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n})\n\n# Second session: get user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email: foo@langchain.dev\n```\n\n#### Stream Writer\n\nStream custom updates from tools as they execute using `runtime.stream_writer`. This is useful for providing real-time feedback to users about what a tool is doing.\n\n```python  theme={null}\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather(city: str, runtime: ToolRuntime) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = runtime.stream_writer\n\n    # Stream custom updates as the tool executes\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n\n    return f\"It's always sunny in {city}!\"\n```\n\n<Note>\n  If you use `runtime.stream_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/python/langchain/streaming) for more details.\n</Note>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_71569374",
    "id": "0aa441cf-51e5-438c-95d5-ea9a51ece190",
    "content": "{\n  \"collections\": \"OSS (LangChain and LangGraph),SDKs and APIs\",\n  \"total\": 14,\n  \"articles\": [\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3a32-2f3b-7253-a164-879e1c83c141",
    "content": "**`createAgent` \u9ed8\u8ba4\u652f\u6301\u8c03\u7528**\u591a\u4e2a\u5de5\u5177**\uff0c\u800c\u975e\u4ec5\u9650\u4e8e\u4e00\u4e2a\u5de5\u5177\u3002**\n\n`createAgent` \u901a\u8fc7 ReAct \u5faa\u73af\uff08Reasoning + Acting\uff09\u5b9e\u73b0\uff1aLLM \u6839\u636e\u4efb\u52a1\u51b3\u5b9a\u8c03\u7528\u54ea\u4e9b\u5de5\u5177\uff0c\u652f\u6301**\u5355\u6b21\u8c03\u7528\u591a\u4e2a\u5de5\u5177**\uff08parallel tool calls\uff09\u3001**\u987a\u5e8f\u8c03\u7528**\uff08sequence calls\uff09\uff0c\u5e76\u6839\u636e\u5de5\u5177\u8fd4\u56de\u7ed3\u679c\u52a8\u6001\u9009\u62e9\u540e\u7eed\u5de5\u5177\uff0c\u76f4\u5230\u5b8c\u6210\u4efb\u52a1\u3002\n\n```typescript\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({ query: z.string() }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\", \n    description: \"Get weather information\",\n    schema: z.object({ location: z.string() }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",  // \u652f\u6301\u5de5\u5177\u8c03\u7528\u7684\u6a21\u578b\n  tools: [search, getWeather],  // \u4f20\u5165\u591a\u4e2a\u5de5\u5177\n});\n\n// \u793a\u4f8b\uff1a\u53ef\u80fd\u5148\u8c03\u7528 search(\"wireless headphones\")\uff0c\u7136\u540e getWeather(\"San Francisco\")\nconst result = await agent.invoke({\n  messages: [{ role: \"user\", content: \"Find popular headphones and check SF weather\" }],\n});\n```\n\n## \u5de5\u5177\u8c03\u7528\u673a\u5236\n\nAgent \u5728\u6bcf\u4e2a\u5faa\u73af\u4e2d\uff1a\n1. LLM \u5206\u6790\u5f53\u524d\u72b6\u6001\uff0c**\u9009\u62e9 0 \u4e2a\u6216\u591a\u4e2a\u5de5\u5177**\uff08\u53d6\u51b3\u4e8e\u6a21\u578b\u5982 GPT-4o \u7684 parallel tool calling \u80fd\u529b\uff09\n2. \u5e76\u884c/\u987a\u5e8f\u6267\u884c\u9009\u4e2d\u7684\u5de5\u5177\n3. \u5c06 `ToolMessage` \u8fd4\u56de\u7ed9 LLM\uff0c\u7ee7\u7eed\u51b3\u7b56\n\n\u652f\u6301**\u52a8\u6001\u5de5\u5177\u9009\u62e9**\uff1a\u6839\u636e\u4e0a\u4e00\u6b65\u5de5\u5177\u7ed3\u679c\uff0cLLM \u53ef\u9009\u62e9\u4e0d\u540c\u5de5\u5177\u7ec4\u5408\u3002\n\n**\u76f8\u5173\u6587\u6863\uff1a**\n- [Agents](https://docs.langchain.com/oss/javascript/langchain/agents)\n- [Tools](https://docs.langchain.com/oss/javascript/langchain/tools)"
  }
]