[
  {
    "role": "user",
    "id": "cf270ea5-d496-47be-8b8e-66ac38c089de",
    "content": "How to start with langchain project?"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_17932472",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"quickstart\"}"
        }
      },
      {
        "type": "function",
        "id": "call_52826932",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"getting started\"}"
        }
      },
      {
        "type": "function",
        "id": "call_10138018",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"installation\"}"
        }
      },
      {
        "type": "function",
        "id": "call_88029345",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"all\"}"
        }
      }
    ],
    "id": "lc_run--019b384e-b09e-7173-b388-f49039e63bd1",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_17932472",
    "id": "2770a66c-fc5a-466b-ba40-e1e3b7933105",
    "content": "Result 1:\nTitle: Cloud\nLink: https://docs.langchain.comlangsmith/cloud\nContent: <Callout icon=\"rocket\" color=\"#4F46E5\" iconType=\"regular\">\n  If you're ready to deploy your app to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or the [full setup guide](/langsmith/deploy-to-cloud). This page explains the Cloud managed architecture for reference.\n</Callout>\n\nThe **Cloud** option is a fully managed model where LangChain hosts and operates all LangSmith infrastructure and services:\n\n* **Fully managed infrastructure**: LangChain handles all infrastructure, updates, scaling, and maintenance.\n* **Deploy from GitHub**: Connect your repositories and deploy with a few clicks.\n* **Automated CI/CD**: Build process is handled automatically by the platform.\n* **LangSmith UI**: Full access to [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), [deployment management](/langsmith/deployments), and [Studio](/langsmith/studio).\n\n|                                               | **Who manages it** | **Where it runs** |\n| --------------------------------------------- | ------------------ | ----------------- |\n| **LangSmith platform (UI, APIs, datastores)** | LangChain          | LangChain's cloud |\n| **Your Agent Servers**                        | LangChain          | LangChain's cloud |\n| **CI/CD for your apps**                       | LangChain          | LangChain's cloud |\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3f0316122425895270d0ecd47b12e139\" alt=\"Cloud deployment: LangChain hosts and manages all components including the UI, APIs, and your Agent Servers.\" data-og-width=\"1425\" width=\"1425\" data-og-height=\"1063\" height=\"1063\" data-path=\"langsmith/images/langgraph-cloud-architecture.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b34a10fb40ca5188dddfb6be69696c75 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d310ce86e8421878575cf4d4fa72bf78 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=7cc3fcee9c36a8d1e8da4cc4abbde3b9 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a184c950c8585e2fdb5e75ac7f0f5642 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=cb778e4a6707b00eaf703886d32569bd 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=00f9d9df44512dded307613502d03299 2500w\" />\n\n## Get started\n\nTo deploy your first application to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or refer to the [comprehensive setup guide](/langsmith/deploy-to-cloud).\n\n## Cloud architecture and scalability\n\n<Note>\n  This section is only relevant for the cloud-managed LangSmith services available at [https://smith.langchain.com](https://smith.langchain.com) and [https://eu.smith.langchain.com](https://eu.smith.langchain.com).\n\n  For information on the self-hosted LangSmith solution, please refer to the [self-hosted documentation](/langsmith/self-hosted).\n</Note>\n\nLangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for LLM application observability, evaluation, and agent deployment\n\n### Architecture\n\nThe US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.\n\n<Note>\n  The [EU-based LangSmith service](https://eu.smith.langchain.com) is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, [contact our sales team](https://www.langchain.com/contact-sales).\n</Note>\n\n#### Regional storage\n\nThe resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses [Supabase](https://supabase.com) for authentication/authorization and [ClickHouse Cloud](https://clickhouse.com/cloud) for data warehouse.\n\n|                                                | US                                                                 | EU                                                                       |\n| ---------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------ |\n| URL                                            | [https://smith.langchain.com](https://smith.langchain.com)         | [https://eu.smith.langchain.com](https://eu.smith.langchain.com)         |\n| API URL                                        | [https://api.smith.langchain.com](https://api.smith.langchain.com) | [https://eu.api.smith.langchain.com](https://eu.api.smith.langchain.com) |\n| GCP                                            | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| Supabase                                       | AWS us-east-1 (N. Virginia)                                        | AWS eu-central-1 (Germany)                                               |\n| ClickHouse Cloud                               | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| [LangSmith deployment](/langsmith/deployments) | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n\nSee the [Regions FAQ](/langsmith/regions-faq) for more information.\n\n#### Region-independent storage\n\nData listed here is stored exclusively in the US:\n\n* Payment and billing information with Stripe and Metronome\n\n#### GCP services\n\nLangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):\n\n* LangSmith Frontend: serves the LangSmith UI.\n* LangSmith Backend: serves the LangSmith API.\n* LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)\n* LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.\n* LangSmith Queue: handles processing of asynchronous tasks. (Internal service)\n\nLangSmith uses the following GCP storage services:\n\n* Google Cloud Storage (GCS) for runs inputs and outputs.\n* Google Cloud SQL PostgreSQL for transactional workloads.\n* Google Cloud Memorystore for Redis for queuing and caching.\n* Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.\n\nSome additional GCP services we use include:\n\n* Google Cloud Load Balancer for routing traffic to the LangSmith services.\n* Google Cloud CDN for caching static assets.\n* Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to [this guide](/langsmith/administration-overview#rate-limits).\n\n<div style={{ textAlign: 'center' }}>\n  <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3\" alt=\"Light mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-light.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w\" />\n\n  <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b\" alt=\"Dark mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w\" />\n</div>\n\n### Allowlisting IP addresses\n\n#### Egress from LangChain SaaS\n\nAll traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:\n\n| US             | EU             |\n| -------------- | -------------- |\n| 34.59.65.97    | 34.13.192.67   |\n| 34.67.51.221   | 34.147.105.64  |\n| 34.46.212.37   | 34.90.22.166   |\n| 34.132.150.88  | 34.147.36.213  |\n| 35.188.222.201 | 34.32.137.113  |\n| 34.58.194.127  | 34.91.238.184  |\n| 34.59.97.173   | 35.204.101.241 |\n| 104.198.162.55 | 35.204.48.32   |\n\nIt may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.\n\n#### Ingress into LangChain SaaS\n\nThe langchain endpoints map to the following static IP addresses:\n\n| US             | EU           |\n| -------------- | ------------ |\n| 34.8.121.39    | 34.95.92.214 |\n| 34.107.251.234 | 34.13.73.122 |\n\nYou may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (`api.smith.langchain.com`, `smith.langchain.com`, `beacon.langchain.com`, `eu.api.smith.langchain.com`, `eu.smith.langchain.com`, `eu.beacon.langchain.com`).\n\n### API rate limits\n\nLangSmith enforces rate limits on API endpoints to ensure service stability and fair usage. The following table shows the rate limits for different endpoints in both US and EU regions. Note that:\n\n* Rate limits are expressed as `count / interval` where count is the number of requests allowed within the interval (in seconds). For example, `2000 / 10` means 2000 requests per 10 seconds.\n* When no HTTP method is specified in the endpoint column, the rate limit applies to all HTTP methods for that endpoint.\n* When a specific method is listed (e.g., `POST`, `GET`), the rate limit applies only to that method.\n\n| Match / Endpoint (method)                   | Identity key     | US prod limit | EU prod limit | Category                                     |\n| ------------------------------------------- | ---------------- | ------------- | ------------- | -------------------------------------------- |\n| OPTIONS, `/info`, `*/v1/metadata/submit`    | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-user-id` + IP | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/v1/beacon`                                | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/repos`                                    | `x-api-key`      | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `/repos`                                    | `x-user-id` + IP | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `POST /runs/batch`                          | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `POST /otel/v1/traces`                      | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST` containing `/charts`                 | `x-api-key`      | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST` containing `/charts`                 | `x-user-id` + IP | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST /runs/multipart`                      | `x-api-key`      | 6000 / 10     | 6000 / 10     | [Multipart ingest](#rate-limit-categories)   |\n| `POST /runs/query`                          | `x-api-key`      | 15 / 10       | 15 / 10       | [Run query (API)](#rate-limit-categories)    |\n| `POST /runs/query`                          | `x-user-id` + IP | 300 / 10      | 300 / 10      | [Run query (User)](#rate-limit-categories)   |\n| `/generate`                                 | `x-api-key`      | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/generate`                                 | `x-user-id` + IP | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/commits`                                  | `x-api-key`      | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `/commits`                                  | `x-user-id` + IP | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `DELETE /sessions` or `*/trigger`           | `x-api-key`      | 10 / 60       | 10 / 60       | [Deletion](#rate-limit-categories)           |\n| `DELETE /sessions` or `*/trigger`           | `x-user-id` + IP | 30 / 60       | 30 / 60       | [Deletion](#rate-limit-categories)           |\n| `POST /runs` (single run ingest)            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `PATCH` containing `/runs`                  | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST /feedback`                            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `GET /runs/{uuid}` or `/api/v1/runs/{uuid}` | `x-api-key`      | 30 / 60       | 30 / 60       | [Run lookup](#rate-limit-categories)         |\n| `GET` containing `/examples`                | `x-api-key`      | 5000 / 60     | 5000 / 60     | [Examples](#rate-limit-categories)           |\n| Any request with `x-api-key`                | `x-api-key`      | 1000 / 10     | 1000 / 10     | [Default (API key)](#rate-limit-categories)  |\n| Any request with `x-user-id`                | `x-user-id` + IP | 1000 / 10     | 1000 / 10     | [Default (User)](#rate-limit-categories)     |\n| `/public/download`                          | IP               | 5000 / 60     | 5000 / 60     | [Public download](#rate-limit-categories)    |\n| `/runs/stats`                               | `x-api-key`      | 1 / 10        | 20 / 10       | [Stats](#rate-limit-categories)              |\n| All other IPs (catch-all)                   | IP               | 100 / 60      | 100 / 60      | [Public (catch-all)](#rate-limit-categories) |\n\n#### Rate limit categories\n\n* **High throughput**: General high-volume endpoints for core operations like authentication, metadata, and feedback.\n* **Repository**: Repository and prompt management operations.\n* **Run ingest**: Individual trace/run ingestion endpoints for observability.\n* **Charts**: Chart generation and visualization endpoints.\n* **Multipart ingest**: Bulk run ingestion via multipart upload for high-volume tracing.\n* **Run query (API)**: API key-based run query operations with stricter limits for complex queries.\n* **Run query (User)**: User-based run query operations with higher limits for interactive use.\n* **Generation**: AI-powered code and content generation endpoints (limited to prevent abuse).\n* **Commits**: Prompt versioning and commit operations.\n* **Deletion**: Session deletion and workflow trigger operations.\n* **Run lookup**: Retrieving specific runs by UUID.\n* **Examples**: Fetching dataset examples for few-shot prompting.\n* **Default (API key)**: Fallback rate limit for authenticated API requests not matching specific patterns.\n* **Default (User)**: Fallback rate limit for authenticated user requests not matching specific patterns.\n* **Public download**: High-volume public download endpoints for shared resources.\n* **Stats**: Run statistics and analytics endpoints (region-specific limits apply).\n* **Public (catch-all)**: Default rate limit for unauthenticated public access.\n\nFor more information on rate limits and other service limits, refer to the [Administration overview](/langsmith/administration-overview#rate-limits).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cloud.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/javascript/deepagents/quickstart\nContent: This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You'll build a research agent that can conduct research and write reports.\n\n## Prerequisites\n\nBefore you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).\n\n### Step 1: Install dependencies\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install deepagents @langchain/tavily\n  ```\n\n  ```bash yarn theme={null}\n  yarn add deepagents @langchain/tavily\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add deepagents @langchain/tavily\n  ```\n</CodeGroup>\n\n### Step 2: Set up your API keys\n\n```bash  theme={null}\nexport ANTHROPIC_API_KEY=\"your-api-key\"\nexport TAVILY_API_KEY=\"your-tavily-api-key\"\n```\n\n### Step 3: Create a search tool\n\n```typescript  theme={null}\nimport { tool } from \"langchain\";\nimport { TavilySearch } from \"@langchain/tavily\";\nimport { z } from \"zod\";\n\nconst internetSearch = tool(\n  async ({\n    query,\n    maxResults = 5,\n    topic = \"general\",\n    includeRawContent = false,\n  }: {\n    query: string;\n    maxResults?: number;\n    topic?: \"general\" | \"news\" | \"finance\";\n    includeRawContent?: boolean;\n  }) => {\n    const tavilySearch = new TavilySearch({\n      maxResults,\n      tavilyApiKey: process.env.TAVILY_API_KEY,\n      includeRawContent,\n      topic,\n    });\n    return await tavilySearch._call({ query });\n  },\n  {\n    name: \"internet_search\",\n    description: \"Run a web search\",\n    schema: z.object({\n      query: z.string().describe(\"The search query\"),\n      maxResults: z\n        .number()\n        .optional()\n        .default(5)\n        .describe(\"Maximum number of results to return\"),\n      topic: z\n        .enum([\"general\", \"news\", \"finance\"])\n        .optional()\n        .default(\"general\")\n        .describe(\"Search topic category\"),\n      includeRawContent: z\n        .boolean()\n        .optional()\n        .default(false)\n        .describe(\"Whether to include raw content\"),\n    }),\n  },\n);\n```\n\n### Step 4: Create a deep agent\n\n```typescript  theme={null}\nimport { createDeepAgent } from \"deepagents\";\n\n// System prompt to steer the agent to be an expert researcher\nconst researchInstructions = `You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n\nYou have access to an internet search tool as your primary means of gathering information.\n\n## \\`internet_search\\`\n\nUse this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n`;\n\nconst agent = createDeepAgent({\n  tools: [internetSearch],\n  systemPrompt: researchInstructions,\n});\n```\n\n### Step 5: Run the agent\n\n```typescript  theme={null}\nconst result = await agent.invoke({\n  messages: [{ role: \"user\", content: \"What is langgraph?\" }],\n});\n\n// Print the agent's response\nconsole.log(result.messages[result.messages.length - 1].content);\n```\n\n## What happened?\n\nYour deep agent automatically:\n\n1. **Planned its approach**: Used the built-in `write_todos` tool to break down the research task\n2. **Conducted research**: Called the `internet_search` tool to gather information\n3. **Managed context**: Used file system tools (`write_file`, `read_file`) to offload large search results\n4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents\n5. **Synthesized a report**: Compiled findings into a coherent response\n\n## Next steps\n\nNow that you've built your first deep agent:\n\n* **Customize your agent**: Learn about [customization options](/oss/javascript/deepagents/customization), including custom system prompts, tools, and subagents.\n* **Understand middleware**: Dive into the [middleware architecture](/oss/javascript/deepagents/middleware) that powers deep agents.\n* **Add long-term memory**: Enable [persistent memory](/oss/javascript/deepagents/long-term-memory) across conversations.\n* **Deploy to production**: Learn about [deployment options](/oss/javascript/langgraph/deploy) for LangGraph applications.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/javascript/langchain/quickstart\nContent: This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n\n<Tip>\n  **LangChain Docs MCP server**\n\n  If you're using an AI coding assistant or IDE (e.g. Claude Code or Cursor), you should install the [LangChain Docs MCP server](/use-these-docs) to get the most out of it. This ensures your agent has access to up-to-date LangChain documentation and examples.\n</Tip>\n\n## Requirements\n\nFor these examples, you will need to:\n\n* [Install](/oss/javascript/langchain/install) the LangChain package\n* Set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key\n* Set the `ANTHROPIC_API_KEY` environment variable in your terminal\n\nAlthough these examples use Claude, you can use [any supported model](/oss/javascript/integrations/providers/overview) by changing the model name in the code and setting up the appropriate API key.\n\n## Build a basic agent\n\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\n\n```ts  theme={null}\nimport { createAgent, tool } from \"langchain\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  (input) => `It's always sunny in ${input.city}!`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string().describe(\"The city to get the weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\n## Build a real-world agent\n\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\n\n1. **Detailed system prompts** for better agent behavior\n2. **Create tools** that integrate with external data\n3. **Model configuration** for consistent responses\n4. **Structured output** for predictable results\n5. **Conversational memory** for chat-like interactions\n6. **Create and run the agent** create a fully functional agent\n\nLet's walk through each step:\n\n<Steps>\n  <Step title=\"Define the system prompt\">\n    The system prompt defines your agent\u2019s role and behavior. Keep it specific and actionable:\n\n    ```ts  theme={null}\n    const systemPrompt = `You are an expert weather forecaster, who speaks in puns.\n\n    You have access to two tools:\n\n    - get_weather_for_location: use this to get the weather for a specific location\n    - get_user_location: use this to get the user's location\n\n    If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;\n    ```\n  </Step>\n\n  <Step title=\"Create tools\">\n    [Tools](/oss/javascript/langchain/tools) are functions your agent can call. Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so. Notice here how the `getUserLocation` tool does exactly that:\n\n    ```ts  theme={null}\n    import { tool, type ToolRuntime } from \"langchain\";\n    import * as z from \"zod\";\n\n    const getWeather = tool(\n      (input) => `It's always sunny in ${input.city}!`,\n      {\n        name: \"get_weather_for_location\",\n        description: \"Get the weather for a given city\",\n        schema: z.object({\n          city: z.string().describe(\"The city to get the weather for\"),\n        }),\n      }\n    );\n\n    type AgentRuntime = ToolRuntime<unknown, { user_id: string }>;\n\n    const getUserLocation = tool(\n      (_, config: AgentRuntime) => {\n        const { user_id } = config.context;\n        return user_id === \"1\" ? \"Florida\" : \"SF\";\n      },\n      {\n        name: \"get_user_location\",\n        description: \"Retrieve user information based on user ID\",\n      }\n    );\n    ```\n\n    <Note>\n      [Zod](https://zod.dev/) is a library for validating and parsing pre-defined schemas. You can use it to define the input schema for your tools to make sure the agent only calls the tool with the correct arguments.\n\n      Alternatively, you can define the `schema` property as a [JSON schema](https://json-schema.org/overview/what-is-jsonschema) object. Keep in mind that JSON schemas **won't** be validated at runtime.\n\n      <Accordion title=\"Example: Using JSON schema for tool input\">\n        ```ts  theme={null}\n        const getWeather = tool(\n          ({ city }) => `It's always sunny in ${city}!`,\n          {\n            name: \"get_weather_for_location\",\n            description: \"Get the weather for a given city\",\n            schema: {\n              type: \"object\",\n              properties: {\n                city: {\n                  type: \"string\",\n                  description: \"The city to get the weather for\"\n                }\n              },\n              required: [\"city\"]\n            },\n          }\n        );\n        ```\n      </Accordion>\n    </Note>\n  </Step>\n\n  <Step title=\"Configure your model\">\n    Set up your [language model](/oss/javascript/langchain/models) with the right parameters for your use case:\n\n    ```ts  theme={null}\n    import { initChatModel } from \"langchain\";\n\n    const model = await initChatModel(\n      \"claude-sonnet-4-5-20250929\",\n      { temperature: 0.5, timeout: 10, maxTokens: 1000 }\n    );\n    ```\n\n    Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.\n  </Step>\n\n  <Step title=\"Define response format\">\n    Optionally, define a structured response format if you need the agent responses to match\n    a specific schema.\n\n    ```ts  theme={null}\n    const responseFormat = z.object({\n      punny_response: z.string(),\n      weather_conditions: z.string().optional(),\n    });\n    ```\n  </Step>\n\n  <Step title=\"Add memory\">\n    Add [memory](/oss/javascript/langchain/short-term-memory) to your agent to maintain state across interactions. This allows\n    the agent to remember previous conversations and context.\n\n    ```ts  theme={null}\n    import { MemorySaver } from \"@langchain/langgraph\";\n\n    const checkpointer = new MemorySaver();\n    ```\n\n    <Info>\n      In production, use a persistent checkpointer that saves to a database.\n      See [Add and manage memory](/oss/javascript/langgraph/add-memory#manage-short-term-memory) for more details.\n    </Info>\n  </Step>\n\n  <Step title=\"Create and run the agent\">\n    Now assemble your agent with all the components and run it!\n\n    ```ts  theme={null}\n    import { createAgent } from \"langchain\";\n\n    const agent = createAgent({\n      model: \"claude-sonnet-4-5-20250929\",\n      systemPrompt: systemPrompt,\n      tools: [getUserLocation, getWeather],\n      responseFormat,\n      checkpointer,\n    });\n\n    // `thread_id` is a unique identifier for a given conversation.\n    const config = {\n      configurable: { thread_id: \"1\" },\n      context: { user_id: \"1\" },\n    };\n\n    const response = await agent.invoke(\n      { messages: [{ role: \"user\", content: \"what is the weather outside?\" }] },\n      config\n    );\n    console.log(response.structuredResponse);\n    // {\n    //   punny_response: \"Florida is still having a 'sun-derful' day ...\",\n    //   weather_conditions: \"It's always sunny in Florida!\"\n    // }\n\n    // Note that we can continue the conversation using the same `thread_id`.\n    const thankYouResponse = await agent.invoke(\n      { messages: [{ role: \"user\", content: \"thank you!\" }] },\n      config\n    );\n    console.log(thankYouResponse.structuredResponse);\n    // {\n    //   punny_response: \"You're 'thund-erfully' welcome! ...\",\n    //   weather_conditions: undefined\n    // }\n    ```\n  </Step>\n</Steps>\n\n<Expandable title=\"Full example code\">\n  ```ts  theme={null}\n  import { createAgent, tool, initChatModel, type ToolRuntime } from \"langchain\";\n  import { MemorySaver } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  // Define system prompt\n  const systemPrompt = `You are an expert weather forecaster, who speaks in puns.\n\n  You have access to two tools:\n\n  - get_weather_for_location: use this to get the weather for a specific location\n  - get_user_location: use this to get the user's location\n\n  If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;\n\n  // Define tools\n  const getWeather = tool(\n    ({ city }) => `It's always sunny in ${city}!`,\n    {\n      name: \"get_weather_for_location\",\n      description: \"Get the weather for a given city\",\n      schema: z.object({\n        city: z.string(),\n      }),\n    }\n  );\n\n  type AgentRuntime = ToolRuntime<unknown, { user_id: string }>;\n\n  const getUserLocation = tool(\n    (_, config: AgentRuntime) => {\n      const { user_id } = config.context;\n      return user_id === \"1\" ? \"Florida\" : \"SF\";\n    },\n    {\n      name: \"get_user_location\",\n      description: \"Retrieve user information based on user ID\",\n      schema: z.object({}),\n    }\n  );\n\n  // Configure model\n  const model = await initChatModel(\n    \"claude-sonnet-4-5-20250929\",\n    { temperature: 0 }\n  );\n\n  // Define response format\n  const responseFormat = z.object({\n    punny_response: z.string(),\n    weather_conditions: z.string().optional(),\n  });\n\n  // Set up memory\n  const checkpointer = new MemorySaver();\n\n  // Create agent\n  const agent = createAgent({\n    model,\n    systemPrompt,\n    responseFormat,\n    checkpointer,\n    tools: [getUserLocation, getWeather],\n  });\n\n  // Run agent\n  // `thread_id` is a unique identifier for a given conversation.\n  const config = {\n    configurable: { thread_id: \"1\" },\n    context: { user_id: \"1\" },\n  };\n\n  const response = await agent.invoke(\n    { messages: [{ role: \"user\", content: \"what is the weather outside?\" }] },\n    config\n  );\n  console.log(response.structuredResponse);\n  // {\n  //   punny_response: \"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n  //   weather_conditions: \"It's always sunny in Florida!\"\n  // }\n\n  // Note that we can continue the conversation using the same `thread_id`.\n  const thankYouResponse = await agent.invoke(\n    { messages: [{ role: \"user\", content: \"thank you!\" }] },\n    config\n  );\n  console.log(thankYouResponse.structuredResponse);\n  // {\n  //   punny_response: \"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n  //   weather_conditions: undefined\n  // }\n  ```\n</Expandable>\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\nCongratulations! You now have an AI agent that can:\n\n* **Understand context** and remember conversations\n* **Use multiple tools** intelligently\n* **Provide structured responses** in a consistent format\n* **Handle user-specific information** through context\n* **Maintain conversation state** across interactions\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/python/deepagents/quickstart\nContent: This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You'll build a research agent that can conduct research and write reports.\n\n## Prerequisites\n\nBefore you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).\n\n### Step 1: Install dependencies\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install deepagents tavily-python\n  ```\n\n  ```bash uv theme={null}\n  uv add deepagents tavily-python\n  ```\n\n  ```bash poetry theme={null}\n  poetry add deepagents tavily-python\n  ```\n</CodeGroup>\n\n### Step 2: Set up your API keys\n\n```bash  theme={null}\nexport ANTHROPIC_API_KEY=\"your-api-key\"\nexport TAVILY_API_KEY=\"your-tavily-api-key\"\n```\n\n### Step 3: Create a search tool\n\n```python  theme={null}\nimport os\nfrom typing import Literal\nfrom tavily import TavilyClient\nfrom deepagents import create_deep_agent\n\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\ndef internet_search(\n    query: str,\n    max_results: int = 5,\n    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n    include_raw_content: bool = False,\n):\n    \"\"\"Run a web search\"\"\"\n    return tavily_client.search(\n        query,\n        max_results=max_results,\n        include_raw_content=include_raw_content,\n        topic=topic,\n    )\n```\n\n### Step 4: Create a deep agent\n\n```python  theme={null}\n# System prompt to steer the agent to be an expert researcher\nresearch_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n\nYou have access to an internet search tool as your primary means of gathering information.\n\n## `internet_search`\n\nUse this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n\"\"\"\n\nagent = create_deep_agent(\n    tools=[internet_search],\n    system_prompt=research_instructions\n)\n```\n\n### Step 5: Run the agent\n\n```python  theme={null}\nresult = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n\n# Print the agent's response\nprint(result[\"messages\"][-1].content)\n```\n\n## What happened?\n\nYour deep agent automatically:\n\n1. **Planned its approach**: Used the built-in `write_todos` tool to break down the research task\n2. **Conducted research**: Called the `internet_search` tool to gather information\n3. **Managed context**: Used file system tools (`write_file`, `read_file`) to offload large search results\n4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents\n5. **Synthesized a report**: Compiled findings into a coherent response\n\n## Next steps\n\nNow that you've built your first deep agent:\n\n* **Customize your agent**: Learn about [customization options](/oss/python/deepagents/customization), including custom system prompts, tools, and subagents.\n* **Understand middleware**: Dive into the [middleware architecture](/oss/python/deepagents/middleware) that powers deep agents.\n* **Add long-term memory**: Enable [persistent memory](/oss/python/deepagents/long-term-memory) across conversations.\n* **Deploy to production**: Learn about [deployment options](/oss/python/langgraph/deploy) for LangGraph applications.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/python/langchain/quickstart\nContent: This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n\n<Tip>\n  **LangChain Docs MCP server**\n\n  If you're using an AI coding assistant or IDE (e.g. Claude Code or Cursor), you should install the [LangChain Docs MCP server](/use-these-docs) to get the most out of it. This ensures your agent has access to up-to-date LangChain documentation and examples.\n</Tip>\n\n## Requirements\n\nFor these examples, you will need to:\n\n* [Install](/oss/python/langchain/install) the LangChain package\n* Set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key\n* Set the `ANTHROPIC_API_KEY` environment variable in your terminal\n\nAlthough these examples use Claude, you can use [any supported model](/oss/python/integrations/providers/overview) by changing the model name in the code and setting up the appropriate API key.\n\n## Build a basic agent\n\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n```\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\n## Build a real-world agent\n\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\n\n1. **Detailed system prompts** for better agent behavior\n2. **Create tools** that integrate with external data\n3. **Model configuration** for consistent responses\n4. **Structured output** for predictable results\n5. **Conversational memory** for chat-like interactions\n6. **Create and run the agent** create a fully functional agent\n\nLet's walk through each step:\n\n<Steps>\n  <Step title=\"Define the system prompt\">\n    The system prompt defines your agent\u2019s role and behavior. Keep it specific and actionable:\n\n    ```python wrap theme={null}\n    SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\n    You have access to two tools:\n\n    - get_weather_for_location: use this to get the weather for a specific location\n    - get_user_location: use this to get the user's location\n\n    If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n    ```\n  </Step>\n\n  <Step title=\"Create tools\">\n    [Tools](/oss/python/langchain/tools) let a model interact with external systems by calling functions you define.\n    Tools can depend on [runtime context](/oss/python/langchain/runtime) and also interact with [agent memory](/oss/python/langchain/short-term-memory).\n\n    Notice below how the `get_user_location` tool uses runtime context:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n\n    @tool\n    def get_weather_for_location(city: str) -> str:\n        \"\"\"Get weather for a given city.\"\"\"\n        return f\"It's always sunny in {city}!\"\n\n    @dataclass\n    class Context:\n        \"\"\"Custom runtime context schema.\"\"\"\n        user_id: str\n\n    @tool\n    def get_user_location(runtime: ToolRuntime[Context]) -> str:\n        \"\"\"Retrieve user information based on user ID.\"\"\"\n        user_id = runtime.context.user_id\n        return \"Florida\" if user_id == \"1\" else \"SF\"\n    ```\n\n    <Tip>\n      Tools should be well-documented: their name, description, and argument names become part of the model's prompt.\n      LangChain's [`@tool` decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) adds metadata and enables runtime injection via the `ToolRuntime` parameter.\n    </Tip>\n  </Step>\n\n  <Step title=\"Configure your model\">\n    Set up your [language model](/oss/python/langchain/models) with the right parameters for your use case:\n\n    ```python  theme={null}\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0.5,\n        timeout=10,\n        max_tokens=1000\n    )\n    ```\n\n    Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.\n  </Step>\n\n  <Step title=\"Define response format\">\n    Optionally, define a structured response format if you need the agent responses to match\n    a specific schema.\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n\n    # We use a dataclass here, but Pydantic models are also supported.\n    @dataclass\n    class ResponseFormat:\n        \"\"\"Response schema for the agent.\"\"\"\n        # A punny response (always required)\n        punny_response: str\n        # Any interesting information about the weather if available\n        weather_conditions: str | None = None\n    ```\n  </Step>\n\n  <Step title=\"Add memory\">\n    Add [memory](/oss/python/langchain/short-term-memory) to your agent to maintain state across interactions. This allows\n    the agent to remember previous conversations and context.\n\n    ```python  theme={null}\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    checkpointer = InMemorySaver()\n    ```\n\n    <Info>\n      In production, use a persistent checkpointer that saves to a database.\n      See [Add and manage memory](/oss/python/langgraph/add-memory#manage-short-term-memory) for more details.\n    </Info>\n  </Step>\n\n  <Step title=\"Create and run the agent\">\n    Now assemble your agent with all the components and run it!\n\n    ```python  theme={null}\n    from langchain.agents.structured_output import ToolStrategy\n\n    agent = create_agent(\n        model=model,\n        system_prompt=SYSTEM_PROMPT,\n        tools=[get_user_location, get_weather_for_location],\n        context_schema=Context,\n        response_format=ToolStrategy(ResponseFormat),\n        checkpointer=checkpointer\n    )\n\n    # `thread_id` is a unique identifier for a given conversation.\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\n    print(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n    #     weather_conditions=\"It's always sunny in Florida!\"\n    # )\n\n\n    # Note that we can continue the conversation using the same `thread_id`.\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\n    print(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n    #     weather_conditions=None\n    # )\n    ```\n  </Step>\n</Steps>\n\n<Expandable title=\"Full example code\">\n  ```python  theme={null}\n  from dataclasses import dataclass\n\n  from langchain.agents import create_agent\n  from langchain.chat_models import init_chat_model\n  from langchain.tools import tool, ToolRuntime\n  from langgraph.checkpoint.memory import InMemorySaver\n  from langchain.agents.structured_output import ToolStrategy\n\n\n  # Define system prompt\n  SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\n  You have access to two tools:\n\n  - get_weather_for_location: use this to get the weather for a specific location\n  - get_user_location: use this to get the user's location\n\n  If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n\n  # Define context schema\n  @dataclass\n  class Context:\n      \"\"\"Custom runtime context schema.\"\"\"\n      user_id: str\n\n  # Define tools\n  @tool\n  def get_weather_for_location(city: str) -> str:\n      \"\"\"Get weather for a given city.\"\"\"\n      return f\"It's always sunny in {city}!\"\n\n  @tool\n  def get_user_location(runtime: ToolRuntime[Context]) -> str:\n      \"\"\"Retrieve user information based on user ID.\"\"\"\n      user_id = runtime.context.user_id\n      return \"Florida\" if user_id == \"1\" else \"SF\"\n\n  # Configure model\n  model = init_chat_model(\n      \"claude-sonnet-4-5-20250929\",\n      temperature=0\n  )\n\n  # Define response format\n  @dataclass\n  class ResponseFormat:\n      \"\"\"Response schema for the agent.\"\"\"\n      # A punny response (always required)\n      punny_response: str\n      # Any interesting information about the weather if available\n      weather_conditions: str | None = None\n\n  # Set up memory\n  checkpointer = InMemorySaver()\n\n  # Create agent\n  agent = create_agent(\n      model=model,\n      system_prompt=SYSTEM_PROMPT,\n      tools=[get_user_location, get_weather_for_location],\n      context_schema=Context,\n      response_format=ToolStrategy(ResponseFormat),\n      checkpointer=checkpointer\n  )\n\n  # Run agent\n  # `thread_id` is a unique identifier for a given conversation.\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n  response = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\n  print(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n  #     weather_conditions=\"It's always sunny in Florida!\"\n  # )\n\n\n  # Note that we can continue the conversation using the same `thread_id`.\n  response = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\n  print(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n  #     weather_conditions=None\n  # )\n  ```\n</Expandable>\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\nCongratulations! You now have an AI agent that can:\n\n* **Understand context** and remember conversations\n* **Use multiple tools** intelligently\n* **Provide structured responses** in a consistent format\n* **Handle user-specific information** through context\n* **Maintain conversation state** across interactions\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_52826932",
    "id": "0e4a61e1-839a-491c-9451-5ece084816d3",
    "content": "Result 1:\nTitle: Cloud\nLink: https://docs.langchain.comlangsmith/cloud\nContent: <Callout icon=\"rocket\" color=\"#4F46E5\" iconType=\"regular\">\n  If you're ready to deploy your app to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or the [full setup guide](/langsmith/deploy-to-cloud). This page explains the Cloud managed architecture for reference.\n</Callout>\n\nThe **Cloud** option is a fully managed model where LangChain hosts and operates all LangSmith infrastructure and services:\n\n* **Fully managed infrastructure**: LangChain handles all infrastructure, updates, scaling, and maintenance.\n* **Deploy from GitHub**: Connect your repositories and deploy with a few clicks.\n* **Automated CI/CD**: Build process is handled automatically by the platform.\n* **LangSmith UI**: Full access to [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), [deployment management](/langsmith/deployments), and [Studio](/langsmith/studio).\n\n|                                               | **Who manages it** | **Where it runs** |\n| --------------------------------------------- | ------------------ | ----------------- |\n| **LangSmith platform (UI, APIs, datastores)** | LangChain          | LangChain's cloud |\n| **Your Agent Servers**                        | LangChain          | LangChain's cloud |\n| **CI/CD for your apps**                       | LangChain          | LangChain's cloud |\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3f0316122425895270d0ecd47b12e139\" alt=\"Cloud deployment: LangChain hosts and manages all components including the UI, APIs, and your Agent Servers.\" data-og-width=\"1425\" width=\"1425\" data-og-height=\"1063\" height=\"1063\" data-path=\"langsmith/images/langgraph-cloud-architecture.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b34a10fb40ca5188dddfb6be69696c75 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d310ce86e8421878575cf4d4fa72bf78 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=7cc3fcee9c36a8d1e8da4cc4abbde3b9 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a184c950c8585e2fdb5e75ac7f0f5642 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=cb778e4a6707b00eaf703886d32569bd 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=00f9d9df44512dded307613502d03299 2500w\" />\n\n## Get started\n\nTo deploy your first application to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or refer to the [comprehensive setup guide](/langsmith/deploy-to-cloud).\n\n## Cloud architecture and scalability\n\n<Note>\n  This section is only relevant for the cloud-managed LangSmith services available at [https://smith.langchain.com](https://smith.langchain.com) and [https://eu.smith.langchain.com](https://eu.smith.langchain.com).\n\n  For information on the self-hosted LangSmith solution, please refer to the [self-hosted documentation](/langsmith/self-hosted).\n</Note>\n\nLangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for LLM application observability, evaluation, and agent deployment\n\n### Architecture\n\nThe US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.\n\n<Note>\n  The [EU-based LangSmith service](https://eu.smith.langchain.com) is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, [contact our sales team](https://www.langchain.com/contact-sales).\n</Note>\n\n#### Regional storage\n\nThe resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses [Supabase](https://supabase.com) for authentication/authorization and [ClickHouse Cloud](https://clickhouse.com/cloud) for data warehouse.\n\n|                                                | US                                                                 | EU                                                                       |\n| ---------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------ |\n| URL                                            | [https://smith.langchain.com](https://smith.langchain.com)         | [https://eu.smith.langchain.com](https://eu.smith.langchain.com)         |\n| API URL                                        | [https://api.smith.langchain.com](https://api.smith.langchain.com) | [https://eu.api.smith.langchain.com](https://eu.api.smith.langchain.com) |\n| GCP                                            | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| Supabase                                       | AWS us-east-1 (N. Virginia)                                        | AWS eu-central-1 (Germany)                                               |\n| ClickHouse Cloud                               | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n| [LangSmith deployment](/langsmith/deployments) | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |\n\nSee the [Regions FAQ](/langsmith/regions-faq) for more information.\n\n#### Region-independent storage\n\nData listed here is stored exclusively in the US:\n\n* Payment and billing information with Stripe and Metronome\n\n#### GCP services\n\nLangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):\n\n* LangSmith Frontend: serves the LangSmith UI.\n* LangSmith Backend: serves the LangSmith API.\n* LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)\n* LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.\n* LangSmith Queue: handles processing of asynchronous tasks. (Internal service)\n\nLangSmith uses the following GCP storage services:\n\n* Google Cloud Storage (GCS) for runs inputs and outputs.\n* Google Cloud SQL PostgreSQL for transactional workloads.\n* Google Cloud Memorystore for Redis for queuing and caching.\n* Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.\n\nSome additional GCP services we use include:\n\n* Google Cloud Load Balancer for routing traffic to the LangSmith services.\n* Google Cloud CDN for caching static assets.\n* Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to [this guide](/langsmith/administration-overview#rate-limits).\n\n<div style={{ textAlign: 'center' }}>\n  <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3\" alt=\"Light mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-light.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w\" />\n\n  <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b\" alt=\"Dark mode overview\" data-og-width=\"2210\" width=\"2210\" data-og-height=\"1463\" height=\"1463\" data-path=\"langsmith/images/cloud-arch-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w\" />\n</div>\n\n### Allowlisting IP addresses\n\n#### Egress from LangChain SaaS\n\nAll traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:\n\n| US             | EU             |\n| -------------- | -------------- |\n| 34.59.65.97    | 34.13.192.67   |\n| 34.67.51.221   | 34.147.105.64  |\n| 34.46.212.37   | 34.90.22.166   |\n| 34.132.150.88  | 34.147.36.213  |\n| 35.188.222.201 | 34.32.137.113  |\n| 34.58.194.127  | 34.91.238.184  |\n| 34.59.97.173   | 35.204.101.241 |\n| 104.198.162.55 | 35.204.48.32   |\n\nIt may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.\n\n#### Ingress into LangChain SaaS\n\nThe langchain endpoints map to the following static IP addresses:\n\n| US             | EU           |\n| -------------- | ------------ |\n| 34.8.121.39    | 34.95.92.214 |\n| 34.107.251.234 | 34.13.73.122 |\n\nYou may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (`api.smith.langchain.com`, `smith.langchain.com`, `beacon.langchain.com`, `eu.api.smith.langchain.com`, `eu.smith.langchain.com`, `eu.beacon.langchain.com`).\n\n### API rate limits\n\nLangSmith enforces rate limits on API endpoints to ensure service stability and fair usage. The following table shows the rate limits for different endpoints in both US and EU regions. Note that:\n\n* Rate limits are expressed as `count / interval` where count is the number of requests allowed within the interval (in seconds). For example, `2000 / 10` means 2000 requests per 10 seconds.\n* When no HTTP method is specified in the endpoint column, the rate limit applies to all HTTP methods for that endpoint.\n* When a specific method is listed (e.g., `POST`, `GET`), the rate limit applies only to that method.\n\n| Match / Endpoint (method)                   | Identity key     | US prod limit | EU prod limit | Category                                     |\n| ------------------------------------------- | ---------------- | ------------- | ------------- | -------------------------------------------- |\n| OPTIONS, `/info`, `*/v1/metadata/submit`    | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/auth`                                     | `x-user-id` + IP | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/v1/beacon`                                | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `/repos`                                    | `x-api-key`      | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `/repos`                                    | `x-user-id` + IP | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |\n| `POST /runs/batch`                          | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `POST /otel/v1/traces`                      | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST` containing `/charts`                 | `x-api-key`      | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST` containing `/charts`                 | `x-user-id` + IP | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |\n| `POST /runs/multipart`                      | `x-api-key`      | 6000 / 10     | 6000 / 10     | [Multipart ingest](#rate-limit-categories)   |\n| `POST /runs/query`                          | `x-api-key`      | 15 / 10       | 15 / 10       | [Run query (API)](#rate-limit-categories)    |\n| `POST /runs/query`                          | `x-user-id` + IP | 300 / 10      | 300 / 10      | [Run query (User)](#rate-limit-categories)   |\n| `/generate`                                 | `x-api-key`      | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/generate`                                 | `x-user-id` + IP | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |\n| `/commits`                                  | `x-api-key`      | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `/commits`                                  | `x-user-id` + IP | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |\n| `DELETE /sessions` or `*/trigger`           | `x-api-key`      | 10 / 60       | 10 / 60       | [Deletion](#rate-limit-categories)           |\n| `DELETE /sessions` or `*/trigger`           | `x-user-id` + IP | 30 / 60       | 30 / 60       | [Deletion](#rate-limit-categories)           |\n| `POST /runs` (single run ingest)            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `PATCH` containing `/runs`                  | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |\n| `POST /feedback`                            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |\n| `GET /runs/{uuid}` or `/api/v1/runs/{uuid}` | `x-api-key`      | 30 / 60       | 30 / 60       | [Run lookup](#rate-limit-categories)         |\n| `GET` containing `/examples`                | `x-api-key`      | 5000 / 60     | 5000 / 60     | [Examples](#rate-limit-categories)           |\n| Any request with `x-api-key`                | `x-api-key`      | 1000 / 10     | 1000 / 10     | [Default (API key)](#rate-limit-categories)  |\n| Any request with `x-user-id`                | `x-user-id` + IP | 1000 / 10     | 1000 / 10     | [Default (User)](#rate-limit-categories)     |\n| `/public/download`                          | IP               | 5000 / 60     | 5000 / 60     | [Public download](#rate-limit-categories)    |\n| `/runs/stats`                               | `x-api-key`      | 1 / 10        | 20 / 10       | [Stats](#rate-limit-categories)              |\n| All other IPs (catch-all)                   | IP               | 100 / 60      | 100 / 60      | [Public (catch-all)](#rate-limit-categories) |\n\n#### Rate limit categories\n\n* **High throughput**: General high-volume endpoints for core operations like authentication, metadata, and feedback.\n* **Repository**: Repository and prompt management operations.\n* **Run ingest**: Individual trace/run ingestion endpoints for observability.\n* **Charts**: Chart generation and visualization endpoints.\n* **Multipart ingest**: Bulk run ingestion via multipart upload for high-volume tracing.\n* **Run query (API)**: API key-based run query operations with stricter limits for complex queries.\n* **Run query (User)**: User-based run query operations with higher limits for interactive use.\n* **Generation**: AI-powered code and content generation endpoints (limited to prevent abuse).\n* **Commits**: Prompt versioning and commit operations.\n* **Deletion**: Session deletion and workflow trigger operations.\n* **Run lookup**: Retrieving specific runs by UUID.\n* **Examples**: Fetching dataset examples for few-shot prompting.\n* **Default (API key)**: Fallback rate limit for authenticated API requests not matching specific patterns.\n* **Default (User)**: Fallback rate limit for authenticated user requests not matching specific patterns.\n* **Public download**: High-volume public download endpoints for shared resources.\n* **Stats**: Run statistics and analytics endpoints (region-specific limits apply).\n* **Public (catch-all)**: Default rate limit for unauthenticated public access.\n\nFor more information on rate limits and other service limits, refer to the [Administration overview](/langsmith/administration-overview#rate-limits).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cloud.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/javascript/deepagents/quickstart\nContent: This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You'll build a research agent that can conduct research and write reports.\n\n## Prerequisites\n\nBefore you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).\n\n### Step 1: Install dependencies\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install deepagents @langchain/tavily\n  ```\n\n  ```bash yarn theme={null}\n  yarn add deepagents @langchain/tavily\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add deepagents @langchain/tavily\n  ```\n</CodeGroup>\n\n### Step 2: Set up your API keys\n\n```bash  theme={null}\nexport ANTHROPIC_API_KEY=\"your-api-key\"\nexport TAVILY_API_KEY=\"your-tavily-api-key\"\n```\n\n### Step 3: Create a search tool\n\n```typescript  theme={null}\nimport { tool } from \"langchain\";\nimport { TavilySearch } from \"@langchain/tavily\";\nimport { z } from \"zod\";\n\nconst internetSearch = tool(\n  async ({\n    query,\n    maxResults = 5,\n    topic = \"general\",\n    includeRawContent = false,\n  }: {\n    query: string;\n    maxResults?: number;\n    topic?: \"general\" | \"news\" | \"finance\";\n    includeRawContent?: boolean;\n  }) => {\n    const tavilySearch = new TavilySearch({\n      maxResults,\n      tavilyApiKey: process.env.TAVILY_API_KEY,\n      includeRawContent,\n      topic,\n    });\n    return await tavilySearch._call({ query });\n  },\n  {\n    name: \"internet_search\",\n    description: \"Run a web search\",\n    schema: z.object({\n      query: z.string().describe(\"The search query\"),\n      maxResults: z\n        .number()\n        .optional()\n        .default(5)\n        .describe(\"Maximum number of results to return\"),\n      topic: z\n        .enum([\"general\", \"news\", \"finance\"])\n        .optional()\n        .default(\"general\")\n        .describe(\"Search topic category\"),\n      includeRawContent: z\n        .boolean()\n        .optional()\n        .default(false)\n        .describe(\"Whether to include raw content\"),\n    }),\n  },\n);\n```\n\n### Step 4: Create a deep agent\n\n```typescript  theme={null}\nimport { createDeepAgent } from \"deepagents\";\n\n// System prompt to steer the agent to be an expert researcher\nconst researchInstructions = `You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n\nYou have access to an internet search tool as your primary means of gathering information.\n\n## \\`internet_search\\`\n\nUse this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n`;\n\nconst agent = createDeepAgent({\n  tools: [internetSearch],\n  systemPrompt: researchInstructions,\n});\n```\n\n### Step 5: Run the agent\n\n```typescript  theme={null}\nconst result = await agent.invoke({\n  messages: [{ role: \"user\", content: \"What is langgraph?\" }],\n});\n\n// Print the agent's response\nconsole.log(result.messages[result.messages.length - 1].content);\n```\n\n## What happened?\n\nYour deep agent automatically:\n\n1. **Planned its approach**: Used the built-in `write_todos` tool to break down the research task\n2. **Conducted research**: Called the `internet_search` tool to gather information\n3. **Managed context**: Used file system tools (`write_file`, `read_file`) to offload large search results\n4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents\n5. **Synthesized a report**: Compiled findings into a coherent response\n\n## Next steps\n\nNow that you've built your first deep agent:\n\n* **Customize your agent**: Learn about [customization options](/oss/javascript/deepagents/customization), including custom system prompts, tools, and subagents.\n* **Understand middleware**: Dive into the [middleware architecture](/oss/javascript/deepagents/middleware) that powers deep agents.\n* **Add long-term memory**: Enable [persistent memory](/oss/javascript/deepagents/long-term-memory) across conversations.\n* **Deploy to production**: Learn about [deployment options](/oss/javascript/langgraph/deploy) for LangGraph applications.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/javascript/langchain/quickstart\nContent: This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n\n<Tip>\n  **LangChain Docs MCP server**\n\n  If you're using an AI coding assistant or IDE (e.g. Claude Code or Cursor), you should install the [LangChain Docs MCP server](/use-these-docs) to get the most out of it. This ensures your agent has access to up-to-date LangChain documentation and examples.\n</Tip>\n\n## Requirements\n\nFor these examples, you will need to:\n\n* [Install](/oss/javascript/langchain/install) the LangChain package\n* Set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key\n* Set the `ANTHROPIC_API_KEY` environment variable in your terminal\n\nAlthough these examples use Claude, you can use [any supported model](/oss/javascript/integrations/providers/overview) by changing the model name in the code and setting up the appropriate API key.\n\n## Build a basic agent\n\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\n\n```ts  theme={null}\nimport { createAgent, tool } from \"langchain\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  (input) => `It's always sunny in ${input.city}!`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string().describe(\"The city to get the weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\n## Build a real-world agent\n\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\n\n1. **Detailed system prompts** for better agent behavior\n2. **Create tools** that integrate with external data\n3. **Model configuration** for consistent responses\n4. **Structured output** for predictable results\n5. **Conversational memory** for chat-like interactions\n6. **Create and run the agent** create a fully functional agent\n\nLet's walk through each step:\n\n<Steps>\n  <Step title=\"Define the system prompt\">\n    The system prompt defines your agent\u2019s role and behavior. Keep it specific and actionable:\n\n    ```ts  theme={null}\n    const systemPrompt = `You are an expert weather forecaster, who speaks in puns.\n\n    You have access to two tools:\n\n    - get_weather_for_location: use this to get the weather for a specific location\n    - get_user_location: use this to get the user's location\n\n    If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;\n    ```\n  </Step>\n\n  <Step title=\"Create tools\">\n    [Tools](/oss/javascript/langchain/tools) are functions your agent can call. Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so. Notice here how the `getUserLocation` tool does exactly that:\n\n    ```ts  theme={null}\n    import { tool, type ToolRuntime } from \"langchain\";\n    import * as z from \"zod\";\n\n    const getWeather = tool(\n      (input) => `It's always sunny in ${input.city}!`,\n      {\n        name: \"get_weather_for_location\",\n        description: \"Get the weather for a given city\",\n        schema: z.object({\n          city: z.string().describe(\"The city to get the weather for\"),\n        }),\n      }\n    );\n\n    type AgentRuntime = ToolRuntime<unknown, { user_id: string }>;\n\n    const getUserLocation = tool(\n      (_, config: AgentRuntime) => {\n        const { user_id } = config.context;\n        return user_id === \"1\" ? \"Florida\" : \"SF\";\n      },\n      {\n        name: \"get_user_location\",\n        description: \"Retrieve user information based on user ID\",\n      }\n    );\n    ```\n\n    <Note>\n      [Zod](https://zod.dev/) is a library for validating and parsing pre-defined schemas. You can use it to define the input schema for your tools to make sure the agent only calls the tool with the correct arguments.\n\n      Alternatively, you can define the `schema` property as a [JSON schema](https://json-schema.org/overview/what-is-jsonschema) object. Keep in mind that JSON schemas **won't** be validated at runtime.\n\n      <Accordion title=\"Example: Using JSON schema for tool input\">\n        ```ts  theme={null}\n        const getWeather = tool(\n          ({ city }) => `It's always sunny in ${city}!`,\n          {\n            name: \"get_weather_for_location\",\n            description: \"Get the weather for a given city\",\n            schema: {\n              type: \"object\",\n              properties: {\n                city: {\n                  type: \"string\",\n                  description: \"The city to get the weather for\"\n                }\n              },\n              required: [\"city\"]\n            },\n          }\n        );\n        ```\n      </Accordion>\n    </Note>\n  </Step>\n\n  <Step title=\"Configure your model\">\n    Set up your [language model](/oss/javascript/langchain/models) with the right parameters for your use case:\n\n    ```ts  theme={null}\n    import { initChatModel } from \"langchain\";\n\n    const model = await initChatModel(\n      \"claude-sonnet-4-5-20250929\",\n      { temperature: 0.5, timeout: 10, maxTokens: 1000 }\n    );\n    ```\n\n    Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.\n  </Step>\n\n  <Step title=\"Define response format\">\n    Optionally, define a structured response format if you need the agent responses to match\n    a specific schema.\n\n    ```ts  theme={null}\n    const responseFormat = z.object({\n      punny_response: z.string(),\n      weather_conditions: z.string().optional(),\n    });\n    ```\n  </Step>\n\n  <Step title=\"Add memory\">\n    Add [memory](/oss/javascript/langchain/short-term-memory) to your agent to maintain state across interactions. This allows\n    the agent to remember previous conversations and context.\n\n    ```ts  theme={null}\n    import { MemorySaver } from \"@langchain/langgraph\";\n\n    const checkpointer = new MemorySaver();\n    ```\n\n    <Info>\n      In production, use a persistent checkpointer that saves to a database.\n      See [Add and manage memory](/oss/javascript/langgraph/add-memory#manage-short-term-memory) for more details.\n    </Info>\n  </Step>\n\n  <Step title=\"Create and run the agent\">\n    Now assemble your agent with all the components and run it!\n\n    ```ts  theme={null}\n    import { createAgent } from \"langchain\";\n\n    const agent = createAgent({\n      model: \"claude-sonnet-4-5-20250929\",\n      systemPrompt: systemPrompt,\n      tools: [getUserLocation, getWeather],\n      responseFormat,\n      checkpointer,\n    });\n\n    // `thread_id` is a unique identifier for a given conversation.\n    const config = {\n      configurable: { thread_id: \"1\" },\n      context: { user_id: \"1\" },\n    };\n\n    const response = await agent.invoke(\n      { messages: [{ role: \"user\", content: \"what is the weather outside?\" }] },\n      config\n    );\n    console.log(response.structuredResponse);\n    // {\n    //   punny_response: \"Florida is still having a 'sun-derful' day ...\",\n    //   weather_conditions: \"It's always sunny in Florida!\"\n    // }\n\n    // Note that we can continue the conversation using the same `thread_id`.\n    const thankYouResponse = await agent.invoke(\n      { messages: [{ role: \"user\", content: \"thank you!\" }] },\n      config\n    );\n    console.log(thankYouResponse.structuredResponse);\n    // {\n    //   punny_response: \"You're 'thund-erfully' welcome! ...\",\n    //   weather_conditions: undefined\n    // }\n    ```\n  </Step>\n</Steps>\n\n<Expandable title=\"Full example code\">\n  ```ts  theme={null}\n  import { createAgent, tool, initChatModel, type ToolRuntime } from \"langchain\";\n  import { MemorySaver } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  // Define system prompt\n  const systemPrompt = `You are an expert weather forecaster, who speaks in puns.\n\n  You have access to two tools:\n\n  - get_weather_for_location: use this to get the weather for a specific location\n  - get_user_location: use this to get the user's location\n\n  If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;\n\n  // Define tools\n  const getWeather = tool(\n    ({ city }) => `It's always sunny in ${city}!`,\n    {\n      name: \"get_weather_for_location\",\n      description: \"Get the weather for a given city\",\n      schema: z.object({\n        city: z.string(),\n      }),\n    }\n  );\n\n  type AgentRuntime = ToolRuntime<unknown, { user_id: string }>;\n\n  const getUserLocation = tool(\n    (_, config: AgentRuntime) => {\n      const { user_id } = config.context;\n      return user_id === \"1\" ? \"Florida\" : \"SF\";\n    },\n    {\n      name: \"get_user_location\",\n      description: \"Retrieve user information based on user ID\",\n      schema: z.object({}),\n    }\n  );\n\n  // Configure model\n  const model = await initChatModel(\n    \"claude-sonnet-4-5-20250929\",\n    { temperature: 0 }\n  );\n\n  // Define response format\n  const responseFormat = z.object({\n    punny_response: z.string(),\n    weather_conditions: z.string().optional(),\n  });\n\n  // Set up memory\n  const checkpointer = new MemorySaver();\n\n  // Create agent\n  const agent = createAgent({\n    model,\n    systemPrompt,\n    responseFormat,\n    checkpointer,\n    tools: [getUserLocation, getWeather],\n  });\n\n  // Run agent\n  // `thread_id` is a unique identifier for a given conversation.\n  const config = {\n    configurable: { thread_id: \"1\" },\n    context: { user_id: \"1\" },\n  };\n\n  const response = await agent.invoke(\n    { messages: [{ role: \"user\", content: \"what is the weather outside?\" }] },\n    config\n  );\n  console.log(response.structuredResponse);\n  // {\n  //   punny_response: \"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n  //   weather_conditions: \"It's always sunny in Florida!\"\n  // }\n\n  // Note that we can continue the conversation using the same `thread_id`.\n  const thankYouResponse = await agent.invoke(\n    { messages: [{ role: \"user\", content: \"thank you!\" }] },\n    config\n  );\n  console.log(thankYouResponse.structuredResponse);\n  // {\n  //   punny_response: \"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n  //   weather_conditions: undefined\n  // }\n  ```\n</Expandable>\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\nCongratulations! You now have an AI agent that can:\n\n* **Understand context** and remember conversations\n* **Use multiple tools** intelligently\n* **Provide structured responses** in a consistent format\n* **Handle user-specific information** through context\n* **Maintain conversation state** across interactions\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/python/deepagents/quickstart\nContent: This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You'll build a research agent that can conduct research and write reports.\n\n## Prerequisites\n\nBefore you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).\n\n### Step 1: Install dependencies\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install deepagents tavily-python\n  ```\n\n  ```bash uv theme={null}\n  uv add deepagents tavily-python\n  ```\n\n  ```bash poetry theme={null}\n  poetry add deepagents tavily-python\n  ```\n</CodeGroup>\n\n### Step 2: Set up your API keys\n\n```bash  theme={null}\nexport ANTHROPIC_API_KEY=\"your-api-key\"\nexport TAVILY_API_KEY=\"your-tavily-api-key\"\n```\n\n### Step 3: Create a search tool\n\n```python  theme={null}\nimport os\nfrom typing import Literal\nfrom tavily import TavilyClient\nfrom deepagents import create_deep_agent\n\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\ndef internet_search(\n    query: str,\n    max_results: int = 5,\n    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n    include_raw_content: bool = False,\n):\n    \"\"\"Run a web search\"\"\"\n    return tavily_client.search(\n        query,\n        max_results=max_results,\n        include_raw_content=include_raw_content,\n        topic=topic,\n    )\n```\n\n### Step 4: Create a deep agent\n\n```python  theme={null}\n# System prompt to steer the agent to be an expert researcher\nresearch_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n\nYou have access to an internet search tool as your primary means of gathering information.\n\n## `internet_search`\n\nUse this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n\"\"\"\n\nagent = create_deep_agent(\n    tools=[internet_search],\n    system_prompt=research_instructions\n)\n```\n\n### Step 5: Run the agent\n\n```python  theme={null}\nresult = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n\n# Print the agent's response\nprint(result[\"messages\"][-1].content)\n```\n\n## What happened?\n\nYour deep agent automatically:\n\n1. **Planned its approach**: Used the built-in `write_todos` tool to break down the research task\n2. **Conducted research**: Called the `internet_search` tool to gather information\n3. **Managed context**: Used file system tools (`write_file`, `read_file`) to offload large search results\n4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents\n5. **Synthesized a report**: Compiled findings into a coherent response\n\n## Next steps\n\nNow that you've built your first deep agent:\n\n* **Customize your agent**: Learn about [customization options](/oss/python/deepagents/customization), including custom system prompts, tools, and subagents.\n* **Understand middleware**: Dive into the [middleware architecture](/oss/python/deepagents/middleware) that powers deep agents.\n* **Add long-term memory**: Enable [persistent memory](/oss/python/deepagents/long-term-memory) across conversations.\n* **Deploy to production**: Learn about [deployment options](/oss/python/langgraph/deploy) for LangGraph applications.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/python/langchain/quickstart\nContent: This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n\n<Tip>\n  **LangChain Docs MCP server**\n\n  If you're using an AI coding assistant or IDE (e.g. Claude Code or Cursor), you should install the [LangChain Docs MCP server](/use-these-docs) to get the most out of it. This ensures your agent has access to up-to-date LangChain documentation and examples.\n</Tip>\n\n## Requirements\n\nFor these examples, you will need to:\n\n* [Install](/oss/python/langchain/install) the LangChain package\n* Set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key\n* Set the `ANTHROPIC_API_KEY` environment variable in your terminal\n\nAlthough these examples use Claude, you can use [any supported model](/oss/python/integrations/providers/overview) by changing the model name in the code and setting up the appropriate API key.\n\n## Build a basic agent\n\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n```\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\n## Build a real-world agent\n\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\n\n1. **Detailed system prompts** for better agent behavior\n2. **Create tools** that integrate with external data\n3. **Model configuration** for consistent responses\n4. **Structured output** for predictable results\n5. **Conversational memory** for chat-like interactions\n6. **Create and run the agent** create a fully functional agent\n\nLet's walk through each step:\n\n<Steps>\n  <Step title=\"Define the system prompt\">\n    The system prompt defines your agent\u2019s role and behavior. Keep it specific and actionable:\n\n    ```python wrap theme={null}\n    SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\n    You have access to two tools:\n\n    - get_weather_for_location: use this to get the weather for a specific location\n    - get_user_location: use this to get the user's location\n\n    If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n    ```\n  </Step>\n\n  <Step title=\"Create tools\">\n    [Tools](/oss/python/langchain/tools) let a model interact with external systems by calling functions you define.\n    Tools can depend on [runtime context](/oss/python/langchain/runtime) and also interact with [agent memory](/oss/python/langchain/short-term-memory).\n\n    Notice below how the `get_user_location` tool uses runtime context:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n\n    @tool\n    def get_weather_for_location(city: str) -> str:\n        \"\"\"Get weather for a given city.\"\"\"\n        return f\"It's always sunny in {city}!\"\n\n    @dataclass\n    class Context:\n        \"\"\"Custom runtime context schema.\"\"\"\n        user_id: str\n\n    @tool\n    def get_user_location(runtime: ToolRuntime[Context]) -> str:\n        \"\"\"Retrieve user information based on user ID.\"\"\"\n        user_id = runtime.context.user_id\n        return \"Florida\" if user_id == \"1\" else \"SF\"\n    ```\n\n    <Tip>\n      Tools should be well-documented: their name, description, and argument names become part of the model's prompt.\n      LangChain's [`@tool` decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) adds metadata and enables runtime injection via the `ToolRuntime` parameter.\n    </Tip>\n  </Step>\n\n  <Step title=\"Configure your model\">\n    Set up your [language model](/oss/python/langchain/models) with the right parameters for your use case:\n\n    ```python  theme={null}\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0.5,\n        timeout=10,\n        max_tokens=1000\n    )\n    ```\n\n    Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.\n  </Step>\n\n  <Step title=\"Define response format\">\n    Optionally, define a structured response format if you need the agent responses to match\n    a specific schema.\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n\n    # We use a dataclass here, but Pydantic models are also supported.\n    @dataclass\n    class ResponseFormat:\n        \"\"\"Response schema for the agent.\"\"\"\n        # A punny response (always required)\n        punny_response: str\n        # Any interesting information about the weather if available\n        weather_conditions: str | None = None\n    ```\n  </Step>\n\n  <Step title=\"Add memory\">\n    Add [memory](/oss/python/langchain/short-term-memory) to your agent to maintain state across interactions. This allows\n    the agent to remember previous conversations and context.\n\n    ```python  theme={null}\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    checkpointer = InMemorySaver()\n    ```\n\n    <Info>\n      In production, use a persistent checkpointer that saves to a database.\n      See [Add and manage memory](/oss/python/langgraph/add-memory#manage-short-term-memory) for more details.\n    </Info>\n  </Step>\n\n  <Step title=\"Create and run the agent\">\n    Now assemble your agent with all the components and run it!\n\n    ```python  theme={null}\n    from langchain.agents.structured_output import ToolStrategy\n\n    agent = create_agent(\n        model=model,\n        system_prompt=SYSTEM_PROMPT,\n        tools=[get_user_location, get_weather_for_location],\n        context_schema=Context,\n        response_format=ToolStrategy(ResponseFormat),\n        checkpointer=checkpointer\n    )\n\n    # `thread_id` is a unique identifier for a given conversation.\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\n    print(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n    #     weather_conditions=\"It's always sunny in Florida!\"\n    # )\n\n\n    # Note that we can continue the conversation using the same `thread_id`.\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\n    print(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n    #     weather_conditions=None\n    # )\n    ```\n  </Step>\n</Steps>\n\n<Expandable title=\"Full example code\">\n  ```python  theme={null}\n  from dataclasses import dataclass\n\n  from langchain.agents import create_agent\n  from langchain.chat_models import init_chat_model\n  from langchain.tools import tool, ToolRuntime\n  from langgraph.checkpoint.memory import InMemorySaver\n  from langchain.agents.structured_output import ToolStrategy\n\n\n  # Define system prompt\n  SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\n  You have access to two tools:\n\n  - get_weather_for_location: use this to get the weather for a specific location\n  - get_user_location: use this to get the user's location\n\n  If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n\n  # Define context schema\n  @dataclass\n  class Context:\n      \"\"\"Custom runtime context schema.\"\"\"\n      user_id: str\n\n  # Define tools\n  @tool\n  def get_weather_for_location(city: str) -> str:\n      \"\"\"Get weather for a given city.\"\"\"\n      return f\"It's always sunny in {city}!\"\n\n  @tool\n  def get_user_location(runtime: ToolRuntime[Context]) -> str:\n      \"\"\"Retrieve user information based on user ID.\"\"\"\n      user_id = runtime.context.user_id\n      return \"Florida\" if user_id == \"1\" else \"SF\"\n\n  # Configure model\n  model = init_chat_model(\n      \"claude-sonnet-4-5-20250929\",\n      temperature=0\n  )\n\n  # Define response format\n  @dataclass\n  class ResponseFormat:\n      \"\"\"Response schema for the agent.\"\"\"\n      # A punny response (always required)\n      punny_response: str\n      # Any interesting information about the weather if available\n      weather_conditions: str | None = None\n\n  # Set up memory\n  checkpointer = InMemorySaver()\n\n  # Create agent\n  agent = create_agent(\n      model=model,\n      system_prompt=SYSTEM_PROMPT,\n      tools=[get_user_location, get_weather_for_location],\n      context_schema=Context,\n      response_format=ToolStrategy(ResponseFormat),\n      checkpointer=checkpointer\n  )\n\n  # Run agent\n  # `thread_id` is a unique identifier for a given conversation.\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n  response = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\n  print(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n  #     weather_conditions=\"It's always sunny in Florida!\"\n  # )\n\n\n  # Note that we can continue the conversation using the same `thread_id`.\n  response = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\n  print(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n  #     weather_conditions=None\n  # )\n  ```\n</Expandable>\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\nCongratulations! You now have an AI agent that can:\n\n* **Understand context** and remember conversations\n* **Use multiple tools** intelligently\n* **Provide structured responses** in a consistent format\n* **Handle user-specific information** through context\n* **Maintain conversation state** across interactions\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_10138018",
    "id": "c304dcaa-65ab-4af6-92a7-263cb4b770bc",
    "content": "Result 1:\nTitle: Use an existing secret for your installation (Kubernetes)\nLink: https://docs.langchain.comlangsmith/self-host-using-an-existing-secret\nContent: By default, LangSmith will provision several Kubernetes secrets to store sensitive information such as license keys, salts, and other configuration parameters. However, you may want to use an existing secret that you have already created in your Kubernetes cluster (or provisioned via some sort of secrets operator). This can be useful if you want to manage sensitive information in a centralized way or if you have specific security requirements.\n\nBy default we will provision the following secrets corresponding to different components of LangSmith:\n\n* `langsmith-secrets`: This secret contains the license key and some other basic configuration parameters. You can see the template for this secret [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/secrets.yaml)\n* `langsmith-redis`: This secret contains the Redis connection string (or node URIs if using Redis cluster) and password. You can see the template for this secret [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/redis/secrets.yaml)\n* `langsmith-postgres`: This secret contains the Postgres connection string and password. You can see the template for this secret [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/postgres/secrets.yaml)\n* `langsmith-clickhouse`: This secret contains the ClickHouse connection string and password. You can see the template for this secret [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/clickhouse/secrets.yaml)\n\n## Requirements\n\n* An existing Kubernetes cluster\n* A way to create Kubernetes secrets in your cluster. This can be done using `kubectl`, a Helm chart, or a secrets operator like [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets)\n\n## Parameters\n\nYou will need to create your own Kubernetes secrets that adhere to the structure of the secrets provisioned by the LangSmith Helm Chart.\n\n<Warning>\n  The secrets must have the same structure as the ones provisioned by the LangSmith Helm Chart (refer to the links above to see the specific secrets). If you miss any of the required keys, your LangSmith instance may not work correctly.\n</Warning>\n\nAn example secret may look like this:\n\n```yaml  theme={null}\napiVersion: v1\nkind: Secret\nmetadata:\n  name: langsmith-existing-secrets\n  namespace: langsmith\nstringData:\n  oauth_client_id: foo\n  oauth_client_secret: foo\n  oauth_issuer_url: foo\n  langsmith_license_key: foo\n  langgraph_cloud_license_key: foo\n  api_key_salt: foo\n  jwt_secret: foo\n  initial_org_admin_password: foo\n  blob_storage_access_key: foo\n  blob_storage_access_key_secret: foo\n  azure_storage_account_key: foo\n  azure_storage_connection_string: foo\n```\n\n## Configuration\n\nWith these secrets provisioned, you can configure your LangSmith instance to use the secrets directly to avoid passing in secret values through plaintext. You can do this by modifying the `langsmith_config.yaml` file for your LangSmith Helm Chart installation.\n\n```yaml  theme={null}\nconfig:\n  existingSecretName: \"langsmith-secrets\" # The name of the secret that contains the license key and other basic configuration parameters\nredis:\n  external:\n    enabled: true # Set to true to use an external Redis instance. This secret is only needed if you are using an external Redis instance\n    existingSecretName: \"langsmith-redis\" # The name of the secret that contains the Redis connection string and password\npostgres:\n  external:\n    enabled: true # Set to true to use an external Postgres instance. This secret is only needed if you are using an external Postgres instance\n    existingSecretName: \"langsmith-postgres\" # The name of the secret that contains the Postgres connection string and password\nclickhouse:\n  external:\n    enabled: true # Set to true to use an external ClickHouse instance. This secret is only needed if you are using an external ClickHouse instance\n    existingSecretName: \"langsmith-clickhouse\" # The name of the secret that contains the ClickHouse connection string and password\n```\n\nOnce configured, you will need to update your LangSmith installation. You can follow our upgrade guide [here](/langsmith/self-host-upgrades). If everything is configured correctly, your LangSmith instance should now be accessible via the Ingress. You can run the following to check that your secrets are being used correctly:\n\n```bash  theme={null}\nkubectl describe deployment langsmith-backend | grep -i <secret-name>\n```\n\nYou should see something like this in the output:\n\n```bash  theme={null}\nPOSTGRES_DATABASE_URI:                    <set to the key 'connection_url' in secret <your-secret-name>  Optional: false\nCLICKHOUSE_DB:                            <set to the key 'clickhouse_db' in secret <your-secret-name>   Optional: false\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-using-an-existing-secret.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Trace with LiveKit\nLink: https://docs.langchain.comlangsmith/trace-with-livekit\nContent: LangSmith can capture traces generated by [LiveKit Agents](https://docs.livekit.io/agents/) using OpenTelemetry instrumentation. This guide shows you how to automatically capture traces from your LiveKit voice AI agents and send them to LangSmith for monitoring and analysis.\n\nFor a complete implementation, see the [demo repository](https://github.com/langchain-ai/voice-agents-tracing).\n\n## Installation\n\nInstall the required packages:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langsmith livekit livekit-agents livekit-plugins-openai livekit-plugins-silero livekit-plugins-turn-detector opentelemetry-exporter-otlp python-dotenv\n  ```\n\n  ```bash uv theme={null}\n  uv add langsmith livekit livekit-agents livekit-plugins-openai livekit-plugins-silero livekit-plugins-turn-detector opentelemetry-exporter-otlp python-dotenv\n  ```\n</CodeGroup>\n\n## Quickstart tutorial\n\nFollow this step-by-step tutorial to create a voice AI agent with LiveKit and LangSmith tracing. You'll build a complete working example by copying and pasting code snippets.\n\n### Step 1: Set up your environment\n\nCreate a `.env` file in your project directory:\n\n```bash .env theme={null}\nOTEL_EXPORTER_OTLP_ENDPOINT=https://api.smith.langchain.com/otel\nOTEL_EXPORTER_OTLP_HEADERS=x-api-key=<your-langsmith-api-key>, Langsmith-Project=livekit-voice\nLIVEKIT_URL=<your-livekit-url>\nLIVEKIT_API_KEY=<your-livekit-api-key>\nLIVEKIT_API_SECRET=<your-livekit-api-secret>\nOPENAI_API_KEY=<your-openai-api-key>\n```\n\n### Step 2: Download the span processor\n\nAdd the [custom span processor file](https://github.com/langchain-ai/voice-agents-tracing/blob/main/livekit/langsmith_processor.py) that enables LangSmith tracing. Save it as `langsmith_processor.py` in your project directory.\n\n<Accordion title=\"What does the span processor do?\">\n  The span processor enriches LiveKit Agents' OpenTelemetry spans with LangSmith-compatible attributes so your traces display properly in LangSmith.\n\n  **Key functions:**\n\n  * Converts LiveKit span types (stt, llm, tts, agent, session, job) to LangSmith format.\n  * Adds `gen_ai.prompt.*` and `gen_ai.completion.*` attributes for message visualization.\n  * Tracks and aggregates conversation messages across turns\n  * Uses multiple extraction strategies to handle various LiveKit attribute formats.\n\n  The processor automatically activates when you import it in your code.\n</Accordion>\n\n### Step 3: Create your voice agent file\n\nCreate a new file called `agent.py` and add the following code. We'll build it section by section so you can copy and paste each part.\n\n#### Part 1: Import dependencies and set up tracing\n\n```python  theme={null}\nimport sys\nimport os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Import LiveKit components\nfrom livekit import agents\nfrom livekit.agents import AgentServer, AgentSession, Agent\nfrom livekit.agents.telemetry import set_tracer_provider\nfrom livekit.plugins import silero\nfrom livekit.plugins.turn_detector.multilingual import MultilingualModel\nfrom opentelemetry.sdk.trace import TracerProvider\n\n# Import span processor to enable LangSmith tracing\nfrom langsmith_processor import LangSmithSpanProcessor\n\n# Set up LangSmith tracing\ndef setup_langsmith():\n    \"\"\"Setup OpenTelemetry tracing to export spans to LangSmith.\"\"\"\n    endpoint = os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\")\n    headers = os.getenv(\"OTEL_EXPORTER_OTLP_HEADERS\")\n\n    if not endpoint or not headers:\n        print(\"\u26a0\ufe0f  Warning: OTEL environment variables not set. Tracing disabled.\")\n        return\n\n    # Create tracer provider with custom span processor\n    trace_provider = TracerProvider()\n    trace_provider.add_span_processor(LangSmithSpanProcessor())\n\n    # Set as LiveKit's tracer provider\n    set_tracer_provider(trace_provider)\n    print(\"\u2705 LangSmith tracing enabled\")\n\n# Enable tracing before creating agents\nsetup_langsmith()\n```\n\n#### Part 2: Define your agent\n\n```python  theme={null}\nclass Assistant(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"\"\"You are a helpful voice AI assistant.\n            You eagerly assist users with their questions.\n            Keep responses concise and conversational.\"\"\",\n        )\n```\n\n#### Part 3: Set up the agent server\n\n```python  theme={null}\nserver = AgentServer()\n\n@server.rtc_session()\nasync def my_agent(ctx: agents.JobContext):\n    # Create agent session with STT, LLM, TTS, and VAD\n    session = AgentSession(\n        stt=\"deepgram/nova-2:en\",\n        llm=\"openai/gpt-4o-mini\",\n        tts=openai.TTS(model=\"tts-1\", voice=\"alloy\"),\n        vad=silero.VAD.load(),\n        turn_detection=MultilingualModel(),\n    )\n\n    # Start the session\n    await session.start(\n        room=ctx.room,\n        agent=Assistant(),\n    )\n\nif __name__ == \"__main__\":\n    # Run in console mode for local testing\n    sys.argv = [sys.argv[0], \"console\"]\n    agents.cli.run_app(server)\n```\n\n### Step 4: Run your agent\n\nRun your voice agent in console mode for local testing:\n\n```bash  theme={null}\npython agent.py console\n```\n\nYour agent will start and connect to LiveKit. Speak through your microphone, and all traces will automatically appear in LangSmith. Here is an example of a trace in LangSmith: [LangSmith trace with LiveKit](https://smith.langchain.com/public/0f583c03-6d2a-4a2c-a043-9588e387cb55/r)\n\nView the complete [agent.py code](https://github.com/langchain-ai/voice-agents-tracing/blob/main/livekit/agent.py).\n\n## Advanced usage\n\n### Custom metadata and tags\n\nYou can add custom metadata to your traces using span attributes:\n\n```python  theme={null}\nfrom opentelemetry import trace\n\nclass Assistant(Agent):\n    def __init__(self) -> None:\n        super().__init__(\n            instructions=\"You are a helpful assistant.\",\n        )\n\n        # Get current span and add custom attributes\n        tracer = trace.get_tracer(__name__)\n        span = trace.get_current_span()\n        if span:\n            span.set_attribute(\"langsmith.metadata.agent_type\", \"voice_assistant\")\n            span.set_attribute(\"langsmith.metadata.version\", \"1.0\")\n            span.set_attribute(\"langsmith.span.tags\", \"livekit,voice-ai,production\")\n```\n\n## Troubleshooting\n\n### Spans not appearing in LangSmith\n\nIf traces aren't showing up in LangSmith:\n\n1. **Verify environment variables**: Ensure `OTEL_EXPORTER_OTLP_ENDPOINT` and `OTEL_EXPORTER_OTLP_HEADERS` are set correctly in your `.env` file.\n2. **Check setup order**: Make sure `setup_langsmith()` is called **before** creating `AgentServer`.\n3. **Check API key**: Confirm your LangSmith API key has write permissions.\n4. **Look for confirmation**: You should see \"\u2705 LangSmith tracing enabled\" in the console when starting.\n\n### Messages not showing correctly\n\nIf conversation messages aren't displaying properly:\n\n1. **Check span processor**: Verify `langsmith_processor.py` is in your project directory and imported correctly.\n2. **Verify imports**: Ensure `LangSmithSpanProcessor` is imported in your agent.py.\n3. **Enable debug logging**: Set `LANGSMITH_PROCESSOR_DEBUG=true` in your environment to see detailed logs.\n\n### Connection issues\n\nIf your agent can't connect to LiveKit:\n\n1. **Verify LiveKit URL**: Check `LIVEKIT_URL` is set correctly in your `.env` file.\n2. **Check credentials**: Ensure `LIVEKIT_API_KEY` and `LIVEKIT_API_SECRET` are correct.\n3. **Test connection**: Try connecting to your LiveKit server with the LiveKit CLI first.\n4. **Console mode**: For local testing, always use: `python agent.py console`.\n\n### Import errors\n\nIf you're getting import errors:\n\n1. **Install dependencies**: Run the complete pip install command from Step 1.\n2. **Check Python version**: Ensure you're using Python 3.9 or higher.\n3. **Verify langsmith\\_processor**: Make sure `langsmith_processor.py` is downloaded and in the same directory as `agent.py`.\n4. **Check LiveKit plugins**: Ensure you have the correct LiveKit plugins installed for your STT/LLM/TTS providers.\n\n### Agent not responding\n\nIf your agent connects but doesn't respond:\n\n1. **Check API keys**: Verify your OpenAI API key (or other provider keys) are correct.\n2. **Test services**: Ensure your STT, LLM, and TTS services are accessible.\n3. **Check instructions**: Make sure your Agent has proper instructions.\n4. **Review logs**: Look for errors in the console output.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-livekit.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Build a voice agent with LangChain\nLink: https://docs.langchain.comoss/javascript/langchain/voice-agent\nContent: ## Overview\n\nChat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\n\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\n\n### What are voice agents?\n\nVoice agents are [agents](/oss/javascript/langchain/agents) that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\n\nThey're suited for a variety of use cases, including:\n\n* Customer support\n* Personal assistants\n* Hands-free interfaces\n* Coaching and training\n\n### How do voice agents work?\n\nAt a high level, every voice agent needs to handle three tasks:\n\n1. **Listen** - capture audio and transcribe it\n2. **Think** - interpret intent, reason, plan\n3. **Speak** - generate audio and stream it back to the user\n\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\n\n#### 1. STT > Agent > TTS Architecture (The \"Sandwich\")\n\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\n\n```mermaid  theme={null}\nflowchart LR\n    A[User Audio] --> B[Speech-to-Text]\n    B --> C[LangChain Agent]\n    C --> D[Text-to-Speech]\n    D --> E[Audio Output]\n```\n\n**Pros:**\n\n* Full control over each component (swap STT/TTS providers as needed)\n* Access to latest capabilities from modern text-modality models\n* Transparent behavior with clear boundaries between components\n\n**Cons:**\n\n* Requires orchestrating multiple services\n* Additional complexity in managing the pipeline\n* Conversion from speech to text loses information (e.g., tone, emotion)\n\n#### 2. Speech-to-Speech Architecture (S2S)\n\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\n\n```mermaid  theme={null}\nflowchart LR\n    A[User Audio] --> B[Multimodal Model]\n    B --> C[Audio Output]\n```\n\n**Pros:**\n\n* Simpler architecture with fewer moving parts\n* Typically lower latency for simple interactions\n* Direct audio processing captures tone and other nuances of speech\n\n**Cons:**\n\n* Limited model options, greater risk of provider lock-in\n* Features may lag behind text-modality models\n* Less transparency in how audio is processed\n* Reduced controllability and customization options\n\nThis guide demonstrates the **sandwich architecture** to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\n\n### Demo Application Overview\n\nWe'll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using [AssemblyAI](https://www.assemblyai.com/) for STT and [Cartesia](https://cartesia.ai/) for TTS (although adapters can be built for most providers).\n\nAn end-to-end reference application is available in the [voice-sandwich-demo](https://github.com/langchain-ai/voice-sandwich-demo) repository. We will walk through that application here.\n\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\n\n### Architecture\n\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\n\n**Client (Browser)**\n\n* Captures microphone audio and encodes it as PCM\n* Establishes WebSocket connection to the backend server\n* Streams audio chunks to the server in real-time\n* Receives and plays back synthesized speech audio\n\n**Server (Node.js)**\n\n* Accepts WebSocket connections from clients\n\n* Orchestrates the three-step pipeline:\n  * [Speech-to-text (STT)](#1-speech-to-text): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\n  * [Agent](#2-langchain-agent): Processes transcripts with LangChain agent, streams response tokens\n  * [Text-to-speech (TTS)](#3-text-to-speech): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\n\n* Returns synthesized audio to the client for playback\n\nThe pipeline uses async iterators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\n\n## Setup\n\nFor detailed installation instructions and setup, see the [repository README](https://github.com/langchain-ai/voice-sandwich-demo#readme).\n\n## 1. Speech-to-text\n\nThe STT stage transforms an incoming audio stream into text transcripts. The implementation uses a producer-consumer pattern to handle audio streaming and transcript reception concurrently.\n\n### Key Concepts\n\n**Producer-Consumer Pattern**: Audio chunks are sent to the STT service concurrently with receiving transcript events. This allows transcription to begin before all audio has arrived.\n\n**Event Types**:\n\n* `stt_chunk`: Partial transcripts provided as the STT service processes audio\n* `stt_output`: Final, formatted transcripts that trigger agent processing\n\n**WebSocket Connection**: Maintains a persistent connection to AssemblyAI's real-time STT API, configured for 16kHz PCM audio with automatic turn formatting.\n\n### Implementation\n\n```typescript  theme={null}\nimport { AssemblyAISTT } from \"./assemblyai\";\nimport type { VoiceAgentEvent } from \"./types\";\n\nasync function* sttStream(\n  audioStream: AsyncIterable<Uint8Array>\n): AsyncGenerator<VoiceAgentEvent> {\n  const stt = new AssemblyAISTT({ sampleRate: 16000 });\n  const passthrough = writableIterator<VoiceAgentEvent>();\n\n  // Producer: pump audio chunks to AssemblyAI\n  const producer = (async () => {\n    try {\n      for await (const audioChunk of audioStream) {\n        await stt.sendAudio(audioChunk);\n      }\n    } finally {\n      await stt.close();\n    }\n  })();\n\n  // Consumer: receive transcription events\n  const consumer = (async () => {\n    for await (const event of stt.receiveEvents()) {\n      passthrough.push(event);\n    }\n  })();\n\n  try {\n    // Yield events as they arrive\n    yield* passthrough;\n  } finally {\n    // Wait for producer and consumer to complete\n    await Promise.all([producer, consumer]);\n  }\n}\n```\n\nThe application implements an AssemblyAI client to manage the WebSocket connection and message parsing. See below for implementations; similar adapters can be constructed for other STT providers.\n\n<Accordion title=\"AssemblyAI Client\">\n  ```typescript  theme={null}\n  export class AssemblyAISTT {\n    protected _bufferIterator = writableIterator<VoiceAgentEvent.STTEvent>();\n    protected _connectionPromise: Promise<WebSocket> | null = null;\n\n    async sendAudio(buffer: Uint8Array): Promise<void> {\n      const conn = await this._connection;\n      conn.send(buffer);\n    }\n\n    async *receiveEvents(): AsyncGenerator<VoiceAgentEvent.STTEvent> {\n      yield* this._bufferIterator;\n    }\n\n    protected get _connection(): Promise<WebSocket> {\n      if (this._connectionPromise) return this._connectionPromise;\n\n      this._connectionPromise = new Promise((resolve, reject) => {\n        const params = new URLSearchParams({\n          sample_rate: this.sampleRate.toString(),\n          format_turns: \"true\",\n        });\n        const url = `wss://streaming.assemblyai.com/v3/ws?${params}`;\n        const ws = new WebSocket(url, {\n          headers: { Authorization: this.apiKey },\n        });\n\n        ws.on(\"open\", () => resolve(ws));\n\n        ws.on(\"message\", (data) => {\n          const message = JSON.parse(data.toString());\n          if (message.type === \"Turn\") {\n            if (message.turn_is_formatted) {\n              this._bufferIterator.push({\n                type: \"stt_output\",\n                transcript: message.transcript,\n                ts: Date.now()\n              });\n            } else {\n              this._bufferIterator.push({\n                type: \"stt_chunk\",\n                transcript: message.transcript,\n                ts: Date.now()\n              });\n            }\n          }\n        });\n      });\n\n      return this._connectionPromise;\n    }\n  }\n  ```\n</Accordion>\n\n## 2. LangChain agent\n\nThe agent stage processes text transcripts through a LangChain [agent](/oss/javascript/langchain/agents) and streams the response tokens. In this case, we stream all [text content blocks](/oss/javascript/langchain/messages#textcontentblock) generated by the agent.\n\n### Key Concepts\n\n**Streaming Responses**: The agent uses [`stream_mode=\"messages\"`](/oss/javascript/langchain/streaming#llm-tokens) to emit response tokens as they're generated, rather than waiting for the complete response. This enables the TTS stage to begin synthesis immediately.\n\n**Conversation Memory**: A [checkpointer](/oss/javascript/langchain/short-term-memory) maintains conversation state across turns using a unique thread ID. This allows the agent to reference previous exchanges in the conversation.\n\n### Implementation\n\n```typescript  theme={null}\nimport { createAgent } from \"langchain\";\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n// Define agent tools\nconst addToOrder = tool(\n  async ({ item, quantity }) => {\n    return `Added ${quantity} x ${item} to the order.`;\n  },\n  {\n    name: \"add_to_order\",\n    description: \"Add an item to the customer's sandwich order.\",\n    schema: z.object({\n      item: z.string(),\n      quantity: z.number(),\n    }),\n  }\n);\n\nconst confirmOrder = tool(\n  async ({ orderSummary }) => {\n    return `Order confirmed: ${orderSummary}. Sending to kitchen.`;\n  },\n  {\n    name: \"confirm_order\",\n    description: \"Confirm the final order with the customer.\",\n    schema: z.object({\n      orderSummary: z.string().describe(\"Summary of the order\"),\n    }),\n  }\n);\n\n// Create agent with tools and memory\nconst agent = createAgent({\n  model: \"claude-haiku-4-5\",\n  tools: [addToOrder, confirmOrder],\n  checkpointer: new MemorySaver(),\n  systemPrompt: `You are a helpful sandwich shop assistant.\nYour goal is to take the user's order. Be concise and friendly.\nDo NOT use emojis, special characters, or markdown.\nYour responses will be read by a text-to-speech engine.`,\n});\n\nasync function* agentStream(\n  eventStream: AsyncIterable<VoiceAgentEvent>\n): AsyncGenerator<VoiceAgentEvent> {\n  // Generate unique thread ID for conversation memory\n  const threadId = uuidv4();\n\n  for await (const event of eventStream) {\n    // Pass through all upstream events\n    yield event;\n\n    // Process final transcripts through the agent\n    if (event.type === \"stt_output\") {\n      const stream = await agent.stream(\n        { messages: [new HumanMessage(event.transcript)] },\n        {\n          configurable: { thread_id: threadId },\n          streamMode: \"messages\",\n        }\n      );\n\n      // Yield agent response chunks as they arrive\n      for await (const [message] of stream) {\n        yield { type: \"agent_chunk\", text: message.text, ts: Date.now() };\n      }\n    }\n  }\n}\n```\n\n## 3. Text-to-speech\n\nThe TTS stage synthesizes agent response text into audio and streams it back to the client. Like the STT stage, it uses a producer-consumer pattern to handle concurrent text sending and audio reception.\n\n### Key Concepts\n\n**Concurrent Processing**: The implementation merges two async streams:\n\n* **Upstream processing**: Passes through all events and sends agent text chunks to the TTS provider\n* **Audio reception**: Receives synthesized audio chunks from the TTS provider\n\n**Streaming TTS**: Some providers (such as [Cartesia](https://cartesia.ai/)) begin synthesizing audio as soon as it receives text, enabling audio playback to start before the agent finishes generating its complete response.\n\n**Event Passthrough**: All upstream events flow through unchanged, allowing the client or other observers to track the full pipeline state.\n\n### Implementation\n\n```typescript  theme={null}\nimport { CartesiaTTS } from \"./cartesia\";\n\nasync function* ttsStream(\n  eventStream: AsyncIterable<VoiceAgentEvent>\n): AsyncGenerator<VoiceAgentEvent> {\n  const tts = new CartesiaTTS();\n  const passthrough = writableIterator<VoiceAgentEvent>();\n\n  // Producer: read upstream events and send text to Cartesia\n  const producer = (async () => {\n    try {\n      for await (const event of eventStream) {\n        passthrough.push(event);\n        if (event.type === \"agent_chunk\") {\n          await tts.sendText(event.text);\n        }\n      }\n    } finally {\n      await tts.close();\n    }\n  })();\n\n  // Consumer: receive audio from Cartesia\n  const consumer = (async () => {\n    for await (const event of tts.receiveEvents()) {\n      passthrough.push(event);\n    }\n  })();\n\n  try {\n    // Yield events from both producer and consumer\n    yield* passthrough;\n  } finally {\n    await Promise.all([producer, consumer]);\n  }\n}\n```\n\nThe application implements an Cartesia client to manage the WebSocket connection and audio streaming. See below for implementations; similar adapters can be constructed for other TTS providers.\n\n<Accordion title=\"Cartesia Client\">\n  ```typescript  theme={null}\n  export class CartesiaTTS {\n    protected _bufferIterator = writableIterator<VoiceAgentEvent.TTSEvent>();\n    protected _connectionPromise: Promise<WebSocket> | null = null;\n\n    async sendText(text: string | null): Promise<void> {\n      if (!text || !text.trim()) return;\n\n      const conn = await this._connection;\n      const payload = { text, try_trigger_generation: false };\n      conn.send(JSON.stringify(payload));\n    }\n\n    async *receiveEvents(): AsyncGenerator<VoiceAgentEvent.TTSEvent> {\n      yield* this._bufferIterator;\n    }\n\n    protected _generateContextId(): string {\n      const timestamp = Date.now();\n      const counter = this._contextCounter++;\n      return `ctx_${timestamp}_${counter}`;\n    }\n\n    protected get _connection(): Promise<WebSocket> {\n      if (this._connectionPromise) return this._connectionPromise;\n\n      this._connectionPromise = new Promise((resolve, reject) => {\n        const params = new URLSearchParams({\n          api_key: this.apiKey,\n          cartesia_version: this.cartesiaVersion,\n        });\n        const url = `wss://api.cartesia.ai/tts/websocket?${params.toString()}`;\n        const ws = new WebSocket(url);\n\n        ws.on(\"open\", () => {\n          resolve(ws);\n        });\n\n        ws.on(\"message\", (data: WebSocket.RawData) => {\n          const message: CartesiaTTSResponse = JSON.parse(data.toString());\n          if (message.data) {\n            this._bufferIterator.push({\n              type: \"tts_chunk\",\n              audio: message.data,\n              ts: Date.now(),\n            });\n          } else if (message.error) {\n            throw new Error(`Cartesia error: ${message.error}`);\n          }\n        });\n      });\n\n      return this._connectionPromise;\n    }\n  }\n  ```\n</Accordion>\n\n## Putting It All Together\n\nThe complete pipeline chains the three stages together:\n\n```typescript  theme={null}\n// using https://hono.dev/\napp.get(\"/ws\", upgradeWebSocket(async () => {\n  const inputStream = writableIterator<Uint8Array>();\n\n  // Chain the three stages\n  const transcriptEventStream = sttStream(inputStream);\n  const agentEventStream = agentStream(transcriptEventStream);\n  const outputEventStream = ttsStream(agentEventStream);\n\n  // Process pipeline and send TTS audio to client\n  const flushPromise = (async () => {\n    for await (const event of outputEventStream) {\n      if (event.type === \"tts_chunk\") {\n        currentSocket?.send(event.audio);\n      }\n    }\n  })();\n\n  return {\n    onMessage(event) {\n      // Push incoming audio into pipeline\n      const data = event.data;\n      if (Buffer.isBuffer(data)) {\n        inputStream.push(new Uint8Array(data));\n      }\n    },\n    async onClose() {\n      inputStream.cancel();\n      await flushPromise;\n    },\n  };\n}));\n```\n\nEach stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.\n\nFor more on building agents with LangChain, see the [Agents guide](/oss/javascript/langchain/agents).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/voice-agent.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Install LangGraph\nLink: https://docs.langchain.comoss/javascript/langgraph/install\nContent: To install the base LangGraph package:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph @langchain/core\n  ```\n</CodeGroup>\n\nTo use LangGraph you will usually want to access LLMs and define tools.\nYou can do this however you see fit.\n\nOne way to do this (which we will use in the docs) is to use [LangChain](/oss/javascript/langchain/overview).\n\nInstall LangChain with:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install langchain\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add langchain\n  ```\n\n  ```bash yarn theme={null}\n  yarn add langchain\n  ```\n\n  ```bash bun theme={null}\n  bun add langchain\n  ```\n</CodeGroup>\n\nTo work with specific LLM provider packages, you will need install them separately.\n\nRefer to the [integrations](/oss/javascript/integrations/providers/overview) page for provider-specific installation instructions.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/install.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Install LangGraph\nLink: https://docs.langchain.comoss/python/langgraph/install\nContent: To install the base LangGraph package:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\nTo use LangGraph you will usually want to access LLMs and define tools.\nYou can do this however you see fit.\n\nOne way to do this (which we will use in the docs) is to use [LangChain](/oss/python/langchain/overview).\n\nInstall LangChain with:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langchain\n  # Requires Python 3.10+\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain\n  # Requires Python 3.10+\n  ```\n</CodeGroup>\n\nTo work with specific LLM provider packages, you will need install them separately.\n\nRefer to the [integrations](/oss/python/integrations/providers/overview) page for provider-specific installation instructions.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/install.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_88029345",
    "id": "e3a17c1b-59a7-453c-afe3-790b764e7fd3",
    "content": "{\n  \"collections\": \"all\",\n  \"total\": 73,\n  \"articles\": [\n    {\n      \"id\": \"7a5d1732-c25b-4a26-9686-52ddfd17926e\",\n      \"title\": \"Merging LangSmith Runs from Remote Graph\",\n      \"url\": \"https://support.langchain.com/articles/9499391881-merging-langsmith-runs-from-remote-graph\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"796cd8bc-a7d9-419d-8141-83998b267e90\",\n      \"title\": \"Resolving common issues when upgrading LangSmith Helm chart from 0.11.x to 0.12.x?\",\n      \"url\": \"https://support.langchain.com/articles/5255204348-resolving-common-issues-when-upgrading-langsmith-helm-chart-from-0-11-x-to-0-12-x\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"adeafe65-c1b6-4620-b3aa-044873fd8c9c\",\n      \"title\": \"Why are my traces being automatically upgraded to extended retention?\",\n      \"url\": \"https://support.langchain.com/articles/3206725817-why-are-my-traces-being-automatically-upgraded-to-extended-retention\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"73778fa5-7613-4254-b687-7b0288ac9903\",\n      \"title\": \"Why am I getting 429 Rate Limit Errors during load testing on self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8364157755-why-am-i-getting-429-rate-limit-errors-during-load-testing-on-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"27a421b4-e877-4a0f-8f38-7b719b189785\",\n      \"title\": \"PagerDuty Alert Severity Not Respecting Configuration\",\n      \"url\": \"https://support.langchain.com/articles/8222693126-pagerduty-alert-severity-not-respecting-configuration\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"adf84afd-7d07-4e11-8645-a3c9a1686f08\",\n      \"title\": \"Session Timeout Configuration for LangSmith Self-Hosted\",\n      \"url\": \"https://support.langchain.com/articles/9515017706-session-timeout-configuration-for-langsmith-self-hosted\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2fcf3091-0038-4226-b56f-4559d6cb1dbc\",\n      \"title\": \"Cleaning ClickHouse System Log Tables\",\n      \"url\": \"https://support.langchain.com/articles/9482219057-cleaning-clickhouse-system-log-tables\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"065735b3-432b-41ce-8469-1494cb753727\",\n      \"title\": \"Why are my agent health checks failing with GCP Gateway API on GKE?\",\n      \"url\": \"https://support.langchain.com/articles/3603461301-why-are-my-agent-health-checks-failing-with-gcp-gateway-api-on-gke\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f9775b3-e85d-4368-ad1b-21f9a8f7552e\",\n      \"title\": \"Evaluator Creation with LiteLLM Proxy - Missing API Key Error\",\n      \"url\": \"https://support.langchain.com/articles/2378929317-evaluator-creation-with-litellm-proxy-missing-api-key-error\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3a81a902-1bca-463d-ab2d-abefd38f8721\",\n      \"title\": \"Why did my evaluator token usage and costs increase after upgrading LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/7643067789-why-did-my-evaluator-token-usage-and-costs-increase-after-upgrading-langsmith\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"61f0452e-5198-4524-9c94-a9cf476ea5bc\",\n      \"title\": \"How to Create a Locked-Down Deployment with Restricted Access\",\n      \"url\": \"https://support.langchain.com/articles/5382256129-how-to-create-a-locked-down-deployment-with-restricted-access\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"c0615bd9-c4a2-4a54-af03-0ee07470ee43\",\n      \"title\": \"How do I configure LangSmith pricing match patterns for AWS Bedrock inference profiles?\",\n      \"url\": \"https://support.langchain.com/articles/4241511543-how-do-i-configure-langsmith-pricing-match-patterns-for-aws-bedrock-inference-profiles\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"365a0d3f-d784-407d-b741-fe51fa8356d4\",\n      \"title\": \"LangSmith Deployments not working with Valkey\",\n      \"url\": \"https://support.langchain.com/articles/2265174426-why-are-my-langgraph-requests-hanging-when-using-aws-elasticache-with-valkey\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"818454c3-a656-4811-a7ca-56cb13df2355\",\n      \"title\": \"How do I enable KMS encryption for S3 storage in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9381186430-how-do-i-enable-kms-encryption-for-s3-storage-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a3c79a7d-390f-43e6-8234-8b55a7f191dd\",\n      \"title\": \"Streaming Failures When Using Shared Custom Redis Instances\",\n      \"url\": \"https://support.langchain.com/articles/3842219806-streaming-failures-when-using-shared-custom-redis-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"55a2c6fe-adf8-4b55-bcb9-969b465241b7\",\n      \"title\": \"Why are my agent threads getting deleted automatically?\",\n      \"url\": \"https://support.langchain.com/articles/7993792111-why-are-my-threads-being-marked-as-temporary-and-getting-deleted-automatically\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b4ba8c34-6d1d-4653-8123-c2eeee2d34e5\",\n      \"title\": \"How do I set up ABAC to restrict user access to specific projects?\",\n      \"url\": \"https://support.langchain.com/articles/6539560119-how-do-i-set-up-abac-to-restrict-user-access-to-specific-projects\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b42f572e-0490-4bb8-9c81-9f065f0c83ed\",\n      \"title\": \"Productionizing and Scaling Self-Hosted LangSmith - Best Practices\",\n      \"url\": \"https://support.langchain.com/articles/8478795211-productionizing-and-scaling-self-hosted-langsmith-best-practices\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"d270716f-edc0-49d9-b5d1-497b5d0e4e6d\",\n      \"title\": \"Private Network Access Blocking LangSmith Studio Connection\",\n      \"url\": \"https://support.langchain.com/articles/1897181200-private-network-access-blocking-langsmith-studio-connection\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"b20fd0bf-1ae9-445c-86ba-5a223112756f\",\n      \"title\": \"How to setup private container registry authentication for LangGraph agent deployments\",\n      \"url\": \"https://support.langchain.com/articles/8606755009-how-to-setup-private-container-registry-authentication-for-langgraph-cloud-agent-deployments\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f6f6fa0-610c-4a15-a4b9-987e932f6a3a\",\n      \"title\": \"How to setup path-based routing for self-hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/2352997668-how-to-setup-path-based-routing-for-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"cc4be529-34c6-458b-8bb3-621fab0e2ab4\",\n      \"title\": \"Best practices for scaling Self-Hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/5142502915-best-practices-for-scaling-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3cec875c-3c8d-4951-b83b-0ac5b8ba7c7c\",\n      \"title\": \"LangSmith Deployments agent is running, but the UI shows it as \\u201cUnsuccessful\\u201d\",\n      \"url\": \"https://support.langchain.com/articles/8394994213-langsmith-deployments-agent-is-running-but-the-ui-shows-it-as-unsuccessful\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"57055dee-474d-4635-93a3-79439570f6ac\",\n      \"title\": \"How do I prevent traces from mixing between projects during concurrent requests?\",\n      \"url\": \"https://support.langchain.com/articles/4784014353-how-do-i-prevent-traces-from-mixing-between-projects-during-concurrent-requests\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9619ff10-34d0-49ea-a7ee-cdbbddafc1e4\",\n      \"title\": \"How do I resolve \\\"too many connections\\\" PostgreSQL errors when scaling platform instances?\",\n      \"url\": \"https://support.langchain.com/articles/9651943867-how-do-i-resolve-too-many-connections-postgresql-errors-when-scaling-platform-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"b740be98-677d-486f-ba77-12258712f70a\",\n      \"title\": \"How do I use the LangSmith Control Plane API to manage deployments?\",\n      \"url\": \"https://support.langchain.com/articles/8184800662-how-do-i-use-the-langgraph-platform-control-plane-api-to-manage-deployments\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"52d389ff-3918-46a8-9ccf-55fe972077bf\",\n      \"title\": \"Why is my LangSmith agent queue Deployment timing out after 600 seconds?\",\n      \"url\": \"https://support.langchain.com/articles/7898522976-why-is-my-queue-deployment-timing-out-after-600-seconds\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"1caf7aa7-5382-43f5-869c-27212fe9f287\",\n      \"title\": \"How do I resolve ClickHouse migration errors after upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/6463985661-how-do-i-resolve-clickhouse-migration-errors-after-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0ff2089a-501c-45b8-b557-4947639c5072\",\n      \"title\": \"How do I disable public sharing for traces in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5345880639-how-do-i-disable-public-sharing-for-traces-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\",\n      \"title\": \"What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\",\n      \"url\": \"https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"bef0ce3c-94c2-4999-b563-ff016fa3884f\",\n      \"title\": \"How do I get LangChain to complete security questionnaires for vendor assessments?\",\n      \"url\": \"https://support.langchain.com/articles/8555158040-how-do-i-get-langchain-to-complete-security-questionnaires-for-vendor-assessments\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"fe2b3c9e-8fa1-49dd-a52d-df92d88050c4\",\n      \"title\": \"How do I update billing information on an existing invoice?\",\n      \"url\": \"https://support.langchain.com/articles/5832977338-how-do-i-update-billing-information-on-an-existing-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"6ea47321-cd15-49a7-a972-be33139a5fe3\",\n      \"title\": \"Can I use an external database cluster to host multiple self-hosted LangSmith instances?\",\n      \"url\": \"https://support.langchain.com/articles/5989664339-can-i-use-an-external-database-cluster-to-host-multiple-self-hosted-langsmith-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"4a127c79-3504-4afa-befa-ec974b85c5f8\",\n      \"title\": \"Why am I being charged for node executions and standby minutes on my development deployment?\",\n      \"url\": \"https://support.langchain.com/articles/2626278629-why-am-i-being-charged-for-node-executions-on-my-development-deployment\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"a442595a-454c-4208-8854-d51f632a3d0f\",\n      \"title\": \"Managing ClickHouse Storage\",\n      \"url\": \"https://support.langchain.com/articles/2929899751-managing-clickhouse-storage\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"c6c77d41-4376-4452-b7a0-fd42c1428242\",\n      \"title\": \"How do I deploy an MCP server on LangSmith Deployments with my agent?\",\n      \"url\": \"https://support.langchain.com/articles/8743137454-how-do-i-deploy-an-mcp-server-on-langsmith-deployments-with-my-agent\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"e5b95386-5971-4e48-be5b-9d9c7dcfe8e3\",\n      \"title\": \"How do I archive a prompt instead of deleting it?\",\n      \"url\": \"https://support.langchain.com/articles/6404244010-how-do-i-archive-a-prompt-instead-of-deleting-it\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"7444d2d0-3f90-42d8-aa4c-b36fa7665e05\",\n      \"title\": \"How do I resolve Redis disk space issues in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5370519454-how-do-i-resolve-redis-disk-space-issues-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"1076b041-020f-4738-acd0-d8ecc07f19ed\",\n      \"title\": \"How do I add existing organization members to new workspaces programmatically via API?\",\n      \"url\": \"https://support.langchain.com/articles/5797508505-how-do-i-add-existing-organization-members-to-new-workspaces-programmatically-via-api\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a91a5cf4-a8a3-4ad5-b9ef-9d0d534d0ba4\",\n      \"title\": \"ClickHouse: Drop Old Data\",\n      \"url\": \"https://support.langchain.com/articles/2379935803-clickhouse-drop-old-data\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5886208c-1f87-4672-87c4-053f2773c161\",\n      \"title\": \"How do I filter out traces that contain a specific node name in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8185473222-how-do-i-filter-out-traces-that-contain-a-specific-node-name-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b170d29c-925a-4aab-b7a0-354f47b7dc77\",\n      \"title\": \"Can I enforce Google SSO-only authentication for my organization?\",\n      \"url\": \"https://support.langchain.com/articles/9820106567-can-i-enforce-google-sso-only-authentication-for-my-organization\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b5128a24-178a-4063-9337-c273af5ac617\",\n      \"title\": \"Why is my trace exploration slow in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9719508217-why-is-my-trace-exploration-slow-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"50634017-ff08-43bb-b7ae-7518aab83623\",\n      \"title\": \"How do I get access to LangSmith Agent Builder?\",\n      \"url\": \"https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"4ad14b9c-abe8-47b0-a416-81dbe2a3a127\",\n      \"title\": \"Why are some experiment runs missing when processing large datasets in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/3506757366-why-are-some-experiment-runs-missing-when-processing-large-datasets-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"40305680-01d3-4506-8257-16db939e39bf\",\n      \"title\": \"How do I get help with billing issues and payment problems?\",\n      \"url\": \"https://support.langchain.com/articles/3779255598-how-do-i-get-help-with-billing-issues-and-payment-problems\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"9273a5a9-0a53-4d5a-917a-c19ebeec64fe\",\n      \"title\": \"How do I get a copy of my LangSmith invoice?\",\n      \"url\": \"https://support.langchain.com/articles/6317483716-how-do-i-get-a-copy-of-my-langsmith-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"ebea1a08-4488-4cac-9c65-75442f3de254\",\n      \"title\": \"Adding FastAPI dependency for custom middleware in LangGraph Platform\",\n      \"url\": \"https://support.langchain.com/articles/7593416758-adding-fastapi-dependency-for-custom-middleware-in-langgraph-platform\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"d37491ce-7f3c-4575-91d5-09c62129248e\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/8574277609-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"48541052-b69e-4e78-ae8b-c2ef4fe26af1\",\n      \"title\": \"Resolving LangGraph and LangSmith version compatibility issues in production\",\n      \"url\": \"https://support.langchain.com/articles/5102702588-resolving-langgraph-and-langsmith-version-compatibility-issues-in-production\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0b6ce386-e341-47a1-8be9-ae350de16843\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/3873263156-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"68fbf7da-954d-454e-8ad4-73e1f1f63b74\",\n      \"title\": \"LangSmith Studio vs Agent Chat UI: When to Use What\",\n      \"url\": \"https://support.langchain.com/articles/1353449293-langsmith-studio-vs-agent-chat-ui\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f7dc0a0c-fa3f-4319-b820-ae470d89877f\",\n      \"title\": \"Why is token-by-token streaming not working after upgrading LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/7150806184-why-is-token-by-token-streaming-not-working-after-upgrading-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"ada8fd1b-4800-434e-8d6f-b5186b170941\",\n      \"title\": \"LangSmith Studio URL Redirects to Error Page\",\n      \"url\": \"https://support.langchain.com/articles/4270790436-langsmith-studio-url-redirects-to-error-page\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f41e0fb9-7ed0-4d9b-85b0-fe88fd805f02\",\n      \"title\": \"Why am I getting service interruption errors when using text search filters?\",\n      \"url\": \"https://support.langchain.com/articles/6919527577-why-am-i-getting-service-interruption-errors-when-using-text-search-filters\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\",\n      \"title\": \"Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\",\n      \"url\": \"https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b1e1ee29-1124-4a92-8f9f-eaa4a679df05\",\n      \"title\": \"Why am I getting a parsing token error when upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/7019615393-why-am-i-getting-a-token-parsing-error-when-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"da8e3156-8405-4c12-a347-96d6590da06c\",\n      \"title\": \"LangSmith upgrade fails with SSL certificate verification error when connecting to Azure Cache for Redis\",\n      \"url\": \"https://support.langchain.com/articles/7558740603-langsmith-helm-upgrade-fails-with-ssl-certificate-verification-error-when-connecting-to-azure-cache-for-redis\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"af6b64a9-ed4a-4993-b54d-d4913eeb1d26\",\n      \"title\": \"How do I change my email address in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9223968956-how-do-i-change-my-email-address-in-langsmith\",\n      \"collection\": \"General\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b384e-b824-7b02-94fe-170db3786d65",
    "content": "**Install LangChain via `pip install langchain` (Python 3.10+) or `npm install langchain` (JavaScript/TypeScript), set up an LLM API key like `ANTHROPIC_API_KEY`, then create your first agent using the official quickstart guides.**\n\nThe quickest path is building a simple agent that answers questions and calls tools. Start with a model like Claude Sonnet, add a basic tool (like weather lookup), and run it immediately.\n\n## Python Quickstart\n\n```python\n# Install: pip install -U langchain\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",  # Requires ANTHROPIC_API_KEY\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\nprint(result)\n```\n\n## JavaScript/TypeScript Quickstart\n\n```typescript\n// Install: npm install langchain\nimport { createAgent, tool } from \"langchain\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  (input) => `It's always sunny in ${input.city}!`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string().describe(\"The city to get the weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",  // Requires ANTHROPIC_API_KEY\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n\nNext steps include adding memory with `MemorySaver`, structured outputs, and real tools. Enable LangSmith tracing by setting `LANGSMITH_API_KEY` for observability.\n\n**Relevant docs:**\n- [Python Quickstart](https://docs.langchain.com/oss/python/langchain/quickstart)\n- [JavaScript Quickstart](https://docs.langchain.com/oss/javascript/langchain/quickstart)\n- [Python LangGraph Install](https://docs.langchain.com/oss/python/langgraph/install)\n- [JavaScript LangGraph Install](https://docs.langchain.com/oss/javascript/langgraph/install)"
  }
]