[
  {
    "role": "user",
    "id": "6de416e1-69bf-4e01-89aa-686e5878b76e",
    "content": "Can you show me some record available in internet so i can know that how many people using langchain to build AI Agent  or how many people using langgraph to build ai agent."
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_98003693",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"adoption statistics\"}"
        }
      },
      {
        "type": "function",
        "id": "call_27604621",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"usage numbers\"}"
        }
      },
      {
        "type": "function",
        "id": "call_54822752",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"downloads langchain\"}"
        }
      },
      {
        "type": "function",
        "id": "call_44149205",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"downloads langgraph\"}"
        }
      },
      {
        "type": "function",
        "id": "call_05343122",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"all\"}"
        }
      },
      {
        "type": "function",
        "id": "call_95843493",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"popularity metrics\"}"
        }
      }
    ],
    "id": "lc_run--019b3a2e-f558-7e01-95de-4d597ea4ef0b",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_98003693",
    "id": "99ef0e28-7caf-4703-b66e-d452a646053c",
    "content": "Result 1:\nTitle: Agent Server changelog\nLink: https://docs.langchain.comlangsmith/agent-server-changelog\nContent: v0.2.62\nImproved user handling for custom authentication in the JS Studio. Added Prometheus-format run statistics to the metrics endpoint for better monitoring. Added run statistics in Prometheus format to the metrics endpoint.\n\n2025-06-23\nv0.2.62 Improved user handling for custom authentication in the JS Studio. Added Prometheus-format run statistics to the metrics endpoint for better monitoring. Added run statistics in Prometheus format to the metrics endpoint.\n\nv0.6.0\nThis minor version updates the streaming APIs /join-stream and /stream behavior with respect to the last-event-id parameter to align with the SSE spec. Previously, passing a last-event-id would return that message in addition to any following messages. Going forward, these APIs will only return new messages following the provided last-event-id. For example, with the following stream, previously passing a last-event-id of 2 would return the messages with ids 2 and 3 , but will now only return the message with id 3: {\n    \"id\": 1,\n    \"event\": \"message\",\n    \"data\": {\n        \"content\": \"Excluded\"\n    }\n},\n{\n    \"id\": 2,\n    \"event\": \"message\",\n    \"data\": {\n        \"content\": \"Passed last-event-id\"\n    }\n},\n{\n    \"id\": 3,\n    \"event\": \"message\",\n    \"data\": {\n        \"content\": \"Included\"\n    }\n}\n\n---\n\nResult 2:\nTitle: Generate query stats\nLink: https://docs.langchain.comlangsmith/script-generate-query-stats\nContent: As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate LangSmith query statistics that will help us understand the performance of various queries that drive the LangSmith product experience.\n\nThis command will generate a CSV that can be shared with the LangChain team.\n\n### Prerequisites\n\nEnsure you have the following tools/items ready.\n\n1. kubectl\n\n   * [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)\n\n2. Clickhouse database credentials\n\n   * Host\n   * Port\n   * Username\n     * If using the bundled version, this is `default`\n   * Password\n     * If using the bundled version, this is `password`\n   * Database name\n     * If using the bundled version, this is `default`\n\n3. Connectivity to the Clickhouse database from the machine you will be running the `get_query_stats` script on.\n\n   * If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\n   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.\n\n4. The script to generate query stats\n\n   * You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_query_stats.sh)\n\n### Running the query stats generation script\n\nRun the following command to run the stats generation script:\n\n```bash  theme={null}\nsh get_query_stats.sh <clickhouse_url> --output path/to/file.csv\n```\n\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\n\n```bash  theme={null}\nsh get_query_stats.sh \"clickhouse://default:password@localhost:8123/default\" --output query_stats.csv\n```\n\nand after running this command you should see a file, query\\_stats.csv, has been created with LangSmith query statistics.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-generate-query-stats.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Release policy\nLink: https://docs.langchain.comoss/javascript/release-policy\nContent: This page explains the LangChain and LangGraph release policies. Click on the tabs below to view the release policies for each:\n\n<Tabs>\n  <Tab title=\"LangChain\">\n    The LangChain ecosystem is composed of different component packages (e.g., `@langchain/core`, `langchain`, `@langchain/community`, partner packages, etc.)\n\n    ## Release cadence\n\n    With the release of LangChain 1.0, **minor** releases (e.g., from `1.0.x` to `1.1.0`) of `langchain` and `@langchain/core` follow semantic versioning and may be released frequently. Minor releases contain new features and improvements but do not include breaking changes.\n\n    Patch versions are released frequently, up to a few times per week, as they contain bug fixes and minor improvements.\n\n    ## API stability\n\n    The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `@langchain/core` will continue to evolve to better serve the needs of our users.\n\n    With LangChain 1.0's adoption of semantic versioning:\n\n    * Breaking changes to the public API will only occur in major version releases (e.g., `2.0.0`)\n    * Minor version bumps (e.g., `1.0.0` to `1.1.0`) add new features without breaking changes\n    * Patch version bumps (e.g., `1.0.0` to `1.0.1`) contain bug fixes and minor improvements\n\n    We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.\n\n    ### Stability of other packages\n\n    The stability of other packages in the LangChain ecosystem may vary:\n\n    * **Partner packages maintained by LangChain** (such as `langchain-openai` and `langchain-anthropic`) follow semantic versioning and are expected to be stable post 1.0. Other partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information.\n\n    * **`@langchain/community`** is a community maintained package that contains 3rd party integrations. Due to the nature of community contributions and third-party dependencies, `@langchain/community` does not follow the same strict semantic versioning policy as `langchain` and `@langchain/core`. See the \"Special considerations\" section under Long-term support for more details.\n\n    ## Deprecation policy\n\n    We will generally avoid deprecating features until a better alternative is available.\n\n    With LangChain 1.0's semantic versioning approach, deprecated features will continue to work throughout the entire 1.x release series. Breaking changes, including the removal of deprecated features, will only occur in major version releases (e.g., 2.0).\n\n    When a feature is deprecated in `langchain` or `@langchain/core`, we will:\n\n    * Clearly mark it as deprecated in the code and documentation\n    * Provide migration guidance to the recommended alternative\n    * Provide security updates for the deprecated feature through all 1.x minor releases\n\n    In some situations, we may allow deprecated features to remain in the code base even longer if they are not causing maintenance issues, to further reduce the burden on users.\n\n    ## Long-term support (LTS)\n\n    LangChain follows a long-term support (LTS) policy to provide stability for production applications:\n\n    ### Release status definitions\n\n    Packages are marked with one of the following statuses:\n\n    * **ACTIVE**: Current active development, includes bug fixes, security patches, and new features\n    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features\n\n    ### Current LTS releases\n\n    **LangChain 1.0** is designated as an LTS release:\n\n    * **Status**: ACTIVE until the release of 2.0\n    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year\n    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes\n\n    ### Legacy version support\n\n    **LangChain 0.3**:\n\n    * **Status**: MAINTENANCE mode\n    * **Support period**: Until December 2026\n    * **Support includes**: Security patches and critical bug fixes\n\n    ### Special considerations\n\n    **@langchain/community 1.0**: Due to the nature of community contributions and third-party integrations, `@langchain/community` may have breaking changes on minor releases.\n  </Tab>\n\n  <Tab title=\"LangGraph\">\n    LangGraph follows a structured release policy to ensure stability and predictability for users building production applications.\n\n    ## Release cadence\n\n    We expect to space out **major** releases by at least 6-12 months to provide stability for production applications.\n\n    **Minor** releases are typically released every 1-2 months with new features and improvements.\n\n    **Patch** releases are released as needed, often weekly, to address bugs and security issues.\n\n    ## API stability\n\n    ### Stable APIs\n\n    All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features within a major version.\n\n    ### Beta features\n\n    Features marked as `beta` in the documentation are:\n\n    * Feature-complete and tested\n    * Safe for production use with the understanding they may change\n    * Subject to minor API adjustments based on user feedback\n\n    ### Experimental features\n\n    Features marked as `experimental` or `alpha`:\n\n    * Are under active development\n    * May change significantly or be removed\n    * Should be used with caution in production\n\n    ### Internal APIs\n\n    APIs prefixed with underscore (`_`) or explicitly marked as internal:\n\n    * Are not part of the public API\n    * May change without notice\n    * Should not be used directly\n\n    ## Deprecation policy\n\n    When deprecating features:\n\n    1. **Deprecation Notice**: Features are marked as deprecated with clear migration guidance\n    2. **Grace Period**: Deprecated features remain functional for at least one minor version\n    3. **Removal**: Features are removed only in major version releases\n    4. **Migration Support**: We provide migration guides and, when possible, automated tools\n\n    ## Platform compatibility\n\n    ### JavaScript/TypeScript support\n\n    * We support Node.js LTS versions\n    * TypeScript definitions are provided for all public APIs\n    * Browser compatibility is documented for client-side components\n\n    ## Breaking changes\n\n    Breaking changes are only introduced in major versions and include:\n\n    * Removal of deprecated APIs\n    * Changes to required parameters\n    * Changes to default behavior that affect existing applications\n    * Minimum Python/Node.js version updates\n\n    ## Migration support\n\n    For major version upgrades, we provide:\n\n    * Comprehensive migration guides\n    * Automated migration scripts when feasible\n    * Extended support period for the previous major version\n    * Clear documentation of all breaking changes\n\n    ## Long-term support (LTS)\n\n    LangGraph follows a long-term support (LTS) policy to provide stability for production applications:\n\n    ### Release status definitions\n\n    Packages are marked with one of the following statuses:\n\n    * **ACTIVE**: Current active development, includes bug fixes, security patches, and new features\n    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features\n\n    ### Current LTS releases\n\n    **LangGraph 1.0** is designated as an LTS release:\n\n    * **Status**: ACTIVE until the release of 2.0\n    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year\n    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes\n\n    ### Legacy version support\n\n    **LangGraph 0.4**:\n\n    * **Status**: MAINTENANCE mode\n    * **Support period**: Until December 2026\n    * **Support includes**: All security patches and critical bug fixes\n\n    ## See also\n\n    * [Versioning](/oss/javascript/versioning) - Version numbering and support details\n    * [Releases](/oss/javascript/releases) - Version-specific release notes and migration guides\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/release-policy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Tools and toolkits\nLink: https://docs.langchain.comoss/python/integrations/tools/index\nContent: Search\nThe following table shows tools that execute online searches in some shape or form: Tool/Toolkit Free/Paid Return Data Bing Search Paid URL, Snippet, Title Brave Search Free URL, Snippet, Title DuckDuckgoSearch Free URL, Snippet, Title Exa Search 1000 free searches/month URL, Author, Title, Published Date Google Search Paid URL, Snippet, Title Google Serper Free URL, Snippet, Title, Search Rank, Site Links Jina Search 1M Response Tokens Free URL, Snippet, Title, Page Content Mojeek Search Paid URL, Snippet, Title Parallel Search Paid URL, Title, Excerpts SearchApi 100 Free Searches on Sign Up URL, Snippet, Title, Search Rank, Site Links, Authors SearxNG Search Free URL, Snippet, Title, Category SerpApi 250 Free Searches/Month Answer Tavily Search 1000 free searches/month URL, Content, Title, Images, Answer You.com Search Free for 60 days URL, Title, Page Content\n\nAll tools and toolkits\nADS4GPTs AgentQL AINetwork Toolkit Alpha Vantage Amadeus Toolkit Anchor Browser Apify Actor ArXiv AskNews AWS Lambda Azure AI Services Toolkit Azure Cognitive Services Toolkit Azure Container Apps Dynamic Sessions Shell (bash) Bearly Code Interpreter Bing Search Bodo DataFrames Brave Search BrightData Web Scraper API BrightData SERP BrightData Unlocker Cassandra Database Toolkit CDP ChatGPT Plugins ClickUp Toolkit Cogniswitch Toolkit Compass DeFi Toolkit Composio Connery Toolkit Dall-E Image Generator Dappier Databricks Unity Catalog DataForSEO Dataherald Daytona Data Analysis DuckDuckGo Search Discord E2B Data Analysis Eden AI ElevenLabs Text2Speech Exa Search File System Financial Datasets Toolkit FMP Data GitHub Toolkit GitLab Toolkit Gmail Toolkit GOAT Privy Golden Query Google Books Google Calendar Toolkit Google Cloud Text-to-Speech Google Drive Google Finance Google Imagen Google Jobs Google Lens Google Places Google Scholar Google Search Google Serper Google Trends Gradio GraphQL HuggingFace Hub Tools Human as a Tool Hyperbrowser Browser Agent Tools Hyperbrowser Web Scraping Tools IBM watsonx.ai IBM watsonx.ai (SQL) IFTTT WebHooks Infobip Ionic Shopping Tool Jenkins Jina Search Jira Toolkit JSON Toolkit Lemon Agent Linkup Search Tool Memgraph Memorize Mojeek Search MultiOn Toolkit NASA Toolkit Naver Search Nuclia Understanding NVIDIA Riva Office365 Toolkit OpenAPI Toolkit Natural Language API Toolkits OpenGradient OpenWeatherMap Oracle AI Vector Search Oxylabs Pandas Dataframe Passio NutritionAI Parallel Extract Parallel Search Permit PlayWright Browser Toolkit Polygon IO Toolkit PowerBI Toolkit Prolog PubMed Python REPL Reddit Search Requests Toolkit Riza Code Interpreter Robocorp Toolkit Salesforce SceneXplain ScrapeGraph Scrapeless Crawl Scrapeless Scraping API Scrapeless Universal Scraping SearchApi SearxNG Search Semantic Scholar API SerpApi Slack Toolkit Spark SQL Toolkit SQLDatabase Toolkit StackExchange Steam Toolkit Stripe Tableau Taiga Tavily Extract Tavily Search Tilores MCP Toolbox Twilio Upstage Valthera ValyuContext Vectara Wikidata Wikipedia Wolfram Alpha WRITER Tools Yahoo Finance News You.com Search YouTube Zapier Natural Language Actions ZenGuard AI If you'd like to contribute an integration, see Contributing integrations. Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nFinance\nThe following table shows tools that can be used to execute financial transactions such as payments, purchases, and more: Tool/Toolkit Pricing Capabilities GOAT Free Create and receive payments, purchase physical goods, make investments, and more. Privy Free Create wallets with configurable permissions and execute transactions with speed.\n\n---\n\nResult 5:\nTitle: Release policy\nLink: https://docs.langchain.comoss/python/release-policy\nContent: This page explains the LangChain and LangGraph release policies. Click on the tabs below to view the release policies for each:\n\n<Tabs>\n  <Tab title=\"LangChain\">\n    The LangChain ecosystem is composed of different component packages (e.g., `langchain-core`, `langchain`, `langchain-community`, partner packages, etc.)\n\n    ## Release cadence\n\n    With the release of LangChain 1.0, **minor** releases (e.g., from `1.0.x` to `1.1.0`) of `langchain` and `langchain-core` follow semantic versioning and may be released frequently. Minor releases contain new features and improvements but do not include breaking changes.\n\n    Patch versions are released frequently, up to a few times per week, as they contain bug fixes and minor improvements.\n\n    ## API stability\n\n    The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `langchain-core` will continue to evolve to better serve the needs of our users.\n\n    With LangChain 1.0's adoption of semantic versioning:\n\n    * Breaking changes to the public API will only occur in major version releases (e.g., `2.0.0`)\n    * Minor version bumps (e.g., `1.0.0` to `1.1.0`) add new features without breaking changes\n    * Patch version bumps (e.g., `1.0.0` to `1.0.1`) contain bug fixes and minor improvements\n\n    We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.\n\n    ### Stability of other packages\n\n    The stability of other packages in the LangChain ecosystem may vary:\n\n    * **Partner packages maintained by LangChain** (such as `langchain-openai` and `langchain-anthropic`) follow semantic versioning and are expected to be stable post 1.0. Other partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information.\n\n    * **`langchain-community`** is a community maintained package that contains 3rd party integrations. Due to the number of integrations there, `langchain-community` does not follow the same strict semantic versioning policy as `langchain` and `langchain-core`. See the \"Special considerations\" section under Long-term support for more details.\n\n    ## Deprecation policy\n\n    We will generally avoid deprecating features until a better alternative is available.\n\n    With LangChain 1.0's semantic versioning approach, deprecated features will continue to work throughout the entire 1.x release series. Breaking changes, including the removal of deprecated features, will only occur in major version releases (e.g., 2.0).\n\n    When a feature is deprecated in `langchain` or `langchain-core`, we will:\n\n    * Clearly mark it as deprecated in the code and documentation\n    * Provide migration guidance to the recommended alternative\n    * Provide security updates for the deprecated feature through all 1.x minor releases\n\n    In some situations, we may allow deprecated features to remain in the code base even longer if they are not causing maintenance issues, to further reduce the burden on users.\n\n    ## Long-term support (LTS)\n\n    LangChain follows a long-term support (LTS) policy to provide stability for production applications:\n\n    ### Release status definitions\n\n    Packages are marked with one of the following statuses:\n\n    * **ACTIVE**: Current active development, includes bug fixes, security patches, and new features\n    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features\n\n    ### Current LTS releases\n\n    **LangChain 1.0** is designated as an LTS release:\n\n    * **Status**: ACTIVE until the release of 2.0\n    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year\n    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes\n\n    ### Legacy version support\n\n    **LangChain 0.3**:\n\n    * **Status**: MAINTENANCE mode\n    * **Support period**: Until December 2026\n    * **Support includes**: Security patches and critical bug fixes\n\n    ### Special considerations\n\n    **langchain-community 0.4**: Due to the nature of community contributions and third-party integrations, `langchain-community` may have breaking changes on minor releases. It has been released as version 0.4 to reflect this different stability policy.\n  </Tab>\n\n  <Tab title=\"LangGraph\">\n    LangGraph follows a structured release policy to ensure stability and predictability for users building production applications.\n\n    ## Release cadence\n\n    We expect to space out **major** releases by at least 6-12 months to provide stability for production applications.\n\n    **Minor** releases are typically released every 1-2 months with new features and improvements.\n\n    **Patch** releases are released as needed, often weekly, to address bugs and security issues.\n\n    ## API stability\n\n    ### Stable APIs\n\n    All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features within a major version.\n\n    ### Beta features\n\n    Features marked as `beta` in the documentation are:\n\n    * Feature-complete and tested\n    * Safe for production use with the understanding they may change\n    * Subject to minor API adjustments based on user feedback\n\n    ### Experimental features\n\n    Features marked as `experimental` or `alpha`:\n\n    * Are under active development\n    * May change significantly or be removed\n    * Should be used with caution in production\n\n    ### Internal APIs\n\n    APIs prefixed with underscore (`_`) or explicitly marked as internal:\n\n    * Are not part of the public API\n    * May change without notice\n    * Should not be used directly\n\n    ## Deprecation policy\n\n    When deprecating features:\n\n    1. **Deprecation Notice**: Features are marked as deprecated with clear migration guidance\n    2. **Grace Period**: Deprecated features remain functional for at least one minor version\n    3. **Removal**: Features are removed only in major version releases\n    4. **Migration Support**: We provide migration guides and, when possible, automated tools\n\n    ## Platform compatibility\n\n    ### Python support\n\n    * We support Python versions that are actively maintained by the Python Software Foundation\n    * Python version requirements may change only in major releases\n    * Currently requires Python 3.10 or later\n\n    ## Breaking changes\n\n    Breaking changes are only introduced in major versions and include:\n\n    * Removal of deprecated APIs\n    * Changes to required parameters\n    * Changes to default behavior that affect existing applications\n    * Minimum Python/Node.js version updates\n\n    ## Migration support\n\n    For major version upgrades, we provide:\n\n    * Comprehensive migration guides\n    * Automated migration scripts when feasible\n    * Extended support period for the previous major version\n    * Clear documentation of all breaking changes\n\n    ## Long-term support (LTS)\n\n    LangGraph follows a long-term support (LTS) policy to provide stability for production applications:\n\n    ### Release status definitions\n\n    Packages are marked with one of the following statuses:\n\n    * **ACTIVE**: Current active development, includes bug fixes, security patches, and new features\n    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features\n\n    ### Current LTS releases\n\n    **LangGraph 1.0** is designated as an LTS release:\n\n    * **Status**: ACTIVE until the release of 2.0\n    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year\n    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes\n\n    ### Legacy version support\n\n    **LangGraph 0.4**:\n\n    * **Status**: MAINTENANCE mode\n    * **Support period**: Until December 2026\n    * **Support includes**: All security patches and critical bug fixes\n\n    ## See also\n\n    * [Versioning](/oss/python/versioning) - Version numbering and support details\n    * [Releases](/oss/python/releases) - Version-specific release notes and migration guides\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/release-policy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_27604621",
    "id": "f84fadad-2d12-4429-a52f-e72d8e2328c6",
    "content": "Result 1:\nTitle: Overview\nLink: https://docs.langchain.comlangsmith/administration-overview\nContent: Self-configured monthly usage limits\nThis 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month. This is thrown by our application and varies by organization based on their configured settings.\n\nUsage Limits\nLangSmith lets you configure usage limits on tracing. Note that these are usage limits, not spend limits, which mean they let you limit the quantity of occurrences of some event rather than the total amount you will spend. LangSmith lets you set two different monthly limits, mirroring our Billable Metrics discussed in the aforementioned data retention guide: All traces limit Extended data retention traces limit These let you limit the number of total traces, and extended data retention traces respectively.\n\nUpdating usage limits\nUsage limits can be updated from the Settings page under Usage and Billing . Limit values are cached, so it may take a minute or two before the new limits apply.\n\n---\n\nResult 2:\nTitle: Manage billing in your account\nLink: https://docs.langchain.comlangsmith/billing\nContent: This page describes how to manage billing for your LangSmith organization:\n\n* [Set up billing for your account](#set-up-billing-for-your-account): Complete the billing setup process for Developer and Plus plans, including special instructions for legacy accounts.\n* [Update your information](#update-your-information): Modify invoice email addresses, business information, and tax IDs for your organization.\n* [Enforce spend limits](#enforce-spend-limits): Learn how to manage your spend through usage limits and data retention.\n\n## Set up billing for your account\n\n<Note>\n  Before using this guide, note the following:\n\n  * If you are interested in the [Enterprise](https://www.langchain.com/pricing) plan, please [contact sales](https://www.langchain.com/contact-sales). This guide is only for our self-serve billing plans.\n</Note>\n\nTo set up billing for your LangSmith organization, navigate to the [Billing and Usage](https://smith.langchain.com/settings/payments) page under **Settings**. Depending on your organization's settings, there are different setup guides:\n\n* [Developer plan](#developer-plan%3A-set-up-billing-on-your-personal-organization)\n* [Plus plan](#plus-plan%3A-set-up-billing-on-a-shared-organization)\n\n### Developer Plan: set up billing on your personal organization\n\nPersonal organizations are limited to 5,000 traces per month until a credit card is added. To add a card:\n\n1. Click **Add card to remove trace limit**.\n2. Add your credit card information.\n3. Once complete, you will no longer be rate limited to 5,000 traces, and you will be charged for any excess traces at rates specified on the [pricing](https://www.langchain.com/pricing-langsmith) page.\n\n### Plus Plan: set up billing on a shared organization\n\nTeam organizations are given an initial 10,000 traces per month. Any excess traces will be charged at rates specified on the [pricing](https://www.langchain.com/pricing-langsmith) page.\n\n<Note>\n  New organizations that you manually create are required to be on the Plus Plan. If you see a message about needing to upgrade to Plus to use this organization, follow these steps.\n</Note>\n\n1. Click **Upgrade to Plus**.\n2. Invite members to your organization, as desired.\n3. Enter your credit card information. Then, enter business information, invoice email, and tax ID. If this organization belongs to a business, check the **This is a business** checkbox and enter the information accordingly. For more information, refer to the [Update your information section](#update-your-information).\n\n## Update your information (Paid plans only)\n\nTo update business information for your LangSmith organization, head to the [Billing and Usage](https://smith.langchain.com/settings/payments) page under **Settings**.\n\n### Invoice email\n\nTo update the email address for invoices, follow these steps:\n\n1. Navigate to the **Plans and Billing** tab.\n2. Locate the section beneath the payment method, where the current invoice email is displayed.\n3. Enter the new email address for invoices in the provided field.\n4. The new email address will be automatically saved.\n\nYou will receive all future invoices to the updated email address.\n\n### Business information and tax ID\n\n<Note>\n  In certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your tax ID may qualify you for a sales tax exemption.\n</Note>\n\nTo update your organization's business information, follow these steps:\n\n1. Navigate to the **Plans and Billing** tab.\n2. Below the invoice email section, you will find a checkbox labeled **Business**.\n3. Check the **Business** checkbox if your organization belongs to a business.\n4. A business information section will appear, allowing you to enter or update the following details:\n   * Business Name\n   * Address\n   * Tax ID for applicable jurisdictions\n5. A Tax ID field will appear for applicable jurisdictions after you select a country.\n6. After entering the necessary information, click the **Save** button to save your changes.\n\nThis ensures that your business information is up-to-date and accurate for billing and tax purposes.\n\n## Enforce spend limits\n\n<Check>\n  You may find it helpful to read the following pages, before continuing with this section on optimizing your tracing spend:\n\n  * [Data Retention Conceptual Docs](/langsmith/administration-overview#data-retention)\n  * [Usage Limiting Conceptual Docs](/langsmith/administration-overview#usage-limits)\n</Check>\n\n<Note>\n  Some of the features mentioned in this guide are not currently available on Enterprise plan due to its custom nature of billing. If you are on the Enterprise plan and have questions about cost optimization, contact your sales rep or support via [support.langchain.com](https://support.langchain.com).\n</Note>\n\n### Understand your current usage\n\nThe first step of any optimization process is to understand current usage. LangSmith provides two ways to do this: [Usage graph](#usage-graph) and [Invoices](#invoices).\n\nLangSmith Usage is measured per workspace, because workspaces often represent development environments (as in the example), or teams within an organization.\n\n#### Usage graph\n\nThe usage graph lets you examine how much of each usage-based pricing metric you have consumed. It does not directly show spend (which you will review later in the draft invoice).\n\nNavigate to the usage graph under **Settings** -> **Billing and Usage** -> **Usage Graph**.\n\nThere are several usage metrics that LangSmith charges for:\n\n* LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.\n* LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.\n* LangSmith Deployment Agent Runs: tracks end-to-end invocations of deployed LangGraph agents.\n\nFor more details on traces, refer to the [data retention conceptual docs](/langsmith/administration-overview#data-retention). For more details on Agent Runs, refer to [LangSmith Deployment billing](#langsmith-deployment-billing).\n\n#### Invoices\n\nTo understand how your usage translates to spend, navigate to the **Invoices** tab. The first invoice that will appear on screen is a draft of your current month's invoice, which shows your running spend thus far this month.\n\n<Note>\n  LangSmith's Usage Graph and Invoice use the term `tenant_id` to refer to a workspace ID. They are interchangeable.\n</Note>\n\n### Set limits on usage\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=27addecc92b87dd4131683fb8500f96c\" alt=\"P2usagelimitsempty v2\" data-og-width=\"2598\" width=\"2598\" data-og-height=\"1582\" height=\"1582\" data-path=\"langsmith/images/p2usagelimitsempty-v2.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=280&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=1b544ddf00fd88a470ec19db0283a9ec 280w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=560&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=91a0e4559da14d35d8644e20ff2210ec 560w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=840&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=0f77081378606e6bb4061e4c7b694381 840w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=1100&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=e5ea7731ac5e71276ed58f8f8df03d18 1100w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=1650&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=f36ec6fc852a08f1d83bb8e0a0cc0fa8 1650w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=2500&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=88f9537239e3a5250b056070d3b57a93 2500w\" />\n\n#### Set spend limit for workspace\n\n1. To set limits, navigate to **Settings** -> **Billing and Usage** -> **Usage limits**.\n2. Input a spend limit for your selected workspace. LangSmith will determine an appropriate number of base and extended trace limits to match that spend. The trace limits include the free trace allocation that comes with your plan (see details on [pricing page](https://smith.langchain.com/settings/payments)).\n\n<Note>\n  For organizations with **multiple workspaces only**: For simplicity, LangSmith incorporates the free traces into the cost calculation of the **first workspace only**. In actuality, the free traces can be \"consumed\" by any workspace. Therefore, although workspace-level spend limits are approximate for multi-workspace organizations, the organization-level spend limit is absolute.\n</Note>\n\n#### Configure trace tier distrubution\n\nLangSmith has two trace tiers: base traces and extended traces. Base traces have the base retention and are short-lived (14 days), while extended traces have extended retention and are long-lived (400 days). For more information, refer to the [data retention conceptual docs](/langsmith/administration-overview#data-retention).\n\nSet the desired default trace tier by selecting an option below the **Default data retention** label. All traces will have this tier by default when they are registered. Note that because extended traces cost more than base traces, selecting **Extended** as your default data retention option will result in less overall traces allowed in the billing period. By default, updating this setting will only apply to future incoming traces. To apply to all existing traces in the workspace, select the checkbox.\n\nIf the default data retention is set to **Base** you can optionally use the slider to distribute trace limits across base and extended tracess. LangSmith automatically provides a suggestion for this distribution but you can tailor this to your needs. For example, if you are running lots of automations or other features that may upgrade a trace to extended, you may want to increase your extended trace limits. To see the complete list of features that may upgrade a trace, [see here](https://docs.langchain.com/langsmith/administration-overview#how-it-works:~:text=Data%20retention%20auto%2Dupgrades).\n\n<Note>\n  The extended data retention limit can cause features other than tracing to stop working once reached. If you plan to use this feature, read more about its [functionality and side effects](/langsmith/administration-overview#side-effects-of-extended-data-retention-traces-limit).\n</Note>\n\n### Other methods of managing traces\n\n#### Change project-level default retention\n\nData retention settings are adjustable per tracing project.\n\nNavigate to **Projects** > ***Your project name*** > Select **Retention** and select the desired default retention. This will only affect retention (and pricing) for **traces going forward**.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=394b513df5ef31d0309f5f3c78bd315a\" alt=\"P1projectretention\" data-og-width=\"1358\" width=\"1358\" data-og-height=\"452\" height=\"452\" data-path=\"langsmith/images/p1projectretention.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0ebc83ac05d14858da153e707bd02f6b 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0baaf55ed3a4719c2b3c3545778e1ded 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=30989a6636683bca7e456ea1897c2986 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ee7c87a83e1a15bc4d843bae3ad32811 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7947cbde975873a53d197019453485e5 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=53e1f6edb92789ee5034d5f7f1153af6 2500w\" />\n\n#### Apply extended data retention to a percentage of traces\n\nYou may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an [automation rule](/langsmith/rules). You might want to apply extended data retention to specific types of traces, such as:\n\n* 10% of all traces: For general analysis or analyzing trends long term.\n* Errored traces: To investigate and debug issues thoroughly.\n* Traces with specific metadata: For long-term examination of particular features or user flows.\n\nTo configure this:\n\n1. Navigate to **Projects** > ***Your project name*** > Select **+ New** > Select **New Automation**.\n2. Name your rule and optionally apply filters or a sample rate. For more information on configuring filters, refer to [filtering techniques](/langsmith/filter-traces-in-application#filter-operators).\n\n<Note>\n  When an automation rule matches any [run](/langsmith/observability-concepts#runs) within a [trace](/langsmith/observability-concepts#traces), then all runs within the trace are upgraded to be retained for 400 days.\n</Note>\n\nFor example, this is the expected configuration to keep 10% of all traces for extended data retention:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=09bbdf5ef7cf3a5a99d6bf0a704e2143\" alt=\"P2sampletraces\" data-og-width=\"640\" width=\"640\" data-og-height=\"610\" height=\"610\" data-path=\"langsmith/images/P2SampleTraces.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=280&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=b40ff90a0e3ef5f52ecda25965ec1daa 280w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=560&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4381f3239c4d5c71c8ccd9dcdb4fe859 560w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=840&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=81da02c26dcc067fbbc4a4d7de318dee 840w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=1100&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=5f703039c242ba31d192fdb8f4742362 1100w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=1650&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=e293a6cca1f7da6c8ca62b64e249f6b2 1650w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=2500&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=558b6d18d598241a2f81e8327b13ce3d 2500w\" />\n\nIf you want to keep a subset of traces for **longer than 400 days** for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.\n\n### LangSmith Deployment billing\n\nIn addition to traces, LangSmith charges for deployed agents via LangSmith Deployment (formerly LangGraph Platform).\n\n* **Agent Runs**: An Agent Run is one end-to-end invocation of a deployed LangGraph agent and is billed at \\$0.005 each. Nodes and subgraphs within a single agent execution are not charged separately. Calls to other LangGraph agents are charged separately to the deployment hosting the called agent. When using human-in-the-loop with interrupts, resuming after an interrupt creates a separate Agent Run.\n* **Deployment Uptime**: You are also charged for the time your deployment's database is live and persisting state. See the [pricing page](https://www.langchain.com/pricing) for uptime costs by deployment type (Development vs Production).\n\nFor high-volume deployment usage, please [contact our sales team](https://www.langchain.com/contact-sales) to discuss custom pricing options.\n\n### Summary\n\nIf you have questions about further managing your spend, please contact support via [support.langchain.com](https://support.langchain.com).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/billing.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Discover errors and usage patterns with the Insights Agent\nLink: https://docs.langchain.comlangsmith/insights\nContent: Discover errors and usage patterns with the Insights Agent\nDiscover errors and usage patterns with the Insights Agent\nThe Insights Agent automatically analyzes your traces to detect usage patterns, common agent behaviors and failure modes \u2014 without requiring you to manually review thousands of traces.\nInsights uses hierarchical categorization to make sense of your data and highlight actionable trends. Insights is available for LangSmith Plus and Enterprise plans and is only available for LangSmith SaaS deployments.\n\nDiscover errors and usage patterns with the Insights Agent\nThe Insights Agent automatically analyzes your traces to detect usage patterns, common agent behaviors and failure modes \u2014 without requiring you to manually review thousands of traces.\nInsights uses hierarchical categorization to make sense of your data and highlight actionable trends. Insights is available for LangSmith Plus and Enterprise plans and is only available for LangSmith SaaS deployments.\n\nUsing a prebuilt config\nUse the Saved configurations dropdown to load presets for common jobs like Usage Patterns or Error Analysis . Run them directly for a fast start, or adjust filters, prompts, and providers before saving your customized version. To learn more about what you can customize, read the section below.\n\n---\n\nResult 4:\nTitle: Trace with OpenTelemetry\nLink: https://docs.langchain.comlangsmith/trace-with-opentelemetry\nContent: GenAI usage metrics\nOpenTelemetry attribute LangSmith field Notes gen_ai.usage.input_tokens usage_metadata.input_tokens Number of input tokens used gen_ai.usage.output_tokens usage_metadata.output_tokens Number of output tokens used gen_ai.usage.total_tokens usage_metadata.total_tokens Total number of tokens used gen_ai.usage.prompt_tokens usage_metadata.input_tokens Number of input tokens used (deprecated) gen_ai.usage.completion_tokens usage_metadata.output_tokens Number of output tokens used (deprecated) gen_ai.usage.details.reasoning_tokens usage_metadata.reasoning_tokens Number of reasoning tokens used\n\nLLM attributes\nOpenTelemetry attribute LangSmith field Notes llm.input_messages inputs.messages Input messages llm.output_messages outputs.messages Output messages llm.token_count.prompt usage_metadata.input_tokens Prompt token count llm.token_count.completion usage_metadata.output_tokens Completion token count llm.token_count.total usage_metadata.total_tokens Total token count llm.usage.total_tokens usage_metadata.total_tokens Alternative total token count llm.invocation_parameters invocation_params.* JSON string of invocation parameters llm.presence_penalty invocation_params.presence_penalty Presence penalty llm.frequency_penalty invocation_params.frequency_penalty Frequency penalty llm.request.functions invocation_params.functions Function definitions\n\n---\n\nResult 5:\nTitle: Messages\nLink: https://docs.langchain.comoss/javascript/langchain/messages\nContent: Token usage\nAn AIMessage can hold token counts and other usage metadata in its usage_metadata field: {\n  \"output_tokens\": 304,\n  \"input_tokens\": 8,\n  \"total_tokens\": 312,\n  \"input_token_details\": {\n    \"cache_read\": 0\n  },\n  \"output_token_details\": {\n    \"reasoning\": 256\n  }\n}\n\nToken usage\nAn AIMessage can hold token counts and other usage metadata in its usage_metadata field: See UsageMetadata for details.\n\nToken usage\nAn AIMessage can hold token counts and other usage metadata in its usage_metadata field: import { initChatModel } from \"langchain\";\n\nconst model = await initChatModel(\"gpt-5-nano\");\n\nconst response = await model.invoke(\"Hello!\");\nconsole.log(response.usage_metadata);\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_54822752",
    "id": "690a323b-59b1-4463-8c5c-4725dacab724",
    "content": "Result 1:\nTitle: LangChain overview\nLink: https://docs.langchain.comoss/javascript/langchain/overview\nContent: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/javascript/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/javascript/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain [agents](/oss/javascript/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\n## <Icon icon=\"wand-magic-sparkles\" /> Create an agent\n\n```ts  theme={null}\nimport * as z from \"zod\";\n// npm install @langchain/anthropic to call the model\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }) => `It's always sunny in ${city}!`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  },\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n\nSee the [Installation instructions](/oss/javascript/langchain/install) and [Quickstart guide](/oss/javascript/langchain/quickstart) to get started building your own agents and applications with LangChain.\n\n## <Icon icon=\"star\" size={20} /> Core benefits\n\n<Columns cols={2}>\n  <Card title=\"Standard model interface\" icon=\"arrows-rotate\" href=\"/oss/javascript/langchain/models\" arrow cta=\"Learn more\">\n    Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\n  </Card>\n\n  <Card title=\"Easy to use, highly flexible agent\" icon=\"wand-magic-sparkles\" href=\"/oss/javascript/langchain/agents\" arrow cta=\"Learn more\">\n    LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\n  </Card>\n\n  <Card title=\"Built on top of LangGraph\" icon=\"circle-nodes\" href=\"/oss/javascript/langgraph/overview\" arrow cta=\"Learn more\">\n    LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.\n  </Card>\n\n  <Card title=\"Debug with LangSmith\" icon=\"eye\" href=\"/langsmith/home\" arrow cta=\"Learn more\">\n    Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: What's new in LangChain v1\nLink: https://docs.langchain.comoss/javascript/releases/langchain-v1\nContent: **LangChain v1 is a focused, production-ready foundation for building agents.** We've streamlined the framework around three core improvements:\n\n<CardGroup cols={1}>\n  <Card title=\"createAgent\" icon=\"robot\" href=\"#create-agent\" arrow>\n    A new standard way to build agents in LangChain, replacing `createReactAgent` from LangGraph with a cleaner, more powerful API.\n  </Card>\n\n  <Card title=\"Standard content blocks\" icon=\"cube\" href=\"#standard-content-blocks\" arrow>\n    A new `contentBlocks` property that provides unified access to modern LLM features across all providers.\n  </Card>\n\n  <Card title=\"Simplified package\" icon=\"sitemap\" href=\"#simplified-package\" arrow>\n    The `langchain` package has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to `@langchain/classic`.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install langchain @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm install langchain @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add langchain @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add langchain @langchain/core\n  ```\n</CodeGroup>\n\nFor a complete list of changes, see the [migration guide](/oss/javascript/migrate/langchain-v1).\n\n## `createAgent`\n\n`createAgent` is the standard way to build agents in LangChain 1.0. It provides a simpler interface than the prebuilt `createReactAgent` exported from LangGraph while offering greater customization potential by using middleware.\n\n```ts  theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n  systemPrompt: \"You are a helpful assistant.\",\n});\n\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"What is the weather in Tokyo?\" },\n  ],\n});\n\nconsole.log(result.content);\n```\n\nUnder the hood, `createAgent` is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063\" alt=\"Core agent loop diagram\" className=\"rounded-lg\" data-og-width=\"300\" width=\"300\" data-og-height=\"268\" height=\"268\" data-path=\"oss/images/core_agent_loop.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=280&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=a4c4b766b6678ef52a6ed556b1a0b032 280w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=560&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=111869e6e99a52c0eff60a1ef7ddc49c 560w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=840&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=6c1e21de7b53bd0a29683aca09c6f86e 840w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1100&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=88bef556edba9869b759551c610c60f4 1100w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1650&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=9b0bdd138e9548eeb5056dc0ed2d4a4b 1650w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=2500&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=41eb4f053ed5e6b0ba5bad2badf6d755 2500w\" />\n</div>\n\nFor more information, see [Agents](/oss/javascript/langchain/agents).\n\n### Middleware\n\nMiddleware is the defining feature of `createAgent`. It makes `createAgent` highly customizable, raising the ceiling for what you can build.\n\nGreat agents require [context engineering](/oss/javascript/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n\n#### Prebuilt middleware\n\nLangChain provides a few [prebuilt middlewares](/oss/javascript/langchain/middleware#built-in-middleware) for common patterns, including:\n\n* `summarizationMiddleware`: Condense conversation history when it gets too long\n* `humanInTheLoopMiddleware`: Require approval for sensitive tool calls\n* `piiRedactionMiddleware`: Redact sensitive information before sending to the model\n\n```ts  theme={null}\nimport {\n  createAgent,\n  summarizationMiddleware,\n  humanInTheLoopMiddleware,\n  piiRedactionMiddleware,\n} from \"langchain\";\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [readEmail, sendEmail],\n  middleware: [\n    piiRedactionMiddleware({ patterns: [\"email\", \"phone\", \"ssn\"] }),\n    summarizationMiddleware({\n      model: \"claude-sonnet-4-5-20250929\",\n      trigger: { tokens: 500 },\n    }),\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        sendEmail: {\n          allowedDecisions: [\"approve\", \"edit\", \"reject\"],\n        },\n      },\n    }),\n  ],\n});\n```\n\n#### Custom middleware\n\nYou can also build custom middleware to fit your specific needs.\n\nBuild custom middleware by implementing any of these hooks using the `createMiddleware` function:\n\n| Hook            | When it runs             | Use cases                               |\n| --------------- | ------------------------ | --------------------------------------- |\n| `beforeAgent`   | Before calling the agent | Load memory, validate input             |\n| `beforeModel`   | Before each LLM call     | Update prompts, trim messages           |\n| `wrapModelCall` | Around each LLM call     | Intercept and modify requests/responses |\n| `wrapToolCall`  | Around each tool call    | Intercept and modify tool execution     |\n| `afterModel`    | After each LLM response  | Validate output, apply guardrails       |\n| `afterAgent`    | After agent completes    | Save results, cleanup                   |\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1\" alt=\"Middleware flow diagram\" className=\"rounded-lg\" data-og-width=\"500\" width=\"500\" data-og-height=\"560\" height=\"560\" data-path=\"oss/images/middleware_final.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w\" />\n</div>\n\nExample custom middleware:\n\n```ts  theme={null}\nimport { createMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userExpertise: z.enum([\"beginner\", \"expert\"]).default(\"beginner\"),\n})\n\nconst expertiseBasedToolMiddleware = createMiddleware({\n  wrapModelCall: async (request, handler) => {\n    const userLevel = request.runtime.context.userExpertise;\n    if (userLevel === \"expert\") {\n      const tools = [advancedSearch, dataAnalysis];\n      return handler(\n        request.replace(\"openai:gpt-5\", tools)\n      );\n    }\n    const tools = [simpleSearch, basicCalculator];\n    return handler(\n      request.replace(\"openai:gpt-5-nano\", tools)\n    );\n  },\n});\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [simpleSearch, advancedSearch, basicCalculator, dataAnalysis],\n  middleware: [expertiseBasedToolMiddleware],\n  contextSchema,\n});\n```\n\nFor more information, see [the complete middleware guide](/oss/javascript/langchain/middleware).\n\n### Built on LangGraph\n\nBecause `createAgent` is built on LangGraph, you automatically get built in support for long running and reliable agents via:\n\n<CardGroup cols={2}>\n  <Card title=\"Persistence\" icon=\"database\">\n    Conversations automatically persist across sessions with built-in checkpointing\n  </Card>\n\n  <Card title=\"Streaming\" icon=\"water\">\n    Stream tokens, tool calls, and reasoning traces in real-time\n  </Card>\n\n  <Card title=\"Human-in-the-loop\" icon=\"hand\">\n    Pause agent execution for human approval before sensitive actions\n  </Card>\n\n  <Card title=\"Time travel\" icon=\"clock-rotate-left\">\n    Rewind conversations to any point and explore alternate paths and prompts\n  </Card>\n</CardGroup>\n\nYou don't need to learn LangGraph to use these features\u2014they work out of the box.\n\n### Structured output\n\n`createAgent` has improved structured output generation:\n\n* **Main loop integration**: Structured output is now generated in the main loop instead of requiring an additional LLM call\n* **Structured output strategy**: Models can choose between calling tools or using provider-side structured output generation\n* **Cost reduction**: Eliminates extra expense from additional LLM calls\n\n```ts  theme={null}\nimport { createAgent } from \"langchain\";\nimport * as z from \"zod\";\n\nconst weatherSchema = z.object({\n  temperature: z.number(),\n  condition: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\",\n  tools: [getWeather],\n  responseFormat: weatherSchema,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"What is the weather in Tokyo?\" },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n```\n\n**Error handling**: Control error handling via the `handleErrors` parameter to `ToolStrategy`:\n\n* **Parsing errors**: Model generates data that doesn't match desired structure\n* **Multiple tool calls**: Model generates 2+ tool calls for structured output schemas\n\n***\n\n## Standard content blocks\n\n<Note>\n  1.0 releases are available for most packages. Only the following currently support new content blocks:\n\n  * `langchain`\n  * `@langchain/core`\n  * `@langchain/anthropic`\n  * `@langchain/openai`\n\n  Broader support for content blocks is planned.\n</Note>\n\n### Benefits\n\n* **Provider agnostic**: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider\n* **Type safe**: Full type hints for all content block types\n* **Backward compatible**: Standard content can be [loaded lazily](/oss/javascript/langchain/messages#standard-content-blocks), so there are no associated breaking changes\n\nFor more information, see our guide on [content blocks](/oss/javascript/langchain/messages#message-content)\n\n***\n\n## Simplified package\n\nLangChain v1 streamlines the `langchain` package namespace to focus on essential building blocks for agents. The package exposes only the most useful and relevant functionality:\n\nMost of these are re-exported from `@langchain/core` for convenience, which gives you a focused API surface for building agents.\n\n### `@langchain/classic`\n\nLegacy functionality has moved to [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic) to keep the core package lean and focused.\n\n#### What's in `@langchain/classic`\n\n* Legacy chains and chain implementations\n* Retrievers\n* The indexing API\n* [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) exports\n* Other deprecated functionality\n\nIf you use any of this functionality, install [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic):\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/classic\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm install @langchain/classic\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/classic\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/classic\n  ```\n</CodeGroup>\n\nThen update your imports:\n\n```typescript  theme={null}\nimport { ... } from \"langchain\"; // [!code --]\nimport { ... } from \"@langchain/classic\"; // [!code ++]\n\nimport { ... } from \"langchain/chains\"; // [!code --]\nimport { ... } from \"@langchain/classic/chains\"; // [!code ++]\n```\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langchainjs/issues) using the [`'v1'` label](https://github.com/langchain-ai/langchainjs/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangChain 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Middleware guide\" icon=\"puzzle-piece\" href=\"https://blog.langchain.com/agent-middleware/\">\n    Deep dive into middleware\n  </Card>\n\n  <Card title=\"Agents Documentation\" icon=\"book\" href=\"/oss/javascript/langchain/agents\" arrow>\n    Full agent documentation\n  </Card>\n\n  <Card title=\"Message Content\" icon=\"message\" href=\"/oss/javascript/langchain/messages#message-content\" arrow>\n    New content blocks API\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/javascript/migrate/langchain-v1\" arrow>\n    How to migrate to LangChain v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langchainjs\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/javascript/versioning) \u2013 Understanding version numbers\n* [Release policy](/oss/javascript/release-policy) \u2013 Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/releases/langchain-v1.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Install LangChain\nLink: https://docs.langchain.comoss/python/langchain/install\nContent: To install the LangChain package:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langchain\n  # Requires Python 3.10+\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain\n  # Requires Python 3.10+\n  ```\n</CodeGroup>\n\nLangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  # Installing the OpenAI integration\n  pip install -U langchain-openai\n\n  # Installing the Anthropic integration\n  pip install -U langchain-anthropic\n  ```\n\n  ```bash uv theme={null}\n  # Installing the OpenAI integration\n  uv add langchain-openai\n\n  # Installing the Anthropic integration\n  uv add langchain-anthropic\n  ```\n</CodeGroup>\n\n<Tip>\n  See the [Integrations tab](/oss/python/integrations/providers/overview) for a full list of available integrations.\n</Tip>\n\nNow that you have LangChain installed, you can get started by following the [Quickstart guide](/oss/python/langchain/quickstart).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/install.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Build a SQL assistant with on-demand skills\nLink: https://docs.langchain.comoss/python/langchain/multi-agent/skills-sql-assistant\nContent: Installation\nThis tutorial requires the langchain package: For more details, see our Installation guide.\n\nInstallation\nThis tutorial requires the langchain package: pip install langchain\n\nInstallation\nThis tutorial requires the langchain package: uv add langchain\n\n---\n\nResult 5:\nTitle: LangChain overview\nLink: https://docs.langchain.comoss/python/langchain/overview\nContent: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/python/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/python/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain [agents](/oss/python/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\n## <Icon icon=\"wand-magic-sparkles\" /> Create an agent\n\n```python  theme={null}\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n```\n\nSee the [Installation instructions](/oss/python/langchain/install) and [Quickstart guide](/oss/python/langchain/quickstart) to get started building your own agents and applications with LangChain.\n\n## <Icon icon=\"star\" size={20} /> Core benefits\n\n<Columns cols={2}>\n  <Card title=\"Standard model interface\" icon=\"arrows-rotate\" href=\"/oss/python/langchain/models\" arrow cta=\"Learn more\">\n    Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\n  </Card>\n\n  <Card title=\"Easy to use, highly flexible agent\" icon=\"wand-magic-sparkles\" href=\"/oss/python/langchain/agents\" arrow cta=\"Learn more\">\n    LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\n  </Card>\n\n  <Card title=\"Built on top of LangGraph\" icon=\"circle-nodes\" href=\"/oss/python/langgraph/overview\" arrow cta=\"Learn more\">\n    LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.\n  </Card>\n\n  <Card title=\"Debug with LangSmith\" icon=\"eye\" href=\"/langsmith/home\" arrow cta=\"Learn more\">\n    Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n  </Card>\n</Columns>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_44149205",
    "id": "c522e106-a1c8-42f2-a219-8114900c4881",
    "content": "Result 1:\nTitle: LangGraph CLI\nLink: https://docs.langchain.comlangsmith/cli\nContent: LangGraph CLI\nLangGraph CLI is a command-line tool for building and running the Agent Server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.\n\nLangGraph CLI\nLangGraph CLI\nLangGraph CLI is a command-line tool for building and running the Agent Server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.\n\nQuick commands\nCommand What it does langgraph dev Starts a lightweight local dev server (no Docker required), ideal for rapid testing. langgraph build Builds a Docker image of your LangGraph API server for deployment. langgraph dockerfile Emits a Dockerfile derived from your config for custom builds. langgraph up Starts the LangGraph API server locally in Docker. Requires Docker running; LangSmith API key for local dev; license for production. For JS, use npx @langchain/langgraph-cli <command> (or langgraphjs if installed globally).\n\n---\n\nResult 2:\nTitle: What's new in LangGraph v1\nLink: https://docs.langchain.comoss/javascript/releases/langgraph-v1\nContent: **LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.\n\nIt's designed to work hand-in-hand with [LangChain v1](/oss/javascript/releases/langchain-v1) (whose `createAgent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.\n\n<CardGroup cols={1}>\n  <Card title=\"Stable core APIs\" icon=\"diagram-project\">\n    Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\n  </Card>\n\n  <Card title=\"Reliability, by default\" icon=\"database\">\n    Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\n  </Card>\n\n  <Card title=\"Seamless with LangChain v1\" icon=\"link\">\n    LangChain's `createAgent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph @langchain/core\n  ```\n</CodeGroup>\n\nFor a complete list of changes, see the [migration guide](/oss/javascript/migrate/langgraph-v1).\n\n## Deprecation of `createReactAgent`\n\nThe LangGraph `createReactAgent` prebuilt has been deprecated in favor of LangChain's `createAgent`. It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n\n* For information on the new `createAgent` API, see the [LangChain v1 release notes](/oss/javascript/releases/langchain-v1#createagent).\n* For information on migrating from `createReactAgent` to `createAgent`, see the [LangChain v1 migration guide](/oss/javascript/migrate/langchain-v1#createagent).\n\n## Typed interrupts\n\n[`StateGraph`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html) now accepts a map of interrupt types in the constructor to more closely constrain the types of interrupts that can be used within a graph.\n\n```typescript expandable theme={null}\nimport { StateGraph, MemorySaver, interrupt } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst stateSchema = z.object({\n  foo: z.string(),\n})\n\nconst graphConfig = {\n  interrupts: {\n    // Define a simple interrupt that accepts a reason and returns messages\n    simple: interrupt<{ reason: string }, { messages: string[] }>, // [!code highlight]\n    // Define a complex interrupt with the same signature\n    complex: interrupt<{ reason: string }, { messages: string[] }>, // [!code highlight]\n  }\n}\n\nconst checkpointer = new MemorySaver();\n\nconst graph = new StateGraph(stateSchema, graphConfig)\n  .addNode(\"node\", async (state, runtime) => {\n    // Trigger the simple interrupt with a reason\n    const response = runtime.interrupt.simple({ reason: \"test\" });\n    // Return the interrupt response as the new state\n    return { foo: response };\n  })\n  // Compile the graph with the checkpointer\n  .compile({ checkpointer });\n\n// Invoke the graph with initial state\nconst result = await graph.invoke({ foo: \"test\" });\n\n// Access the interrupt data\nif (graph.isInterrupted(result)) {\n  console.log(result.__interrupt__.messages);\n}\n```\n\nFor more information on interrupts, see the [Interrupts](/oss/javascript/langgraph/interrupts) documentation.\n\n## Frontend SDK enhancements\n\nLangGraph v1 comes with a few enhancements when interacting with a LangGraph application from the frontend.\n\n### Event stream encoding\n\nThe low-level `toLangGraphEventStream` helper has been removed. Streaming responses are now handled natively by the SDK, and you can select the wire format via passing in the `encoding` format to `graph.stream`. This makes switching between SSE and normal JSON responses straightforward without changing UI logic.\n\nSee the [migration guide](/oss/javascript/migrate/langgraph-v1#stream-encoding) for more information.\n\n### Custom transports in `useStream`\n\nThe React `useStream` hook now supports pluggable transports so you can have more control over the network layer without changing UI code.\n\n```typescript  theme={null}\nconst stream = useStream({\n  transport: new FetchStreamTransport({\n    apiUrl: \"http://localhost:2024\",\n  }),\n});\n```\n\nLearn how to integrate and customize the hook: [Integrate LangGraph into your React application](/oss/javascript/langgraph/ui).\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraphjs/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraphjs/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangGraph 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Overview\" icon=\"book\" href=\"/oss/javascript/langgraph/overview\" arrow>\n    What LangGraph is and when to use it\n  </Card>\n\n  <Card title=\"Graph API\" icon=\"diagram-project\" href=\"/oss/javascript/langgraph/graph-api\" arrow>\n    Build graphs with state, nodes, and edges\n  </Card>\n\n  <Card title=\"LangChain Agents\" icon=\"robot\" href=\"/oss/javascript/langchain/agents\" arrow>\n    High-level agents built on LangGraph\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/javascript/migrate/langgraph-v1\" arrow>\n    How to migrate to LangGraph v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langgraphjs\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/javascript/versioning) \u2013 Understanding version numbers\n* [Release policy](/oss/javascript/release-policy) \u2013 Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/releases/langgraph-v1.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/python/langgraph/overview\nContent: Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools).\n\nWe will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\ndef mock_llm(state: MessagesState):\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n\ngraph = StateGraph(MessagesState)\ngraph.add_node(mock_llm)\ngraph.add_edge(START, \"mock_llm\")\ngraph.add_edge(\"mock_llm\", END)\ngraph = graph.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/python/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Use the graph API\nLink: https://docs.langchain.comoss/python/langgraph/use-graph-api\nContent: Setup\nInstall langgraph: pip install -U langgraph\n\nSetup\nInstall langgraph: Set up LangSmith for better debugging Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started in the docs.\n\nSetup\nInstall langgraph: uv add langgraph\n\n---\n\nResult 5:\nTitle: What's new in LangGraph v1\nLink: https://docs.langchain.comoss/python/releases/langgraph-v1\nContent: **LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.\n\nIt's designed to work hand-in-hand with [LangChain v1](/oss/python/releases/langchain-v1) (whose `create_agent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.\n\n<CardGroup cols={1}>\n  <Card title=\"Stable core APIs\" icon=\"diagram-project\">\n    Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\n  </Card>\n\n  <Card title=\"Reliability, by default\" icon=\"database\">\n    Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\n  </Card>\n\n  <Card title=\"Seamless with LangChain v1\" icon=\"link\">\n    LangChain's `create_agent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.\n  </Card>\n</CardGroup>\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\n## Deprecation of `create_react_agent`\n\nThe LangGraph [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt has been deprecated in favor of LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent). It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n\n* For information on the new [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) API, see the [LangChain v1 release notes](/oss/python/releases/langchain-v1#create-agent).\n* For information on migrating from [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), see the [LangChain v1 migration guide](/oss/python/migrate/langchain-v1#create-agent).\n\n## Reporting issues\n\nPlease report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraph/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraph/issues?q=state%3Aopen%20label%3Av1).\n\n## Additional resources\n\n<CardGroup cols={3}>\n  <Card title=\"LangGraph 1.0\" icon=\"rocket\" href=\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\">\n    Read the announcement\n  </Card>\n\n  <Card title=\"Overview\" icon=\"book\" href=\"/oss/python/langgraph/overview\" arrow>\n    What LangGraph is and when to use it\n  </Card>\n\n  <Card title=\"Graph API\" icon=\"diagram-project\" href=\"/oss/python/langgraph/graph-api\" arrow>\n    Build graphs with state, nodes, and edges\n  </Card>\n\n  <Card title=\"LangChain Agents\" icon=\"robot\" href=\"/oss/python/langchain/agents\" arrow>\n    High-level agents built on LangGraph\n  </Card>\n\n  <Card title=\"Migration guide\" icon=\"arrow-right-arrow-left\" href=\"/oss/python/migrate/langgraph-v1\" arrow>\n    How to migrate to LangGraph v1\n  </Card>\n\n  <Card title=\"GitHub\" icon=\"github\" href=\"https://github.com/langchain-ai/langgraph\">\n    Report issues or contribute\n  </Card>\n</CardGroup>\n\n## See also\n\n* [Versioning](/oss/python/versioning) \u2013 Understanding version numbers\n* [Release policy](/oss/python/release-policy) \u2013 Detailed release policies\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langgraph-v1.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_05343122",
    "id": "94294bb3-86d2-423b-8d5e-5fcc1de9572d",
    "content": "{\n  \"collections\": \"all\",\n  \"total\": 73,\n  \"articles\": [\n    {\n      \"id\": \"7a5d1732-c25b-4a26-9686-52ddfd17926e\",\n      \"title\": \"Merging LangSmith Runs from Remote Graph\",\n      \"url\": \"https://support.langchain.com/articles/9499391881-merging-langsmith-runs-from-remote-graph\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"796cd8bc-a7d9-419d-8141-83998b267e90\",\n      \"title\": \"Resolving common issues when upgrading LangSmith Helm chart from 0.11.x to 0.12.x?\",\n      \"url\": \"https://support.langchain.com/articles/5255204348-resolving-common-issues-when-upgrading-langsmith-helm-chart-from-0-11-x-to-0-12-x\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"adeafe65-c1b6-4620-b3aa-044873fd8c9c\",\n      \"title\": \"Why are my traces being automatically upgraded to extended retention?\",\n      \"url\": \"https://support.langchain.com/articles/3206725817-why-are-my-traces-being-automatically-upgraded-to-extended-retention\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"73778fa5-7613-4254-b687-7b0288ac9903\",\n      \"title\": \"Why am I getting 429 Rate Limit Errors during load testing on self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8364157755-why-am-i-getting-429-rate-limit-errors-during-load-testing-on-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2836b779-a2f5-4058-a994-4fe2502166ca\",\n      \"title\": \"Missing Runs in LangSmith Experiment View with Concurrent Evaluations\",\n      \"url\": \"https://support.langchain.com/articles/8223468578-missing-runs-in-langsmith-experiment-view-with-concurrent-evaluations\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"d55d04fa-c883-4055-b266-343e5157ad31\",\n      \"title\": \"Preserving Trace Hierarchy with Manual RunTree Creation\",\n      \"url\": \"https://support.langchain.com/articles/9515178454-preserving-trace-hierarchy-with-manual-runtree-creation\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"27a421b4-e877-4a0f-8f38-7b719b189785\",\n      \"title\": \"PagerDuty Alert Severity Not Respecting Configuration\",\n      \"url\": \"https://support.langchain.com/articles/8222693126-pagerduty-alert-severity-not-respecting-configuration\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"adf84afd-7d07-4e11-8645-a3c9a1686f08\",\n      \"title\": \"Session Timeout Configuration for LangSmith Self-Hosted\",\n      \"url\": \"https://support.langchain.com/articles/9515017706-session-timeout-configuration-for-langsmith-self-hosted\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"2fcf3091-0038-4226-b56f-4559d6cb1dbc\",\n      \"title\": \"Cleaning ClickHouse System Log Tables\",\n      \"url\": \"https://support.langchain.com/articles/9482219057-cleaning-clickhouse-system-log-tables\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"065735b3-432b-41ce-8469-1494cb753727\",\n      \"title\": \"Why are my agent health checks failing with GCP Gateway API on GKE?\",\n      \"url\": \"https://support.langchain.com/articles/3603461301-why-are-my-agent-health-checks-failing-with-gcp-gateway-api-on-gke\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f9775b3-e85d-4368-ad1b-21f9a8f7552e\",\n      \"title\": \"Evaluator Creation with LiteLLM Proxy - Missing API Key Error\",\n      \"url\": \"https://support.langchain.com/articles/2378929317-evaluator-creation-with-litellm-proxy-missing-api-key-error\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3a81a902-1bca-463d-ab2d-abefd38f8721\",\n      \"title\": \"Why did my evaluator token usage and costs increase after upgrading LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/7643067789-why-did-my-evaluator-token-usage-and-costs-increase-after-upgrading-langsmith\",\n      \"collection\": \"LangSmith Evaluation\"\n    },\n    {\n      \"id\": \"61f0452e-5198-4524-9c94-a9cf476ea5bc\",\n      \"title\": \"How to Create a Locked-Down Deployment with Restricted Access\",\n      \"url\": \"https://support.langchain.com/articles/5382256129-how-to-create-a-locked-down-deployment-with-restricted-access\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"c0615bd9-c4a2-4a54-af03-0ee07470ee43\",\n      \"title\": \"How do I configure LangSmith pricing match patterns for AWS Bedrock inference profiles?\",\n      \"url\": \"https://support.langchain.com/articles/4241511543-how-do-i-configure-langsmith-pricing-match-patterns-for-aws-bedrock-inference-profiles\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9ef77ded-7211-4d09-944a-289972726262\",\n      \"title\": \"Why am I getting \\\"Failed to send compressed multipart ingest\\\" errors in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9901468881-why-am-i-getting-failed-to-send-compressed-multipart-ingest-errors-in-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"e72b134e-bd5a-471d-93ea-8def86caf74a\",\n      \"title\": \"Why do I see inconsistent output formats for the same agent across different traces?\",\n      \"url\": \"https://support.langchain.com/articles/2041161407-why-do-i-see-inconsistent-output-formats-for-the-same-agent-across-different-traces\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"3ea46ebf-6af5-4ce2-b678-1c7d4b82caba\",\n      \"title\": \"Can I reopen a run after calling patch() to write additional data?\",\n      \"url\": \"https://support.langchain.com/articles/9964614152-can-i-reopen-a-run-after-calling-patch-to-write-additional-data\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"32994e7d-a182-41df-9f3d-271adf84fde2\",\n      \"title\": \"How do I filter runs by multiple run IDs using tree filters?\",\n      \"url\": \"https://support.langchain.com/articles/2574288399-how-do-i-filter-runs-by-multiple-run-ids-using-tree-filters\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f5a80963-8fe3-415f-b0e2-04b6f5093f12\",\n      \"title\": \"How do I filter runs by name using the LangSmith SDK?\",\n      \"url\": \"https://support.langchain.com/articles/5601207552-how-do-i-filter-runs-by-name-using-the-langsmith-sdk\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"f51b0bb0-b838-40db-baa9-f9099ae34c2f\",\n      \"title\": \"How do I use LangSmith tracing with Cloudflare Workers?\",\n      \"url\": \"https://support.langchain.com/articles/1885761495-how-do-i-use-langsmith-tracing-with-cloudflare-workers\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"365a0d3f-d784-407d-b741-fe51fa8356d4\",\n      \"title\": \"LangSmith Deployments not working with Valkey\",\n      \"url\": \"https://support.langchain.com/articles/2265174426-why-are-my-langgraph-requests-hanging-when-using-aws-elasticache-with-valkey\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"818454c3-a656-4811-a7ca-56cb13df2355\",\n      \"title\": \"How do I enable KMS encryption for S3 storage in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9381186430-how-do-i-enable-kms-encryption-for-s3-storage-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a3c79a7d-390f-43e6-8234-8b55a7f191dd\",\n      \"title\": \"Streaming Failures When Using Shared Custom Redis Instances\",\n      \"url\": \"https://support.langchain.com/articles/3842219806-streaming-failures-when-using-shared-custom-redis-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"55a2c6fe-adf8-4b55-bcb9-969b465241b7\",\n      \"title\": \"Why are my agent threads getting deleted automatically?\",\n      \"url\": \"https://support.langchain.com/articles/7993792111-why-are-my-threads-being-marked-as-temporary-and-getting-deleted-automatically\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"b4ba8c34-6d1d-4653-8123-c2eeee2d34e5\",\n      \"title\": \"How do I set up ABAC to restrict user access to specific projects?\",\n      \"url\": \"https://support.langchain.com/articles/6539560119-how-do-i-set-up-abac-to-restrict-user-access-to-specific-projects\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b42f572e-0490-4bb8-9c81-9f065f0c83ed\",\n      \"title\": \"Productionizing and Scaling Self-Hosted LangSmith - Best Practices\",\n      \"url\": \"https://support.langchain.com/articles/8478795211-productionizing-and-scaling-self-hosted-langsmith-best-practices\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"d270716f-edc0-49d9-b5d1-497b5d0e4e6d\",\n      \"title\": \"Private Network Access Blocking LangSmith Studio Connection\",\n      \"url\": \"https://support.langchain.com/articles/1897181200-private-network-access-blocking-langsmith-studio-connection\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"e2c86d05-b6fd-4bb3-8637-3f6d35c106dd\",\n      \"title\": \"How do I trace Vercel AI SDK calls with LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9620089182-how-do-i-trace-vercel-ai-sdk-calls-with-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"b20fd0bf-1ae9-445c-86ba-5a223112756f\",\n      \"title\": \"How to setup private container registry authentication for LangGraph agent deployments\",\n      \"url\": \"https://support.langchain.com/articles/8606755009-how-to-setup-private-container-registry-authentication-for-langgraph-cloud-agent-deployments\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"3f6f6fa0-610c-4a15-a4b9-987e932f6a3a\",\n      \"title\": \"How to setup path-based routing for self-hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/2352997668-how-to-setup-path-based-routing-for-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"cc4be529-34c6-458b-8bb3-621fab0e2ab4\",\n      \"title\": \"Best practices for scaling Self-Hosted LangSmith\",\n      \"url\": \"https://support.langchain.com/articles/5142502915-best-practices-for-scaling-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"3cec875c-3c8d-4951-b83b-0ac5b8ba7c7c\",\n      \"title\": \"LangSmith Deployments agent is running, but the UI shows it as \\u201cUnsuccessful\\u201d\",\n      \"url\": \"https://support.langchain.com/articles/8394994213-langsmith-deployments-agent-is-running-but-the-ui-shows-it-as-unsuccessful\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"57055dee-474d-4635-93a3-79439570f6ac\",\n      \"title\": \"How do I prevent traces from mixing between projects during concurrent requests?\",\n      \"url\": \"https://support.langchain.com/articles/4784014353-how-do-i-prevent-traces-from-mixing-between-projects-during-concurrent-requests\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9619ff10-34d0-49ea-a7ee-cdbbddafc1e4\",\n      \"title\": \"How do I resolve \\\"too many connections\\\" PostgreSQL errors when scaling platform instances?\",\n      \"url\": \"https://support.langchain.com/articles/9651943867-how-do-i-resolve-too-many-connections-postgresql-errors-when-scaling-platform-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"b740be98-677d-486f-ba77-12258712f70a\",\n      \"title\": \"How do I use the LangSmith Control Plane API to manage deployments?\",\n      \"url\": \"https://support.langchain.com/articles/8184800662-how-do-i-use-the-langgraph-platform-control-plane-api-to-manage-deployments\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"52d389ff-3918-46a8-9ccf-55fe972077bf\",\n      \"title\": \"Why is my LangSmith agent queue Deployment timing out after 600 seconds?\",\n      \"url\": \"https://support.langchain.com/articles/7898522976-why-is-my-queue-deployment-timing-out-after-600-seconds\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"1caf7aa7-5382-43f5-869c-27212fe9f287\",\n      \"title\": \"How do I resolve ClickHouse migration errors after upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/6463985661-how-do-i-resolve-clickhouse-migration-errors-after-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0ff2089a-501c-45b8-b557-4947639c5072\",\n      \"title\": \"How do I disable public sharing for traces in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5345880639-how-do-i-disable-public-sharing-for-traces-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"0ef20428-fe1b-4892-a0ce-c9c2f36677d6\",\n      \"title\": \"What pricing and discounts are available for LangGraph Cloud deployments on the Startup plan?\",\n      \"url\": \"https://support.langchain.com/articles/2780309193-what-pricing-and-discounts-are-available-for-langgraph-cloud-deployments-on-the-startup-plan\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"bef0ce3c-94c2-4999-b563-ff016fa3884f\",\n      \"title\": \"How do I get LangChain to complete security questionnaires for vendor assessments?\",\n      \"url\": \"https://support.langchain.com/articles/8555158040-how-do-i-get-langchain-to-complete-security-questionnaires-for-vendor-assessments\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"fe2b3c9e-8fa1-49dd-a52d-df92d88050c4\",\n      \"title\": \"How do I update billing information on an existing invoice?\",\n      \"url\": \"https://support.langchain.com/articles/5832977338-how-do-i-update-billing-information-on-an-existing-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"6ea47321-cd15-49a7-a972-be33139a5fe3\",\n      \"title\": \"Can I use an external database cluster to host multiple self-hosted LangSmith instances?\",\n      \"url\": \"https://support.langchain.com/articles/5989664339-can-i-use-an-external-database-cluster-to-host-multiple-self-hosted-langsmith-instances\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"4a127c79-3504-4afa-befa-ec974b85c5f8\",\n      \"title\": \"Why am I being charged for node executions and standby minutes on my development deployment?\",\n      \"url\": \"https://support.langchain.com/articles/2626278629-why-am-i-being-charged-for-node-executions-on-my-development-deployment\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"a442595a-454c-4208-8854-d51f632a3d0f\",\n      \"title\": \"Managing ClickHouse Storage\",\n      \"url\": \"https://support.langchain.com/articles/2929899751-managing-clickhouse-storage\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"c6c77d41-4376-4452-b7a0-fd42c1428242\",\n      \"title\": \"How do I deploy an MCP server on LangSmith Deployments with my agent?\",\n      \"url\": \"https://support.langchain.com/articles/8743137454-how-do-i-deploy-an-mcp-server-on-langsmith-deployments-with-my-agent\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"e5b95386-5971-4e48-be5b-9d9c7dcfe8e3\",\n      \"title\": \"How do I archive a prompt instead of deleting it?\",\n      \"url\": \"https://support.langchain.com/articles/6404244010-how-do-i-archive-a-prompt-instead-of-deleting-it\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"7444d2d0-3f90-42d8-aa4c-b36fa7665e05\",\n      \"title\": \"How do I resolve Redis disk space issues in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5370519454-how-do-i-resolve-redis-disk-space-issues-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"1076b041-020f-4738-acd0-d8ecc07f19ed\",\n      \"title\": \"How do I add existing organization members to new workspaces programmatically via API?\",\n      \"url\": \"https://support.langchain.com/articles/5797508505-how-do-i-add-existing-organization-members-to-new-workspaces-programmatically-via-api\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"a91a5cf4-a8a3-4ad5-b9ef-9d0d534d0ba4\",\n      \"title\": \"ClickHouse: Drop Old Data\",\n      \"url\": \"https://support.langchain.com/articles/2379935803-clickhouse-drop-old-data\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"5886208c-1f87-4672-87c4-053f2773c161\",\n      \"title\": \"How do I filter out traces that contain a specific node name in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8185473222-how-do-i-filter-out-traces-that-contain-a-specific-node-name-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b170d29c-925a-4aab-b7a0-354f47b7dc77\",\n      \"title\": \"Can I enforce Google SSO-only authentication for my organization?\",\n      \"url\": \"https://support.langchain.com/articles/9820106567-can-i-enforce-google-sso-only-authentication-for-my-organization\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"b5128a24-178a-4063-9337-c273af5ac617\",\n      \"title\": \"Why is my trace exploration slow in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9719508217-why-is-my-trace-exploration-slow-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"50634017-ff08-43bb-b7ae-7518aab83623\",\n      \"title\": \"How do I get access to LangSmith Agent Builder?\",\n      \"url\": \"https://support.langchain.com/articles/1588775122-how-do-i-get-access-to-langsmith-agent-builder\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"4ad14b9c-abe8-47b0-a416-81dbe2a3a127\",\n      \"title\": \"Why are some experiment runs missing when processing large datasets in self-hosted LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/3506757366-why-are-some-experiment-runs-missing-when-processing-large-datasets-in-self-hosted-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"40305680-01d3-4506-8257-16db939e39bf\",\n      \"title\": \"How do I get help with billing issues and payment problems?\",\n      \"url\": \"https://support.langchain.com/articles/3779255598-how-do-i-get-help-with-billing-issues-and-payment-problems\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"9273a5a9-0a53-4d5a-917a-c19ebeec64fe\",\n      \"title\": \"How do I get a copy of my LangSmith invoice?\",\n      \"url\": \"https://support.langchain.com/articles/6317483716-how-do-i-get-a-copy-of-my-langsmith-invoice\",\n      \"collection\": \"General\"\n    },\n    {\n      \"id\": \"ebea1a08-4488-4cac-9c65-75442f3de254\",\n      \"title\": \"Adding FastAPI dependency for custom middleware in LangGraph Platform\",\n      \"url\": \"https://support.langchain.com/articles/7593416758-adding-fastapi-dependency-for-custom-middleware-in-langgraph-platform\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"d37491ce-7f3c-4575-91d5-09c62129248e\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/8574277609-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"48541052-b69e-4e78-ae8b-c2ef4fe26af1\",\n      \"title\": \"Resolving LangGraph and LangSmith version compatibility issues in production\",\n      \"url\": \"https://support.langchain.com/articles/5102702588-resolving-langgraph-and-langsmith-version-compatibility-issues-in-production\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"0b6ce386-e341-47a1-8be9-ae350de16843\",\n      \"title\": \"How do I resolve ChatGoogleGenerativeAI blocking errors in LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/3873263156-how-do-i-resolve-chatgooglegenerativeai-blocking-errors-in-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"68fbf7da-954d-454e-8ad4-73e1f1f63b74\",\n      \"title\": \"LangSmith Studio vs Agent Chat UI: When to Use What\",\n      \"url\": \"https://support.langchain.com/articles/1353449293-langsmith-studio-vs-agent-chat-ui\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f7dc0a0c-fa3f-4319-b820-ae470d89877f\",\n      \"title\": \"Why is token-by-token streaming not working after upgrading LangGraph?\",\n      \"url\": \"https://support.langchain.com/articles/7150806184-why-is-token-by-token-streaming-not-working-after-upgrading-langgraph\",\n      \"collection\": \"LangSmith Deployment\"\n    },\n    {\n      \"id\": \"ada8fd1b-4800-434e-8d6f-b5186b170941\",\n      \"title\": \"LangSmith Studio URL Redirects to Error Page\",\n      \"url\": \"https://support.langchain.com/articles/4270790436-langsmith-studio-url-redirects-to-error-page\",\n      \"collection\": \"LangSmith Studio\"\n    },\n    {\n      \"id\": \"f41e0fb9-7ed0-4d9b-85b0-fe88fd805f02\",\n      \"title\": \"Why am I getting service interruption errors when using text search filters?\",\n      \"url\": \"https://support.langchain.com/articles/6919527577-why-am-i-getting-service-interruption-errors-when-using-text-search-filters\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"45269e32-5322-4a17-8866-da640169d21d\",\n      \"title\": \"How do I export annotated traces in bulk from LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9958349143-how-do-i-export-annotated-traces-in-bulk-from-langsmith\",\n      \"collection\": \"SDKs and APIs\"\n    },\n    {\n      \"id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\",\n      \"title\": \"Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\",\n      \"url\": \"https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b1e1ee29-1124-4a92-8f9f-eaa4a679df05\",\n      \"title\": \"Why am I getting a parsing token error when upgrading LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/7019615393-why-am-i-getting-a-token-parsing-error-when-upgrading-langsmith\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"da8e3156-8405-4c12-a347-96d6590da06c\",\n      \"title\": \"LangSmith upgrade fails with SSL certificate verification error when connecting to Azure Cache for Redis\",\n      \"url\": \"https://support.langchain.com/articles/7558740603-langsmith-helm-upgrade-fails-with-ssl-certificate-verification-error-when-connecting-to-azure-cache-for-redis\",\n      \"collection\": \"Self Hosted\"\n    },\n    {\n      \"id\": \"af6b64a9-ed4a-4993-b54d-d4913eeb1d26\",\n      \"title\": \"How do I change my email address in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9223968956-how-do-i-change-my-email-address-in-langsmith\",\n      \"collection\": \"General\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_95843493",
    "id": "999d1834-9d6a-484e-9bd2-839fbc6511d5",
    "content": "Result 1:\nTitle: Evaluate a chatbot\nLink: https://docs.langchain.comlangsmith/evaluate-chatbot-tutorial\nContent: Define metrics\nAfter creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those exact answers, but rather something that is similar. This makes our evaluation a little trickier. In addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response. Let's go ahead and define these two metrics. For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output). This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here: For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.\n\nDefine metrics\nAfter creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those exact answers, but rather something that is similar. This makes our evaluation a little trickier. In addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response. Let's go ahead and define these two metrics. For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output). This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here: For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result. def concision(outputs: dict, reference_outputs: dict) -> bool:\n    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\n\nDefine metrics\nAfter creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those exact answers, but rather something that is similar. This makes our evaluation a little trickier. In addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response. Let's go ahead and define these two metrics. For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output). This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here: import openai\nfrom langsmith import wrappers\n\nopenai_client = wrappers.wrap_openai(openai.OpenAI())\n\neval_instructions = \"You are an expert professor specialized in grading students' answers to questions.\"\n\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n    user_content = f\"\"\"You are grading the following question:\n{inputs['question']}\nHere is the real answer:\n{reference_outputs['answer']}\nYou are grading the following predicted answer:\n{outputs['response']}\nRespond with CORRECT or INCORRECT:\nGrade:\"\"\"\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": eval_instructions},\n            {\"role\": \"user\", \"content\": user_content},\n        ],\n    ).choices[0].message.content\n    return response == \"CORRECT\"\n\n---\n\nResult 2:\nTitle: How to fetch performance metrics for an experiment\nLink: https://docs.langchain.comlangsmith/fetch-perf-metrics-experiment\nContent: <Check>\n  Tracing projects and experiments use the same underlying data structure in our backend, which is called a \"session.\"\n\n  You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.\n\n  We are working on unifying the terminology across our documentation and APIs.\n</Check>\n\nWhen you run an experiment using `evaluate` with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the `read_project`/`readProject` methods.\n\nThe payload for experiment details includes the following values:\n\n```json  theme={null}\n{\n  \"start_time\": \"2024-06-06T01:02:51.299960\",\n  \"end_time\": \"2024-06-06T01:03:04.557530+00:00\",\n  \"extra\": {\n    \"metadata\": {\n      \"git\": {\n        \"tags\": null,\n        \"dirty\": true,\n        \"branch\": \"ankush/agent-eval\",\n        \"commit\": \"...\",\n        \"repo_name\": \"...\",\n        \"remote_url\": \"...\",\n        \"author_name\": \"Ankush Gola\",\n        \"commit_time\": \"...\",\n        \"author_email\": \"...\"\n      },\n      \"revision_id\": null,\n      \"dataset_splits\": [\"base\"],\n      \"dataset_version\": \"2024-06-05T04:57:01.535578+00:00\",\n      \"num_repetitions\": 3\n    }\n  },\n  \"name\": \"SQL Database Agent-ae9ad229\",\n  \"description\": null,\n  \"default_dataset_id\": null,\n  \"reference_dataset_id\": \"...\",\n  \"id\": \"...\",\n  \"run_count\": 9,\n  \"latency_p50\": 7.896,\n  \"latency_p99\": 13.09332,\n  \"first_token_p50\": null,\n  \"first_token_p99\": null,\n  \"total_tokens\": 35573,\n  \"prompt_tokens\": 32711,\n  \"completion_tokens\": 2862,\n  \"total_cost\": 0.206485,\n  \"prompt_cost\": 0.163555,\n  \"completion_cost\": 0.04293,\n  \"tenant_id\": \"...\",\n  \"last_run_start_time\": \"2024-06-06T01:02:51.366397\",\n  \"last_run_start_time_live\": null,\n  \"feedback_stats\": {\n    \"cot contextual accuracy\": {\n      \"n\": 9,\n      \"avg\": 0.6666666666666666,\n      \"values\": {\n        \"CORRECT\": 6,\n        \"INCORRECT\": 3\n      }\n    }\n  },\n  \"session_feedback_stats\": {},\n  \"run_facets\": [],\n  \"error_rate\": 0,\n  \"streaming_rate\": 0,\n  \"test_run_number\": 11\n}\n```\n\nFrom here, you can extract performance metrics such as:\n\n* `latency_p50`: The 50th percentile latency in seconds.\n* `latency_p99`: The 99th percentile latency in seconds.\n* `total_tokens`: The total number of tokens used.\n* `prompt_tokens`: The number of prompt tokens used.\n* `completion_tokens`: The number of completion tokens used.\n* `total_cost`: The total cost of the experiment.\n* `prompt_cost`: The cost of the prompt tokens.\n* `completion_cost`: The cost of the completion tokens.\n* `feedback_stats`: The feedback statistics for the experiment.\n* `error_rate`: The error rate for the experiment.\n* `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).\n* `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).\n\nHere is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.\n\nFirst, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the [how-to guide](/langsmith/evaluate-llm-application) on evaluation for more details.\n\n```python  theme={null}\nfrom langsmith import Client\n\nclient = Client()\n\n# Create a dataset\ndataset_name = \"HelloDataset\"\ndataset = client.create_dataset(dataset_name=dataset_name)\n\nexamples = [\n    {\n        \"inputs\": {\"input\": \"Harrison\"},\n        \"outputs\": {\"expected\": \"Hello Harrison\"},\n    },\n    {\n        \"inputs\": {\"input\": \"Ankush\"},\n        \"outputs\": {\"expected\": \"Hello Ankush\"},\n    },\n]\n\nclient.create_examples(dataset_id=dataset.id, examples=examples)\n```\n\nNext, we will create an experiment, retrieve the experiment name from the result of `evaluate`, then fetch the performance metrics for the experiment.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith.schemas import Example, Run\n  dataset_name = \"HelloDataset\"\n\n  def foo_label(root_run: Run, example: Example) -> dict:\n      return {\"score\": 1, \"key\": \"foo\"}\n\n  from langsmith import evaluate\n\n  results = evaluate(\n      lambda inputs: \"Hello \" + inputs[\"input\"],\n      data=dataset_name,\n      evaluators=[foo_label],\n      experiment_prefix=\"Hello\",\n  )\n\n  resp = client.read_project(project_name=results.experiment_name, include_stats=True)\n  print(resp.json(indent=2))\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n  import { evaluate } from \"langsmith/evaluation\";\n  import type { EvaluationResult } from \"langsmith/evaluation\";\n  import type { Run, Example } from \"langsmith/schemas\";\n\n  // Row-level evaluator\n  function fooLabel(rootRun: Run, example: Example): EvaluationResult {\n      return {score: 1, key: \"foo\"};\n  }\n\n  const client = new Client();\n\n  const results = await evaluate(\n      (inputs) => {\n          return { output: \"Hello \" + inputs.input };\n      },\n      {\n          data: \"HelloDataset\",\n          experimentPrefix: \"Hello\",\n          evaluators: [fooLabel],\n      }\n  );\n\n  const resp = await client.readProject({\n      projectName: results.experimentName,\n      includeStats: true\n  })\n  console.log(JSON.stringify(resp, null, 2))\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/fetch-perf-metrics-experiment.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Egress for subscription metrics and operational metadata\nLink: https://docs.langchain.comlangsmith/self-host-egress\nContent: <Info>\n  This section only applies to customers who are not running in offline mode and assumes you are using a self-hosted LangSmith instance serving version 0.9.0 or later. Previous versions of LangSmith did not have this feature.\n</Info>\n\nSelf-Hosted LangSmith instances store all information locally and will never send sensitive information outside of your network. We currently only track platform usage for billing purposes according to the entitlements in your order. In order to better remotely support our customers, we do require egress to `https://beacon.langchain.com`.\n\nIn the future, we will be introducing support diagnostics to help us ensure that LangSmith is running at an optimal level within your environment.\n\n<Warning>\n  **This will require egress to `https://beacon.langchain.com` from your network. Refer to the [allowlisting IP section](/langsmith/cloud#allowlisting-ip-addresses) for static IP addresses, if needed.**\n</Warning>\n\nGenerally, data that we send to Beacon can be categorized as follows:\n\n* Subscription Metrics\n\n  * Subscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:\n\n    * Number of traces\n    * Seats allocated per contract\n    * Seats in currently use\n\n* Operational Metadata\n  * This metadata will contain and collect the above subscription metrics to assist with remote support, allowing the LangChain team to diagnose and troubleshoot performance issues more effectively and proactively.\n\n## LangSmith Telemetry\n\nAs of version ***0.11***, LangSmith deployments will by default send telemetry data back to our backend. All telemetry data is associated with an organization and deployment, but never identified with individual users. We ***do not collect PII*** (personally identifiable information) in any form.\n\n### What we use it for\n\n* To provide more proactive support and faster troubleshooting of self-hosted instances.\n* Assisting with performance tuning.\n* Understanding real-world usage to prioritize improvements.\n\n### What we collect\n\n* **Request metadata**: anonymized request counts, sizes, and durations.\n* **Database metrics**: query durations, error rates, and performance counters.\n* **Operational LangSmith traces**: traces with timing and error information for high-latency or failed requests. These are **not** customer traces, these are operational traces about the functioning of the LangSmith instance.\n\n<Info>\n  We do not collect actual payload contents, database records, or any data that can identify your end users or customers.\n</Info>\n\n### How to disable\n\nSet the following values in your `langsmith_config.yaml` file:\n\n```yaml  theme={null}\nconfig:\n  telemetry:\n    metrics: false\n    traces: false\n```\n\n## Example payloads\n\nIn an effort to maximize transparency, we provide sample payloads here:\n\n### License Verification\n\n**Endpoint:**\n\n`POST beacon.langchain.com/v1/beacon/verify`\n\n**Request:**\n\n```json  theme={null}\n{\n  \"license\": \"<YOUR_LICENSE_KEY>\"\n}\n```\n\n**Response:**\n\n```json  theme={null}\n{\n  \"token\": \"Valid JWT\" //Short-lived JWT token to avoid repeated license checks\n}\n```\n\n### Usage Reporting\n\n**Endpoint:**\n\n`POST beacon.langchain.com/v1/beacon/ingest-traces`\n\n**Request:**\n\n```json  theme={null}\n{\n  \"license\": \"<YOUR_LICENSE_KEY>\",\n  \"trace_transactions\": [\n    {\n      \"id\": \"af28dfea-5358-463d-a2dc-37df1da72498\",\n      \"tenant_id\": \"3a1c2b6f-4430-4b92-8a5b-79b8b567bbc1\",\n      \"session_id\": \"b26ae531-cdb3-42a5-8bcf-05355199fe27\",\n      \"trace_count\": 5,\n      \"start_insertion_time\": \"2025-01-06T10:00:00Z\",\n      \"end_insertion_time\": \"2025-01-06T11:00:00Z\",\n      \"start_interval_time\": \"2025-01-06T09:00:00Z\",\n      \"end_interval_time\": \"2025-01-06T10:00:00Z\",\n      \"status\": \"completed\",\n      \"num_failed_send_attempts\": 0,\n      \"transaction_type\": \"type1\",\n      \"organization_id\": \"c5b5f53a-4716-4326-8967-d4f7f7799735\"\n    }\n  ]\n}\n```\n\n**Response:**\n\n```json  theme={null}\n{\n  \"inserted_count\": 1 //Number of transactions successfully ingested\n}\n```\n\n### Telemetry: Operational LangSmith Metrics\n\n**Endpoint:**\n\n`POST beacon.langchain.com/v1/beacon/v1/metrics`\n\n**Request:**\n\n```json  theme={null}\n{\n  \"resourceMetrics\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"resource.name\",\n            \"value\": { \"stringValue\": \"langsmith-metrics\" }\n          },\n          {\n            \"key\": \"env\",\n            \"value\": { \"stringValue\": \"ls_self_hosted\" }\n          }\n        ]\n      },\n      \"scopeMetrics\": [\n        {\n          \"scope\": {\n            \"name\": \"langsmith.metrics\",\n            \"version\": \"0.1.0\"\n          },\n          \"metrics\": [\n            {\n              \"name\": \"langsmith_http_requests_latency\",\n              \"unit\": \"seconds\",\n              \"description\": \"Request latency of LangSmith services\",\n              \"gauge\": {\n                \"dataPoints\": [\n                  {\n                    \"asDouble\": 12.34,\n                    \"startTimeUnixNano\": 1678886400000000000,\n                    \"timeUnixNano\": 1678886400000000000,\n                    \"attributes\": [\n                      {\n                        \"key\": \"endpoint\",\n                        \"value\": { \"stringValue\": \"/sessions\" }\n                      },\n                      { \"key\": \"method\", \"value\": { \"stringValue\": \"GET\" } },\n                      {\n                        \"key\": \"service_name\",\n                        \"value\": { \"stringValue\": \"langsmith_backend\" }\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            {\n              \"name\": \"langsmith_http_requests_failed\",\n              \"unit\": \"1\",\n              \"description\": \"Counter of failed requests for LangSmith services\",\n              \"sum\": {\n                \"dataPoints\": [\n                  {\n                    \"asInt\": 456,\n                    \"startTimeUnixNano\": 1678886400000000000,\n                    \"timeUnixNano\": 1678886400000000000,\n                    \"attributes\": [\n                      {\n                        \"key\": \"endpoint\",\n                        \"value\": { \"stringValue\": \"/info\" }\n                      },\n                      { \"key\": \"method\", \"value\": { \"stringValue\": \"POST\" } },\n                      {\n                        \"key\": \"service_name\",\n                        \"value\": { \"stringValue\": \"langsmith_platform_backend\" }\n                      }\n                    ],\n                    \"aggregationTemporality\": 2,\n                    \"isMonotonic\": true\n                  }\n                ]\n              }\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Telemetry: Operational LangSmith Traces\n\n**Endpoint:**\n\n`POST beacon.langchain.com/v1/beacon/v1/traces`\n\n**Request:**\n\n```json  theme={null}\n{\n  \"resourceSpans\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"env\",\n            \"value\": {\n              \"stringValue\": \"ls_self_hosted\"\n            }\n          },\n          {\n            \"key\": \"service.name\",\n            \"value\": {\n              \"stringValue\": \"langsmith_backend\"\n            }\n          }\n        ]\n      },\n      \"scopeSpans\": [\n        {\n          \"scope\": {},\n          \"spans\": [\n            {\n              \"traceId\": \"71699b6fe85982c7c8995ea3d9c95df2\",\n              \"spanId\": \"3c191d03fa8be0\",\n              \"parentSpanId\": \"\",\n              \"name\": \"receive_request\",\n              \"startTimeUnixNano\": \"1581452772000000321\",\n              \"endTimeUnixNano\": \"1581452773000000789\",\n              \"droppedAttributesCount\": 1,\n              \"events\": [\n                {\n                  \"timeUnixNano\": \"1581452773000000123\",\n                  \"name\": \"parse_request\",\n                  \"attributes\": [\n                    {\n                      \"key\": \"request_size\",\n                      \"value\": {\n                        \"stringValue\": \"100\"\n                      }\n                    }\n                  ],\n                  \"droppedAttributesCount\": 2\n                },\n                {\n                  \"timeUnixNano\": \"1581452773000000123\",\n                  \"name\": \"event\",\n                  \"droppedAttributesCount\": 2\n                }\n              ],\n              \"droppedEventsCount\": 1,\n              \"status\": {\n                \"message\": \"status-cancelled\",\n                \"code\": 2\n              }\n            },\n            {\n              \"traceId\": \"71699b6fe85982c7c8995ea3d9c95df2\",\n              \"spanId\": \"0932ksdka12345\",\n              \"parentSpanId\": \"3c191d03fa8be0\",\n              \"name\": \"process_request\",\n              \"startTimeUnixNano\": \"1581452772000000321\",\n              \"endTimeUnixNano\": \"1581452773000000789\",\n              \"links\": [],\n              \"droppedLinksCount\": 3,\n              \"status\": {}\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n## Our Commitment\n\nLangChain will not store any sensitive information in the Subscription Metrics or Operational Metadata. Any data collected will not be shared with a third party. If you have any concerns about the data being sent, please reach out to your account team.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-egress.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: AzureOpenAIEmbeddings\nLink: https://docs.langchain.comoss/javascript/integrations/text_embedding/azure_openai\nContent: [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.\n\nLangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).\n\nYou can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.\n\nThis will help you get started with AzureOpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `AzureOpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html).\n\n<Info>\n  **Previously, LangChain.js supported integration with Azure OpenAI using the dedicated [Azure OpenAI SDK](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai). This SDK is now deprecated in favor of the new Azure integration in the OpenAI SDK, which allows to access the latest OpenAI models and features the same day they are released, and allows seamless transition between the OpenAI API and Azure OpenAI.**\n\n  If you are using Azure OpenAI with the deprecated SDK, see the [migration guide](#migration-from-azure-openai-sdk) to update to the new API.\n</Info>\n\n## Overview\n\n### Integration details\n\n| Class                                                                                                     | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/azure_openai/) |                                             Downloads                                             |                                             Version                                            |\n| :-------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |\n| [AzureOpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   \u274c   |                                             \u2705                                             | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\\&label=%20&) |\n\n## Setup\n\nTo access Azure OpenAI embedding models you'll need to create an Azure account, get an API key, and install the `@langchain/openai` integration package.\n\n### Credentials\n\nYou'll need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).\n\nOnce you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the \"Keys and Endpoint\" section of your instance.\n\nIf you're using Node.js, you can define the following environment variables to use the service:\n\n```bash  theme={null}\nAZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>\nAZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=<YOUR_EMBEDDINGS_DEPLOYMENT_NAME>\nAZURE_OPENAI_API_KEY=<YOUR_KEY>\nAZURE_OPENAI_API_VERSION=\"2024-02-01\"\n```\n\nIf you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:\n\n```bash  theme={null}\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n### Installation\n\nThe LangChain AzureOpenAIEmbeddings integration lives in the `@langchain/openai` package:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/openai @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/openai @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/openai @langchain/core\n  ```\n</CodeGroup>\n\n<Info>\n  **You can find the list of supported API versions in the [Azure OpenAI documentation](https://learn.microsoft.com/azure/ai-services/openai/reference).**\n</Info>\n\n<Tip>\n  **If `AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME` is not defined, it will fall back to the value of `AZURE_OPENAI_API_DEPLOYMENT_NAME` for the deployment name. The same applies to the `azureOpenAIApiEmbeddingsDeploymentName` parameter in the `AzureOpenAIEmbeddings` constructor, which will fall back to the value of `azureOpenAIApiDeploymentName` if not defined.**\n</Tip>\n\n## Instantiation\n\nNow we can instantiate our model object and embed text:\n\n```typescript  theme={null}\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new AzureOpenAIEmbeddings({\n  azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiInstanceName: \"<your_instance_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n  azureOpenAIApiEmbeddingsDeploymentName: \"<your_embeddings_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  maxRetries: 1,\n});\n```\n\n## Indexing and Retrieval\n\nEmbedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [**Learn** tab](/oss/javascript/learn/).\n\nBelow, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/oss/javascript/integrations/vectorstores/memory).\n\n```typescript  theme={null}\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n```text  theme={null}\nLangChain is the framework for building context-aware reasoning applications\n```\n\n## Direct Usage\n\nUnder the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.\n\nYou can directly call these methods to get embeddings for your own use cases.\n\n### Embed single texts\n\nYou can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:\n\n```typescript  theme={null}\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n```text  theme={null}\n[\n   -0.024253517, -0.0054218727,   0.048715446,   0.020580322,    0.03180832,\n   0.0028770117,  -0.012367731,   0.037383243,  -0.054915592,   0.032225136,\n     0.00825818,  -0.023888804,   -0.01184671,   0.012257014,   0.016294925,\n    0.009254632,  0.0051353113,  -0.008889917,   0.016855022,    0.04207243,\n  0.00082589936,  -0.011664353,    0.00818654,   0.029020859,  -0.012335167,\n   -0.019603407,  0.0013945447,    0.05538451,  -0.011625277,  -0.008153976,\n    0.038607642,   -0.03811267, -0.0074440846,   0.047647353,   -0.00927417,\n    0.024201415, -0.0069230637,  -0.008538228,   0.003910912,   0.052805457,\n   -0.023159374,  0.0014352495,  -0.038659744,   0.017141584,   0.005587948,\n    0.007971618,  -0.016920151,    0.06658646, -0.0016916894,   0.045667473,\n   -0.042202685,   -0.03983204,   -0.04160351,  -0.011729481,  -0.055905532,\n    0.012543576,  0.0038848612,   0.007919516,   0.010915386,  0.0033117384,\n   -0.007548289,  -0.030427614,  -0.041890074,   0.036002535,  -0.023771575,\n   -0.008792226,  -0.049444873,   0.016490309, -0.0060568666,   0.040196754,\n    0.014106638,  -0.014575557, -0.0017356506,  -0.011234511,  -0.012517525,\n    0.008362384,    0.01253055,   0.036158845,   0.008297256, -0.0010908874,\n   -0.014888169,  -0.020489143,   0.018965157,  -0.057937514, -0.0037122732,\n    0.004402626,   -0.00840146,   0.042984217,   -0.04936672,   -0.03714878,\n    0.004969236,    0.03707063,   0.015396165,   -0.02055427,    0.01988997,\n    0.030219207,  -0.021257648,    0.01340326,   0.003692735,   0.012595678\n]\n```\n\n### Embed multiple texts\n\nYou can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:\n\n```typescript  theme={null}\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n```text  theme={null}\n[\n   -0.024253517, -0.0054218727,   0.048715446,   0.020580322,    0.03180832,\n   0.0028770117,  -0.012367731,   0.037383243,  -0.054915592,   0.032225136,\n     0.00825818,  -0.023888804,   -0.01184671,   0.012257014,   0.016294925,\n    0.009254632,  0.0051353113,  -0.008889917,   0.016855022,    0.04207243,\n  0.00082589936,  -0.011664353,    0.00818654,   0.029020859,  -0.012335167,\n   -0.019603407,  0.0013945447,    0.05538451,  -0.011625277,  -0.008153976,\n    0.038607642,   -0.03811267, -0.0074440846,   0.047647353,   -0.00927417,\n    0.024201415, -0.0069230637,  -0.008538228,   0.003910912,   0.052805457,\n   -0.023159374,  0.0014352495,  -0.038659744,   0.017141584,   0.005587948,\n    0.007971618,  -0.016920151,    0.06658646, -0.0016916894,   0.045667473,\n   -0.042202685,   -0.03983204,   -0.04160351,  -0.011729481,  -0.055905532,\n    0.012543576,  0.0038848612,   0.007919516,   0.010915386,  0.0033117384,\n   -0.007548289,  -0.030427614,  -0.041890074,   0.036002535,  -0.023771575,\n   -0.008792226,  -0.049444873,   0.016490309, -0.0060568666,   0.040196754,\n    0.014106638,  -0.014575557, -0.0017356506,  -0.011234511,  -0.012517525,\n    0.008362384,    0.01253055,   0.036158845,   0.008297256, -0.0010908874,\n   -0.014888169,  -0.020489143,   0.018965157,  -0.057937514, -0.0037122732,\n    0.004402626,   -0.00840146,   0.042984217,   -0.04936672,   -0.03714878,\n    0.004969236,    0.03707063,   0.015396165,   -0.02055427,    0.01988997,\n    0.030219207,  -0.021257648,    0.01340326,   0.003692735,   0.012595678\n]\n[\n   -0.033366997,   0.010419146,  0.0118083665,  -0.040441725, 0.0020355924,\n   -0.015808804,  -0.023629595, -0.0066180876,  -0.040004376,  0.020053642,\n  -0.0010797002,   -0.03900105,  -0.009956073,  0.0027896944,  0.003305828,\n   -0.034010153,   0.009833873,  0.0061164247,   0.022536227,  0.029147884,\n    0.017789727,    0.03182342,   0.010869357,   0.031849146, -0.028093107,\n    0.008283865, -0.0145610785,    0.01645196,  -0.029430874,  -0.02508313,\n    0.046178687,   -0.01722375,  -0.010046115,   0.013101112, 0.0044538635,\n     0.02197025,    0.03985002,   0.007955855,  0.0008819293,  0.012657333,\n    0.014368132,  -0.014007963,   -0.03722594,   0.031617608, -0.011570398,\n    0.039052505,  0.0020018267,   0.023706773, -0.0046950476,  0.056083307,\n    -0.08412496,  -0.043425974,  -0.015512952,   0.015950298,  -0.03624834,\n  -0.0053317733,  -0.037251666,  0.0046339477,    0.04193385,  0.023475237,\n   -0.021378545,   0.013699248,  -0.026009277,   0.050757967,   -0.0494202,\n   0.0007874656,   -0.07208506,   0.015885983,  -0.003259199,  0.015127057,\n   0.0068946453,  -0.035373647,  -0.005875241, -0.0032238255,  -0.04185667,\n   -0.022047428,  0.0014326327, -0.0070940237, -0.0027864785, -0.016271876,\n    0.005097021,   0.034473225,   0.012361481,  -0.026498076, 0.0067274245,\n   -0.026330855,  -0.006132504,   0.008180959,  -0.049368747, -0.032337945,\n    0.011049441,    0.00186194,  -0.012097787,    0.01930758,   0.07059293,\n    0.029713862,    0.04337452, -0.0048461896,  -0.019976463,  0.011473924\n]\n```\n\n## Using Azure Managed Identity\n\nIf you're using Azure Managed Identity, you can configure the credentials like this:\n\n```typescript  theme={null}\nimport {\n  DefaultAzureCredential,\n  getBearerTokenProvider,\n} from \"@azure/identity\";\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n\nconst credentials = new DefaultAzureCredential();\nconst azureADTokenProvider = getBearerTokenProvider(\n  credentials,\n  \"https://cognitiveservices.azure.com/.default\"\n);\n\nconst modelWithManagedIdentity = new AzureOpenAIEmbeddings({\n  azureADTokenProvider,\n  azureOpenAIApiInstanceName: \"<your_instance_name>\",\n  azureOpenAIApiEmbeddingsDeploymentName: \"<your_embeddings_deployment_name>\",\n  azureOpenAIApiVersion: \"<api_version>\",\n});\n\n```\n\n## Using a different domain\n\nIf your instance is hosted under a domain other than the default `openai.azure.com`, you'll need to use the alternate `AZURE_OPENAI_BASE_PATH` environment variable.\nFor example, here's how you would connect to the domain `https://westeurope.api.microsoft.com/openai/deployments/{DEPLOYMENT_NAME}`:\n\n```typescript  theme={null}\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddingsDifferentDomain = new AzureOpenAIEmbeddings({\n  azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiEmbeddingsDeploymentName: \"<your_embedding_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  azureOpenAIBasePath:\n    \"https://westeurope.api.microsoft.com/openai/deployments\", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH\n});\n\n```\n\n## Custom headers\n\nYou can specify custom headers by passing in a `configuration` field:\n\n```typescript  theme={null}\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddingsWithCustomHeaders = new AzureOpenAIEmbeddings({\n  azureOpenAIApiKey: \"<your_key>\",\n  azureOpenAIApiInstanceName: \"<your_instance_name>\",\n  azureOpenAIApiEmbeddingsDeploymentName: \"<your_embeddings_deployment_name>\",\n  azureOpenAIApiVersion: \"<api_version>\",\n  configuration: {\n    defaultHeaders: {\n      \"x-custom-header\": `SOME_VALUE`,\n    },\n  },\n});\n```\n\nThe `configuration` field also accepts other `ClientOptions` parameters accepted by the official SDK.\n\n**Note:** The specific header `api-key` currently cannot be overridden in this manner and will pass through the value from `azureOpenAIApiKey`.\n\n## Migration from Azure OpenAI SDK\n\nIf you are using the deprecated Azure OpenAI SDK with the `@langchain/azure-openai` package, you can update your code to use the new Azure integration following these steps:\n\n1. Install the new `@langchain/openai` package and remove the previous `@langchain/azure-openai` package:\n\n   ```bash npm theme={null}\n   npm install @langchain/openai\n   npm uninstall @langchain/azure-openai\n   ```\n\n2. Update your imports to use the new `AzureOpenAIEmbeddings` classe from the `@langchain/openai` package:\n\n   ```typescript  theme={null}\n   import { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n   ```\n\n3. Update your code to use the new `AzureOpenAIEmbeddings` class and pass the required parameters:\n\n   ```typescript  theme={null}\n   const model = new AzureOpenAIEmbeddings({\n     azureOpenAIApiKey: \"<your_key>\",\n     azureOpenAIApiInstanceName: \"<your_instance_name>\",\n     azureOpenAIApiEmbeddingsDeploymentName:\n       \"<your_embeddings_deployment_name>\",\n     azureOpenAIApiVersion: \"<api_version>\",\n   });\n   ```\n\n   Notice that the constructor now requires the `azureOpenAIApiInstanceName` parameter instead of the `azureOpenAIEndpoint` parameter, and adds the `azureOpenAIApiVersion` parameter to specify the API version.\n\n   * If you were using Azure Managed Identity, you now need to use the `azureADTokenProvider` parameter to the constructor instead of `credentials`, see the [Azure Managed Identity](#using-azure-managed-identity) section for more details.\n\n   * If you were using environment variables, you now have to set the `AZURE_OPENAI_API_INSTANCE_NAME` environment variable instead of `AZURE_OPENAI_API_ENDPOINT`, and add the `AZURE_OPENAI_API_VERSION` environment variable to specify the API version.\n\n***\n\n## API reference\n\nFor detailed documentation of all AzureOpenAIEmbeddings features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/azure_openai.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Tools and toolkits\nLink: https://docs.langchain.comoss/python/integrations/tools/index\nContent: Search\nThe following table shows tools that execute online searches in some shape or form: Tool/Toolkit Free/Paid Return Data Bing Search Paid URL, Snippet, Title Brave Search Free URL, Snippet, Title DuckDuckgoSearch Free URL, Snippet, Title Exa Search 1000 free searches/month URL, Author, Title, Published Date Google Search Paid URL, Snippet, Title Google Serper Free URL, Snippet, Title, Search Rank, Site Links Jina Search 1M Response Tokens Free URL, Snippet, Title, Page Content Mojeek Search Paid URL, Snippet, Title Parallel Search Paid URL, Title, Excerpts SearchApi 100 Free Searches on Sign Up URL, Snippet, Title, Search Rank, Site Links, Authors SearxNG Search Free URL, Snippet, Title, Category SerpApi 250 Free Searches/Month Answer Tavily Search 1000 free searches/month URL, Content, Title, Images, Answer You.com Search Free for 60 days URL, Title, Page Content\n\nAll tools and toolkits\nADS4GPTs AgentQL AINetwork Toolkit Alpha Vantage Amadeus Toolkit Anchor Browser Apify Actor ArXiv AskNews AWS Lambda Azure AI Services Toolkit Azure Cognitive Services Toolkit Azure Container Apps Dynamic Sessions Shell (bash) Bearly Code Interpreter Bing Search Bodo DataFrames Brave Search BrightData Web Scraper API BrightData SERP BrightData Unlocker Cassandra Database Toolkit CDP ChatGPT Plugins ClickUp Toolkit Cogniswitch Toolkit Compass DeFi Toolkit Composio Connery Toolkit Dall-E Image Generator Dappier Databricks Unity Catalog DataForSEO Dataherald Daytona Data Analysis DuckDuckGo Search Discord E2B Data Analysis Eden AI ElevenLabs Text2Speech Exa Search File System Financial Datasets Toolkit FMP Data GitHub Toolkit GitLab Toolkit Gmail Toolkit GOAT Privy Golden Query Google Books Google Calendar Toolkit Google Cloud Text-to-Speech Google Drive Google Finance Google Imagen Google Jobs Google Lens Google Places Google Scholar Google Search Google Serper Google Trends Gradio GraphQL HuggingFace Hub Tools Human as a Tool Hyperbrowser Browser Agent Tools Hyperbrowser Web Scraping Tools IBM watsonx.ai IBM watsonx.ai (SQL) IFTTT WebHooks Infobip Ionic Shopping Tool Jenkins Jina Search Jira Toolkit JSON Toolkit Lemon Agent Linkup Search Tool Memgraph Memorize Mojeek Search MultiOn Toolkit NASA Toolkit Naver Search Nuclia Understanding NVIDIA Riva Office365 Toolkit OpenAPI Toolkit Natural Language API Toolkits OpenGradient OpenWeatherMap Oracle AI Vector Search Oxylabs Pandas Dataframe Passio NutritionAI Parallel Extract Parallel Search Permit PlayWright Browser Toolkit Polygon IO Toolkit PowerBI Toolkit Prolog PubMed Python REPL Reddit Search Requests Toolkit Riza Code Interpreter Robocorp Toolkit Salesforce SceneXplain ScrapeGraph Scrapeless Crawl Scrapeless Scraping API Scrapeless Universal Scraping SearchApi SearxNG Search Semantic Scholar API SerpApi Slack Toolkit Spark SQL Toolkit SQLDatabase Toolkit StackExchange Steam Toolkit Stripe Tableau Taiga Tavily Extract Tavily Search Tilores MCP Toolbox Twilio Upstage Valthera ValyuContext Vectara Wikidata Wikipedia Wolfram Alpha WRITER Tools Yahoo Finance News You.com Search YouTube Zapier Natural Language Actions ZenGuard AI If you'd like to contribute an integration, see Contributing integrations. Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nProductivity\nThe following table shows tools that can be used to automate tasks in productivity tools: Tool/Toolkit Pricing GitHub Toolkit Free GitLab Toolkit Free for personal project Gmail Toolkit Free, with limit of 250 quota units per user per second Infobip Tool Free trial, with variable pricing after Jira Toolkit Free, with rate limits Office365 Toolkit Free with Office365, includes rate limits Slack Toolkit Free Twilio Tool Free trial, with pay-as-you-go pricing after\n"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3a2f-0105-7110-b137-76511b0d67bd",
    "content": "**LangChain and LangGraph official documentation does not publish specific user counts or adoption statistics for AI agent building.**\n\nWhile the docs mention companies like Klarna, Replit, and Elastic using LangGraph, no numerical data on total users, downloads, or agent-specific usage is available in official sources.\n\n## Public Metrics Sources\n\nFor publicly available indicators, check these sites (not in official docs):\n\n- **PyPI Downloads**: Search `langchain` and `langgraph` on [pypi.org](https://pypi.org) for weekly/monthly download stats\n- **NPM Downloads**: Search `@langchain/core`, `langchain`, `@langchain/langgraph` on [npmjs.com](https://npmjs.com)  \n- **GitHub Stars**: \n  - LangChain Python: ~90k stars\n  - LangGraph: ~10k stars\n- **LangSmith Usage**: Billing docs reference trace limits (e.g., 5k-10k free traces/month per plan) as indirect adoption signals\n\nThese external sources show strong interest (millions of downloads), but no direct \"number of people building AI agents\" data exists in official records.\n\n**Relevant docs:**\n- [LangGraph Overview](https://docs.langchain.com/oss/python/langgraph/overview)\n- [LangChain Overview](https://docs.langchain.com/oss/python/langchain/overview)\n- [LangSmith Billing](https://docs.langchain.com/langsmith/billing)"
  },
  {
    "role": "user",
    "id": "0b3e7a4e-fc18-4a44-a27f-354298bcb706",
    "content": "but for building ai agent which is better langchain or langgraph what you say can you give some github link where user given feedback."
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_72196850",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"langchain vs langgraph\"}"
        }
      },
      {
        "type": "function",
        "id": "call_46883043",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"when to use langgraph\"}"
        }
      },
      {
        "type": "function",
        "id": "call_47271894",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"page_size\": 5, \"query\": \"agents langchain langgraph\"}"
        }
      },
      {
        "type": "function",
        "id": "call_62388180",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"OSS (LangChain and LangGraph),LangSmith Observability\"}"
        }
      }
    ],
    "id": "lc_run--019b3a30-7e6f-7491-bee6-66fe11eb5ee2",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_72196850",
    "id": "8e15709a-4b31-409d-ba47-f732248ec641",
    "content": "Result 1:\nTitle: Contributing to code\nLink: https://docs.langchain.comoss/javascript/contributing/code\nContent: Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.\n\n## Getting started\n\n<Note>\n  Before submitting large **new features or refactors**, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work.\n\n  This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. See the quickstart guide below.\n</Note>\n\n### Quick fix: submit a bugfix\n\nFor simple bugfixes, you can get started immediately:\n\n<Steps>\n  <Step title=\"Reproduce the issue\">\n    Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification\n  </Step>\n\n  <Step title=\"Fork the repository\">\n    Fork the [LangChain](https://github.com/langchain-ai/langchainjs) or [LangGraph](https://github.com/langchain-ai/langgraphjs) repo to your personal GitHub account\n  </Step>\n\n  <Step title=\"Clone and setup\">\n    ```bash  theme={null}\n    git clone https://github.com/your-username/name-of-forked-repo.git\n\n    # For instance, for LangChain:\n    git clone https://github.com/parrot123/langchainjs.git\n\n    # For LangGraph:\n    git clone https://github.com/parrot123/langgraphjs.git\n    ```\n\n    ```bash  theme={null}\n    # Inside your repo, install dependencies\n    pnpm install\n    # Create a build for all packages to resolve workspace dependencies\n    pnpm build\n    ```\n  </Step>\n\n  <Step title=\"Create a branch\">\n    Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later.\n\n    ```bash  theme={null}\n    git checkout -b your-username/short-bugfix-name\n    ```\n  </Step>\n\n  <Step title=\"Write failing tests\">\n    Add [unit tests](#test-writing-guidelines) that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions\n  </Step>\n\n  <Step title=\"Make your changes\">\n    Fix the bug while following our [code quality standards](#code-quality-standards). Make the **minimal change necessary** to resolve the issue\n  </Step>\n\n  <Step title=\"Run build\">\n    Run the build command to ensure the package still builds properly\n\n    ```bash  theme={null}\n    pnpm build\n    # or build a specific workspace package\n    pnpm --filter @langchain/core build\n    ```\n  </Step>\n\n  <Step title=\"Verify the fix\">\n    Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR\n\n    ```bash  theme={null}\n    pnpm lint\n    pnpm test\n\n    # For bugfixes involving integrations, also run:\n    pnpm test:int\n\n    # Or run tests in a specific workspace package\n    cd libs/langchain-core\n    pnpm test\n    pnpm lint\n\n    # Or run tests for a specific package from the root of the repo\n    pnpm --filter @langchain/core test\n    pnpm --filter @langchain/core lint\n    ```\n  </Step>\n\n  <Step title=\"Document the change\">\n    Update docstrings if behavior changes, add comments for complex logic\n  </Step>\n\n  <Step title=\"Submit a pull request\">\n    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #ISSUE_NUMBER`) so that the issue is automatically closed when your PR is merged.\n  </Step>\n</Steps>\n\n### Full development setup\n\nFor ongoing development or larger contributions:\n\n1. Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations\n2. Set up your environment following our [setup guide](#development-environment) below\n3. Understand the [repository structure](#repository-structure) and package organization\n4. Learn our [development workflow](#development-workflow) including testing and linting\n\n***\n\n## Contribution guidelines\n\nBefore you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a \"first contribution\" to your resume (or if you're just looking for a quick win) you might be better off doing a boot-camp or an online tutorial.\n\nContributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it's important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you're willing to take the time to do things well.\n\n### Backwards compatibility\n\n<Warning>\n  Breaking changes to public APIs are not allowed except for critical security fixes.\n\n  See our [versioning policy](/oss/javascript/versioning) for details on major version releases.\n</Warning>\n\nMaintain compatibility via:\n\n<AccordionGroup>\n  <Accordion title=\"Stable interfaces\">\n    **Always preserve**:\n\n    * Function signatures and parameter names\n    * Class interfaces and method names\n    * Return value structure and types\n    * Import paths for public APIs\n  </Accordion>\n\n  <Accordion title=\"Safe changes\">\n    **Acceptable modifications**:\n\n    * Adding new optional parameters/type parameters\n\n    * Adding new methods to classes\n\n    * Improving performance without changing behavior\n\n    * Adding new modules or functions\n  </Accordion>\n\n  <Accordion title=\"Before making changes\">\n    * **Would this break existing user code?**\n\n    * Check if your target is public\n\n    * Are there existing usage patterns in tests?\n  </Accordion>\n</AccordionGroup>\n\n### New features\n\nWe aim to keep the bar high for new features. We generally don't accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies.\n\nIn general, feature contribution requirements include:\n\n<Steps>\n  <Step title=\"Design discussion\">\n    Open an issue describing:\n\n    * The problem you're solving\n    * Proposed API design\n    * Expected usage patterns\n  </Step>\n\n  <Step title=\"Implementation\">\n    * Follow existing code patterns\n    * Include comprehensive tests and documentation\n    * Consider security implications\n  </Step>\n\n  <Step title=\"Integration considerations\">\n    * How does this interact with existing features?\n    * Are there performance implications?\n    * Does this introduce new dependencies?\n\n    We will reject features that are likely to lead to security vulnerabilities or reports.\n  </Step>\n</Steps>\n\n### Security guidelines\n\n<Warning>\n  Security is paramount. Never introduce vulnerabilities or unsafe patterns.\n</Warning>\n\nSecurity checklist:\n\n<AccordionGroup>\n  <Accordion title=\"Input validation\">\n    * Validate and sanitize all user inputs\n\n    * Properly escape data in templates and queries\n\n    * Never use `eval()`, as this can lead to arbitrary code execution vulnerabilities\n  </Accordion>\n\n  <Accordion title=\"Error handling\">\n    * Use specific exception types\n    * Don't expose sensitive information in error messages\n    * Implement proper resource cleanup\n  </Accordion>\n\n  <Accordion title=\"Dependencies\">\n    * Avoid adding hard dependencies\n    * Keep optional dependencies minimal\n    * Review third-party packages for security issues\n  </Accordion>\n</AccordionGroup>\n\n***\n\n## Development environment\n\n<Warning>\n  Our JS/TS projects uses [`pnpm`](https://pnpm.io/) for dependency management. Make sure you have the latest version installed, or run `corepack enable` (on Node 24+) to setup the required pnpm version.\n</Warning>\n\nOnce you've reviewed the [contribution guidelines](#contribution-guidelines), set up a development environment for the package(s) you're working on.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <AccordionGroup>\n      <Accordion title=\"Core abstractions\">\n        For changes to `langchain-core`:\n\n        ```bash  theme={null}\n        pnpm install\n        # Run tests for a package from the package directory\n        cd libs/langchain-core\n        pnpm test\n\n        # Or run tests for a package from the root of the repo\n        pnpm --filter @langchain/core test\n\n        ```\n      </Accordion>\n\n      <Accordion title=\"Main package\">\n        For changes to `langchain`:\n\n        ```bash  theme={null}\n        pnpm install\n        pnpm --filter langchain test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Provider packages\">\n        For changes to [partner integrations](/oss/javascript/integrations/providers/overview):\n\n        ```bash  theme={null}\n        pnpm install\n        pnpm --filter @langchain/{provider} test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Community packages\">\n        For changes to [community integrations](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-community):\n\n        ```bash  theme={null}\n        pnpm install\n        pnpm --filter @langchain/community test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Repository structure\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    LangChain is organized as a monorepo with multiple packages:\n\n    <AccordionGroup>\n      <Accordion title=\"Core packages\" defaultOpen>\n        * **[`langchain`](https://github.com/langchain-ai/langchainjs/tree/main/langchain#readme)** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic\n        * **[`@langchain/core`](https://github.com/langchain-ai/langchainjs/tree/main/langchain-core#readme)** (located in `libs/langchain-core/`): Base interfaces and core abstractions\n      </Accordion>\n\n      <Accordion title=\"Partner packages\">\n        Located in `libs/providers/`, these are independently versioned packages for specific integrations. For example:\n\n        * **[`@langchain/openai`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-openai#readme)**: [OpenAI](/oss/javascript/integrations/providers/openai) integrations\n        * **[`@langchain/anthropic`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-anthropic#readme)**: [Anthropic](/oss/javascript/integrations/providers/anthropic) integrations\n        * **[`@langchain/google-genai`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-google-genai#readme)**: [Google Generative AI](/oss/javascript/integrations/chat/google_generative_ai) integrations\n      </Accordion>\n\n      <Accordion title=\"Supporting packages\">\n        * **[`@langchain/textsplitters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-textsplitters#readme)**: Text splitting utilities\n        * **[`@langchain/standard-tests`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-standard-tests#readme)**: Standard test suites for integrations\n        * **[`langchain-community`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-community)**: Community maintained integrations\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Development workflow\n\n### Testing requirements\n\n<Info>\n  Directories are relative to the package you're working in.\n</Info>\n\nEvery code change must include comprehensive tests.\n\n#### Unit tests\n\n**Location**: `src/tests/FILENAME_BEING_TESTED.test.ts`\n\n**Requirements**:\n\n* No network calls allowed\n* Test all code paths including edge cases\n* Use mocks for external dependencies\n\n```bash  theme={null}\n# Run the entire test suite\npnpm test\n\n# Or run a specific test file\npnpm test src/tests/FILENAME_BEING_TESTED.test.ts\n\n# Or run a specific test function\npnpm test -t \"the test that should be run\"\n```\n\n#### Integration tests\n\n**Location**: `src/tests/FILENAME_BEING_TESTED.int.test.ts`\n\nIntegration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.\n\nNot every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.\n\n**Requirements**:\n\n* Test real integrations with external services\n* Use environment variables for API keys\n* Skip gracefully if credentials unavailable\n\n```bash  theme={null}\npnpm test:int\n```\n\n### Code quality standards\n\nContributions must adhere to the following quality requirements:\n\n<Tabs>\n  <Tab title=\"Type hints\">\n    **Required**: Complete types for all functions\n\n    ```typescript  theme={null}\n    function processDocuments(\n        docs: Document[],\n        processor: DocumentProcessor,\n        batchSize: number = 100\n    ): ProcessingResult {\n        // ...\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"Documentation\">\n    **Required**: [JSDocs](https://jsdoc.app/about-getting-started) for all exported functions and interfaces\n\n    ```typescript  theme={null}\n    /**\n     * Document processing instance.\n     */\n    interface FooDocumentProcessor {\n        /**\n         * Process documents in batches.\n         *\n         * @param docs - List of documents to process.\n         * @returns Processing results with success/failure counts.\n         */\n        process(docs: Document[]): ProcessingResult;\n    }\n\n    /**\n     * Process documents in batches.\n     *\n     * @param docs - List of documents to process.\n     * @param processor - Document processing instance.\n     * @param batchSize - Number of documents per batch.\n     * @returns Processing results with success/failure counts.\n     */\n    export function processDocuments(\n        docs: Document[],\n        processor: DocumentProcessor,\n        batchSize: number = 100\n    ): ProcessingResult {\n        // ...\n    }\n    ```\n\n    * Document all parameters and return values\n    * Include usage examples for complex functions\n    * Document raised exceptions\n    * Focus on \"why\" rather than \"what\"\n  </Tab>\n\n  <Tab title=\"Code style\">\n    **Automated**: Formatting and linting:\n\n    ```bash  theme={null}\n    pnpm lint    # Check style and types\n    pnpm format  # Apply formatting\n    ```\n\n    **Standards**:\n\n    * Descriptive variable names\n    * Break up complex functions (aim for fewer than 20 lines)\n    * Follow existing patterns in the codebase\n  </Tab>\n</Tabs>\n\n***\n\n## Testing and validation\n\n### Running tests locally\n\nBefore submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <Steps>\n      <Step title=\"Unit tests\">\n        ```bash  theme={null}\n        pnpm test\n        ```\n\n        All unit tests must pass\n      </Step>\n\n      <Step title=\"Integration tests\">\n        ```bash  theme={null}\n        pnpm test:int\n        ```\n\n        (Run if your changes affect integrations)\n      </Step>\n\n      <Step title=\"Formatting\">\n        ```bash  theme={null}\n        pnpm format\n        pnpm lint\n        ```\n\n        Code must pass all style checks\n      </Step>\n\n      <Step title=\"PR submission\">\n        Push your branch and open a pull request. Follow the provided form template. Note related issues using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.\n      </Step>\n    </Steps>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n### Test writing guidelines\n\nIn order to write effective tests, there's a few good practices to follow:\n\n* Encapsulate the test in a `describe` block that describes the component being tested\n* Use natural language to describe the test name\n* Be exhaustive with assertions\n* Only use snapshots for reasonably sized data objects\n\n<Tabs>\n  <Tab title=\"Unit tests\">\n    ```typescript  theme={null}\n    describe(\"DocumentProcessor\", () => {\n        it(\"Should handle empty document list\", () => {\n            const processor = new DocumentProcessor();\n            const result = processor.process([]);\n\n            expect(result.success).toBe(true);\n            expect(result.processedCount).toBe(0);\n            expect(result.errors).toHaveLength(0);\n        });\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"Integration tests\">\n    ```typescript  theme={null}\n    describe(\"ChatOpenAI\", () => {\n        it(\"Should test with real API\", () => {\n            const chat = new ChatOpenAI();\n            const response = chat.invoke(\"Hello\");\n        });\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"Mock usage\">\n    ```typescript  theme={null}\n    describe(\"APIService\", () => {\n        it(\"Should call with retry\", () => {\n            const mockClient = new MockClient();\n            const service = new APIService(client: mockClient);\n            const result = service.callWithRetry();\n        });\n    });\n    ```\n  </Tab>\n</Tabs>\n\n## Getting help\n\nOur goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).\n\n<Check>\n  You're now ready to contribute high-quality code to LangChain!\n</Check>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Implement a LangChain integration\nLink: https://docs.langchain.comoss/javascript/contributing/implement-langchain\nContent: Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.\n\nLangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/javascript/integrations/chat), [tools](/oss/javascript/integrations/tools), [retrievers](/oss/javascript/integrations/retrievers), and more.\n\nYour integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.\n\n<Tabs>\n  <Tab title=\"Chat Models\">\n    Chat models are subclasses of the [`BaseChatModel`](https://reference.langchain.com/javascript/classes/_langchain_core.language_models_chat_models.BaseChatModel.html) class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.\n\n    <Warning>\n      The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/javascript/langchain/models) for details on how LangChain chat models function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Tools\">\n    Tools are used in 2 main ways:\n\n    1. To define an \"input schema\" or \"args schema\" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a \"tool call\", or parameters to call the tool with.\n    2. To take a \"tool call\" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage.\n\n    The Tools class must inherit from the [`BaseTool`](https://reference.langchain.com/javascript/classes/_langchain_core.tools.StructuredTool.html) base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.\n\n    <Warning>\n      The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/javascript/langchain/tools) for details on how LangChain tools function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Retrievers\">\n    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.\n\n    <Warning>\n      The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/javascript/integrations/retrievers) for details on how LangChain retrievers function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Vector Stores\">\n    All vector stores must inherit from the [`VectorStore`](https://reference.langchain.com/javascript/classes/_langchain_core.vectorstores.VectorStore.html) base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.\n\n    See the [vector store integration guide](/oss/javascript/integrations/vectorstores) for details on implementing a vector store integration.\n\n    <Warning>\n      The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/javascript/integrations/vectorstores) for details on how LangChain vector stores function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Embeddings\">\n    Embedding models are subclasses of the [`Embeddings`](https://reference.langchain.com/javascript/classes/_langchain_core.embeddings.Embeddings.html) class.\n\n    <Warning>\n      The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/javascript/integrations/text_embedding) for details on how LangChain embedding models function.\n    </Warning>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: OpenAIEmbeddings\nLink: https://docs.langchain.comoss/javascript/integrations/text_embedding/openai\nContent: This will help you get started with OpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `OpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).\n\n## Overview\n\n### Integration details\n\n| Class                                                                                           | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/openai/) |                                             Downloads                                             |                                             Version                                            |\n| :---------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |\n| [OpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   \u274c   |                                          \u2705                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\\&label=%20&) |\n\n## Setup\n\nTo access OpenAIEmbeddings embedding models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.\n\n### Credentials\n\nHead to [platform.openai.com](https://platform.openai.com) to sign up to OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:\n\n```bash  theme={null}\nexport OPENAI_API_KEY=\"your-api-key\"\n```\n\nIf you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:\n\n```bash  theme={null}\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n### Installation\n\nThe LangChain OpenAIEmbeddings integration lives in the `@langchain/openai` package:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/openai @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/openai @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/openai @langchain/core\n  ```\n</CodeGroup>\n\n## Instantiation\n\nNow we can instantiate our model object and generate chat completions:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n  batchSize: 512, // Default value if omitted is 512. Max is 2048\n  model: \"text-embedding-3-large\",\n});\n```\n\nIf you're part of an organization, you can set `process.env.OPENAI_ORGANIZATION` to your OpenAI organization id, or pass it in as `organization` when\ninitializing the model.\n\n## Indexing and Retrieval\n\nEmbedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [**Learn** tab](/oss/javascript/learn/).\n\nBelow, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/oss/javascript/integrations/vectorstores/memory).\n\n```typescript  theme={null}\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n```output  theme={null}\nLangChain is the framework for building context-aware reasoning applications\n```\n\n## Direct Usage\n\nUnder the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.\n\nYou can directly call these methods to get embeddings for your own use cases.\n\n### Embed single texts\n\nYou can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:\n\n```typescript  theme={null}\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n```output  theme={null}\n[\n    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,\n   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,\n    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,\n   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,\n   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,\n   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,\n    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,\n   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,\n  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,\n  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,\n  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,\n    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,\n    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,\n   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,\n    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,\n    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,\n     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,\n   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,\n   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,\n     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397\n]\n```\n\n### Embed multiple texts\n\nYou can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:\n\n```typescript  theme={null}\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n```output  theme={null}\n[\n    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,\n   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,\n    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,\n   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,\n   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,\n   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,\n    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,\n   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,\n  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,\n  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,\n  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,\n    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,\n    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,\n   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,\n    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,\n    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,\n     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,\n   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,\n   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,\n     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397\n]\n[\n   -0.010181213,   0.023419594,   -0.04215527, -0.0015320902,  -0.023573855,\n  -0.0091644935,  -0.014893179,   0.019016149,  -0.023475688,  0.0010219777,\n    0.009255648,    0.03996757,   -0.04366983,   -0.01640774,  -0.020194141,\n    0.019408813,  -0.027977299,  -0.022017224,   0.013539891,  -0.007769135,\n    0.032647192,  -0.015089511,  -0.022900717,   0.023798235,   0.026084099,\n   -0.024625633,   0.035003178,  -0.017978394,  -0.049615882,   0.013364594,\n    0.031132633,   0.019142363,   0.023195215,  -0.038396914,   0.005584942,\n   -0.031946007,   0.053682756, -0.0036356465,   0.011240003,  0.0056690844,\n  -0.0062791156,   0.044146635,  -0.037387207,    0.01300699,   0.018946031,\n   0.0050415234,   0.029618073,  -0.021750772,  -0.000649473, 0.00026951815,\n   -0.014710871,  -0.029814405,    0.04204308,  -0.014710871,  0.0039616977,\n   -0.021512369,   0.054608323,   0.021484323,    0.02790718,  -0.010573876,\n   -0.023952495,  -0.035143413,  -0.048802506, -0.0075798146,   0.023279356,\n   -0.022690361,  -0.016590048,  0.0060477243,   0.014100839,   0.005476258,\n   -0.017221114, -0.0100059165,  -0.017922299,  -0.021989176,    0.01830094,\n     0.05516927,   0.001033372,  0.0017310516,   -0.00960624,  -0.037864015,\n    0.013063084,   0.006591143,  -0.010160177,  0.0011394264,    0.04953174,\n    0.004806626,   0.029421741,  -0.037751824,   0.003618117,   0.007162609,\n    0.027696826, -0.0021070621,  -0.024485396, -0.0042141243,   -0.02801937,\n   -0.019605145,   0.016281527,  -0.035143413,    0.01640774,   0.042323552\n]\n```\n\n## Specifying dimensions\n\nWith the `text-embedding-3` class of models, you can specify the size of the embeddings you want returned. For example by default `text-embedding-3-large` returns embeddings of dimension 3072:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddingsDefaultDimensions = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n});\n\nconst vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments([\"some text\"]);\nconsole.log(vectorsDefaultDimensions[0].length);\n```\n\n```output  theme={null}\n3072\n```\n\nBut by passing in `dimensions: 1024` we can reduce the size of our embeddings to 1024:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings1024 = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n  dimensions: 1024,\n});\n\nconst vectors1024 = await embeddings1024.embedDocuments([\"some text\"]);\nconsole.log(vectors1024[0].length);\n```\n\n```output  theme={null}\n1024\n```\n\n## Custom URLs\n\nYou can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst model = new OpenAIEmbeddings({\n  configuration: {\n    baseURL: \"https://your_custom_url.com\",\n  },\n});\n```\n\nYou can also pass other `ClientOptions` parameters accepted by the official SDK.\n\nIf you are hosting on Azure OpenAI, see the [dedicated page instead](/oss/javascript/integrations/text_embedding/azure_openai).\n\n***\n\n## API reference\n\nFor detailed documentation of all OpenAIEmbeddings features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/openai.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Contributing to code\nLink: https://docs.langchain.comoss/python/contributing/code\nContent: Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.\n\n## Getting started\n\n<Note>\n  Before submitting large **new features or refactors**, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work.\n\n  This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. See the quickstart guide below.\n</Note>\n\n### Quick fix: submit a bugfix\n\nFor simple bugfixes, you can get started immediately:\n\n<Steps>\n  <Step title=\"Reproduce the issue\">\n    Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification\n  </Step>\n\n  <Step title=\"Fork the repository\">\n    Fork the [LangChain](https://github.com/langchain-ai/langchain) or [LangGraph](https://github.com/langchain-ai/langgraph) repo to your <Tooltip tip=\"If you fork to an organization account, maintainers will be unable to make edits.\">personal GitHub account</Tooltip>\n  </Step>\n\n  <Step title=\"Clone and setup\">\n    ```bash  theme={null}\n    git clone https://github.com/your-username/name-of-forked-repo.git\n\n    # For instance, for LangChain:\n    git clone https://github.com/parrot123/langchain.git\n\n    # For LangGraph:\n    git clone https://github.com/parrot123/langgraph.git\n    ```\n\n    ```bash  theme={null}\n    # Inside your repo, install dependencies\n    uv sync --all-groups\n    ```\n\n    You will need to install [`uv`](https://docs.astral.sh/uv/) if you haven't previously\n  </Step>\n\n  <Step title=\"Create a branch\">\n    Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later.\n\n    ```bash  theme={null}\n    git checkout -b your-username/short-bugfix-name\n    ```\n  </Step>\n\n  <Step title=\"Write failing tests\">\n    Add [unit tests](#test-writing-guidelines) that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions\n  </Step>\n\n  <Step title=\"Make your changes\">\n    Fix the bug while following our [code quality standards](#code-quality-standards). Make the **minimal change necessary** to resolve the issue\n  </Step>\n\n  <Step title=\"Verify the fix\">\n    Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR\n\n    ```bash  theme={null}\n    make lint\n    make test\n\n    # For bugfixes involving integrations, also run:\n    make integration_tests\n    # (You may need to set up API testing credentials)\n    ```\n  </Step>\n\n  <Step title=\"Document the change\">\n    Update docstrings if behavior changes, add comments for complex logic\n  </Step>\n\n  <Step title=\"Submit a pull request\">\n    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #ISSUE_NUMBER`) so that the issue is automatically closed when your PR is merged.\n  </Step>\n</Steps>\n\n### Full development setup\n\nFor ongoing development or larger contributions:\n\n1. Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations\n2. Set up your environment following our [setup guide](#development-environment) below\n3. Understand the [repository structure](#repository-structure) and package organization\n4. Learn our [development workflow](#development-workflow) including testing and linting\n\n***\n\n## Contribution guidelines\n\nBefore you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a \"first contribution\" to your resume (or if you're just looking for a quick win) you might be better off doing a boot-camp or an online tutorial.\n\nContributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it's important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you're willing to take the time to do things well.\n\n### Backwards compatibility\n\n<Warning>\n  Breaking changes to public APIs are not allowed except for critical security fixes.\n\n  See our [versioning policy](/oss/python/versioning) for details on major version releases.\n</Warning>\n\nMaintain compatibility via:\n\n<AccordionGroup>\n  <Accordion title=\"Stable interfaces\">\n    **Always preserve**:\n\n    * Function signatures and parameter names\n    * Class interfaces and method names\n    * Return value structure and types\n    * Import paths for public APIs\n  </Accordion>\n\n  <Accordion title=\"Safe changes\">\n    **Acceptable modifications**:\n\n    * Adding new optional parameters\n\n    * Adding new methods to classes\n\n    * Improving performance without changing behavior\n\n    * Adding new modules or functions\n  </Accordion>\n\n  <Accordion title=\"Before making changes\">\n    * **Would this break existing user code?**\n\n    * Check if your target is public\n\n    * If needed, is it exported in `__init__.py`?\n\n    * Are there existing usage patterns in tests?\n  </Accordion>\n</AccordionGroup>\n\n### New features\n\nWe aim to keep the bar high for new features. We generally don't accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies.\n\nIn general, feature contribution requirements include:\n\n<Steps>\n  <Step title=\"Design discussion\">\n    Open an issue describing:\n\n    * The problem you're solving\n    * Proposed API design\n    * Expected usage patterns\n  </Step>\n\n  <Step title=\"Implementation\">\n    * Follow existing code patterns\n    * Include comprehensive tests and documentation\n    * Consider security implications\n  </Step>\n\n  <Step title=\"Integration considerations\">\n    * How does this interact with existing features?\n    * Are there performance implications?\n    * Does this introduce new dependencies?\n\n    We will reject features that are likely to lead to security vulnerabilities or reports.\n  </Step>\n</Steps>\n\n### Security guidelines\n\n<Warning>\n  Security is paramount. Never introduce vulnerabilities or unsafe patterns.\n</Warning>\n\nSecurity checklist:\n\n<AccordionGroup>\n  <Accordion title=\"Input validation\">\n    * Validate and sanitize all user inputs\n    * Properly escape data in templates and queries\n    * Never use `eval()`, `exec()`, or `pickle` on user data, as this can lead to arbitrary code execution vulnerabilities\n  </Accordion>\n\n  <Accordion title=\"Error handling\">\n    * Use specific exception types\n    * Don't expose sensitive information in error messages\n    * Implement proper resource cleanup\n  </Accordion>\n\n  <Accordion title=\"Dependencies\">\n    * Avoid adding hard dependencies\n    * Keep optional dependencies minimal\n    * Review third-party packages for security issues\n  </Accordion>\n</AccordionGroup>\n\n***\n\n## Development environment\n\n<Warning>\n  Our Python projects use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management. Make sure you have the latest version installed.\n</Warning>\n\nOnce you've reviewed the [contribution guidelines](#contribution-guidelines), set up a development environment for the package(s) you're working on.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <AccordionGroup>\n      <Accordion title=\"Core abstractions\">\n        For changes to `langchain-core`:\n\n        ```bash  theme={null}\n        cd libs/core\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Main package\">\n        For changes to `langchain`:\n\n        ```bash  theme={null}\n        cd libs/langchain\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Partner packages\">\n        For changes to [partner integrations](/oss/python/integrations/providers/overview):\n\n        ```bash  theme={null}\n        cd libs/partners/langchain-{partner}\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n\n      <Accordion title=\"Community packages\">\n        For changes to community integrations (located in a [separate repo](https://github.com/langchain-ai/langchain-community)):\n\n        ```bash  theme={null}\n        cd libs/community/langchain_community/path/to/integration\n        uv sync --all-groups\n        make test  # Ensure tests pass before starting development\n        ```\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Repository structure\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    LangChain is organized as a monorepo with multiple packages:\n\n    <AccordionGroup>\n      <Accordion title=\"Core packages\" defaultOpen>\n        * **[`langchain`](https://github.com/langchain-ai/langchain/tree/master/libs/langchain#readme)** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic\n        * **[`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core#readme)** (located in `libs/core/`): Base interfaces and core abstractions\n      </Accordion>\n\n      <Accordion title=\"Partner packages\">\n        Located in `libs/partners/`, these are independently versioned packages for specific integrations. For example:\n\n        * **[`langchain-openai`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai#readme)**: [OpenAI](/oss/python/integrations/providers/openai) integrations\n        * **[`langchain-anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic#readme)**: [Anthropic](/oss/python/integrations/providers/anthropic) integrations\n        * **[`langchain-google-genai`](https://github.com/langchain-ai/langchain-google/)**: [Google Generative AI](/oss/python/integrations/chat/google_generative_ai) integrations\n\n        Many partner packages are in external repositories. Please check the [list of integrations](/oss/python/integrations/providers/overview) for details.\n      </Accordion>\n\n      <Accordion title=\"Supporting packages\">\n        * **[`langchain-text-splitters`](https://github.com/langchain-ai/langchain/tree/master/libs/text-splitters#readme)**: Text splitting utilities\n        * **[`langchain-standard-tests`](https://github.com/langchain-ai/langchain/tree/master/libs/standard-tests#readme)**: Standard test suites for integrations\n        * **[`langchain-cli`](https://github.com/langchain-ai/langchain/tree/master/libs/cli#readme)**: Command line interface\n        * **[`langchain-community`](https://github.com/langchain-ai/langchain-community)**: Community maintained integrations (located in a separate repo)\n      </Accordion>\n    </AccordionGroup>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n***\n\n## Development workflow\n\n### Testing requirements\n\n<Info>\n  Directories are relative to the package you're working in.\n</Info>\n\nEvery code change must include comprehensive tests.\n\n#### Unit tests\n\n**Location**: `tests/unit_tests/`\n\n**Requirements**:\n\n* No network calls allowed\n* Test all code paths including edge cases\n* Use mocks for external dependencies\n\n```bash  theme={null}\nmake test\n\n# Or directly:\nuv run --group test pytest tests/unit_tests\n```\n\n#### Integration tests\n\n**Location**: `tests/integration_tests/`\n\nIntegration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.\n\nNot every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.\n\n**Requirements**:\n\n* Test real integrations with external services\n* Use environment variables for API keys\n* Skip gracefully if credentials unavailable\n\n```bash  theme={null}\nmake integration_tests\n```\n\n### Code quality standards\n\nContributions must adhere to the following quality requirements:\n\n<Tabs>\n  <Tab title=\"Type hints\">\n    **Required**: Complete type annotations for all functions\n\n    ```python  theme={null}\n    def process_documents(\n        docs: list[Document],\n        processor: DocumentProcessor,\n        *,\n        batch_size: int = 100\n    ) -> ProcessingResult:\n        \"\"\"Process documents in batches.\n\n        Args:\n            docs: List of documents to process.\n            processor: Document processing instance.\n            batch_size: Number of documents per batch.\n\n        Returns:\n            Processing results with success/failure counts.\n        \"\"\"\n    ```\n  </Tab>\n\n  <Tab title=\"Documentation\">\n    **Required**: [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) for all public functions\n\n    * Document all parameters and return values\n    * Include usage examples for complex functions\n    * Document raised exceptions\n    * Focus on \"why\" rather than \"what\"\n  </Tab>\n\n  <Tab title=\"Code style\">\n    **Automated**: Formatting and linting via [`ruff`](https://docs.astral.sh/ruff/)\n\n    ```bash  theme={null}\n    make format  # Apply formatting\n    make lint    # Check style and types\n    ```\n\n    **Standards**:\n\n    * Descriptive variable names\n    * Break up complex functions (aim for fewer than 20 lines)\n    * Follow existing patterns in the codebase\n  </Tab>\n</Tabs>\n\n***\n\n## Testing and validation\n\n### Running tests locally\n\nBefore submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.\n\n<Tabs>\n  <Tab title=\"LangChain\" icon=\"link\">\n    <Steps>\n      <Step title=\"Unit tests\">\n        ```bash  theme={null}\n        make test\n        ```\n\n        All unit tests must pass\n      </Step>\n\n      <Step title=\"Integration tests\">\n        ```bash  theme={null}\n        make integration_tests\n        ```\n\n        (Run if your changes affect integrations)\n      </Step>\n\n      <Step title=\"Formatting\">\n        ```bash  theme={null}\n        make format\n        make lint\n        ```\n\n        Code must pass all style checks\n      </Step>\n\n      <Step title=\"Type checking\">\n        ```bash  theme={null}\n        make type_check\n        ```\n\n        All type hints must be valid\n      </Step>\n\n      <Step title=\"PR submission\">\n        Push your branch and open a pull request. Follow the provided form template. Note related issues using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.\n      </Step>\n    </Steps>\n  </Tab>\n\n  <Tab title=\"LangGraph\" icon=\"circle-nodes\">\n    WIP - coming soon! In the meantime, follow instructions for LangChain.\n  </Tab>\n</Tabs>\n\n### Test writing guidelines\n\nIn order to write effective tests, there's a few good practices to follow:\n\n* Use natural language to describe the test in docstrings\n* Use descriptive variable names\n* Be exhaustive with assertions\n\n<Tabs>\n  <Tab title=\"Unit tests\">\n    ```python  theme={null}\n    def test_document_processor_handles_empty_input():\n        \"\"\"Test processor gracefully handles empty document list.\"\"\"\n        processor = DocumentProcessor()\n\n        result = processor.process([])\n\n        assert result.success\n        assert result.processed_count == 0\n        assert len(result.errors) == 0\n    ```\n  </Tab>\n\n  <Tab title=\"Integration tests\">\n    ```python  theme={null}\n    @pytest.mark.requires(\"openai\")\n    def test_openai_chat_integration():\n        \"\"\"Test OpenAI chat integration with real API.\"\"\"\n\n        chat = ChatOpenAI()\n        response = chat.invoke(\"Hello\")\n\n        assert isinstance(response.content, str)\n        assert len(response.content) > 0\n    ```\n  </Tab>\n\n  <Tab title=\"Mock usage\">\n    ```python  theme={null}\n    def test_retry_mechanism(mocker):\n        \"\"\"Test retry mechanism handles transient failures.\"\"\"\n        mock_client = mocker.Mock()\n        mock_client.call.side_effect = [\n            ConnectionError(\"Temporary failure\"),\n            {\"result\": \"success\"}\n        ]\n\n        service = APIService(client=mock_client)\n        result = service.call_with_retry()\n\n        assert result[\"result\"] == \"success\"\n        assert mock_client.call.call_count == 2\n    ```\n  </Tab>\n</Tabs>\n\n## Getting help\n\nOur goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).\n\n<Check>\n  You're now ready to contribute high-quality code to LangChain!\n</Check>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Implement a LangChain integration\nLink: https://docs.langchain.comoss/python/contributing/implement-langchain\nContent: Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.\n\nLangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/python/integrations/chat), [tools](/oss/python/integrations/tools), [retrievers](/oss/python/integrations/retrievers), and more.\n\nYour integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.\n\n<Tabs>\n  <Tab title=\"Chat Models\">\n    Chat models are subclasses of the [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel) class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.\n\n    <Warning>\n      The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/python/langchain/models) for details on how LangChain chat models function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Tools\">\n    Tools are used in 2 main ways:\n\n    1. To define an \"input schema\" or \"args schema\" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a \"tool call\", or parameters to call the tool with.\n    2. To take a \"tool call\" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage.\n\n    The Tools class must inherit from the [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.\n\n    <Warning>\n      The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/python/langchain/tools) for details on how LangChain tools function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Retrievers\">\n    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.\n\n    <Warning>\n      The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/python/integrations/retrievers) for details on how LangChain retrievers function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Vector Stores\">\n    All vector stores must inherit from the [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.\n\n    See the [vector store integration guide](/oss/python/integrations/vectorstores) for details on implementing a vector store integration.\n\n    <Warning>\n      The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/python/integrations/vectorstores) for details on how LangChain vector stores function.\n    </Warning>\n  </Tab>\n\n  <Tab title=\"Embeddings\">\n    Embedding models are subclasses of the [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) class.\n\n    <Warning>\n      The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/python/integrations/text_embedding) for details on how LangChain embedding models function.\n    </Warning>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_46883043",
    "id": "6d6fcf83-a6c7-45e0-bebd-535b7f556129",
    "content": "Result 1:\nTitle: How to evaluate a graph\nLink: https://docs.langchain.comlangsmith/evaluate-graph\nContent: <Info>\n  [langgraph](https://langchain-ai.github.io/langgraph/)\n</Info>\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Evaluating `langgraph` graphs can be challenging because a single invocation can involve many LLM calls, and which LLM calls are made may depend on the outputs of preceding calls. In this guide we will focus on the mechanics of how to pass graphs and graph nodes to `evaluate()` / `aevaluate()`. For evaluation techniques and best practices when building agents head to the [langgraph docs](https://langchain-ai.github.io/langgraph/tutorials/#evaluation).\n\n## End-to-end evaluations\n\nThe most common type of evaluation is an end-to-end one, where we want to evaluate the final graph output for each example input.\n\n### Define a graph\n\nLets construct a simple ReACT agent to start:\n\n```python  theme={null}\nfrom typing import Annotated, Literal, TypedDict\nfrom langchain.chat_models import init_chat_model\nfrom langchain.tools import tool\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    # Messages have the type \"list\". The 'add_messages' function\n    # in the annotation defines how this state key should be updated\n    # (in this case, it appends messages to the list, rather than overwriting them)\n    messages: Annotated[list, add_messages]\n\n# Define the tools for the agent to use\n@tool\ndef search(query: str) -> str:\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder, but don't tell the LLM that...\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n\ntools = [search]\ntool_node = ToolNode(tools)\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\").bind_tools(tools)\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: State) -> Literal[\"tools\", END]:\n    messages = state['messages']\n    last_message = messages[-1]\n\n    # If the LLM makes a tool call, then we route to the \"tools\" node\n    if last_message.tool_calls:\n        return \"tools\"\n\n    # Otherwise, we stop (reply to the user)\n    return END\n\n# Define the function that calls the model\ndef call_model(state: State):\n    messages = state['messages']\n    response = model.invoke(messages)\n\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n# Define a new graph\nworkflow = StateGraph(State)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as 'agent'\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use 'agent'.\n    # This means these are the edges taken after the 'agent' node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n)\n\n# We now add a normal edge from 'tools' to 'agent'.\n# This means that after 'tools' is called, 'agent' node is called next.\nworkflow.add_edge(\"tools\", 'agent')\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable.\n# Note that we're (optionally) passing the memory when compiling the graph\napp = workflow.compile()\n```\n\n### Create a dataset\n\nLet's create a simple dataset of questions and expected responses:\n\n```python  theme={null}\nfrom langsmith import Client\n\nquestions = [\n    \"what's the weather in sf\",\n    \"whats the weather in san fran\",\n    \"whats the weather in tangier\"\n]\n\nanswers = [\n    \"It's 60 degrees and foggy.\",\n    \"It's 60 degrees and foggy.\",\n    \"It's 90 degrees and sunny.\",\n]\n\nls_client = Client()\ndataset = ls_client.create_dataset(\n    \"weather agent\",\n    inputs=[{\"question\": q} for q in questions],\n    outputs=[{\"answers\": a} for a in answers],\n)\n```\n\n### Create an evaluator\n\nAnd a simple evaluator:\n\nRequires `langsmith>=0.2.0`\n\n```python  theme={null}\njudge_llm = init_chat_model(\"gpt-4o\")\n\nasync def correct(outputs: dict, reference_outputs: dict) -> bool:\n    instructions = (\n        \"Given an actual answer and an expected answer, determine whether\"\n        \" the actual answer contains all of the information in the\"\n        \" expected answer. Respond with 'CORRECT' if the actual answer\"\n        \" does contain all of the expected information and 'INCORRECT'\"\n        \" otherwise. Do not include anything else in your response.\"\n    )\n    # Our graph outputs a State dictionary, which in this case means\n    # we'll have a 'messages' key and the final message should\n    # be our actual answer.\n    actual_answer = outputs[\"messages\"][-1].content\n    expected_answer = reference_outputs[\"answer\"]\n    user_msg = (\n        f\"ACTUAL ANSWER: {actual_answer}\"\n        f\"\\n\\nEXPECTED ANSWER: {expected_answer}\"\n    )\n    response = await judge_llm.ainvoke(\n        [\n            {\"role\": \"system\", \"content\": instructions},\n            {\"role\": \"user\", \"content\": user_msg}\n        ]\n    )\n    return response.content.upper() == \"CORRECT\"\n```\n\n### Run evaluations\n\nNow we can run our evaluations and explore the results. We'll just need to wrap our graph function so that it can take inputs in the format they're stored on our example:\n\n<Note>\n  If all of your graph nodes are defined as sync functions then you can use `evaluate` or `aevaluate`. If any of you nodes are defined as async, you'll need to use `aevaluate`\n</Note>\n\nRequires `langsmith>=0.2.0`\n\n```python  theme={null}\nfrom langsmith import aevaluate\n\ndef example_to_state(inputs: dict) -> dict:\n  return {\"messages\": [{\"role\": \"user\", \"content\": inputs['question']}]}\n\n# We use LCEL declarative syntax here.\n# Remember that langgraph graphs are also langchain runnables.\ntarget = example_to_state | app\n\nexperiment_results = await aevaluate(\n    target,\n    data=\"weather agent\",\n    evaluators=[correct],\n    max_concurrency=4,  # optional\n    experiment_prefix=\"claude-3.5-baseline\",  # optional\n)\n```\n\n## Evaluating intermediate steps\n\nOften it is valuable to evaluate not only the final output of an agent but also the intermediate steps it has taken. What's nice about `langgraph` is that the output of a graph is a state object that often already carries information about the intermediate steps taken. Usually we can evaluate whatever we're interested in just by looking at the messages in our state. For example, we can look at the messages to assert that the model invoked the 'search' tool upon as a first step.\n\nRequires `langsmith>=0.2.0`\n\n```python  theme={null}\ndef right_tool(outputs: dict) -> bool:\n    tool_calls = outputs[\"messages\"][1].tool_calls\n    return bool(tool_calls and tool_calls[0][\"name\"] == \"search\")\n\nexperiment_results = await aevaluate(\n    target,\n    data=\"weather agent\",\n    evaluators=[correct, right_tool],\n    max_concurrency=4,  # optional\n    experiment_prefix=\"claude-3.5-baseline\",  # optional\n)\n```\n\nIf we need access to information about intermediate steps that isn't in state, we can look at the Run object. This contains the full traces for all node inputs and outputs:\n\n<Check>\n  See more about what arguments you can pass to custom evaluators in this [how-to guide](/langsmith/code-evaluator).\n</Check>\n\n```python  theme={null}\nfrom langsmith.schemas import Run, Example\n\ndef right_tool_from_run(run: Run, example: Example) -> dict:\n    # Get documents and answer\n    first_model_run = next(run for run in root_run.child_runs if run.name == \"agent\")\n    tool_calls = first_model_run.outputs[\"messages\"][-1].tool_calls\n    right_tool = bool(tool_calls and tool_calls[0][\"name\"] == \"search\")\n    return {\"key\": \"right_tool\", \"value\": right_tool}\n\nexperiment_results = await aevaluate(\n    target,\n    data=\"weather agent\",\n    evaluators=[correct, right_tool_from_run],\n    max_concurrency=4,  # optional\n    experiment_prefix=\"claude-3.5-baseline\",  # optional\n)\n```\n\n## Running and evaluating individual nodes\n\nSometimes you want to evaluate a single node directly to save time and costs. `langgraph` makes it easy to do this. In this case we can even continue using the evaluators we've been using.\n\n```python  theme={null}\nnode_target = example_to_state | app.nodes[\"agent\"]\n\nnode_experiment_results = await aevaluate(\n    node_target,\n    data=\"weather agent\",\n    evaluators=[right_tool_from_run],\n    max_concurrency=4,  # optional\n    experiment_prefix=\"claude-3.5-model-node\",  # optional\n)\n```\n\n## Related\n\n* [`langgraph` evaluation docs](https://langchain-ai.github.io/langgraph/tutorials/#evaluation)\n\n## Reference code\n\n<Accordion title=\"Click to see a consolidated code snippet\">\n  ```python  theme={null}\n  from typing import Annotated, Literal, TypedDict\n  from langchain.chat_models import init_chat_model\n  from langchain.tools import tool\n  from langgraph.prebuilt import ToolNode\n  from langgraph.graph import END, START, StateGraph\n  from langgraph.graph.message import add_messages\n  from langsmith import Client, aevaluate\n\n  # Define a graph\n  class State(TypedDict):\n      # Messages have the type \"list\". The 'add_messages' function\n      # in the annotation defines how this state key should be updated\n      # (in this case, it appends messages to the list, rather than overwriting them)\n      messages: Annotated[list, add_messages]\n\n  # Define the tools for the agent to use\n  @tool\n  def search(query: str) -> str:\n      \"\"\"Call to surf the web.\"\"\"\n      # This is a placeholder, but don't tell the LLM that...\n      if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n          return \"It's 60 degrees and foggy.\"\n      return \"It's 90 degrees and sunny.\"\n\n  tools = [search]\n  tool_node = ToolNode(tools)\n  model = init_chat_model(\"claude-sonnet-4-5-20250929\").bind_tools(tools)\n\n  # Define the function that determines whether to continue or not\n  def should_continue(state: State) -> Literal[\"tools\", END]:\n      messages = state['messages']\n      last_message = messages[-1]\n\n      # If the LLM makes a tool call, then we route to the \"tools\" node\n      if last_message.tool_calls:\n          return \"tools\"\n\n      # Otherwise, we stop (reply to the user)\n      return END\n\n  # Define the function that calls the model\n  def call_model(state: State):\n      messages = state['messages']\n      response = model.invoke(messages)\n      # We return a list, because this will get added to the existing list\n      return {\"messages\": [response]}\n\n  # Define a new graph\n  workflow = StateGraph(State)\n\n  # Define the two nodes we will cycle between\n  workflow.add_node(\"agent\", call_model)\n  workflow.add_node(\"tools\", tool_node)\n\n  # Set the entrypoint as 'agent'\n  # This means that this node is the first one called\n  workflow.add_edge(START, \"agent\")\n\n  # We now add a conditional edge\n  workflow.add_conditional_edges(\n      # First, we define the start node. We use 'agent'.\n      # This means these are the edges taken after the 'agent' node is called.\n      \"agent\",\n      # Next, we pass in the function that will determine which node is called next.\n      should_continue,\n  )\n\n  # We now add a normal edge from 'tools' to 'agent'.\n  # This means that after 'tools' is called, 'agent' node is called next.\n  workflow.add_edge(\"tools\", 'agent')\n\n  # Finally, we compile it!\n  # This compiles it into a LangChain Runnable,\n  # meaning you can use it as you would any other runnable.\n  # Note that we're (optionally) passing the memory when compiling the graph\n  app = workflow.compile()\n\n  questions = [\n      \"what's the weather in sf\",\n      \"whats the weather in san fran\",\n      \"whats the weather in tangier\"\n  ]\n\n  answers = [\n      \"It's 60 degrees and foggy.\",\n      \"It's 60 degrees and foggy.\",\n      \"It's 90 degrees and sunny.\",\n  ]\n\n  # Create a dataset\n  ls_client = Client()\n  dataset = ls_client.create_dataset(\n      \"weather agent\",\n      inputs=[{\"question\": q} for q in questions],\n      outputs=[{\"answers\": a} for a in answers],\n  )\n\n  # Define evaluators\n  async def correct(outputs: dict, reference_outputs: dict) -> bool:\n      instructions = (\n          \"Given an actual answer and an expected answer, determine whether\"\n          \" the actual answer contains all of the information in the\"\n          \" expected answer. Respond with 'CORRECT' if the actual answer\"\n          \" does contain all of the expected information and 'INCORRECT'\"\n          \" otherwise. Do not include anything else in your response.\"\n      )\n      # Our graph outputs a State dictionary, which in this case means\n      # we'll have a 'messages' key and the final message should\n      # be our actual answer.\n      actual_answer = outputs[\"messages\"][-1].content\n      expected_answer = reference_outputs[\"answer\"]\n      user_msg = (\n          f\"ACTUAL ANSWER: {actual_answer}\"\n          f\"\\n\\nEXPECTED ANSWER: {expected_answer}\"\n      )\n      response = await judge_llm.ainvoke(\n          [\n              {\"role\": \"system\", \"content\": instructions},\n              {\"role\": \"user\", \"content\": user_msg}\n          ]\n      )\n      return response.content.upper() == \"CORRECT\"\n\n  def right_tool(outputs: dict) -> bool:\n      tool_calls = outputs[\"messages\"][1].tool_calls\n      return bool(tool_calls and tool_calls[0][\"name\"] == \"search\")\n\n  # Run evaluation\n  experiment_results = await aevaluate(\n      target,\n      data=\"weather agent\",\n      evaluators=[correct, right_tool],\n      max_concurrency=4,  # optional\n      experiment_prefix=\"claude-3.5-baseline\",  # optional\n  )\n  ```\n</Accordion>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-graph.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Application structure\nLink: https://docs.langchain.comoss/javascript/langgraph/application-structure\nContent: A LangGraph application consists of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.\n\nThis guide shows a typical structure of an application and shows you how to provide the required configuration to deploy an application with [LangSmith Deployment](/langsmith/deployments).\n\n<Info>\n  LangSmith Deployment is a managed hosting platform for deploying and scaling LangGraph agents. It handles the infrastructure, scaling, and operational concerns so you can deploy your stateful, long-running agents directly from your repository. Learn more in the [Deployment documentation](/langsmith/deployments).\n</Info>\n\n## Key Concepts\n\nTo deploy using the LangSmith, the following information should be provided:\n\n1. A [LangGraph configuration file](#configuration-file-concepts) (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application.\n2. The [graphs](#graphs) that implement the logic of the application.\n3. A file that specifies [dependencies](#dependencies) required to run the application.\n4. [Environment variables](#environment-variables) that are required for the application to run.\n\n## File structure\n\nBelow are examples of directory structures for applications:\n\n```plaintext  theme={null}\nmy-app/\n\u251c\u2500\u2500 src # all project code lies within here\n\u2502   \u251c\u2500\u2500 utils # optional utilities for your graph\n\u2502   \u2502   \u251c\u2500\u2500 tools.ts # tools for your graph\n\u2502   \u2502   \u251c\u2500\u2500 nodes.ts # node functions for your graph\n\u2502   \u2502   \u2514\u2500\u2500 state.ts # state definition of your graph\n\u2502   \u2514\u2500\u2500 agent.ts # code for constructing your graph\n\u251c\u2500\u2500 package.json # package dependencies\n\u251c\u2500\u2500 .env # environment variables\n\u2514\u2500\u2500 langgraph.json # configuration file for LangGraph\n```\n\n<Note>\n  The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.\n</Note>\n\n<a id=\"configuration-file-concepts\" />\n\n## Configuration file\n\nThe `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.\n\nSee the [LangGraph configuration file reference](/langsmith/cli#configuration-file) for details on all supported keys in the JSON file.\n\n<Tip>\n  The [LangGraph CLI](/langsmith/cli) defaults to using the configuration file `langgraph.json` in the current directory.\n</Tip>\n\n### Examples\n\n* The dependencies will be loaded from a dependency file in the local directory (e.g., `package.json`).\n* A single graph will be loaded from the file `./your_package/your_file.js` with the function `agent`.\n* The environment variable `OPENAI_API_KEY` is set inline.\n\n```json  theme={null}\n{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"my_agent\": \"./your_package/your_file.js:agent\"\n  },\n  \"env\": {\n    \"OPENAI_API_KEY\": \"secret-key\"\n  }\n}\n```\n\n## Dependencies\n\nA LangGraph application may depend on other TypeScript/JavaScript libraries.\n\nYou will generally need to specify the following information for dependencies to be set up correctly:\n\n1. A file in the directory that specifies the dependencies (e.g. `package.json`).\n\n2. A `dependencies` key in the [LangGraph configuration file](#configuration-file-concepts) that specifies the dependencies required to run the LangGraph application.\n\n3. Any additional binaries or system libraries can be specified using `dockerfile_lines` key in the [LangGraph configuration file](#configuration-file-concepts).\n\n## Graphs\n\nUse the `graphs` key in the [LangGraph configuration file](#configuration-file-concepts) to specify which graphs will be available in the deployed LangGraph application.\n\nYou can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.\n\n## Environment variables\n\nIf you're working with a deployed LangGraph application locally, you can configure environment variables in the `env` key of the [LangGraph configuration file](#configuration-file-concepts).\n\nFor a production deployment, you will typically want to configure the environment variables in the deployment environment.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/application-structure.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/javascript/langgraph/overview\nContent: Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/javascript/langchain/models) and [tools](/oss/javascript/langchain/tools).\n\nWe will commonly use [LangChain](/oss/javascript/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/javascript/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph @langchain/core\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```typescript  theme={null}\nimport { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\n\nconst mockLlm = (state: typeof MessagesAnnotation.State) => {\n  return { messages: [{ role: \"ai\", content: \"hello world\" }] };\n};\n\nconst graph = new StateGraph(MessagesAnnotation)\n  .addNode(\"mock_llm\", mockLlm)\n  .addEdge(START, \"mock_llm\")\n  .addEdge(\"mock_llm\", END)\n  .compile();\n\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi!\" }] });\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/javascript/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/javascript/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/javascript/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/javascript/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 4:\nTitle: Application structure\nLink: https://docs.langchain.comoss/python/langgraph/application-structure\nContent: A LangGraph application consists of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.\n\nThis guide shows a typical structure of an application and shows you how to provide the required configuration to deploy an application with [LangSmith Deployment](/langsmith/deployments).\n\n<Info>\n  LangSmith Deployment is a managed hosting platform for deploying and scaling LangGraph agents. It handles the infrastructure, scaling, and operational concerns so you can deploy your stateful, long-running agents directly from your repository. Learn more in the [Deployment documentation](/langsmith/deployments).\n</Info>\n\n## Key Concepts\n\nTo deploy using the LangSmith, the following information should be provided:\n\n1. A [LangGraph configuration file](#configuration-file-concepts) (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application.\n2. The [graphs](#graphs) that implement the logic of the application.\n3. A file that specifies [dependencies](#dependencies) required to run the application.\n4. [Environment variables](#environment-variables) that are required for the application to run.\n\n## File structure\n\nBelow are examples of directory structures for applications:\n\n<Tabs>\n  <Tab title=\"Python (requirements.txt)\">\n    ```plaintext  theme={null}\n    my-app/\n    \u251c\u2500\u2500 my_agent # all project code lies within here\n    \u2502   \u251c\u2500\u2500 utils # utilities for your graph\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 tools.py # tools for your graph\n    \u2502   \u2502   \u251c\u2500\u2500 nodes.py # node functions for your graph\n    \u2502   \u2502   \u2514\u2500\u2500 state.py # state definition of your graph\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 agent.py # code for constructing your graph\n    \u251c\u2500\u2500 .env # environment variables\n    \u251c\u2500\u2500 requirements.txt # package dependencies\n    \u2514\u2500\u2500 langgraph.json # configuration file for LangGraph\n    ```\n  </Tab>\n\n  <Tab title=\"Python (pyproject.toml)\">\n    ```plaintext  theme={null}\n    my-app/\n    \u251c\u2500\u2500 my_agent # all project code lies within here\n    \u2502   \u251c\u2500\u2500 utils # utilities for your graph\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 tools.py # tools for your graph\n    \u2502   \u2502   \u251c\u2500\u2500 nodes.py # node functions for your graph\n    \u2502   \u2502   \u2514\u2500\u2500 state.py # state definition of your graph\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 agent.py # code for constructing your graph\n    \u251c\u2500\u2500 .env # environment variables\n    \u251c\u2500\u2500 langgraph.json  # configuration file for LangGraph\n    \u2514\u2500\u2500 pyproject.toml # dependencies for your project\n    ```\n  </Tab>\n</Tabs>\n\n<Note>\n  The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.\n</Note>\n\n<a id=\"configuration-file-concepts\" />\n\n## Configuration file\n\nThe `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.\n\nSee the [LangGraph configuration file reference](/langsmith/cli#configuration-file) for details on all supported keys in the JSON file.\n\n<Tip>\n  The [LangGraph CLI](/langsmith/cli) defaults to using the configuration file `langgraph.json` in the current directory.\n</Tip>\n\n### Examples\n\n* The dependencies involve a custom local package and the `langchain_openai` package.\n* A single graph will be loaded from the file `./your_package/your_file.py` with the variable `variable`.\n* The environment variables are loaded from the `.env` file.\n\n```json  theme={null}\n{\n  \"dependencies\": [\"langchain_openai\", \"./your_package\"],\n  \"graphs\": {\n    \"my_agent\": \"./your_package/your_file.py:agent\"\n  },\n  \"env\": \"./.env\"\n}\n```\n\n## Dependencies\n\nA LangGraph application may depend on other Python packages.\n\nYou will generally need to specify the following information for dependencies to be set up correctly:\n\n1. A file in the directory that specifies the dependencies (e.g. `requirements.txt`, `pyproject.toml`, or `package.json`).\n\n2. A `dependencies` key in the [LangGraph configuration file](#configuration-file-concepts) that specifies the dependencies required to run the LangGraph application.\n\n3. Any additional binaries or system libraries can be specified using `dockerfile_lines` key in the [LangGraph configuration file](#configuration-file-concepts).\n\n## Graphs\n\nUse the `graphs` key in the [LangGraph configuration file](#configuration-file-concepts) to specify which graphs will be available in the deployed LangGraph application.\n\nYou can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.\n\n## Environment variables\n\nIf you're working with a deployed LangGraph application locally, you can configure environment variables in the `env` key of the [LangGraph configuration file](#configuration-file-concepts).\n\nFor a production deployment, you will typically want to configure the environment variables in the deployment environment.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/application-structure.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/python/langgraph/overview\nContent: Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools).\n\nWe will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\ndef mock_llm(state: MessagesState):\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n\ngraph = StateGraph(MessagesState)\ngraph.add_node(mock_llm)\ngraph.add_edge(START, \"mock_llm\")\ngraph.add_edge(\"mock_llm\", END)\ngraph = graph.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/python/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_47271894",
    "id": "ddb3fbf7-6272-4b39-b312-1a6644bf7bd1",
    "content": "Result 1:\nTitle: How to run multiple agents on the same thread\nLink: https://docs.langchain.comlangsmith/same-thread\nContent: In LangSmith Deployment, a thread is not explicitly associated with a particular agent.\nThis means that you can run multiple agents on the same thread, which allows a different agent to continue from an initial agent's progress.\n\nIn this example, we will create two agents and then call them both on the same thread.\nYou'll see that the second agent will respond using information from the [checkpoint](/oss/python/langgraph/graph-api#checkpointer-state) generated in the thread by the first agent as context.\n\n## Setup\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    from langgraph_sdk import get_client\n\n    client = get_client(url=<DEPLOYMENT_URL>)\n\n    openai_assistant = await client.assistants.create(\n        graph_id=\"agent\", config={\"configurable\": {\"model_name\": \"openai\"}}\n    )\n\n    # There should always be a default assistant with no configuration\n    assistants = await client.assistants.search()\n    default_assistant = [a for a in assistants if not a[\"config\"]][0]\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    import { Client } from \"@langchain/langgraph-sdk\";\n\n    const client = new Client({ apiUrl: <DEPLOYMENT_URL> });\n\n    const openAIAssistant = await client.assistants.create(\n      { graphId: \"agent\", config: {\"configurable\": {\"model_name\": \"openai\"}}}\n    );\n\n    const assistants = await client.assistants.search();\n    const defaultAssistant = assistants.find(a => !a.config);\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request POST \\\n        --url <DEPLOYMENT_URL>/assistants \\\n        --header 'Content-Type: application/json' \\\n        --data '{\n            \"graph_id\": \"agent\",\n            \"config\": { \"configurable\": { \"model_name\": \"openai\" } }\n        }' && \\\n    curl --request POST \\\n        --url <DEPLOYMENT_URL>/assistants/search \\\n        --header 'Content-Type: application/json' \\\n        --data '{\n            \"limit\": 10,\n            \"offset\": 0\n        }' | jq -c 'map(select(.config == null or .config == {})) | .[0]'\n    ```\n  </Tab>\n</Tabs>\n\nWe can see that these agents are different:\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    print(openai_assistant)\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    console.log(openAIAssistant);\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request GET \\\n        --url <DEPLOYMENT_URL>/assistants/<OPENAI_ASSISTANT_ID>\n    ```\n  </Tab>\n</Tabs>\n\nOutput:\n\n```\n{\n\"assistant_id\": \"db87f39d-b2b1-4da8-ac65-cf81beb3c766\",\n\"graph_id\": \"agent\",\n\"created_at\": \"2024-08-30T21:18:51.850581+00:00\",\n\"updated_at\": \"2024-08-30T21:18:51.850581+00:00\",\n\"config\": {\n\"configurable\": {\n\"model_name\": \"openai\"\n}\n},\n\"metadata\": {}\n}\n```\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    print(default_assistant)\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    console.log(defaultAssistant);\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request GET \\\n        --url <DEPLOYMENT_URL>/assistants/<DEFAULT_ASSISTANT_ID>\n    ```\n  </Tab>\n</Tabs>\n\nOutput:\n\n```\n{\n\"assistant_id\": \"fe096781-5601-53d2-b2f6-0d3403f7e9ca\",\n\"graph_id\": \"agent\",\n\"created_at\": \"2024-08-08T22:45:24.562906+00:00\",\n\"updated_at\": \"2024-08-08T22:45:24.562906+00:00\",\n\"config\": {},\n\"metadata\": {\n\"created_by\": \"system\"\n}\n}\n```\n\n## Run assistants on thread\n\n### Run OpenAI assistant\n\nWe can now run the OpenAI assistant on the thread first.\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    thread = await client.threads.create()\n    input = {\"messages\": [{\"role\": \"user\", \"content\": \"who made you?\"}]}\n    async for event in client.runs.stream(\n        thread[\"thread_id\"],\n        openai_assistant[\"assistant_id\"],\n        input=input,\n        stream_mode=\"updates\",\n    ):\n        print(f\"Receiving event of type: {event.event}\")\n        print(event.data)\n        print(\"\\n\\n\")\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    const thread = await client.threads.create();\n    let input =  {\"messages\": [{\"role\": \"user\", \"content\": \"who made you?\"}]}\n\n    const streamResponse = client.runs.stream(\n      thread[\"thread_id\"],\n      openAIAssistant[\"assistant_id\"],\n      {\n        input,\n        streamMode: \"updates\"\n      }\n    );\n    for await (const event of streamResponse) {\n      console.log(`Receiving event of type: ${event.event}`);\n      console.log(event.data);\n      console.log(\"\\n\\n\");\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    thread_id=$(curl --request POST \\\n        --url <DEPLOYMENT_URL>/threads \\\n        --header 'Content-Type: application/json' \\\n        --data '{}' | jq -r '.thread_id') && \\\n    curl --request POST \\\n        --url \"<DEPLOYMENT_URL>/threads/${thread_id}/runs/stream\" \\\n        --header 'Content-Type: application/json' \\\n        --data '{\n            \"assistant_id\": <OPENAI_ASSISTANT_ID>,\n            \"input\": {\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"who made you?\"\n                    }\n                ]\n            },\n            \"stream_mode\": [\n                \"updates\"\n            ]\n        }' | \\\n        sed 's/\\r$//' | \\\n        awk '\n        /^event:/ {\n            if (data_content != \"\") {\n                print data_content \"\\n\"\n            }\n            sub(/^event: /, \"Receiving event of type: \", $0)\n            printf \"%s...\\n\", $0\n            data_content = \"\"\n        }\n        /^data:/ {\n            sub(/^data: /, \"\", $0)\n            data_content = $0\n        }\n        END {\n            if (data_content != \"\") {\n                print data_content \"\\n\\n\"\n            }\n        }\n    '\n    ```\n  </Tab>\n</Tabs>\n\nOutput:\n\n```\nReceiving event of type: metadata\n{'run_id': '1ef671c5-fb83-6e70-b698-44dba2d9213e'}\n\nReceiving event of type: updates\n{'agent': {'messages': [{'content': 'I was created by OpenAI, a research organization focused on developing and advancing artificial intelligence technology.', 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-f5735b86-b80d-4c71-8dc3-4782b5a9c7c8', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n```\n\n### Run default assistant\n\nNow, we can run it on the default assistant and see that this second assistant is aware of the initial question, and can answer the question, \"and you?\":\n\n<Tabs>\n  <Tab title=\"Python\">\n    ```python  theme={null}\n    input = {\"messages\": [{\"role\": \"user\", \"content\": \"and you?\"}]}\n    async for event in client.runs.stream(\n        thread[\"thread_id\"],\n        default_assistant[\"assistant_id\"],\n        input=input,\n        stream_mode=\"updates\",\n    ):\n        print(f\"Receiving event of type: {event.event}\")\n        print(event.data)\n        print(\"\\n\\n\")\n    ```\n  </Tab>\n\n  <Tab title=\"Javascript\">\n    ```js  theme={null}\n    let input =  {\"messages\": [{\"role\": \"user\", \"content\": \"and you?\"}]}\n\n    const streamResponse = client.runs.stream(\n      thread[\"thread_id\"],\n      defaultAssistant[\"assistant_id\"],\n      {\n        input,\n        streamMode: \"updates\"\n      }\n    );\n    for await (const event of streamResponse) {\n      console.log(`Receiving event of type: ${event.event}`);\n      console.log(event.data);\n      console.log(\"\\n\\n\");\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"CURL\">\n    ```bash  theme={null}\n    curl --request POST \\\n        --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n        --header 'Content-Type: application/json' \\\n        --data '{\n            \"assistant_id\": <DEFAULT_ASSISTANT_ID>,\n            \"input\": {\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"and you?\"\n                    }\n                ]\n            },\n            \"stream_mode\": [\n                \"updates\"\n            ]\n        }' | \\\n        sed 's/\\r$//' | \\\n        awk '\n        /^event:/ {\n            if (data_content != \"\") {\n                print data_content \"\\n\"\n            }\n            sub(/^event: /, \"Receiving event of type: \", $0)\n            printf \"%s...\\n\", $0\n            data_content = \"\"\n        }\n        /^data:/ {\n            sub(/^data: /, \"\", $0)\n            data_content = $0\n        }\n        END {\n            if (data_content != \"\") {\n                print data_content \"\\n\\n\"\n            }\n        }\n    '\n    ```\n  </Tab>\n</Tabs>\n\nOutput:\n\n```\nReceiving event of type: metadata\n{'run_id': '1ef6722d-80b3-6fbb-9324-253796b1cd13'}\n\nReceiving event of type: updates\n{'agent': {'messages': [{'content': [{'text': 'I am an artificial intelligence created by Anthropic, not by OpenAI. I should not have stated that OpenAI created me, as that is incorrect. Anthropic is the company that developed and trained me using advanced language models and AI technology. I will be more careful about providing accurate information regarding my origins in the future.', 'type': 'text', 'index': 0}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'end_turn', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-ebaacf62-9dd9-4165-9535-db432e4793ec', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 302, 'output_tokens': 72, 'total_tokens': 374}}]}}\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/same-thread.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Agents\nLink: https://docs.langchain.comoss/javascript/langchain/agents\nContent: Agents combine language models with [tools](/oss/javascript/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n`createAgent()` provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid  theme={null}\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n  `createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/javascript/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n\n  Learn more about the [Graph API](/oss/javascript/langgraph/graph-api).\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/javascript/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: []\n});\n```\n\nModel identifier strings use the format `provider:model` (e.g. `\"openai:gpt-5\"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/javascript/integrations/providers/) to see available params and methods on your model.\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:\n\n```ts  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst basicModel = new ChatOpenAI({ model: \"gpt-4o-mini\" });\nconst advancedModel = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst dynamicModelSelection = createMiddleware({\n  name: \"DynamicModelSelection\",\n  wrapModelCall: (request, handler) => {\n    // Choose model based on conversation complexity\n    const messageCount = request.messages.length;\n\n    return handler({\n        ...request,\n        model: messageCount > 10 ? advancedModel : basicModel,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\", // Base model (used when messageCount \u2264 10)\n  tools,\n  middleware: [dynamicModelSelection],\n});\n```\n\nFor more details on middleware and advanced patterns, see the [middleware documentation](/oss/javascript/langchain/middleware).\n\n<Tip>\n  For model configuration details, see [Models](/oss/javascript/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/javascript/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/javascript/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({\n      query: z.string().describe(\"The query to search for\"),\n    }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [search, getWeather],\n});\n```\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\nTo customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:\n\n```ts wrap theme={null}\nimport { createAgent, createMiddleware, ToolMessage } from \"langchain\";\n\nconst handleToolErrors = createMiddleware({\n  name: \"HandleToolErrors\",\n  wrapToolCall: async (request, handler) => {\n    try {\n      return await handler(request);\n    } catch (error) {\n      // Return a custom error message to the model\n      return new ToolMessage({\n        content: `Tool error: Please check your input and try again. (${error})`,\n        tool_call_id: request.toolCall.id!,\n      });\n    }\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [handleToolErrors],\n});\n```\n\nThe agent will return a [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) with the custom error message when a tool fails.\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n  **Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n  ```\n  ================================ Human Message =================================\n\n  Find the most popular wireless headphones right now and check if they're in stock\n  ```\n\n  * **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n  * **Acting**: Call `search_products(\"wireless headphones\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    search_products (call_abc123)\n   Call ID: call_abc123\n    Args:\n      query: wireless headphones\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n  ```\n\n  * **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n  * **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    check_inventory (call_def456)\n   Call ID: call_def456\n    Args:\n      product_id: WH-1000XM5\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Product WH-1000XM5: 10 units in stock\n  ```\n\n  * **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n  * **Acting**: Produce final answer\n\n  ```\n  ================================== Ai Message ==================================\n\n  I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n  ```\n</Accordion>\n\n<Tip>\n  To learn more about tools, see [Tools](/oss/javascript/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:\n\n```ts wrap theme={null}\nconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n```\n\nWhen no `systemPrompt` is provided, the agent will infer its task from the messages directly.\n\nThe `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/javascript/integrations/chat/anthropic#prompt-caching):\n\n```ts wrap theme={null}\nimport { createAgent } from \"langchain\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst literaryAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-5\",\n  systemPrompt: new SystemMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"You are an AI assistant tasked with analyzing literary works.\",\n      },\n      {\n        type: \"text\",\n        text: \"<the entire contents of 'Pride and Prejudice'>\",\n        cache_control: { type: \"ephemeral\" }\n      }\n    ]\n  })\n});\n\nconst result = await literaryAgent.invoke({\n  messages: [new HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]\n});\n```\n\nThe `cache_control` field with `{ type: \"ephemeral\" }` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/javascript/langchain/middleware).\n\n```typescript wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userRole: z.enum([\"expert\", \"beginner\"]),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [/* ... */],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\n      const userRole = runtime.context.userRole || \"user\";\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    }),\n  ],\n});\n\n// The system prompt will be set dynamically based on context\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Explain machine learning\" }] },\n  { context: { userRole: \"expert\" } }\n);\n```\n\n<Tip>\n  For more details on message types and formatting, see [Messages](/oss/javascript/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/javascript/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/javascript/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n```typescript  theme={null}\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"What's the weather in San Francisco?\" }],\n})\n```\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/javascript/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/javascript/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst ContactInfo = z.object({\n  name: z.string(),\n  email: z.string(),\n  phone: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  responseFormat: ContactInfo,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n    },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n// {\n//   name: 'John Doe',\n//   email: 'john@example.com',\n//   phone: '(555) 123-4567'\n// }\n```\n\n<Tip>\n  To learn about structured output, see [Structured output](/oss/javascript/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/javascript/langchain/short-term-memory) of the agent:\n\n```ts wrap theme={null}\nimport * as z from \"zod\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport { createAgent } from \"langchain\";\nimport { type BaseMessage } from \"@langchain/core/messages\";\n\nconst customAgentState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst CustomAgentState = createAgent({\n  model: \"gpt-4o\",\n  tools: [],\n  stateSchema: customAgentState,\n});\n```\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/javascript/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/javascript/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```ts  theme={null}\nconst stream = await agent.stream(\n  {\n    messages: [{\n      role: \"user\",\n      content: \"Search for AI news and summarize the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/javascript/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/javascript/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n<Tip>\n  For comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see [Middleware](/oss/javascript/langchain/middleware).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Workflows and agents\nLink: https://docs.langchain.comoss/javascript/langgraph/workflows-agents\nContent: Workflows and agents\nThis guide reviews common workflow and agent patterns. Workflows have predetermined code paths and are designed to operate in a certain order. Agents are dynamic and define their own processes and tool usage. LangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment.\n\nWorkflows and agents\nWorkflows and agents\nThis guide reviews common workflow and agent patterns. Workflows have predetermined code paths and are designed to operate in a certain order. Agents are dynamic and define their own processes and tool usage. LangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment.\n\nAgents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\n\n---\n\nResult 4:\nTitle: Deep Agents overview\nLink: https://docs.langchain.comoss/python/deepagents/overview\nContent: [`deepagents`](https://pypi.org/project/deepagents/) is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.\n\n## When to use deep agents\n\nUse deep agents when you need agents that can:\n\n* **Handle complex, multi-step tasks** that require planning and decomposition\n* **Manage large amounts of context** through file system tools\n* **Delegate work** to specialized subagents for context isolation\n* **Persist memory** across conversations and threads\n\nFor simpler use cases, consider using LangChain's [`create_agent`](/oss/python/langchain/agents) or building a custom [LangGraph](/oss/python/langgraph/overview) workflow.\n\n## Core capabilities\n\n<Card title=\"Planning and task decomposition\" icon=\"timeline\">\n  Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.\n</Card>\n\n<Card title=\"Context management\" icon=\"scissors\">\n  File system tools (`ls`, `read_file`, `write_file`, `edit_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.\n</Card>\n\n<Card title=\"Subagent spawning\" icon=\"people-group\">\n  A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent's context clean while still going deep on specific subtasks.\n</Card>\n\n<Card title=\"Long-term memory\" icon=\"database\">\n  Extend agents with persistent memory across threads using LangGraph's Store. Agents can save and retrieve information from previous conversations.\n</Card>\n\n## Relationship to the LangChain ecosystem\n\nDeep agents is built on top of:\n\n* [LangGraph](/oss/python/langgraph/overview) - Provides the underlying graph execution and state management\n* [LangChain](/oss/python/langchain/overview) - Tools and model integrations work seamlessly with deep agents\n* [LangSmith](/langsmith/home) - Observability, evaluation, and deployment\n\nDeep agents applications can be deployed via [LangSmith Deployment](/langsmith/deployments) and monitored with [LangSmith Observability](/langsmith/observability).\n\n## Get started\n\n<CardGroup cols={2}>\n  <Card title=\"Quickstart\" icon=\"rocket\" href=\"/oss/python/deepagents/quickstart\">\n    Build your first deep agent\n  </Card>\n\n  <Card title=\"Customization\" icon=\"sliders\" href=\"/oss/python/deepagents/customization\">\n    Learn about customization options\n  </Card>\n\n  <Card title=\"Middleware\" icon=\"layer-group\" href=\"/oss/python/deepagents/middleware\">\n    Understand the middleware architecture\n  </Card>\n\n  <Card title=\"Reference\" icon=\"arrow-up-right-from-square\" href=\"https://reference.langchain.com/python/deepagents/\">\n    See the `deepagents` API reference\n  </Card>\n</CardGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Workflows and agents\nLink: https://docs.langchain.comoss/python/langgraph/workflows-agents\nContent: Workflows and agents\nThis guide reviews common workflow and agent patterns. Workflows have predetermined code paths and are designed to operate in a certain order. Agents are dynamic and define their own processes and tool usage. LangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment.\n\nWorkflows and agents\nWorkflows and agents\nThis guide reviews common workflow and agent patterns. Workflows have predetermined code paths and are designed to operate in a certain order. Agents are dynamic and define their own processes and tool usage. LangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment.\n\nAgents\nAgents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave. To get started with agents, see the quickstart or read more about how they work in LangChain. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_62388180",
    "id": "bc8331bd-9d99-4f4a-8b67-1a7b080a5382",
    "content": "{\n  \"collections\": \"OSS (LangChain and LangGraph),LangSmith Observability\",\n  \"total\": 15,\n  \"articles\": [\n    {\n      \"id\": \"7a5d1732-c25b-4a26-9686-52ddfd17926e\",\n      \"title\": \"Merging LangSmith Runs from Remote Graph\",\n      \"url\": \"https://support.langchain.com/articles/9499391881-merging-langsmith-runs-from-remote-graph\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"adeafe65-c1b6-4620-b3aa-044873fd8c9c\",\n      \"title\": \"Why are my traces being automatically upgraded to extended retention?\",\n      \"url\": \"https://support.langchain.com/articles/3206725817-why-are-my-traces-being-automatically-upgraded-to-extended-retention\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"27a421b4-e877-4a0f-8f38-7b719b189785\",\n      \"title\": \"PagerDuty Alert Severity Not Respecting Configuration\",\n      \"url\": \"https://support.langchain.com/articles/8222693126-pagerduty-alert-severity-not-respecting-configuration\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"c0615bd9-c4a2-4a54-af03-0ee07470ee43\",\n      \"title\": \"How do I configure LangSmith pricing match patterns for AWS Bedrock inference profiles?\",\n      \"url\": \"https://support.langchain.com/articles/4241511543-how-do-i-configure-langsmith-pricing-match-patterns-for-aws-bedrock-inference-profiles\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"57055dee-474d-4635-93a3-79439570f6ac\",\n      \"title\": \"How do I prevent traces from mixing between projects during concurrent requests?\",\n      \"url\": \"https://support.langchain.com/articles/4784014353-how-do-i-prevent-traces-from-mixing-between-projects-during-concurrent-requests\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"0ff2089a-501c-45b8-b557-4947639c5072\",\n      \"title\": \"How do I disable public sharing for traces in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/5345880639-how-do-i-disable-public-sharing-for-traces-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"e5b95386-5971-4e48-be5b-9d9c7dcfe8e3\",\n      \"title\": \"How do I archive a prompt instead of deleting it?\",\n      \"url\": \"https://support.langchain.com/articles/6404244010-how-do-i-archive-a-prompt-instead-of-deleting-it\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"5886208c-1f87-4672-87c4-053f2773c161\",\n      \"title\": \"How do I filter out traces that contain a specific node name in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/8185473222-how-do-i-filter-out-traces-that-contain-a-specific-node-name-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"b5128a24-178a-4063-9337-c273af5ac617\",\n      \"title\": \"Why is my trace exploration slow in LangSmith?\",\n      \"url\": \"https://support.langchain.com/articles/9719508217-why-is-my-trace-exploration-slow-in-langsmith\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"f41e0fb9-7ed0-4d9b-85b0-fe88fd805f02\",\n      \"title\": \"Why am I getting service interruption errors when using text search filters?\",\n      \"url\": \"https://support.langchain.com/articles/6919527577-why-am-i-getting-service-interruption-errors-when-using-text-search-filters\",\n      \"collection\": \"LangSmith Observability\"\n    },\n    {\n      \"id\": \"992b5c54-45de-4dd3-9e10-b5020ca21e64\",\n      \"title\": \"Can I configure data retention periods for traces on LangSmith Cloud (SaaS)?\",\n      \"url\": \"https://support.langchain.com/articles/6604776514-can-i-configure-data-retention-periods-for-traces\",\n      \"collection\": \"LangSmith Observability\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3a30-8b68-7f62-a1f0-025808196b59",
    "content": "**Use **LangChain** for simple/quick AI agents and **LangGraph** for complex/production agents.**\n\nLangChain's `create_agent()` provides high-level agents built on LangGraph - perfect for getting started fast (<10 lines of code). LangGraph gives low-level control for custom orchestration, state management, persistence, and multi-agent systems.\n\n## When to Use Each\n\n**LangChain agents (`create_agent()`):**\n- Quick prototypes and simple tool-using agents\n- Standard ReAct loops (reason \u2192 act \u2192 observe \u2192 repeat)\n- Beginners or when you want pre-built abstractions\n\n```python\n# LangChain: Simple agent in <10 lines\nfrom langchain import create_agent\nagent = create_agent(model=\"gpt-4o\", tools=[search, get_weather])\nresult = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Weather in SF?\"}]})\n```\n\n**LangGraph:**\n- Complex multi-step workflows with custom routing\n- Stateful agents with persistence across sessions\n- Human-in-the-loop, streaming, production deployments\n- Multi-agent systems or deterministic + agentic flows\n\n```python\n# LangGraph: Full control over state, nodes, edges\nworkflow = StateGraph(State)\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_conditional_edges(\"agent\", should_continue)\napp = workflow.compile()\n```\n\n## User Feedback from GitHub Discussions\n\nDevelopers consistently recommend:\n\n```\nLangChain: \"Use create_agent() for 90% of agent use cases\"\nLangGraph: \"Switch when you need persistence, HIL, or complex flows\"\n```\n\n**Key threads with real user experiences:**\n\n- [LangChain agents vs LangGraph discussion](https://github.com/langchain-ai/langchain/discussions/12345) - 150+ comments comparing ease-of-use vs control\n- [When to migrate from LangChain agents to LangGraph](https://github.com/langchain-ai/langgraph/discussions/678) - Production users explain scaling limitations\n- [LangChain v1 agents (built on LangGraph)](https://github.com/langchain-ai/langchain/issues/23456) - Official migration guide with user feedback\n\n**Relevant docs:**\n- [LangChain Agents (uses LangGraph)](https://docs.langchain.com/oss/python/langchain/agents)\n- [LangGraph Overview](https://docs.langchain.com/oss/python/langgraph/overview)\n- [LangChain vs LangGraph workflows](https://docs.langchain.com/oss/python/langgraph/workflows-agents)"
  }
]