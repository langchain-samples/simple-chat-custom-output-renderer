[
  {
    "role": "user",
    "id": "18b46f99-d499-49a5-92bb-c0a8616a4310",
    "content": "\u6211\u60f3\u4e86\u89e3**StateGraph add_node**\u7684\u7528\u6cd5"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_43476211",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"python\", \"page_size\": 5, \"query\": \"stategraph\"}"
        }
      },
      {
        "type": "function",
        "id": "call_70979854",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"python\", \"page_size\": 5, \"query\": \"add_node\"}"
        }
      },
      {
        "type": "function",
        "id": "call_81229289",
        "function": {
          "name": "search_support_articles",
          "arguments": "{\"collections\": \"OSS (LangChain and LangGraph)\"}"
        }
      }
    ],
    "id": "lc_run--019b3a1f-fb02-7f01-9b41-fb2e19dca613",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_43476211",
    "id": "58c3ece1-1371-445d-9d06-91195c3bc0d4",
    "content": "Result 1:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/javascript/langgraph/overview\nContent: Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/javascript/langchain/models) and [tools](/oss/javascript/langchain/tools).\n\nWe will commonly use [LangChain](/oss/javascript/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/javascript/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph @langchain/core\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```typescript  theme={null}\nimport { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\n\nconst mockLlm = (state: typeof MessagesAnnotation.State) => {\n  return { messages: [{ role: \"ai\", content: \"hello world\" }] };\n};\n\nconst graph = new StateGraph(MessagesAnnotation)\n  .addNode(\"mock_llm\", mockLlm)\n  .addEdge(START, \"mock_llm\")\n  .addEdge(\"mock_llm\", END)\n  .compile();\n\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi!\" }] });\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/javascript/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/javascript/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/javascript/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/javascript/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/javascript/langgraph/quickstart\nContent: This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\n\n* [Use the Graph API](#use-the-graph-api) if you prefer to define your agent as a graph of nodes and edges.\n* [Use the Functional API](#use-the-functional-api) if you prefer to define your agent as a single function.\n\nFor conceptual information, see [Graph API overview](/oss/javascript/langgraph/graph-api) and [Functional API overview](/oss/javascript/langgraph/functional-api).\n\n<Info>\n  For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.\n</Info>\n\n<Tabs>\n  <Tab title=\"Use the Graph API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```typescript  theme={null}\n    import { ChatAnthropic } from \"@langchain/anthropic\";\n    import { tool } from \"@langchain/core/tools\";\n    import * as z from \"zod\";\n\n    const model = new ChatAnthropic({\n      model: \"claude-sonnet-4-5-20250929\",\n      temperature: 0,\n    });\n\n    // Define tools\n    const add = tool(({ a, b }) => a + b, {\n      name: \"add\",\n      description: \"Add two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const multiply = tool(({ a, b }) => a * b, {\n      name: \"multiply\",\n      description: \"Multiply two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const divide = tool(({ a, b }) => a / b, {\n      name: \"divide\",\n      description: \"Divide two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    // Augment the LLM with tools\n    const toolsByName = {\n      [add.name]: add,\n      [multiply.name]: multiply,\n      [divide.name]: divide,\n    };\n    const tools = Object.values(toolsByName);\n    const modelWithTools = model.bindTools(tools);\n    ```\n\n    ## 2. Define state\n\n    The graph's state is used to store the messages and the number of LLM calls.\n\n    <Tip>\n      State in LangGraph persists throughout the agent's execution.\n\n      The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.\n    </Tip>\n\n    ```typescript  theme={null}\n    import { StateGraph, START, END } from \"@langchain/langgraph\";\n    import { MessagesZodMeta } from \"@langchain/langgraph\";\n    import { registry } from \"@langchain/langgraph/zod\";\n    import { type BaseMessage } from \"@langchain/core/messages\";\n\n    const MessagesState = z.object({\n      messages: z\n        .array(z.custom<BaseMessage>())\n        .register(registry, MessagesZodMeta),\n      llmCalls: z.number().optional(),\n    });\n    ```\n\n    ## 3. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    ```typescript  theme={null}\n    import { SystemMessage } from \"@langchain/core/messages\";\n    async function llmCall(state: z.infer<typeof MessagesState>) {\n      return {\n        messages: await modelWithTools.invoke([\n          new SystemMessage(\n            \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n          ),\n          ...state.messages,\n        ]),\n        llmCalls: (state.llmCalls ?? 0) + 1,\n      };\n    }\n    ```\n\n    ## 4. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```typescript  theme={null}\n    import { isAIMessage, ToolMessage } from \"@langchain/core/messages\";\n    async function toolNode(state: z.infer<typeof MessagesState>) {\n      const lastMessage = state.messages.at(-1);\n\n      if (lastMessage == null || !isAIMessage(lastMessage)) {\n        return { messages: [] };\n      }\n\n      const result: ToolMessage[] = [];\n      for (const toolCall of lastMessage.tool_calls ?? []) {\n        const tool = toolsByName[toolCall.name];\n        const observation = await tool.invoke(toolCall);\n        result.push(observation);\n      }\n\n      return { messages: result };\n    }\n    ```\n\n    ## 5. Define end logic\n\n    The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.\n\n    ```typescript  theme={null}\n    async function shouldContinue(state: z.infer<typeof MessagesState>) {\n      const lastMessage = state.messages.at(-1);\n      if (lastMessage == null || !isAIMessage(lastMessage)) return END;\n\n      // If the LLM makes a tool call, then perform an action\n      if (lastMessage.tool_calls?.length) {\n        return \"toolNode\";\n      }\n\n      // Otherwise, we stop (reply to the user)\n      return END;\n    }\n    ```\n\n    ## 6. Build and compile the agent\n\n    The agent is built using the [`StateGraph`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html) class and compiled using the [`compile`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html#compile) method.\n\n    ```typescript  theme={null}\n    const agent = new StateGraph(MessagesState)\n      .addNode(\"llmCall\", llmCall)\n      .addNode(\"toolNode\", toolNode)\n      .addEdge(START, \"llmCall\")\n      .addConditionalEdges(\"llmCall\", shouldContinue, [\"toolNode\", END])\n      .addEdge(\"toolNode\", \"llmCall\")\n      .compile();\n\n    // Invoke\n    import { HumanMessage } from \"@langchain/core/messages\";\n    const result = await agent.invoke({\n      messages: [new HumanMessage(\"Add 3 and 4.\")],\n    });\n\n    for (const message of result.messages) {\n      console.log(`[${message.getType()}]: ${message.text}`);\n    }\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Graph API.\n\n    <Accordion title=\"Full code example\">\n      ```typescript  theme={null}\n      // Step 1: Define tools and model\n\n      import { ChatAnthropic } from \"@langchain/anthropic\";\n      import { tool } from \"@langchain/core/tools\";\n      import * as z from \"zod\";\n\n      const model = new ChatAnthropic({\n        model: \"claude-sonnet-4-5-20250929\",\n        temperature: 0,\n      });\n\n      // Define tools\n      const add = tool(({ a, b }) => a + b, {\n        name: \"add\",\n        description: \"Add two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const multiply = tool(({ a, b }) => a * b, {\n        name: \"multiply\",\n        description: \"Multiply two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const divide = tool(({ a, b }) => a / b, {\n        name: \"divide\",\n        description: \"Divide two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      // Augment the LLM with tools\n      const toolsByName = {\n        [add.name]: add,\n        [multiply.name]: multiply,\n        [divide.name]: divide,\n      };\n      const tools = Object.values(toolsByName);\n      const modelWithTools = model.bindTools(tools);\n\n      // Step 2: Define state\n\n      import { StateGraph, START, END } from \"@langchain/langgraph\";\n      import { MessagesZodMeta } from \"@langchain/langgraph\";\n      import { registry } from \"@langchain/langgraph/zod\";\n      import { type BaseMessage } from \"@langchain/core/messages\";\n\n      const MessagesState = z.object({\n        messages: z\n          .array(z.custom<BaseMessage>())\n          .register(registry, MessagesZodMeta),\n        llmCalls: z.number().optional(),\n      });\n\n      // Step 3: Define model node\n\n      import { SystemMessage } from \"@langchain/core/messages\";\n      async function llmCall(state: z.infer<typeof MessagesState>) {\n        return {\n          messages: await modelWithTools.invoke([\n            new SystemMessage(\n              \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n            ),\n            ...state.messages,\n          ]),\n          llmCalls: (state.llmCalls ?? 0) + 1,\n        };\n      }\n\n      // Step 4: Define tool node\n\n      import { isAIMessage, ToolMessage } from \"@langchain/core/messages\";\n      async function toolNode(state: z.infer<typeof MessagesState>) {\n        const lastMessage = state.messages.at(-1);\n\n        if (lastMessage == null || !isAIMessage(lastMessage)) {\n          return { messages: [] };\n        }\n\n        const result: ToolMessage[] = [];\n        for (const toolCall of lastMessage.tool_calls ?? []) {\n          const tool = toolsByName[toolCall.name];\n          const observation = await tool.invoke(toolCall);\n          result.push(observation);\n        }\n\n        return { messages: result };\n      }\n\n      // Step 5: Define logic to determine whether to end\n\n      async function shouldContinue(state: z.infer<typeof MessagesState>) {\n        const lastMessage = state.messages.at(-1);\n        if (lastMessage == null || !isAIMessage(lastMessage)) return END;\n\n        // If the LLM makes a tool call, then perform an action\n        if (lastMessage.tool_calls?.length) {\n          return \"toolNode\";\n        }\n\n        // Otherwise, we stop (reply to the user)\n        return END;\n      }\n\n      // Step 6: Build and compile the agent\n\n      const agent = new StateGraph(MessagesState)\n        .addNode(\"llmCall\", llmCall)\n        .addNode(\"toolNode\", toolNode)\n        .addEdge(START, \"llmCall\")\n        .addConditionalEdges(\"llmCall\", shouldContinue, [\"toolNode\", END])\n        .addEdge(\"toolNode\", \"llmCall\")\n        .compile();\n\n      // Invoke\n      import { HumanMessage } from \"@langchain/core/messages\";\n      const result = await agent.invoke({\n        messages: [new HumanMessage(\"Add 3 and 4.\")],\n      });\n\n      for (const message of result.messages) {\n        console.log(`[${message.getType()}]: ${message.text}`);\n      }\n      ```\n    </Accordion>\n  </Tab>\n\n  <Tab title=\"Use the Functional API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```typescript  theme={null}\n    import { ChatAnthropic } from \"@langchain/anthropic\";\n    import { tool } from \"@langchain/core/tools\";\n    import * as z from \"zod\";\n\n    const model = new ChatAnthropic({\n      model: \"claude-sonnet-4-5-20250929\",\n      temperature: 0,\n    });\n\n    // Define tools\n    const add = tool(({ a, b }) => a + b, {\n      name: \"add\",\n      description: \"Add two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const multiply = tool(({ a, b }) => a * b, {\n      name: \"multiply\",\n      description: \"Multiply two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const divide = tool(({ a, b }) => a / b, {\n      name: \"divide\",\n      description: \"Divide two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    // Augment the LLM with tools\n    const toolsByName = {\n      [add.name]: add,\n      [multiply.name]: multiply,\n      [divide.name]: divide,\n    };\n    const tools = Object.values(toolsByName);\n    const modelWithTools = model.bindTools(tools);\n\n    ```\n\n    ## 2. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    ```typescript  theme={null}\n    import { task, entrypoint } from \"@langchain/langgraph\";\n    import { SystemMessage } from \"@langchain/core/messages\";\n    const callLlm = task({ name: \"callLlm\" }, async (messages: BaseMessage[]) => {\n      return modelWithTools.invoke([\n        new SystemMessage(\n          \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n        ),\n        ...messages,\n      ]);\n    });\n    ```\n\n    ## 3. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```typescript  theme={null}\n    import type { ToolCall } from \"@langchain/core/messages/tool\";\n    const callTool = task({ name: \"callTool\" }, async (toolCall: ToolCall) => {\n      const tool = toolsByName[toolCall.name];\n      return tool.invoke(toolCall);\n    });\n    ```\n\n    ## 4. Define agent\n\n    ```typescript  theme={null}\n    import { addMessages } from \"@langchain/langgraph\";\n    import { type BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n\n    const agent = entrypoint({ name: \"agent\" }, async (messages: BaseMessage[]) => {\n      let modelResponse = await callLlm(messages);\n\n      while (true) {\n        if (!modelResponse.tool_calls?.length) {\n          break;\n        }\n\n        // Execute tools\n        const toolResults = await Promise.all(\n          modelResponse.tool_calls.map((toolCall) => callTool(toolCall))\n        );\n        messages = addMessages(messages, [modelResponse, ...toolResults]);\n        modelResponse = await callLlm(messages);\n      }\n\n      return messages;\n    });\n\n    // Invoke\n    import { HumanMessage } from \"@langchain/core/messages\";\n\n    const result = await agent.invoke([new HumanMessage(\"Add 3 and 4.\")]);\n\n    for (const message of result) {\n      console.log(`[${message.getType()}]: ${message.text}`);\n    }\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Functional API.\n\n    <Accordion title=\"Full code example\" icon=\"code\">\n      ```typescript  theme={null}\n      // Step 1: Define tools and model\n\n      import { ChatAnthropic } from \"@langchain/anthropic\";\n      import { tool } from \"@langchain/core/tools\";\n      import * as z from \"zod\";\n\n      const model = new ChatAnthropic({\n        model: \"claude-sonnet-4-5-20250929\",\n        temperature: 0,\n      });\n\n      // Define tools\n      const add = tool(({ a, b }) => a + b, {\n        name: \"add\",\n        description: \"Add two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const multiply = tool(({ a, b }) => a * b, {\n        name: \"multiply\",\n        description: \"Multiply two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const divide = tool(({ a, b }) => a / b, {\n        name: \"divide\",\n        description: \"Divide two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      // Augment the LLM with tools\n      const toolsByName = {\n        [add.name]: add,\n        [multiply.name]: multiply,\n        [divide.name]: divide,\n      };\n      const tools = Object.values(toolsByName);\n      const modelWithTools = model.bindTools(tools);\n\n      // Step 2: Define model node\n\n      import { task, entrypoint } from \"@langchain/langgraph\";\n      import { SystemMessage } from \"@langchain/core/messages\";\n      const callLlm = task({ name: \"callLlm\" }, async (messages: BaseMessage[]) => {\n        return modelWithTools.invoke([\n          new SystemMessage(\n            \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n          ),\n          ...messages,\n        ]);\n      });\n\n      // Step 3: Define tool node\n\n      import type { ToolCall } from \"@langchain/core/messages/tool\";\n      const callTool = task({ name: \"callTool\" }, async (toolCall: ToolCall) => {\n        const tool = toolsByName[toolCall.name];\n        return tool.invoke(toolCall);\n      });\n\n      // Step 4: Define agent\n      import { addMessages } from \"@langchain/langgraph\";\n      import { type BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n      const agent = entrypoint({ name: \"agent\" }, async (messages: BaseMessage[]) => {\n        let modelResponse = await callLlm(messages);\n\n        while (true) {\n          if (!modelResponse.tool_calls?.length) {\n            break;\n          }\n\n          // Execute tools\n          const toolResults = await Promise.all(\n            modelResponse.tool_calls.map((toolCall) => callTool(toolCall))\n          );\n          messages = addMessages(messages, [modelResponse, ...toolResults]);\n          modelResponse = await callLlm(messages);\n        }\n\n        return messages;\n      });\n\n      // Invoke\n      import { HumanMessage } from \"@langchain/core/messages\";\n      const result = await agent.invoke([new HumanMessage(\"Add 3 and 4.\")]);\n\n      for (const message of result) {\n        console.log(`[${message.getType()}]: ${message.text}`);\n      }\n      ```\n    </Accordion>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/quickstart.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Memory\nLink: https://docs.langchain.comoss/python/langgraph/add-memory\nContent: Use semantic search\nEnable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity. from langchain.embeddings import init_embeddings\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.store.base import BaseStore\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.graph import START, MessagesState, StateGraph\n\nmodel = init_chat_model(\"gpt-4o-mini\")\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\ndef chat(state, *, store: BaseStore):\n    # Search based on user's last message\n    items = store.search(\n        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n    )\n    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n    response = model.invoke(\n        [\n            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n            *state[\"messages\"],\n        ]\n    )\n    return {\"messages\": [response]}\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(chat)\nbuilder.add_edge(START, \"chat\")\ngraph = builder.compile(store=store)\n\nfor message, metadata in graph.stream(\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n    stream_mode=\"messages\",\n):\n    print(message.content, end=\"\")\n\nUse semantic search\nEnable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity.\n\nUse semantic search\nEnable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity. from langchain.embeddings import init_embeddings\nfrom langgraph.store.memory import InMemoryStore\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\nitems = store.search(\n    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n)\n\n---\n\nResult 4:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/python/langgraph/overview\nContent: Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools).\n\nWe will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\ndef mock_llm(state: MessagesState):\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n\ngraph = StateGraph(MessagesState)\ngraph.add_node(mock_llm)\ngraph.add_edge(START, \"mock_llm\")\ngraph.add_edge(\"mock_llm\", END)\ngraph = graph.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/python/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/python/langgraph/quickstart\nContent: This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\n\n* [Use the Graph API](#use-the-graph-api) if you prefer to define your agent as a graph of nodes and edges.\n* [Use the Functional API](#use-the-functional-api) if you prefer to define your agent as a single function.\n\nFor conceptual information, see [Graph API overview](/oss/python/langgraph/graph-api) and [Functional API overview](/oss/python/langgraph/functional-api).\n\n<Info>\n  For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.\n</Info>\n\n<Tabs>\n  <Tab title=\"Use the Graph API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```python  theme={null}\n    from langchain.tools import tool\n    from langchain.chat_models import init_chat_model\n\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0\n    )\n\n\n    # Define tools\n    @tool\n    def multiply(a: int, b: int) -> int:\n        \"\"\"Multiply `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a * b\n\n\n    @tool\n    def add(a: int, b: int) -> int:\n        \"\"\"Adds `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a + b\n\n\n    @tool\n    def divide(a: int, b: int) -> float:\n        \"\"\"Divide `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a / b\n\n\n    # Augment the LLM with tools\n    tools = [add, multiply, divide]\n    tools_by_name = {tool.name: tool for tool in tools}\n    model_with_tools = model.bind_tools(tools)\n    ```\n\n    ## 2. Define state\n\n    The graph's state is used to store the messages and the number of LLM calls.\n\n    <Tip>\n      State in LangGraph persists throughout the agent's execution.\n\n      The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.\n    </Tip>\n\n    ```python  theme={null}\n    from langchain.messages import AnyMessage\n    from typing_extensions import TypedDict, Annotated\n    import operator\n\n\n    class MessagesState(TypedDict):\n        messages: Annotated[list[AnyMessage], operator.add]\n        llm_calls: int\n    ```\n\n    ## 3. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    ```python  theme={null}\n    from langchain.messages import SystemMessage\n\n\n    def llm_call(state: dict):\n        \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n        return {\n            \"messages\": [\n                model_with_tools.invoke(\n                    [\n                        SystemMessage(\n                            content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                        )\n                    ]\n                    + state[\"messages\"]\n                )\n            ],\n            \"llm_calls\": state.get('llm_calls', 0) + 1\n        }\n    ```\n\n    ## 4. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```python  theme={null}\n    from langchain.messages import ToolMessage\n\n\n    def tool_node(state: dict):\n        \"\"\"Performs the tool call\"\"\"\n\n        result = []\n        for tool_call in state[\"messages\"][-1].tool_calls:\n            tool = tools_by_name[tool_call[\"name\"]]\n            observation = tool.invoke(tool_call[\"args\"])\n            result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n        return {\"messages\": result}\n    ```\n\n    ## 5. Define end logic\n\n    The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.\n\n    ```python  theme={null}\n    from typing import Literal\n    from langgraph.graph import StateGraph, START, END\n\n\n    def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n        \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n\n        # If the LLM makes a tool call, then perform an action\n        if last_message.tool_calls:\n            return \"tool_node\"\n\n        # Otherwise, we stop (reply to the user)\n        return END\n    ```\n\n    ## 6. Build and compile the agent\n\n    The agent is built using the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) class and compiled using the [`compile`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.compile) method.\n\n    ```python  theme={null}\n    # Build workflow\n    agent_builder = StateGraph(MessagesState)\n\n    # Add nodes\n    agent_builder.add_node(\"llm_call\", llm_call)\n    agent_builder.add_node(\"tool_node\", tool_node)\n\n    # Add edges to connect nodes\n    agent_builder.add_edge(START, \"llm_call\")\n    agent_builder.add_conditional_edges(\n        \"llm_call\",\n        should_continue,\n        [\"tool_node\", END]\n    )\n    agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n    # Compile the agent\n    agent = agent_builder.compile()\n\n    # Show the agent\n    from IPython.display import Image, display\n    display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n    # Invoke\n    from langchain.messages import HumanMessage\n    messages = [HumanMessage(content=\"Add 3 and 4.\")]\n    messages = agent.invoke({\"messages\": messages})\n    for m in messages[\"messages\"]:\n        m.pretty_print()\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Graph API.\n\n    <Accordion title=\"Full code example\">\n      ```python  theme={null}\n      # Step 1: Define tools and model\n\n      from langchain.tools import tool\n      from langchain.chat_models import init_chat_model\n\n\n      model = init_chat_model(\n          \"claude-sonnet-4-5-20250929\",\n          temperature=0\n      )\n\n\n      # Define tools\n      @tool\n      def multiply(a: int, b: int) -> int:\n          \"\"\"Multiply `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a * b\n\n\n      @tool\n      def add(a: int, b: int) -> int:\n          \"\"\"Adds `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a + b\n\n\n      @tool\n      def divide(a: int, b: int) -> float:\n          \"\"\"Divide `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a / b\n\n\n      # Augment the LLM with tools\n      tools = [add, multiply, divide]\n      tools_by_name = {tool.name: tool for tool in tools}\n      model_with_tools = model.bind_tools(tools)\n\n      # Step 2: Define state\n\n      from langchain.messages import AnyMessage\n      from typing_extensions import TypedDict, Annotated\n      import operator\n\n\n      class MessagesState(TypedDict):\n          messages: Annotated[list[AnyMessage], operator.add]\n          llm_calls: int\n\n      # Step 3: Define model node\n      from langchain.messages import SystemMessage\n\n\n      def llm_call(state: dict):\n          \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n          return {\n              \"messages\": [\n                  model_with_tools.invoke(\n                      [\n                          SystemMessage(\n                              content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                          )\n                      ]\n                      + state[\"messages\"]\n                  )\n              ],\n              \"llm_calls\": state.get('llm_calls', 0) + 1\n          }\n\n\n      # Step 4: Define tool node\n\n      from langchain.messages import ToolMessage\n\n\n      def tool_node(state: dict):\n          \"\"\"Performs the tool call\"\"\"\n\n          result = []\n          for tool_call in state[\"messages\"][-1].tool_calls:\n              tool = tools_by_name[tool_call[\"name\"]]\n              observation = tool.invoke(tool_call[\"args\"])\n              result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n          return {\"messages\": result}\n\n      # Step 5: Define logic to determine whether to end\n\n      from typing import Literal\n      from langgraph.graph import StateGraph, START, END\n\n\n      # Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n      def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n          \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n          messages = state[\"messages\"]\n          last_message = messages[-1]\n\n          # If the LLM makes a tool call, then perform an action\n          if last_message.tool_calls:\n              return \"tool_node\"\n\n          # Otherwise, we stop (reply to the user)\n          return END\n\n      # Step 6: Build agent\n\n      # Build workflow\n      agent_builder = StateGraph(MessagesState)\n\n      # Add nodes\n      agent_builder.add_node(\"llm_call\", llm_call)\n      agent_builder.add_node(\"tool_node\", tool_node)\n\n      # Add edges to connect nodes\n      agent_builder.add_edge(START, \"llm_call\")\n      agent_builder.add_conditional_edges(\n          \"llm_call\",\n          should_continue,\n          [\"tool_node\", END]\n      )\n      agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n      # Compile the agent\n      agent = agent_builder.compile()\n\n\n      from IPython.display import Image, display\n      # Show the agent\n      display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n      # Invoke\n      from langchain.messages import HumanMessage\n      messages = [HumanMessage(content=\"Add 3 and 4.\")]\n      messages = agent.invoke({\"messages\": messages})\n      for m in messages[\"messages\"]:\n          m.pretty_print()\n\n      ```\n    </Accordion>\n  </Tab>\n\n  <Tab title=\"Use the Functional API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```python  theme={null}\n    from langchain.tools import tool\n    from langchain.chat_models import init_chat_model\n\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0\n    )\n\n\n    # Define tools\n    @tool\n    def multiply(a: int, b: int) -> int:\n        \"\"\"Multiply `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a * b\n\n\n    @tool\n    def add(a: int, b: int) -> int:\n        \"\"\"Adds `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a + b\n\n\n    @tool\n    def divide(a: int, b: int) -> float:\n        \"\"\"Divide `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a / b\n\n\n    # Augment the LLM with tools\n    tools = [add, multiply, divide]\n    tools_by_name = {tool.name: tool for tool in tools}\n    model_with_tools = model.bind_tools(tools)\n\n    from langgraph.graph import add_messages\n    from langchain.messages import (\n        SystemMessage,\n        HumanMessage,\n        ToolCall,\n    )\n    from langchain_core.messages import BaseMessage\n    from langgraph.func import entrypoint, task\n    ```\n\n    ## 2. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    <Tip>\n      The [`@task`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.task) decorator marks a function as a task that can be executed as part of the agent. Tasks can be called synchronously or asynchronously within your entrypoint function.\n    </Tip>\n\n    ```python  theme={null}\n    @task\n    def call_llm(messages: list[BaseMessage]):\n        \"\"\"LLM decides whether to call a tool or not\"\"\"\n        return model_with_tools.invoke(\n            [\n                SystemMessage(\n                    content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                )\n            ]\n            + messages\n        )\n    ```\n\n    ## 3. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```python  theme={null}\n    @task\n    def call_tool(tool_call: ToolCall):\n        \"\"\"Performs the tool call\"\"\"\n        tool = tools_by_name[tool_call[\"name\"]]\n        return tool.invoke(tool_call)\n\n    ```\n\n    ## 4. Define agent\n\n    The agent is built using the [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) function.\n\n    <Note>\n      In the Functional API, instead of defining nodes and edges explicitly, you write standard control flow logic (loops, conditionals) within a single function.\n    </Note>\n\n    ```python  theme={null}\n    @entrypoint()\n    def agent(messages: list[BaseMessage]):\n        model_response = call_llm(messages).result()\n\n        while True:\n            if not model_response.tool_calls:\n                break\n\n            # Execute tools\n            tool_result_futures = [\n                call_tool(tool_call) for tool_call in model_response.tool_calls\n            ]\n            tool_results = [fut.result() for fut in tool_result_futures]\n            messages = add_messages(messages, [model_response, *tool_results])\n            model_response = call_llm(messages).result()\n\n        messages = add_messages(messages, model_response)\n        return messages\n\n    # Invoke\n    messages = [HumanMessage(content=\"Add 3 and 4.\")]\n    for chunk in agent.stream(messages, stream_mode=\"updates\"):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Functional API.\n\n    <Accordion title=\"Full code example\" icon=\"code\">\n      ```python  theme={null}\n      # Step 1: Define tools and model\n\n      from langchain.tools import tool\n      from langchain.chat_models import init_chat_model\n\n\n      model = init_chat_model(\n          \"claude-sonnet-4-5-20250929\",\n          temperature=0\n      )\n\n\n      # Define tools\n      @tool\n      def multiply(a: int, b: int) -> int:\n          \"\"\"Multiply `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a * b\n\n\n      @tool\n      def add(a: int, b: int) -> int:\n          \"\"\"Adds `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a + b\n\n\n      @tool\n      def divide(a: int, b: int) -> float:\n          \"\"\"Divide `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a / b\n\n\n      # Augment the LLM with tools\n      tools = [add, multiply, divide]\n      tools_by_name = {tool.name: tool for tool in tools}\n      model_with_tools = model.bind_tools(tools)\n\n      from langgraph.graph import add_messages\n      from langchain.messages import (\n          SystemMessage,\n          HumanMessage,\n          ToolCall,\n      )\n      from langchain_core.messages import BaseMessage\n      from langgraph.func import entrypoint, task\n\n\n      # Step 2: Define model node\n\n      @task\n      def call_llm(messages: list[BaseMessage]):\n          \"\"\"LLM decides whether to call a tool or not\"\"\"\n          return model_with_tools.invoke(\n              [\n                  SystemMessage(\n                      content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                  )\n              ]\n              + messages\n          )\n\n\n      # Step 3: Define tool node\n\n      @task\n      def call_tool(tool_call: ToolCall):\n          \"\"\"Performs the tool call\"\"\"\n          tool = tools_by_name[tool_call[\"name\"]]\n          return tool.invoke(tool_call)\n\n\n      # Step 4: Define agent\n\n      @entrypoint()\n      def agent(messages: list[BaseMessage]):\n          model_response = call_llm(messages).result()\n\n          while True:\n              if not model_response.tool_calls:\n                  break\n\n              # Execute tools\n              tool_result_futures = [\n                  call_tool(tool_call) for tool_call in model_response.tool_calls\n              ]\n              tool_results = [fut.result() for fut in tool_result_futures]\n              messages = add_messages(messages, [model_response, *tool_results])\n              model_response = call_llm(messages).result()\n\n          messages = add_messages(messages, model_response)\n          return messages\n\n      # Invoke\n      messages = [HumanMessage(content=\"Add 3 and 4.\")]\n      for chunk in agent.stream(messages, stream_mode=\"updates\"):\n          print(chunk)\n          print(\"\\n\")\n      ```\n    </Accordion>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/quickstart.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_70979854",
    "id": "2e35d303-5d90-4e49-8e1d-396096e732c1",
    "content": "Result 1:\nTitle: Observability in Studio\nLink: https://docs.langchain.comlangsmith/observability-studio\nContent: LangSmith [Studio](/langsmith/studio) provides tools to inspect, debug, and improve your app beyond execution. By working with traces, datasets, and prompts, you can see how your application behaves in detail, measure its performance, and refine its outputs:\n\n* [Iterate on prompts](#iterate-on-prompts): Modify prompts inside graph nodes directly or with the LangSmith playground.\n* [Run experiments over a dataset](#run-experiments-over-a-dataset): Execute your assistant over a LangSmith dataset to score and compare results.\n* [Debug LangSmith traces](#debug-langsmith-traces): Import traced runs into Studio and optionally clone them into your local agent.\n* [Add a node to a dataset](#add-node-to-dataset): Turn parts of thread history into dataset examples for evaluation or further analysis.\n\n## Iterate on prompts\n\nStudio supports the following methods for modifying prompts in your graph:\n\n* [Direct node editing](#direct-node-editing)\n* [Playground interface](#playground)\n\n### Direct node editing\n\nStudio allows you to edit prompts used inside individual nodes, directly from the graph interface.\n\n### Graph configuration\n\nDefine your [configuration](/oss/python/langgraph/use-graph-api#add-runtime-configuration) to specify prompt fields and their associated nodes using `langgraph_nodes` and `langgraph_type` keys.\n\n#### `langgraph_nodes`\n\n* **Description**: Specifies which nodes of the graph a configuration field is associated with.\n* **Value Type**: Array of strings, where each string is the name of a node in your graph.\n* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata[\"json_schema_extra\"]` dictionary for dataclasses.\n* **Example**:\n  ```python  theme={null}\n  system_prompt: str = Field(\n      default=\"You are a helpful AI assistant.\",\n      json_schema_extra={\"langgraph_nodes\": [\"call_model\", \"other_node\"]},\n  )\n  ```\n\n#### `langgraph_type`\n\n* **Description**: Specifies the type of configuration field, which determines how it's handled in the UI.\n* **Value Type**: String\n* **Supported Values**:\n  * `\"prompt\"`: Indicates the field contains prompt text that should be treated specially in the UI.\n* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata[\"json_schema_extra\"]` dictionary for dataclasses.\n* **Example**:\n  ```python  theme={null}\n  system_prompt: str = Field(\n      default=\"You are a helpful AI assistant.\",\n      json_schema_extra={\n          \"langgraph_nodes\": [\"call_model\"],\n          \"langgraph_type\": \"prompt\",\n      },\n  )\n  ```\n\n<Accordion title=\"Full example configuration\">\n  ```python  theme={null}\n  ## Using Pydantic\n  from pydantic import BaseModel, Field\n  from typing import Annotated, Literal\n\n  class Configuration(BaseModel):\n      \"\"\"The configuration for the agent.\"\"\"\n\n      system_prompt: str = Field(\n          default=\"You are a helpful AI assistant.\",\n          description=\"The system prompt to use for the agent's interactions. \"\n          \"This prompt sets the context and behavior for the agent.\",\n          json_schema_extra={\n              \"langgraph_nodes\": [\"call_model\"],\n              \"langgraph_type\": \"prompt\",\n          },\n      )\n\n      model: Annotated[\n          Literal[\n              \"anthropic/claude-sonnet-4-5-20250929\",\n              \"anthropic/claude-haiku-4-5-20251001\",\n              \"openai/o1\",\n              \"openai/gpt-4o-mini\",\n              \"openai/o1-mini\",\n              \"openai/o3-mini\",\n          ],\n          {\"__template_metadata__\": {\"kind\": \"llm\"}},\n      ] = Field(\n          default=\"openai/gpt-4o-mini\",\n          description=\"The name of the language model to use for the agent's main interactions. \"\n          \"Should be in the form: provider/model-name.\",\n          json_schema_extra={\"langgraph_nodes\": [\"call_model\"]},\n      )\n\n  ## Using Dataclasses\n  from dataclasses import dataclass, field\n\n  @dataclass(kw_only=True)\n  class Configuration:\n      \"\"\"The configuration for the agent.\"\"\"\n\n      system_prompt: str = field(\n          default=\"You are a helpful AI assistant.\",\n          metadata={\n              \"description\": \"The system prompt to use for the agent's interactions. \"\n              \"This prompt sets the context and behavior for the agent.\",\n              \"json_schema_extra\": {\"langgraph_nodes\": [\"call_model\"]},\n          },\n      )\n\n      model: Annotated[str, {\"__template_metadata__\": {\"kind\": \"llm\"}}] = field(\n          default=\"anthropic/claude-3-5-sonnet-20240620\",\n          metadata={\n              \"description\": \"The name of the language model to use for the agent's main interactions. \"\n              \"Should be in the form: provider/model-name.\",\n              \"json_schema_extra\": {\"langgraph_nodes\": [\"call_model\"]},\n          },\n      )\n\n  ```\n</Accordion>\n\n#### Editing prompts in the UI\n\n1. Locate the gear icon on nodes with associated configuration fields.\n2. Click to open the configuration modal.\n3. Edit the values.\n4. Save to update the current assistant version or create a new one.\n\n### Playground\n\nThe [playground](/langsmith/create-a-prompt) interface allows testing individual LLM calls without running the full graph:\n\n1. Select a thread.\n2. Click **View LLM Runs** on a node. This lists all the LLM calls (if any) made inside the node.\n3. Select an LLM run to open in the playground.\n4. Modify prompts and test different model and tool settings.\n5. Copy updated prompts back to your graph.\n\n## Run experiments over a dataset\n\nStudio lets you run [evaluations](/langsmith/evaluation-concepts) by executing your assistant against a predefined LangSmith [dataset](/langsmith/evaluation-concepts#datasets). This allows you to test performance across a variety of inputs, compare outputs to reference answers, and score results with configured [evaluators](/langsmith/evaluation-concepts#evaluators).\n\nThis guide shows you how to run a full end-to-end experiment directly from Studio.\n\n### Prerequisites\n\nBefore running an experiment, ensure you have the following:\n\n* **A LangSmith dataset**: Your dataset should contain the inputs you want to test and optionally, reference outputs for comparison. The schema for the inputs must match the required input schema for the assistant. For more information on schemas, see [here](/oss/python/langgraph/use-graph-api#schema). For more on creating datasets, refer to [How to Manage Datasets](/langsmith/manage-datasets-in-application#set-up-your-dataset).\n* **(Optional) Evaluators**: You can attach evaluators (e.g., LLM-as-a-Judge, heuristics, or custom functions) to your dataset in LangSmith. These will run automatically after the graph has processed all inputs.\n* **A running application**: The experiment can be run against:\n  * An application deployed on [LangSmith](/langsmith/deployments).\n  * A locally running application started via the [langgraph-cli](/langsmith/local-server).\n\n<Note>\n  Studio experiments follow the same [data retention](/langsmith/administration-overview#data-retention) rules as other experiments. By default, traces have base tier retention (14 days). However, traces will automatically upgrade to extended tier retention (400 days) if feedback is added to them. Feedback can be added in one of two ways:\n\n  * The [dataset has evaluators configured](/langsmith/bind-evaluator-to-dataset).\n  * [Feedback](/langsmith/observability-concepts#feedback) is manually added to a trace.\n\n  This auto-upgrade increases both the retention period and the cost of the trace. For more details, refer to [Data retention auto-upgrades](/langsmith/administration-overview#how-it-works).\n</Note>\n\n### Experiment setup\n\n1. Launch the experiment. Click the **Run experiment** button in the top right corner of the Studio page.\n2. Select your dataset. In the modal that appears, select the dataset (or a specific dataset split) to use for the experiment and click **Start**.\n3. Monitor the progress. All of the inputs in the dataset will now be run against the active assistant. Monitor the experiment's progress via the badge in the top right corner.\n4. You can continue to work in Studio while the experiment runs in the background. Click the arrow icon button at any time to navigate to LangSmith and view the detailed experiment results.\n\n## Debug LangSmith traces\n\nThis guide explains how to open LangSmith traces in Studio for interactive investigation and debugging.\n\n### Open deployed threads\n\n1. Open the LangSmith trace, selecting the root run.\n2. Click **Run in Studio**.\n\nThis will open Studio connected to the associated deployment with the trace's parent thread selected.\n\n### Testing local agents with remote traces\n\nThis section explains how to test a local agent against remote traces from LangSmith. This enables you to use production traces as input for local testing, allowing you to debug and verify agent modifications in your development environment.\n\n#### Prerequisites\n\n* A LangSmith traced thread\n* A [locally running agent](/langsmith/local-server#local-development-server).\n\n<Info>\n  **Local agent requirements**\n\n  * langgraph>=0.3.18\n  * langgraph-api>=0.0.32\n  * Contains the same set of nodes present in the remote trace\n</Info>\n\n#### Clone thread\n\n1. Open the LangSmith trace, selecting the root run.\n2. Click the dropdown next to **Run in Studio**.\n3. Enter your local agent's URL.\n4. Select **Clone thread locally**.\n5. If multiple graphs exist, select the target graph.\n\nA new thread will be created in your local agent with the thread history inferred and copied from the remote thread, and you will be navigated to Studio for your locally running application.\n\n## Add node to dataset\n\nAdd [examples](/langsmith/evaluation-concepts#examples) to [LangSmith datasets](/langsmith/manage-datasets) from nodes in the thread log. This is useful to evaluate individual steps of the agent.\n\n1. Select a thread.\n2. Click **Add to Dataset**.\n3. Select nodes whose input/output you want to add to a dataset.\n4. For each selected node, select the target dataset to create the example in. By default a dataset for the specific assistant and node will be selected. If this dataset does not yet exist, it will be created.\n5. Edit the example's input/output as needed before adding it to the dataset.\n6. Select **Add to dataset** at the bottom of the page to add all selected nodes to their respective datasets.\n\nFor more details, refer to [How to evaluate an application's intermediate steps](/langsmith/evaluate-on-intermediate-steps).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-studio.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Graph API overview\nLink: https://docs.langchain.comoss/javascript/langgraph/graph-api\nContent: Nodes\nIn LangGraph, nodes are typically functions (sync or async) that accept the following arguments: state \u2013 The state of the graph config \u2013 A RunnableConfig object that contains configuration information like thread_id and tracing information like tags You can add nodes to a graph using the addNode method. Behind the scenes, functions are converted to RunnableLambda , which add batch and async support to your function, along with native tracing and debugging. If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name. builder.addNode(myNode);\n// You can then create edges to/from this node by referencing it as `\"myNode\"`\n\nSTART Node\nThe START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. import { START } from \"@langchain/langgraph\";\n\ngraph.addEdge(START, \"nodeA\");\n\nNodes\nIn LangGraph, nodes are typically functions (sync or async) that accept the following arguments: state \u2013 The state of the graph config \u2013 A RunnableConfig object that contains configuration information like thread_id and tracing information like tags You can add nodes to a graph using the addNode method. import { StateGraph } from \"@langchain/langgraph\";\nimport { RunnableConfig } from \"@langchain/core/runnables\";\nimport * as z from \"zod\";\n\nconst State = z.object({\n  input: z.string(),\n  results: z.string(),\n});\n\nconst builder = new StateGraph(State);\n  .addNode(\"myNode\", (state, config) => {\n    console.log(\"In node: \", config?.configurable?.user_id);\n    return { results: `Hello, ${state.input}!` };\n  })\n  addNode(\"otherNode\", (state) => {\n    return state;\n  })\n  ...\n\n---\n\nResult 3:\nTitle: Graph API overview\nLink: https://docs.langchain.comoss/python/langgraph/graph-api\nContent: Nodes\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments: state \u2013 The state of the graph config \u2013 A RunnableConfig object that contains configuration information like thread_id and tracing information like tags runtime \u2013 A Runtime object that contains runtime context and other information like store and stream_writer Similar to NetworkX , you add these nodes to a graph using the add_node method: Behind the scenes, functions are converted to RunnableLambda , which add batch and async support to your function, along with native tracing and debugging. If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\n\nNodes\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments: state \u2013 The state of the graph config \u2013 A RunnableConfig object that contains configuration information like thread_id and tracing information like tags runtime \u2013 A Runtime object that contains runtime context and other information like store and stream_writer Similar to NetworkX , you add these nodes to a graph using the add_node method: Behind the scenes, functions are converted to RunnableLambda , which add batch and async support to your function, along with native tracing and debugging. If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\n\nNodes\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments: state \u2013 The state of the graph config \u2013 A RunnableConfig object that contains configuration information like thread_id and tracing information like tags runtime \u2013 A Runtime object that contains runtime context and other information like store and stream_writer Similar to NetworkX , you add these nodes to a graph using the add_node method: from dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\nfrom langgraph.runtime import Runtime\n\nclass State(TypedDict):\n    input: str\n    results: str\n\n@dataclass\nclass Context:\n    user_id: str\n\nbuilder = StateGraph(State)\n\ndef plain_node(state: State):\n    return state\n\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\n    print(\"In node: \", runtime.context.user_id)\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\ndef node_with_config(state: State, config: RunnableConfig):\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\n\nbuilder.add_node(\"plain_node\", plain_node)\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\nbuilder.add_node(\"node_with_config\", node_with_config)\n...\n\n---\n\nResult 4:\nTitle: Use the graph API\nLink: https://docs.langchain.comoss/python/langgraph/use-graph-api\nContent: Add node caching\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph. To configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node: from langgraph.types import CachePolicy\n\nbuilder.add_node(\n    \"node_name\",\n    node_function,\n    cache_policy=CachePolicy(ttl=120),\n)\n\nAdd node caching\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph. To configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node: Then, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available.\n\nAdd node caching\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph. To configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node: Then, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available. from langgraph.cache.memory import InMemoryCache\n\ngraph = builder.compile(cache=InMemoryCache())\n\n---\n\nResult 5:\nTitle: Subgraphs\nLink: https://docs.langchain.comoss/python/langgraph/use-subgraphs\nContent: This guide explains the mechanics of using subgraphs. A subgraph is a [graph](/oss/python/langgraph/graph-api#graphs) that is used as a [node](/oss/python/langgraph/graph-api#nodes) in another graph.\n\nSubgraphs are useful for:\n\n* Building [multi-agent systems](/oss/python/langchain/multi-agent)\n* Re-using a set of nodes in multiple graphs\n* Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph\n\nWhen adding subgraphs, you need to define how the parent graph and the subgraph communicate:\n\n* [Invoke a graph from a node](#invoke-a-graph-from-a-node) \u2014 subgraphs are called from inside a node in the parent graph\n* [Add a graph as a node](#add-a-graph-as-a-node) \u2014 a subgraph is added directly as a node in the parent and **shares [state keys](/oss/python/langgraph/graph-api#state)** with the parent\n\n## Setup\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\n<Tip>\n  **Set up LangSmith for LangGraph development**\n  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started [here](https://docs.smith.langchain.com).\n</Tip>\n\n## Invoke a graph from a node\n\nA simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have **completely different schemas** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](/oss/python/langchain/multi-agent) system.\n\nIf that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.\n\n```python  theme={null}\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass SubgraphState(TypedDict):\n    bar: str\n\n# Subgraph\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"hi! \" + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nclass State(TypedDict):\n    foo: str\n\ndef call_subgraph(state: State):\n    # Transform the state to the subgraph state\n    subgraph_output = subgraph.invoke({\"bar\": state[\"foo\"]})  # [!code highlight]\n    # Transform response back to the parent state\n    return {\"foo\": subgraph_output[\"bar\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", call_subgraph)\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n```\n\n<Accordion title=\"Full example: different state schemas\">\n  ```python  theme={null}\n  from typing_extensions import TypedDict\n  from langgraph.graph.state import StateGraph, START\n\n  # Define subgraph\n  class SubgraphState(TypedDict):\n      # note that none of these keys are shared with the parent graph state\n      bar: str\n      baz: str\n\n  def subgraph_node_1(state: SubgraphState):\n      return {\"baz\": \"baz\"}\n\n  def subgraph_node_2(state: SubgraphState):\n      return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n\n  subgraph_builder = StateGraph(SubgraphState)\n  subgraph_builder.add_node(subgraph_node_1)\n  subgraph_builder.add_node(subgraph_node_2)\n  subgraph_builder.add_edge(START, \"subgraph_node_1\")\n  subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n  subgraph = subgraph_builder.compile()\n\n  # Define parent graph\n  class ParentState(TypedDict):\n      foo: str\n\n  def node_1(state: ParentState):\n      return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n  def node_2(state: ParentState):\n      # Transform the state to the subgraph state\n      response = subgraph.invoke({\"bar\": state[\"foo\"]})\n      # Transform response back to the parent state\n      return {\"foo\": response[\"bar\"]}\n\n\n  builder = StateGraph(ParentState)\n  builder.add_node(\"node_1\", node_1)\n  builder.add_node(\"node_2\", node_2)\n  builder.add_edge(START, \"node_1\")\n  builder.add_edge(\"node_1\", \"node_2\")\n  graph = builder.compile()\n\n  for chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n      print(chunk)\n  ```\n\n  ```\n  ((), {'node_1': {'foo': 'hi! foo'}})\n  (('node_2:577b710b-64ae-31fb-9455-6a4d4cc2b0b9',), {'subgraph_node_1': {'baz': 'baz'}})\n  (('node_2:577b710b-64ae-31fb-9455-6a4d4cc2b0b9',), {'subgraph_node_2': {'bar': 'hi! foobaz'}})\n  ((), {'node_2': {'foo': 'hi! foobaz'}})\n  ```\n</Accordion>\n\n<Accordion title=\"Full example: different state schemas (two levels of subgraphs)\">\n  This is an example with two levels of subgraphs: parent -> child -> grandchild.\n\n  ```python  theme={null}\n  # Grandchild graph\n  from typing_extensions import TypedDict\n  from langgraph.graph.state import StateGraph, START, END\n\n  class GrandChildState(TypedDict):\n      my_grandchild_key: str\n\n  def grandchild_1(state: GrandChildState) -> GrandChildState:\n      # NOTE: child or parent keys will not be accessible here\n      return {\"my_grandchild_key\": state[\"my_grandchild_key\"] + \", how are you\"}\n\n\n  grandchild = StateGraph(GrandChildState)\n  grandchild.add_node(\"grandchild_1\", grandchild_1)\n\n  grandchild.add_edge(START, \"grandchild_1\")\n  grandchild.add_edge(\"grandchild_1\", END)\n\n  grandchild_graph = grandchild.compile()\n\n  # Child graph\n  class ChildState(TypedDict):\n      my_child_key: str\n\n  def call_grandchild_graph(state: ChildState) -> ChildState:\n      # NOTE: parent or grandchild keys won't be accessible here\n      grandchild_graph_input = {\"my_grandchild_key\": state[\"my_child_key\"]}\n      grandchild_graph_output = grandchild_graph.invoke(grandchild_graph_input)\n      return {\"my_child_key\": grandchild_graph_output[\"my_grandchild_key\"] + \" today?\"}\n\n  child = StateGraph(ChildState)\n  # We're passing a function here instead of just compiled graph (`grandchild_graph`)\n  child.add_node(\"child_1\", call_grandchild_graph)\n  child.add_edge(START, \"child_1\")\n  child.add_edge(\"child_1\", END)\n  child_graph = child.compile()\n\n  # Parent graph\n  class ParentState(TypedDict):\n      my_key: str\n\n  def parent_1(state: ParentState) -> ParentState:\n      # NOTE: child or grandchild keys won't be accessible here\n      return {\"my_key\": \"hi \" + state[\"my_key\"]}\n\n  def parent_2(state: ParentState) -> ParentState:\n      return {\"my_key\": state[\"my_key\"] + \" bye!\"}\n\n  def call_child_graph(state: ParentState) -> ParentState:\n      child_graph_input = {\"my_child_key\": state[\"my_key\"]}\n      child_graph_output = child_graph.invoke(child_graph_input)\n      return {\"my_key\": child_graph_output[\"my_child_key\"]}\n\n  parent = StateGraph(ParentState)\n  parent.add_node(\"parent_1\", parent_1)\n  # We're passing a function here instead of just a compiled graph (`child_graph`)\n  parent.add_node(\"child\", call_child_graph)\n  parent.add_node(\"parent_2\", parent_2)\n\n  parent.add_edge(START, \"parent_1\")\n  parent.add_edge(\"parent_1\", \"child\")\n  parent.add_edge(\"child\", \"parent_2\")\n  parent.add_edge(\"parent_2\", END)\n\n  parent_graph = parent.compile()\n\n  for chunk in parent_graph.stream({\"my_key\": \"Bob\"}, subgraphs=True):\n      print(chunk)\n  ```\n\n  ```\n  ((), {'parent_1': {'my_key': 'hi Bob'}})\n  (('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child_1:781bb3b1-3971-84ce-810b-acf819a03f9c'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n  (('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})\n  ((), {'child': {'my_key': 'hi Bob, how are you today?'}})\n  ((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})\n  ```\n</Accordion>\n\n## Add a graph as a node\n\nWhen the parent graph and subgraph can communicate over a shared state key (channel) in the [schema](/oss/python/langgraph/graph-api#state), you can add a graph as a [node](/oss/python/langgraph/graph-api#nodes) in another graph. For example, in [multi-agent](/oss/python/langchain/multi-agent) systems, the agents often communicate over a shared [messages](/oss/python/langgraph/graph-api#why-use-messages) key.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c280df5c968cd4237b0b5d03823d8946\" alt=\"SQL agent graph\" style={{ height: \"450px\" }} data-og-width=\"1177\" width=\"1177\" data-og-height=\"818\" height=\"818\" data-path=\"oss/images/subgraph.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=e3d08dae8fb81e15b4d8069a48999eac 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d8942031ba051119e0cb772ef697e0b 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0d5285bd104c542fe660bc09fed53e5e 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=32bc8ffa0eda13a0f3bb163631774a60 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=6a511f3b9dc44383614803d32390875a 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=169d55e154e5ea0146a57373235f768e 2500w\" />\n\nIf your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:\n\n1. Define the subgraph workflow (`subgraph_builder` in the example below) and compile it\n2. Pass compiled subgraph to the [`add_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_node) method when defining the parent graph workflow\n\n```python  theme={null}\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)  # [!code highlight]\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n```\n\n<Accordion title=\"Full example: shared state schemas\">\n  ```python  theme={null}\n  from typing_extensions import TypedDict\n  from langgraph.graph.state import StateGraph, START\n\n  # Define subgraph\n  class SubgraphState(TypedDict):\n      foo: str  # shared with parent graph state\n      bar: str  # private to SubgraphState\n\n  def subgraph_node_1(state: SubgraphState):\n      return {\"bar\": \"bar\"}\n\n  def subgraph_node_2(state: SubgraphState):\n      # note that this node is using a state key ('bar') that is only available in the subgraph\n      # and is sending update on the shared state key ('foo')\n      return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\n  subgraph_builder = StateGraph(SubgraphState)\n  subgraph_builder.add_node(subgraph_node_1)\n  subgraph_builder.add_node(subgraph_node_2)\n  subgraph_builder.add_edge(START, \"subgraph_node_1\")\n  subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n  subgraph = subgraph_builder.compile()\n\n  # Define parent graph\n  class ParentState(TypedDict):\n      foo: str\n\n  def node_1(state: ParentState):\n      return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n  builder = StateGraph(ParentState)\n  builder.add_node(\"node_1\", node_1)\n  builder.add_node(\"node_2\", subgraph)\n  builder.add_edge(START, \"node_1\")\n  builder.add_edge(\"node_1\", \"node_2\")\n  graph = builder.compile()\n\n  for chunk in graph.stream({\"foo\": \"foo\"}):\n      print(chunk)\n  ```\n\n  ```\n  {'node_1': {'foo': 'hi! foo'}}\n  {'node_2': {'foo': 'hi! foobar'}}\n  ```\n</Accordion>\n\n## Add persistence\n\nYou only need to **provide the checkpointer when compiling the parent graph**. LangGraph will automatically propagate the checkpointer to the child subgraphs.\n\n```python  theme={null}\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n```\n\nIf you want the subgraph to **have its own memory**, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/python/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories:\n\n```python  theme={null}\nsubgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)\n```\n\n## View subgraph state\n\nWhen you enable [persistence](/oss/python/langgraph/persistence), you can [inspect the graph state](/oss/python/langgraph/persistence#checkpoints) (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.\n\nYou can inspect the graph state via `graph.get_state(config)`. To view the subgraph state, you can use `graph.get_state(config, subgraphs=True)`.\n\n<Warning>\n  **Available **only** when interrupted**\n  Subgraph state can only be viewed **when the subgraph is interrupted**. Once you resume the graph, you won't be able to access the subgraph state.\n</Warning>\n\n<Accordion title=\"View interrupted subgraph state\">\n  ```python  theme={null}\n  from langgraph.graph import START, StateGraph\n  from langgraph.checkpoint.memory import MemorySaver\n  from langgraph.types import interrupt, Command\n  from typing_extensions import TypedDict\n\n  class State(TypedDict):\n      foo: str\n\n  # Subgraph\n\n  def subgraph_node_1(state: State):\n      value = interrupt(\"Provide value:\")\n      return {\"foo\": state[\"foo\"] + value}\n\n  subgraph_builder = StateGraph(State)\n  subgraph_builder.add_node(subgraph_node_1)\n  subgraph_builder.add_edge(START, \"subgraph_node_1\")\n\n  subgraph = subgraph_builder.compile()\n\n  # Parent graph\n\n  builder = StateGraph(State)\n  builder.add_node(\"node_1\", subgraph)\n  builder.add_edge(START, \"node_1\")\n\n  checkpointer = MemorySaver()\n  graph = builder.compile(checkpointer=checkpointer)\n\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n  graph.invoke({\"foo\": \"\"}, config)\n  parent_state = graph.get_state(config)\n\n  # This will be available only when the subgraph is interrupted.\n  # Once you resume the graph, you won't be able to access the subgraph state.\n  subgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state\n\n  # resume the subgraph\n  graph.invoke(Command(resume=\"bar\"), config)\n  ```\n\n  1. This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.\n</Accordion>\n\n## Stream subgraph outputs\n\nTo include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\n\n```python  theme={null}\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    subgraphs=True, # [!code highlight]\n    stream_mode=\"updates\",\n):\n    print(chunk)\n```\n\n<Accordion title=\"Stream from subgraphs\">\n  ```python  theme={null}\n  from typing_extensions import TypedDict\n  from langgraph.graph.state import StateGraph, START\n\n  # Define subgraph\n  class SubgraphState(TypedDict):\n      foo: str\n      bar: str\n\n  def subgraph_node_1(state: SubgraphState):\n      return {\"bar\": \"bar\"}\n\n  def subgraph_node_2(state: SubgraphState):\n      # note that this node is using a state key ('bar') that is only available in the subgraph\n      # and is sending update on the shared state key ('foo')\n      return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\n  subgraph_builder = StateGraph(SubgraphState)\n  subgraph_builder.add_node(subgraph_node_1)\n  subgraph_builder.add_node(subgraph_node_2)\n  subgraph_builder.add_edge(START, \"subgraph_node_1\")\n  subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n  subgraph = subgraph_builder.compile()\n\n  # Define parent graph\n  class ParentState(TypedDict):\n      foo: str\n\n  def node_1(state: ParentState):\n      return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n  builder = StateGraph(ParentState)\n  builder.add_node(\"node_1\", node_1)\n  builder.add_node(\"node_2\", subgraph)\n  builder.add_edge(START, \"node_1\")\n  builder.add_edge(\"node_1\", \"node_2\")\n  graph = builder.compile()\n\n  for chunk in graph.stream(\n      {\"foo\": \"foo\"},\n      stream_mode=\"updates\",\n      subgraphs=True, # [!code highlight]\n  ):\n      print(chunk)\n  ```\n\n  ```\n  ((), {'node_1': {'foo': 'hi! foo'}})\n  (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})\n  (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n  ((), {'node_2': {'foo': 'hi! foobar'}})\n  ```\n</Accordion>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-subgraphs.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "search_support_articles",
    "tool_call_id": "call_81229289",
    "id": "c0262b92-be7b-4087-9360-3a8035126dba",
    "content": "{\n  \"collections\": \"OSS (LangChain and LangGraph)\",\n  \"total\": 4,\n  \"articles\": [\n    {\n      \"id\": \"bb651320-775e-4dfa-a054-c955cc3a0d5d\",\n      \"title\": \"Why Do Pydantic\\u00a0date\\u00a0Fields Fail in LangGraph State?\",\n      \"url\": \"https://support.langchain.com/articles/7196116702-why-do-pydantic-date-fields-fail-in-langgraph-state\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"ae959c68-a996-4426-8c8e-ef3fa1dd88cc\",\n      \"title\": \"How do I connect JIRA, Slack, and Confluence MCPs with LangChain?\",\n      \"url\": \"https://support.langchain.com/articles/3473375007-how-do-i-connect-jira-slack-and-confluence-mcps-with-langchain\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\",\n      \"title\": \"Isolating Subgraph State When Calling Graphs from Tools\",\n      \"url\": \"https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    },\n    {\n      \"id\": \"5f2572a3-9086-4d92-8d15-69f7b8e4c8be\",\n      \"title\": \"MCP Server Runtime in Google Colab / Jupyter Notebook\",\n      \"url\": \"https://support.langchain.com/articles/1061688976-mcp-server-runtime-in-google-colab-jupyter-notebook\",\n      \"collection\": \"OSS (LangChain and LangGraph)\"\n    }\n  ],\n  \"note\": \"All articles listed are public and have content. Use IDs to fetch full content.\"\n}"
  },
  {
    "role": "assistant",
    "tool_calls": [
      {
        "type": "function",
        "id": "call_55608934",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"python\", \"page_size\": 5, \"query\": \"stategraph add_node\"}"
        }
      },
      {
        "type": "function",
        "id": "call_90750111",
        "function": {
          "name": "SearchDocsByLangChain",
          "arguments": "{\"language\": \"python\", \"page_size\": 5, \"query\": \"graph api\"}"
        }
      },
      {
        "type": "function",
        "id": "call_04157800",
        "function": {
          "name": "get_article_content",
          "arguments": "{\"article_id\": \"9e06b53a-7b85-4793-b7b6-e179c0c8d815\"}"
        }
      }
    ],
    "id": "lc_run--019b3a20-04f4-7732-9a70-9e8e9e69999b",
    "content": ""
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_55608934",
    "id": "bb9ca6da-1398-40b0-bc46-e7a7649dbfc6",
    "content": "Result 1:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/javascript/langgraph/overview\nContent: Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/javascript/langchain/models) and [tools](/oss/javascript/langchain/tools).\n\nWe will commonly use [LangChain](/oss/javascript/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/javascript/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/langgraph @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/langgraph @langchain/core\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/langgraph @langchain/core\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```typescript  theme={null}\nimport { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\n\nconst mockLlm = (state: typeof MessagesAnnotation.State) => {\n  return { messages: [{ role: \"ai\", content: \"hello world\" }] };\n};\n\nconst graph = new StateGraph(MessagesAnnotation)\n  .addNode(\"mock_llm\", mockLlm)\n  .addEdge(START, \"mock_llm\")\n  .addEdge(\"mock_llm\", END)\n  .compile();\n\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi!\" }] });\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/javascript/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/javascript/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/javascript/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/javascript/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/javascript/langgraph/quickstart\nContent: This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\n\n* [Use the Graph API](#use-the-graph-api) if you prefer to define your agent as a graph of nodes and edges.\n* [Use the Functional API](#use-the-functional-api) if you prefer to define your agent as a single function.\n\nFor conceptual information, see [Graph API overview](/oss/javascript/langgraph/graph-api) and [Functional API overview](/oss/javascript/langgraph/functional-api).\n\n<Info>\n  For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.\n</Info>\n\n<Tabs>\n  <Tab title=\"Use the Graph API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```typescript  theme={null}\n    import { ChatAnthropic } from \"@langchain/anthropic\";\n    import { tool } from \"@langchain/core/tools\";\n    import * as z from \"zod\";\n\n    const model = new ChatAnthropic({\n      model: \"claude-sonnet-4-5-20250929\",\n      temperature: 0,\n    });\n\n    // Define tools\n    const add = tool(({ a, b }) => a + b, {\n      name: \"add\",\n      description: \"Add two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const multiply = tool(({ a, b }) => a * b, {\n      name: \"multiply\",\n      description: \"Multiply two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const divide = tool(({ a, b }) => a / b, {\n      name: \"divide\",\n      description: \"Divide two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    // Augment the LLM with tools\n    const toolsByName = {\n      [add.name]: add,\n      [multiply.name]: multiply,\n      [divide.name]: divide,\n    };\n    const tools = Object.values(toolsByName);\n    const modelWithTools = model.bindTools(tools);\n    ```\n\n    ## 2. Define state\n\n    The graph's state is used to store the messages and the number of LLM calls.\n\n    <Tip>\n      State in LangGraph persists throughout the agent's execution.\n\n      The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.\n    </Tip>\n\n    ```typescript  theme={null}\n    import { StateGraph, START, END } from \"@langchain/langgraph\";\n    import { MessagesZodMeta } from \"@langchain/langgraph\";\n    import { registry } from \"@langchain/langgraph/zod\";\n    import { type BaseMessage } from \"@langchain/core/messages\";\n\n    const MessagesState = z.object({\n      messages: z\n        .array(z.custom<BaseMessage>())\n        .register(registry, MessagesZodMeta),\n      llmCalls: z.number().optional(),\n    });\n    ```\n\n    ## 3. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    ```typescript  theme={null}\n    import { SystemMessage } from \"@langchain/core/messages\";\n    async function llmCall(state: z.infer<typeof MessagesState>) {\n      return {\n        messages: await modelWithTools.invoke([\n          new SystemMessage(\n            \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n          ),\n          ...state.messages,\n        ]),\n        llmCalls: (state.llmCalls ?? 0) + 1,\n      };\n    }\n    ```\n\n    ## 4. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```typescript  theme={null}\n    import { isAIMessage, ToolMessage } from \"@langchain/core/messages\";\n    async function toolNode(state: z.infer<typeof MessagesState>) {\n      const lastMessage = state.messages.at(-1);\n\n      if (lastMessage == null || !isAIMessage(lastMessage)) {\n        return { messages: [] };\n      }\n\n      const result: ToolMessage[] = [];\n      for (const toolCall of lastMessage.tool_calls ?? []) {\n        const tool = toolsByName[toolCall.name];\n        const observation = await tool.invoke(toolCall);\n        result.push(observation);\n      }\n\n      return { messages: result };\n    }\n    ```\n\n    ## 5. Define end logic\n\n    The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.\n\n    ```typescript  theme={null}\n    async function shouldContinue(state: z.infer<typeof MessagesState>) {\n      const lastMessage = state.messages.at(-1);\n      if (lastMessage == null || !isAIMessage(lastMessage)) return END;\n\n      // If the LLM makes a tool call, then perform an action\n      if (lastMessage.tool_calls?.length) {\n        return \"toolNode\";\n      }\n\n      // Otherwise, we stop (reply to the user)\n      return END;\n    }\n    ```\n\n    ## 6. Build and compile the agent\n\n    The agent is built using the [`StateGraph`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html) class and compiled using the [`compile`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html#compile) method.\n\n    ```typescript  theme={null}\n    const agent = new StateGraph(MessagesState)\n      .addNode(\"llmCall\", llmCall)\n      .addNode(\"toolNode\", toolNode)\n      .addEdge(START, \"llmCall\")\n      .addConditionalEdges(\"llmCall\", shouldContinue, [\"toolNode\", END])\n      .addEdge(\"toolNode\", \"llmCall\")\n      .compile();\n\n    // Invoke\n    import { HumanMessage } from \"@langchain/core/messages\";\n    const result = await agent.invoke({\n      messages: [new HumanMessage(\"Add 3 and 4.\")],\n    });\n\n    for (const message of result.messages) {\n      console.log(`[${message.getType()}]: ${message.text}`);\n    }\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Graph API.\n\n    <Accordion title=\"Full code example\">\n      ```typescript  theme={null}\n      // Step 1: Define tools and model\n\n      import { ChatAnthropic } from \"@langchain/anthropic\";\n      import { tool } from \"@langchain/core/tools\";\n      import * as z from \"zod\";\n\n      const model = new ChatAnthropic({\n        model: \"claude-sonnet-4-5-20250929\",\n        temperature: 0,\n      });\n\n      // Define tools\n      const add = tool(({ a, b }) => a + b, {\n        name: \"add\",\n        description: \"Add two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const multiply = tool(({ a, b }) => a * b, {\n        name: \"multiply\",\n        description: \"Multiply two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const divide = tool(({ a, b }) => a / b, {\n        name: \"divide\",\n        description: \"Divide two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      // Augment the LLM with tools\n      const toolsByName = {\n        [add.name]: add,\n        [multiply.name]: multiply,\n        [divide.name]: divide,\n      };\n      const tools = Object.values(toolsByName);\n      const modelWithTools = model.bindTools(tools);\n\n      // Step 2: Define state\n\n      import { StateGraph, START, END } from \"@langchain/langgraph\";\n      import { MessagesZodMeta } from \"@langchain/langgraph\";\n      import { registry } from \"@langchain/langgraph/zod\";\n      import { type BaseMessage } from \"@langchain/core/messages\";\n\n      const MessagesState = z.object({\n        messages: z\n          .array(z.custom<BaseMessage>())\n          .register(registry, MessagesZodMeta),\n        llmCalls: z.number().optional(),\n      });\n\n      // Step 3: Define model node\n\n      import { SystemMessage } from \"@langchain/core/messages\";\n      async function llmCall(state: z.infer<typeof MessagesState>) {\n        return {\n          messages: await modelWithTools.invoke([\n            new SystemMessage(\n              \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n            ),\n            ...state.messages,\n          ]),\n          llmCalls: (state.llmCalls ?? 0) + 1,\n        };\n      }\n\n      // Step 4: Define tool node\n\n      import { isAIMessage, ToolMessage } from \"@langchain/core/messages\";\n      async function toolNode(state: z.infer<typeof MessagesState>) {\n        const lastMessage = state.messages.at(-1);\n\n        if (lastMessage == null || !isAIMessage(lastMessage)) {\n          return { messages: [] };\n        }\n\n        const result: ToolMessage[] = [];\n        for (const toolCall of lastMessage.tool_calls ?? []) {\n          const tool = toolsByName[toolCall.name];\n          const observation = await tool.invoke(toolCall);\n          result.push(observation);\n        }\n\n        return { messages: result };\n      }\n\n      // Step 5: Define logic to determine whether to end\n\n      async function shouldContinue(state: z.infer<typeof MessagesState>) {\n        const lastMessage = state.messages.at(-1);\n        if (lastMessage == null || !isAIMessage(lastMessage)) return END;\n\n        // If the LLM makes a tool call, then perform an action\n        if (lastMessage.tool_calls?.length) {\n          return \"toolNode\";\n        }\n\n        // Otherwise, we stop (reply to the user)\n        return END;\n      }\n\n      // Step 6: Build and compile the agent\n\n      const agent = new StateGraph(MessagesState)\n        .addNode(\"llmCall\", llmCall)\n        .addNode(\"toolNode\", toolNode)\n        .addEdge(START, \"llmCall\")\n        .addConditionalEdges(\"llmCall\", shouldContinue, [\"toolNode\", END])\n        .addEdge(\"toolNode\", \"llmCall\")\n        .compile();\n\n      // Invoke\n      import { HumanMessage } from \"@langchain/core/messages\";\n      const result = await agent.invoke({\n        messages: [new HumanMessage(\"Add 3 and 4.\")],\n      });\n\n      for (const message of result.messages) {\n        console.log(`[${message.getType()}]: ${message.text}`);\n      }\n      ```\n    </Accordion>\n  </Tab>\n\n  <Tab title=\"Use the Functional API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```typescript  theme={null}\n    import { ChatAnthropic } from \"@langchain/anthropic\";\n    import { tool } from \"@langchain/core/tools\";\n    import * as z from \"zod\";\n\n    const model = new ChatAnthropic({\n      model: \"claude-sonnet-4-5-20250929\",\n      temperature: 0,\n    });\n\n    // Define tools\n    const add = tool(({ a, b }) => a + b, {\n      name: \"add\",\n      description: \"Add two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const multiply = tool(({ a, b }) => a * b, {\n      name: \"multiply\",\n      description: \"Multiply two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const divide = tool(({ a, b }) => a / b, {\n      name: \"divide\",\n      description: \"Divide two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    // Augment the LLM with tools\n    const toolsByName = {\n      [add.name]: add,\n      [multiply.name]: multiply,\n      [divide.name]: divide,\n    };\n    const tools = Object.values(toolsByName);\n    const modelWithTools = model.bindTools(tools);\n\n    ```\n\n    ## 2. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    ```typescript  theme={null}\n    import { task, entrypoint } from \"@langchain/langgraph\";\n    import { SystemMessage } from \"@langchain/core/messages\";\n    const callLlm = task({ name: \"callLlm\" }, async (messages: BaseMessage[]) => {\n      return modelWithTools.invoke([\n        new SystemMessage(\n          \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n        ),\n        ...messages,\n      ]);\n    });\n    ```\n\n    ## 3. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```typescript  theme={null}\n    import type { ToolCall } from \"@langchain/core/messages/tool\";\n    const callTool = task({ name: \"callTool\" }, async (toolCall: ToolCall) => {\n      const tool = toolsByName[toolCall.name];\n      return tool.invoke(toolCall);\n    });\n    ```\n\n    ## 4. Define agent\n\n    ```typescript  theme={null}\n    import { addMessages } from \"@langchain/langgraph\";\n    import { type BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n\n    const agent = entrypoint({ name: \"agent\" }, async (messages: BaseMessage[]) => {\n      let modelResponse = await callLlm(messages);\n\n      while (true) {\n        if (!modelResponse.tool_calls?.length) {\n          break;\n        }\n\n        // Execute tools\n        const toolResults = await Promise.all(\n          modelResponse.tool_calls.map((toolCall) => callTool(toolCall))\n        );\n        messages = addMessages(messages, [modelResponse, ...toolResults]);\n        modelResponse = await callLlm(messages);\n      }\n\n      return messages;\n    });\n\n    // Invoke\n    import { HumanMessage } from \"@langchain/core/messages\";\n\n    const result = await agent.invoke([new HumanMessage(\"Add 3 and 4.\")]);\n\n    for (const message of result) {\n      console.log(`[${message.getType()}]: ${message.text}`);\n    }\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Functional API.\n\n    <Accordion title=\"Full code example\" icon=\"code\">\n      ```typescript  theme={null}\n      // Step 1: Define tools and model\n\n      import { ChatAnthropic } from \"@langchain/anthropic\";\n      import { tool } from \"@langchain/core/tools\";\n      import * as z from \"zod\";\n\n      const model = new ChatAnthropic({\n        model: \"claude-sonnet-4-5-20250929\",\n        temperature: 0,\n      });\n\n      // Define tools\n      const add = tool(({ a, b }) => a + b, {\n        name: \"add\",\n        description: \"Add two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const multiply = tool(({ a, b }) => a * b, {\n        name: \"multiply\",\n        description: \"Multiply two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const divide = tool(({ a, b }) => a / b, {\n        name: \"divide\",\n        description: \"Divide two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      // Augment the LLM with tools\n      const toolsByName = {\n        [add.name]: add,\n        [multiply.name]: multiply,\n        [divide.name]: divide,\n      };\n      const tools = Object.values(toolsByName);\n      const modelWithTools = model.bindTools(tools);\n\n      // Step 2: Define model node\n\n      import { task, entrypoint } from \"@langchain/langgraph\";\n      import { SystemMessage } from \"@langchain/core/messages\";\n      const callLlm = task({ name: \"callLlm\" }, async (messages: BaseMessage[]) => {\n        return modelWithTools.invoke([\n          new SystemMessage(\n            \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n          ),\n          ...messages,\n        ]);\n      });\n\n      // Step 3: Define tool node\n\n      import type { ToolCall } from \"@langchain/core/messages/tool\";\n      const callTool = task({ name: \"callTool\" }, async (toolCall: ToolCall) => {\n        const tool = toolsByName[toolCall.name];\n        return tool.invoke(toolCall);\n      });\n\n      // Step 4: Define agent\n      import { addMessages } from \"@langchain/langgraph\";\n      import { type BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n      const agent = entrypoint({ name: \"agent\" }, async (messages: BaseMessage[]) => {\n        let modelResponse = await callLlm(messages);\n\n        while (true) {\n          if (!modelResponse.tool_calls?.length) {\n            break;\n          }\n\n          // Execute tools\n          const toolResults = await Promise.all(\n            modelResponse.tool_calls.map((toolCall) => callTool(toolCall))\n          );\n          messages = addMessages(messages, [modelResponse, ...toolResults]);\n          modelResponse = await callLlm(messages);\n        }\n\n        return messages;\n      });\n\n      // Invoke\n      import { HumanMessage } from \"@langchain/core/messages\";\n      const result = await agent.invoke([new HumanMessage(\"Add 3 and 4.\")]);\n\n      for (const message of result) {\n        console.log(`[${message.getType()}]: ${message.text}`);\n      }\n      ```\n    </Accordion>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/quickstart.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 3:\nTitle: Memory\nLink: https://docs.langchain.comoss/python/langgraph/add-memory\nContent: Use semantic search\nEnable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity. from langchain.embeddings import init_embeddings\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.store.base import BaseStore\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.graph import START, MessagesState, StateGraph\n\nmodel = init_chat_model(\"gpt-4o-mini\")\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\ndef chat(state, *, store: BaseStore):\n    # Search based on user's last message\n    items = store.search(\n        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n    )\n    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n    response = model.invoke(\n        [\n            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n            *state[\"messages\"],\n        ]\n    )\n    return {\"messages\": [response]}\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(chat)\nbuilder.add_edge(START, \"chat\")\ngraph = builder.compile(store=store)\n\nfor message, metadata in graph.stream(\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n    stream_mode=\"messages\",\n):\n    print(message.content, end=\"\")\n\nUse semantic search\nEnable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity.\n\nUse semantic search\nEnable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity. from langchain.embeddings import init_embeddings\nfrom langgraph.store.memory import InMemoryStore\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\nitems = store.search(\n    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n)\n\n---\n\nResult 4:\nTitle: LangGraph overview\nLink: https://docs.langchain.comoss/python/langgraph/overview\nContent: Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools).\n\nWe will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\n## <Icon icon=\"download\" size={20} /> Install\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\nThen, create a simple hello world example:\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\ndef mock_llm(state: MessagesState):\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n\ngraph = StateGraph(MessagesState)\ngraph.add_node(mock_llm)\ngraph.add_edge(START, \"mock_llm\")\ngraph.add_edge(\"mock_llm\", END)\ngraph = graph.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n```\n\n## Core benefits\n\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\n* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\n* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\n## LangGraph ecosystem\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\n<Columns cols={1}>\n  <Card title=\"LangSmith\" icon=\"chart-line\" href=\"http://www.langchain.com/langsmith\" arrow cta=\"Learn more\">\n    Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n  </Card>\n\n  <Card title=\"LangGraph\" icon=\"server\" href=\"/langsmith/agent-server\" arrow cta=\"Learn more\">\n    Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n  </Card>\n\n  <Card title=\"LangChain\" icon=\"link\" href=\"/oss/python/langchain/overview\" arrow cta=\"Learn more\">\n    Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n  </Card>\n</Columns>\n\n## Acknowledgements\n\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Quickstart\nLink: https://docs.langchain.comoss/python/langgraph/quickstart\nContent: This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\n\n* [Use the Graph API](#use-the-graph-api) if you prefer to define your agent as a graph of nodes and edges.\n* [Use the Functional API](#use-the-functional-api) if you prefer to define your agent as a single function.\n\nFor conceptual information, see [Graph API overview](/oss/python/langgraph/graph-api) and [Functional API overview](/oss/python/langgraph/functional-api).\n\n<Info>\n  For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.\n</Info>\n\n<Tabs>\n  <Tab title=\"Use the Graph API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```python  theme={null}\n    from langchain.tools import tool\n    from langchain.chat_models import init_chat_model\n\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0\n    )\n\n\n    # Define tools\n    @tool\n    def multiply(a: int, b: int) -> int:\n        \"\"\"Multiply `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a * b\n\n\n    @tool\n    def add(a: int, b: int) -> int:\n        \"\"\"Adds `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a + b\n\n\n    @tool\n    def divide(a: int, b: int) -> float:\n        \"\"\"Divide `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a / b\n\n\n    # Augment the LLM with tools\n    tools = [add, multiply, divide]\n    tools_by_name = {tool.name: tool for tool in tools}\n    model_with_tools = model.bind_tools(tools)\n    ```\n\n    ## 2. Define state\n\n    The graph's state is used to store the messages and the number of LLM calls.\n\n    <Tip>\n      State in LangGraph persists throughout the agent's execution.\n\n      The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.\n    </Tip>\n\n    ```python  theme={null}\n    from langchain.messages import AnyMessage\n    from typing_extensions import TypedDict, Annotated\n    import operator\n\n\n    class MessagesState(TypedDict):\n        messages: Annotated[list[AnyMessage], operator.add]\n        llm_calls: int\n    ```\n\n    ## 3. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    ```python  theme={null}\n    from langchain.messages import SystemMessage\n\n\n    def llm_call(state: dict):\n        \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n        return {\n            \"messages\": [\n                model_with_tools.invoke(\n                    [\n                        SystemMessage(\n                            content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                        )\n                    ]\n                    + state[\"messages\"]\n                )\n            ],\n            \"llm_calls\": state.get('llm_calls', 0) + 1\n        }\n    ```\n\n    ## 4. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```python  theme={null}\n    from langchain.messages import ToolMessage\n\n\n    def tool_node(state: dict):\n        \"\"\"Performs the tool call\"\"\"\n\n        result = []\n        for tool_call in state[\"messages\"][-1].tool_calls:\n            tool = tools_by_name[tool_call[\"name\"]]\n            observation = tool.invoke(tool_call[\"args\"])\n            result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n        return {\"messages\": result}\n    ```\n\n    ## 5. Define end logic\n\n    The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.\n\n    ```python  theme={null}\n    from typing import Literal\n    from langgraph.graph import StateGraph, START, END\n\n\n    def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n        \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n\n        # If the LLM makes a tool call, then perform an action\n        if last_message.tool_calls:\n            return \"tool_node\"\n\n        # Otherwise, we stop (reply to the user)\n        return END\n    ```\n\n    ## 6. Build and compile the agent\n\n    The agent is built using the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) class and compiled using the [`compile`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.compile) method.\n\n    ```python  theme={null}\n    # Build workflow\n    agent_builder = StateGraph(MessagesState)\n\n    # Add nodes\n    agent_builder.add_node(\"llm_call\", llm_call)\n    agent_builder.add_node(\"tool_node\", tool_node)\n\n    # Add edges to connect nodes\n    agent_builder.add_edge(START, \"llm_call\")\n    agent_builder.add_conditional_edges(\n        \"llm_call\",\n        should_continue,\n        [\"tool_node\", END]\n    )\n    agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n    # Compile the agent\n    agent = agent_builder.compile()\n\n    # Show the agent\n    from IPython.display import Image, display\n    display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n    # Invoke\n    from langchain.messages import HumanMessage\n    messages = [HumanMessage(content=\"Add 3 and 4.\")]\n    messages = agent.invoke({\"messages\": messages})\n    for m in messages[\"messages\"]:\n        m.pretty_print()\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Graph API.\n\n    <Accordion title=\"Full code example\">\n      ```python  theme={null}\n      # Step 1: Define tools and model\n\n      from langchain.tools import tool\n      from langchain.chat_models import init_chat_model\n\n\n      model = init_chat_model(\n          \"claude-sonnet-4-5-20250929\",\n          temperature=0\n      )\n\n\n      # Define tools\n      @tool\n      def multiply(a: int, b: int) -> int:\n          \"\"\"Multiply `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a * b\n\n\n      @tool\n      def add(a: int, b: int) -> int:\n          \"\"\"Adds `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a + b\n\n\n      @tool\n      def divide(a: int, b: int) -> float:\n          \"\"\"Divide `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a / b\n\n\n      # Augment the LLM with tools\n      tools = [add, multiply, divide]\n      tools_by_name = {tool.name: tool for tool in tools}\n      model_with_tools = model.bind_tools(tools)\n\n      # Step 2: Define state\n\n      from langchain.messages import AnyMessage\n      from typing_extensions import TypedDict, Annotated\n      import operator\n\n\n      class MessagesState(TypedDict):\n          messages: Annotated[list[AnyMessage], operator.add]\n          llm_calls: int\n\n      # Step 3: Define model node\n      from langchain.messages import SystemMessage\n\n\n      def llm_call(state: dict):\n          \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n          return {\n              \"messages\": [\n                  model_with_tools.invoke(\n                      [\n                          SystemMessage(\n                              content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                          )\n                      ]\n                      + state[\"messages\"]\n                  )\n              ],\n              \"llm_calls\": state.get('llm_calls', 0) + 1\n          }\n\n\n      # Step 4: Define tool node\n\n      from langchain.messages import ToolMessage\n\n\n      def tool_node(state: dict):\n          \"\"\"Performs the tool call\"\"\"\n\n          result = []\n          for tool_call in state[\"messages\"][-1].tool_calls:\n              tool = tools_by_name[tool_call[\"name\"]]\n              observation = tool.invoke(tool_call[\"args\"])\n              result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n          return {\"messages\": result}\n\n      # Step 5: Define logic to determine whether to end\n\n      from typing import Literal\n      from langgraph.graph import StateGraph, START, END\n\n\n      # Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n      def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n          \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n          messages = state[\"messages\"]\n          last_message = messages[-1]\n\n          # If the LLM makes a tool call, then perform an action\n          if last_message.tool_calls:\n              return \"tool_node\"\n\n          # Otherwise, we stop (reply to the user)\n          return END\n\n      # Step 6: Build agent\n\n      # Build workflow\n      agent_builder = StateGraph(MessagesState)\n\n      # Add nodes\n      agent_builder.add_node(\"llm_call\", llm_call)\n      agent_builder.add_node(\"tool_node\", tool_node)\n\n      # Add edges to connect nodes\n      agent_builder.add_edge(START, \"llm_call\")\n      agent_builder.add_conditional_edges(\n          \"llm_call\",\n          should_continue,\n          [\"tool_node\", END]\n      )\n      agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n      # Compile the agent\n      agent = agent_builder.compile()\n\n\n      from IPython.display import Image, display\n      # Show the agent\n      display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n      # Invoke\n      from langchain.messages import HumanMessage\n      messages = [HumanMessage(content=\"Add 3 and 4.\")]\n      messages = agent.invoke({\"messages\": messages})\n      for m in messages[\"messages\"]:\n          m.pretty_print()\n\n      ```\n    </Accordion>\n  </Tab>\n\n  <Tab title=\"Use the Functional API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```python  theme={null}\n    from langchain.tools import tool\n    from langchain.chat_models import init_chat_model\n\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0\n    )\n\n\n    # Define tools\n    @tool\n    def multiply(a: int, b: int) -> int:\n        \"\"\"Multiply `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a * b\n\n\n    @tool\n    def add(a: int, b: int) -> int:\n        \"\"\"Adds `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a + b\n\n\n    @tool\n    def divide(a: int, b: int) -> float:\n        \"\"\"Divide `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a / b\n\n\n    # Augment the LLM with tools\n    tools = [add, multiply, divide]\n    tools_by_name = {tool.name: tool for tool in tools}\n    model_with_tools = model.bind_tools(tools)\n\n    from langgraph.graph import add_messages\n    from langchain.messages import (\n        SystemMessage,\n        HumanMessage,\n        ToolCall,\n    )\n    from langchain_core.messages import BaseMessage\n    from langgraph.func import entrypoint, task\n    ```\n\n    ## 2. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    <Tip>\n      The [`@task`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.task) decorator marks a function as a task that can be executed as part of the agent. Tasks can be called synchronously or asynchronously within your entrypoint function.\n    </Tip>\n\n    ```python  theme={null}\n    @task\n    def call_llm(messages: list[BaseMessage]):\n        \"\"\"LLM decides whether to call a tool or not\"\"\"\n        return model_with_tools.invoke(\n            [\n                SystemMessage(\n                    content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                )\n            ]\n            + messages\n        )\n    ```\n\n    ## 3. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```python  theme={null}\n    @task\n    def call_tool(tool_call: ToolCall):\n        \"\"\"Performs the tool call\"\"\"\n        tool = tools_by_name[tool_call[\"name\"]]\n        return tool.invoke(tool_call)\n\n    ```\n\n    ## 4. Define agent\n\n    The agent is built using the [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) function.\n\n    <Note>\n      In the Functional API, instead of defining nodes and edges explicitly, you write standard control flow logic (loops, conditionals) within a single function.\n    </Note>\n\n    ```python  theme={null}\n    @entrypoint()\n    def agent(messages: list[BaseMessage]):\n        model_response = call_llm(messages).result()\n\n        while True:\n            if not model_response.tool_calls:\n                break\n\n            # Execute tools\n            tool_result_futures = [\n                call_tool(tool_call) for tool_call in model_response.tool_calls\n            ]\n            tool_results = [fut.result() for fut in tool_result_futures]\n            messages = add_messages(messages, [model_response, *tool_results])\n            model_response = call_llm(messages).result()\n\n        messages = add_messages(messages, model_response)\n        return messages\n\n    # Invoke\n    messages = [HumanMessage(content=\"Add 3 and 4.\")]\n    for chunk in agent.stream(messages, stream_mode=\"updates\"):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Functional API.\n\n    <Accordion title=\"Full code example\" icon=\"code\">\n      ```python  theme={null}\n      # Step 1: Define tools and model\n\n      from langchain.tools import tool\n      from langchain.chat_models import init_chat_model\n\n\n      model = init_chat_model(\n          \"claude-sonnet-4-5-20250929\",\n          temperature=0\n      )\n\n\n      # Define tools\n      @tool\n      def multiply(a: int, b: int) -> int:\n          \"\"\"Multiply `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a * b\n\n\n      @tool\n      def add(a: int, b: int) -> int:\n          \"\"\"Adds `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a + b\n\n\n      @tool\n      def divide(a: int, b: int) -> float:\n          \"\"\"Divide `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a / b\n\n\n      # Augment the LLM with tools\n      tools = [add, multiply, divide]\n      tools_by_name = {tool.name: tool for tool in tools}\n      model_with_tools = model.bind_tools(tools)\n\n      from langgraph.graph import add_messages\n      from langchain.messages import (\n          SystemMessage,\n          HumanMessage,\n          ToolCall,\n      )\n      from langchain_core.messages import BaseMessage\n      from langgraph.func import entrypoint, task\n\n\n      # Step 2: Define model node\n\n      @task\n      def call_llm(messages: list[BaseMessage]):\n          \"\"\"LLM decides whether to call a tool or not\"\"\"\n          return model_with_tools.invoke(\n              [\n                  SystemMessage(\n                      content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                  )\n              ]\n              + messages\n          )\n\n\n      # Step 3: Define tool node\n\n      @task\n      def call_tool(tool_call: ToolCall):\n          \"\"\"Performs the tool call\"\"\"\n          tool = tools_by_name[tool_call[\"name\"]]\n          return tool.invoke(tool_call)\n\n\n      # Step 4: Define agent\n\n      @entrypoint()\n      def agent(messages: list[BaseMessage]):\n          model_response = call_llm(messages).result()\n\n          while True:\n              if not model_response.tool_calls:\n                  break\n\n              # Execute tools\n              tool_result_futures = [\n                  call_tool(tool_call) for tool_call in model_response.tool_calls\n              ]\n              tool_results = [fut.result() for fut in tool_result_futures]\n              messages = add_messages(messages, [model_response, *tool_results])\n              model_response = call_llm(messages).result()\n\n          messages = add_messages(messages, model_response)\n          return messages\n\n      # Invoke\n      messages = [HumanMessage(content=\"Add 3 and 4.\")]\n      for chunk in agent.stream(messages, stream_mode=\"updates\"):\n          print(chunk)\n          print(\"\\n\")\n      ```\n    </Accordion>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/quickstart.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "SearchDocsByLangChain",
    "tool_call_id": "call_90750111",
    "id": "0292d148-4243-43ec-ad98-dc5c29244b8a",
    "content": "Result 1:\nTitle: Data purging for compliance\nLink: https://docs.langchain.comlangsmith/data-purging-compliance\nContent: This guide covers the various features available after data reaches LangSmith Cloud servers to help you achieve your privacy goals.\n\n## Data retention\n\nLangSmith provides automatic data retention capabilities to help with compliance and storage management. Data retention policies can be configured at the organization and project levels.\n\nFor detailed information about data retention configuration and management, please refer to the [Data Retention concepts](/langsmith/administration-overview#data-retention) documentation.\n\n## Trace deletes\n\nYou can use the API to complete trace deletes. The API supports two methods for deleting traces:\n\n1. **By trace IDs and session ID**: Delete specific traces by providing a list of trace IDs and their corresponding session ID (up to 1000 traces per request)\n2. **By metadata**: Delete traces across a workspace that match any of the specified metadata key-value pairs\n\nFor more details, refer to the [API spec](https://api.smith.langchain.com/redoc#tag/run/operation/delete_runs_api_v1_runs_delete_post).\n\n<Warning>\n  All trace deletions will delete related entities like feedbacks, aggregations, and stats across all data storages.\n</Warning>\n\n### Deletion timeline\n\nTrace deletions are processed during non-peak usage times and are not instant. LangChain runs the delete job on the weekend. There is no confirmation of deletion - you'll need to query the data again to verify it has been removed.\n\n### Delete specific traces\n\nTo delete specific traces by their trace IDs from a single session:\n\n```bash  theme={null}\ncurl -X POST \"https://api.smith.langchain.com/api/v1/runs/delete\" \\\n  -H \"x-api-key: YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"trace_ids\": [\"trace-id-1\", \"trace-id-2\", \"trace-id-3\"],\n    \"session_id\": \"session-id-1\"\n  }'\n```\n\n### Delete by metadata\n\nWhen deleting by metadata:\n\n* Accepts a `metadata` object of key/value pairs. KV pair matching uses an **or** condition. A trace will match if it has **any** of the key-value pairs specified in metadata (not all)\n* You don't need to specify a session id when deleting by metadata. Deletes will apply across the workspace.\n\nTo delete traces based on metadata across a workspace (matches **any** of the metadata key-value pairs):\n\n```bash  theme={null}\ncurl -X POST \"https://api.smith.langchain.com/api/v1/runs/delete\" \\\n  -H \"x-api-key: YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"metadata\": {\n      \"user_id\": \"user123\",\n      \"environment\": \"staging\"\n    }\n  }'\n```\n\nThis will delete traces that have either `user_id: \"user123\"` **or** `environment: \"staging\"` in their metadata.\n\n<Warning>\n  Remember that you can only schedule up to 1000 traces per session per request. For larger deletions, you'll need to make multiple requests.\n</Warning>\n\n## Example deletes\n\nYou can delete dataset examples self-serve via our API, which supports both soft and hard deletion methods depending on your data retention needs.\n\n<Warning>\n  Hard deletes will permanently remove inputs, outputs, and metadata from ALL versions of the specified examples across the entire dataset history.\n</Warning>\n\n### Deleting examples is a two-step process\n\nFor bulk operations, example deletion follows a two-step process:\n\n#### 1. Search for examples by metadata\n\nFind all examples with matching metadata across all datasets in a workspace.\n\n[GET /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get)\n\n* `as_of` must be explicitly specified as a timestamp. Only examples created before the `as_of` date will be returned\n\n```bash  theme={null}\ncurl -X GET \"https://api.smith.langchain.com/api/v1/examples?as_of=2024-01-01T00:00:00Z\" \\\n  -H \"x-api-key: YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"metadata\": {\n      \"user_id\": \"user123\",\n      \"environment\": \"staging\"\n    }\n  }'\n```\n\nThis will return examples that have either `user_id: \"user123\"` **or** `environment: \"staging\"` in their metadata across all datasets in your workspace.\n\n#### 2. Hard delete examples\n\nOnce you have the example IDs, send a delete request. This will zero-out the inputs, outputs, and metadata from all versions of the dataset for that example.\n\n[DELETE /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/delete_examples_api_v1_examples_delete)\n\n* Specify example IDs and add `\"hard_delete\": true` to the query params of the request\n\n```bash  theme={null}\ncurl -X DELETE \"https://api.smith.langchain.com/api/v1/examples?hard_delete=true\" \\\n  -H \"x-api-key: YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"example_ids\": [\"example-id-1\", \"example-id-2\", \"example-id-3\"]\n  }'\n```\n\n### Deletion types\n\n#### Soft delete (default)\n\n* Creates tombstoned entries with NULL inputs/outputs in the dataset\n* Preserves historical data and maintains dataset versioning\n* Only affects the current version of the dataset\n\n#### Hard delete\n\n* Permanently removes inputs, outputs, and metadata from ALL dataset versions\n* Complete data removal when compliance requires zero-out across all versions\n* Add `\"hard_delete\": true` to the query parameters\n\nFor more details, refer to the [API spec](https://api.smith.langchain.com/redoc#tag/examples/operation/delete_examples_api_v1_examples_delete).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-purging-compliance.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 2:\nTitle: Query traces (SDK)\nLink: https://docs.langchain.comlangsmith/export-traces\nContent: Query traces (SDK)\nQuery traces (SDK)\nRecommended Reading Before diving into this content, it might be helpful to read the following: Run (span) data format LangSmith trace query syntax If you are looking to export a large volume of traces, we recommend that you use the Bulk Data Export functionality, as it will better handle large data volumes and will support automatic retries and parallelization across partitions. The recommended way to query runs (the span data in LangSmith traces) is to use the list_runs method in the SDK or /runs/query endpoint in the API. LangSmith stores traces in a simple format that is specified in the Run (span) data format.\n\nQuery traces (SDK)\nRecommended Reading Before diving into this content, it might be helpful to read the following: Run (span) data format LangSmith trace query syntax If you are looking to export a large volume of traces, we recommend that you use the Bulk Data Export functionality, as it will better handle large data volumes and will support automatic retries and parallelization across partitions. The recommended way to query runs (the span data in LangSmith traces) is to use the list_runs method in the SDK or /runs/query endpoint in the API. LangSmith stores traces in a simple format that is specified in the Run (span) data format.\n\nTree Filter\nList all runs named \"RetrieveDocs\" whose root run has a \"user_score\" feedback of 1 and any run in the full trace is named \"ExpandQuery\". This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace.\n\n---\n\nResult 3:\nTitle: Configure LangSmith for scale\nLink: https://docs.langchain.comlangsmith/self-host-scale\nContent: Trace querying (read path)\nCommon usage that puts load on the read path: Users on the frontend looking at tracing projects or individual traces Scripts used to query for trace info Hitting either the /runs/query or /runs/<run-id> api endpoints Services that play a large role in querying traces: Backend service: Receives the request and submits a query to ClickHouse to then respond to the request ClickHouse: Persistent storage for traces. This is the main database that is queried when requesting trace info. When scaling up the read path (trace querying), it is helpful to monitor the two services/resources listed above. Here are some typical changes that can help improve performance of trace querying: Increase the number of backend service pods. This would be most impactful if backend service pods are reaching 1 core CPU usage. Give ClickHouse more resources (CPU or Memory). ClickHouse can be very resource intensive, but it should lead to better performance. Move to a replicated ClickHouse cluster . Adding replicas of ClickHouse helps with read performance, but we recommend staying below 5 replicas (start with 3). For more precise guidance on how this translates to helm chart values, refer to the examples the following section . If you are unsure why your LangSmith instance cannot handle a certain load pattern, contact the LangChain team.\n\nTrace ingestion (write path)\nCommon usage that put load on the write path: Ingesting traces via the Python or JavaScript LangSmith SDK Ingesting traces via the @traceable wrapper Submitting traces via the /runs/multipart endpoint Services that play a large role in trace ingestion: Platform backend service: Receives initial request to ingest traces and places traces on a Redis queue Redis cache: Used to queue traces that need to be persisted Queue service: Persists traces for querying ClickHouse: Persistent storage used for traces When scaling up the write path (trace ingestion), it is helpful to monitor the four services/resources listed above. Here are some typical changes that can help increase performance of trace ingestion: Give ClickHouse more resources (CPU and memory) if it is approaching resource limits. Increase the number of platform-backend pods if ingest requests are taking long to respond. Increase queue service pod replicas if traces are not being processed from Redis fast enough. Use a larger Redis cache if you notice that the current Redis instance is reaching resource limits. This could also be a reason why ingest requests take a long time.\n\nExample LangSmith configurations for scale\nBelow we provide some example LangSmith configurations based on expected read and write loads. For read load (trace querying): Low means roughly 5 users looking at traces at a time (about 10 requests per second) Medium means roughly 20 users looking at traces at a time (about 40 requests per second) High means roughly 50 users looking at traces at a time (about 100 requests per second) For write load (trace ingestion): Low means up to 10 traces submitted per second Medium means up to 100 traces submitted per second High means up to 1000 traces submitted per second The exact optimal configuration depends on your usage and trace payloads. Use the examples below in combination with the information above and your specific usage to update your LangSmith configuration as you see fit. If you have any questions, please reach out to the LangChain team.\n\n---\n\nResult 4:\nTitle: Trace query syntax\nLink: https://docs.langchain.comlangsmith/trace-query-syntax\nContent: Using the method in the SDK or endpoint in the API, you can filter runs to analyze and export.\n\n## Filter arguments\n\n| Keys                          | Description                                                                                                                                                                                                                    |\n| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `project_id` / `project_name` | The project(s) to fetch runs from - can be a single project or a list of projects.                                                                                                                                             |\n| `trace_id`                    | Fetch runs that are part of a specific trace.                                                                                                                                                                                  |\n| `run_type`                    | The type of run to get, such as `llm`, `chain`, `tool`, `retriever`, etc.                                                                                                                                                      |\n| `dataset_name` / `dataset_id` | Fetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.                                                                              |\n| `reference_example_id`        | Fetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.                                                                                                   |\n| `parent_run_id`               | Fetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.                                                                  |\n| `error`                       | Fetch runs that errored or did not error.                                                                                                                                                                                      |\n| `run_ids`                     | Fetch runs with a given list of run ids. Note: **This will ignore all other filtering arguments.**                                                                                                                             |\n| `filter`                      | Fetch runs that match a given structured filter statement. See the guide below for more information.                                                                                                                           |\n| `trace_filter`                | Filter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of the root run within a trace.                            |\n| `tree_filter`                 | Filter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of any run within a trace. |\n| `is_root`                     | Only return root runs.                                                                                                                                                                                                         |\n| `select`                      | Select the fields to return in the response. By default, all fields are returned. See [run data format](/langsmith/run-data-format) for available fields.                                                                      |\n| `query` (*experimental*)      | Natural language query, which translates your query into a filter statement.                                                                                                                                                   |\n\n<Note>\n  **Performance tip**: Passing the `select` parameter and excluding `inputs` and `outputs` from the list can significantly improve query performance and reduce response sizes, especially for large runs.\n</Note>\n\n## Filter query language\n\nLangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs.\n\nThe filtering grammar is based on common comparators on fields in the run object. Supported comparators include:\n\n* `gte` (greater than or equal to)\n* `gt` (greater than)\n* `lte` (less than or equal to)\n* `lt` (less than)\n* `eq` (equal to)\n* `neq` (not equal to)\n* `has` (check if run contains a tag or metadata json blob)\n* `search` (search for a substring in a string field)\n\nAdditionally, you can combine multiple comparisons through the `and` operator.\n\nThese can be applied on fields of the run object, such as its `id`, `name`, `run_type`, `start_time` / `end_time`, `latency`, `total_tokens`, `error`, `execution_order`, `tags`, and any associated feedback through `feedback_key` and `feedback_score`.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-query-syntax.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\nResult 5:\nTitle: Trace with API\nLink: https://docs.langchain.comlangsmith/trace-with-api\nContent: Learn how to trace your LLM applications using the LangSmith API directly.\n\nIt is **highly** recommended to use our Python or TypeScript SDKs to send traces to LangSmith. We have designed these SDKs with optimizations like batching and backgrounding to ensure that your application's performance is not impacted by sending traces to LangSmith. However, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application. This guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation  for a full list of endpoints and request/response schemas.\n\n## Basic tracing\n\nThe simplest way to log runs is via the POST and PATCH `/runs` endpoint. These routes expect minimal contextual information about the tree structure to\n\n<Note>\n  When using the LangSmith REST API, you will need to provide your API key in the request headers as `\"x-api-key\"`.\n\n  If your API key is linked to multiple workspaces, you will need to specify the workspace being used in the header with `\"x-tenant-id\"`.\n\n  In the simple example, you do not need to set the `dotted_order` opr `trace_id` fields in the request body. These fields will be automatically generated by the system. Though this is simpler, it is slower and has a lower rate limit in LangSmith.\n</Note>\n\nThe following example shows how you might leverage our API directly in Python. The same principles apply to other languages.\n\n```python  theme={null}\nimport openai\nimport os\nimport requests\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\n\n# Send your API Key in the request headers\nheaders = {\n    \"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"],\n    \"x-tenant-id\": os.environ[\"LANGSMITH_WORKSPACE_ID\"]\n}\n\ndef post_run(run_id, name, run_type, inputs, parent_id=None):\n    \"\"\"Function to post a new run to the API.\"\"\"\n    data = {\n        \"id\": run_id.hex,\n        \"name\": name,\n        \"run_type\": run_type,\n        \"inputs\": inputs,\n        \"start_time\": datetime.utcnow().isoformat(),\n        # \"session_name\": \"project-name\",  # the name of the project to trace to\n        # \"session_id\": \"project-id\",  # the ID of the project to trace to. specify one of session_name or session_id\n    }\n    if parent_id:\n        data[\"parent_run_id\"] = parent_id.hex\n\n    requests.post(\n        \"https://api.smith.langchain.com/runs\",  # Update appropriately for self-hosted installations or the EU region\n        json=data,\n        headers=headers\n    )\n\ndef patch_run(run_id, outputs):\n    \"\"\"Function to patch a run with outputs.\"\"\"\n    requests.patch(\n        f\"https://api.smith.langchain.com/runs/{run_id}\",\n        json={\n            \"outputs\": outputs,\n            \"end_time\": datetime.now(timezone.utc).isoformat(),\n        },\n        headers=headers,\n    )\n\n# This can be a user input to your app\nquestion = \"Can you summarize this morning's meetings?\"\n\n# This can be retrieved in a retrieval step\ncontext = \"During this morning's meeting, we solved all world conflict.\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"},\n    {\"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n]\n\n# Create parent run\nparent_run_id = uuid4()\npost_run(parent_run_id, \"Chat Pipeline\", \"chain\", {\"question\": question})\n\n# Create child run\nchild_run_id = uuid4()\npost_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)\n\n# Generate a completion\nclient = openai.Client()\nchat_completion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages\n)\n\n# End runs\npatch_run(child_run_id, chat_completion.dict())\npatch_run(parent_run_id, {\"answer\": chat_completion.choices[0].message.content})\n```\n\nSee the doc on the [Run (span) data format](/langsmith/run-data-format) for more information.\n\n## Batch Ingestion\n\nFor faster ingestion of runs and higher rate limits, you can use the POST `/runs/multipart` [link](https://api.smith.langchain.com/redoc#tag/run/operation/multipart_ingest_runs_api_v1_runs_multipart_post) endpoint. Below is an example. It requires `orjson` (for fast json ) and `requests_toolbelt` to run\n\n```python  theme={null}\nimport json\nimport os\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Dict, List\nimport requests\nfrom requests_toolbelt import MultipartEncoder\n\ndef create_dotted_order(\n    start_time: datetime | None = None,\n    run_id: uuid.UUID | None = None\n) -> str:\n    \"\"\"Create a dotted order string for run ordering and hierarchy.\n\n    The dotted order is used to establish the sequence and relationships between runs.\n    It combines a timestamp with a unique identifier to ensure proper ordering and tracing.\n    \"\"\"\n    st = start_time or datetime.now(timezone.utc)\n    id_ = run_id or uuid.uuid4()\n    return f\"{st.strftime('%Y%m%dT%H%M%S%fZ')}{id_}\"\n\ndef create_run_base(\n    name: str,\n    run_type: str,\n    inputs: dict,\n    start_time: datetime\n) -> dict:\n    \"\"\"Create the base structure for a run.\"\"\"\n    run_id = uuid.uuid4()\n    return {\n        \"id\": str(run_id),\n        \"trace_id\": str(run_id),\n        \"name\": name,\n        \"start_time\": start_time.isoformat(),\n        \"inputs\": inputs,\n        \"run_type\": run_type,\n    }\n\ndef construct_run(\n    name: str,\n    run_type: str,\n    inputs: dict,\n    parent_dotted_order: str | None = None,\n) -> dict:\n    \"\"\"Construct a run dictionary with the given parameters.\n\n    This function creates a run with a unique ID and dotted order, establishing its place\n    in the trace hierarchy if it's a child run.\n    \"\"\"\n    start_time = datetime.now(timezone.utc)\n    run = create_run_base(name, run_type, inputs, start_time)\n    current_dotted_order = create_dotted_order(start_time, uuid.UUID(run[\"id\"]))\n\n    if parent_dotted_order:\n        current_dotted_order = f\"{parent_dotted_order}.{current_dotted_order}\"\n        run[\"trace_id\"] = parent_dotted_order.split(\".\")[0].split(\"Z\")[1]\n        run[\"parent_run_id\"] = parent_dotted_order.split(\".\")[-1].split(\"Z\")[1]\n\n    run[\"dotted_order\"] = current_dotted_order\n    return run\n\ndef serialize_run(operation: str, run_data: dict) -> List[tuple]:\n    \"\"\"Serialize a run for the multipart request.\n\n    This function separates the run data into parts for efficient transmission and storage.\n    The main run data and optional fields (inputs, outputs, events) are serialized separately.\n    \"\"\"\n    run_id = run_data.get(\"id\", str(uuid.uuid4()))\n\n    # Separate optional fields\n    inputs = run_data.pop(\"inputs\", None)\n    outputs = run_data.pop(\"outputs\", None)\n    events = run_data.pop(\"events\", None)\n\n    parts = []\n\n    # Serialize main run data\n    run_data_json = json.dumps(run_data).encode(\"utf-8\")\n    parts.append(\n        (\n            f\"{operation}.{run_id}\",\n            (\n                None,\n                run_data_json,\n                \"application/json\",\n                {\"Content-Length\": str(len(run_data_json))},\n            ),\n        )\n    )\n\n    # Serialize optional fields\n    for key, value in [(\"inputs\", inputs), (\"outputs\", outputs), (\"events\", events)]:\n        if value:\n            serialized_value = json.dumps(value).encode(\"utf-8\")\n            parts.append(\n                (\n                    f\"{operation}.{run_id}.{key}\",\n                    (\n                        None,\n                        serialized_value,\n                        \"application/json\",\n                        {\"Content-Length\": str(len(serialized_value))},\n                    ),\n                )\n            )\n\n    return parts\n\ndef batch_ingest_runs(\n    api_url: str,\n    api_key: str,\n    posts: list[dict] | None = None,\n    patches: list[dict] | None = None,\n) -> None:\n    \"\"\"Ingest multiple runs in a single batch request.\n\n    This function handles both creating new runs (posts) and updating existing runs (patches).\n    It's more efficient for ingesting multiple runs compared to individual API calls.\n    \"\"\"\n    boundary = uuid.uuid4().hex\n    all_parts = []\n\n    for operation, runs in zip((\"post\", \"patch\"), (posts, patches)):\n        if runs:\n            all_parts.extend(\n                [part for run in runs for part in serialize_run(operation, run)]\n            )\n\n    encoder = MultipartEncoder(fields=all_parts, boundary=boundary)\n    headers = {\"Content-Type\": encoder.content_type, \"x-api-key\": api_key}\n\n    try:\n        response = requests.post(\n            f\"{api_url}/runs/multipart\",\n            data=encoder,\n            headers=headers\n        )\n        response.raise_for_status()\n        print(\"Successfully ingested runs.\")\n    except requests.RequestException as e:\n        print(f\"Error ingesting runs: {e}\")\n        # In a production environment, you might want to log this error or handle it more robustly\n\n# Configure API URL and key\n# For production use, consider using a configuration file or environment variables\napi_url = \"https://api.smith.langchain.com\"\napi_key = os.environ.get(\"LANGSMITH_API_KEY\")\n\nif not api_key:\n    raise ValueError(\"LANGSMITH_API_KEY environment variable is not set\")\n\n# Create a parent run\nparent_run = construct_run(\n    name=\"Parent Run\",\n    run_type=\"chain\",\n    inputs={\"main_question\": \"Tell me about France\"},\n)\n\n# Create a child run, linked to the parent\nchild_run = construct_run(\n    name=\"Child Run\",\n    run_type=\"llm\",\n    inputs={\"question\": \"What is the capital of France?\"},\n    parent_dotted_order=parent_run[\"dotted_order\"],\n)\n\n# First, post the runs to create them\nposts = [parent_run, child_run]\nbatch_ingest_runs(api_url, api_key, posts=posts)\n\n# Then, update the runs with their end times and any outputs\nchild_run_update = {\n    **child_run,\n    \"end_time\": datetime.now(timezone.utc).isoformat(),\n    \"outputs\": {\"answer\": \"Paris is the capital of France.\"},\n}\n\nparent_run_update = {\n    **parent_run,\n    \"end_time\": datetime.now(timezone.utc).isoformat(),\n    \"outputs\": {\"summary\": \"Discussion about France, including its capital.\"},\n}\n\npatches = [parent_run_update, child_run_update]\nbatch_ingest_runs(api_url, api_key, patches=patches)\n\n# Note: This example requires the `requests` and `requests_toolbelt` libraries.\n# You can install them using pip:\n# pip install requests requests_toolbelt\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-api.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n"
  },
  {
    "role": "tool",
    "name": "get_article_content",
    "tool_call_id": "call_04157800",
    "id": "3fd6a663-cc75-4588-acb0-1df31ffd6646",
    "content": "ID: 9e06b53a-7b85-4793-b7b6-e179c0c8d815\nTitle: Isolating Subgraph State When Calling Graphs from Tools\nURL: https://support.langchain.com/articles/6131002418-isolating-subgraph-state-when-calling-graphs-from-tools\nCollection: Customer Support Knowledge Base\n\nContent:\n<h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">Problem</h2><p style=\"padding-left: 0px!important;\">When Graph A calls Graph B from a tool, their states get merged if they share the same checkpointer and thread_id.</p><h2 style=\"padding-left: 0px!important;\" level=\"2\" id=\"\">Solution</h2><p style=\"padding-left: 0px!important;\">Give each graph its own checkpointer instance. This allows them to share the same thread_id (conversation_id) while keeping states isolated.</p><div data-active-tab=\"0\" data-show-line-numbers=\"false\" data-tabs-enabled=\"false\" data-type=\"code-block-group\"><pre data-language=\"python\"><code class=\"language-python\">from typing import Annotated\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_anthropic import ChatAnthropic\nfrom dotenv import load_dotenv\nfrom typing_extensions import TypedDict\n\nload_dotenv()\n\n# === Graph B (subagent) ===\nclass GraphBState(TypedDict):\n    messages: Annotated[list, add_messages]\n\ndef graph_b_node(state: GraphBState):\n    return {\"messages\": [AIMessage(content=\"[GRAPH_B] Response\")]}\n\ncheckpointer_b = MemorySaver()  # Own checkpointer\ngraph_b = StateGraph(GraphBState)\ngraph_b.add_node(\"node\", graph_b_node)\ngraph_b.set_entry_point(\"node\")\ngraph_b.set_finish_point(\"node\")\ngraph_b = graph_b.compile(checkpointer=checkpointer_b)\n\n\n# === Tool that calls Graph B ===\n@tool\ndef call_graph_b(query: str, config: RunnableConfig) -&gt; str:\n    \"\"\"Call Graph B subagent.\"\"\"\n    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n    result = graph_b.invoke(\n        {\"messages\": [HumanMessage(content=query)]},\n        {\"configurable\": {\"thread_id\": thread_id}}\n    )\n    return f\"Done\"\n\n\n# === Graph A (parent agent) ===\ncheckpointer_a = MemorySaver()  # Own checkpointer\nagent = create_react_agent(\n    model=ChatAnthropic(model=\"claude-sonnet-4-20250514\"),\n    tools=[call_graph_b],\n    checkpointer=checkpointer_a,\n)\n\n\n# === Run ===\nthread_id = \"conv_12345\"\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\n\nagent.invoke({\"messages\": [HumanMessage(content=\"call graph_b with 'hello'\")]}, config)\n\ngraph_a_state = agent.get_state(config)\ngraph_b_state = graph_b.get_state({\"configurable\": {\"thread_id\": thread_id}})\n\nprint(f\"Graph A thread_id: {graph_a_state.config['configurable']['thread_id']}\")\nprint(f\"Graph B thread_id: {graph_b_state.config['configurable']['thread_id']}\")\nprint()\nprint(f\"Graph A messages: {len(graph_a_state.values['messages'])}\")\nprint(f\"Graph B messages: {len(graph_b_state.values['messages'])}\")\nprint()\nprint(f\"States isolated: {not any('[GRAPH_B]' in str(m.content) for m in graph_a_state.values['messages'])}\")\n</code></pre></div><p style=\"padding-left: 0px!important;\"></p>"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3a20-150c-7d73-9c55-2baa2be91f36",
    "content": "**`StateGraph.add_node()` \u5c06\u51fd\u6570\u6dfb\u52a0\u5230 LangGraph \u5de5\u4f5c\u6d41\u4e2d\uff0c\u6bcf\u4e2a\u8282\u70b9\u63a5\u6536\u5f53\u524d\u72b6\u6001\u5e76\u8fd4\u56de\u72b6\u6001\u66f4\u65b0\u3002**\n\n\u8282\u70b9\u51fd\u6570\u63a5\u6536\u72b6\u6001\u5b57\u5178\uff08\u5339\u914d `StateGraph` \u5b9a\u4e49\u7684 schema\uff09\uff0c\u5e76\u8fd4\u56de\u5b57\u5178\u5f62\u5f0f\u7684**\u72b6\u6001\u66f4\u65b0**\uff08\u4f7f\u7528 reducer \u5982 `operator.add` \u5408\u5e76\uff09\u3002LangGraph \u81ea\u52a8\u5c06\u666e\u901a\u51fd\u6570\u8f6c\u6362\u4e3a\u652f\u6301\u6279\u5904\u7406\u3001\u5f02\u6b65\u548c\u8ffd\u8e2a\u7684 `RunnableLambda`\u3002\n\n```python\nfrom typing_extensions import TypedDict, Annotated\nfrom langgraph.graph import StateGraph, START, END\nimport operator\nfrom langchain_core.messages import AnyMessage\n\n# \u5b9a\u4e49\u72b6\u6001 schema\nclass MessagesState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]  # \u8ffd\u52a0\u65b0\u6d88\u606f\n    llm_calls: int  # \u8ba1\u6570\u5668\n\n# \u521b\u5efa StateGraph\nbuilder = StateGraph(MessagesState)\n\n# \u5b9a\u4e49\u8282\u70b9\u51fd\u6570\uff1a\u63a5\u6536\u72b6\u6001\uff0c\u8fd4\u56de\u66f4\u65b0\ndef llm_call(state: MessagesState) -> dict:\n    # \u5904\u7406\u5f53\u524d\u72b6\u6001\uff0c\u8fd4\u56de\u90e8\u5206\u66f4\u65b0\n    return {\n        \"messages\": [model.invoke(state[\"messages\"])],  # \u8ffd\u52a0\u65b0\u6d88\u606f\n        \"llm_calls\": state.get(\"llm_calls\", 0) + 1      # \u9012\u589e\u8ba1\u6570\u5668\n    }\n\ndef tool_node(state: MessagesState) -> dict:\n    # \u6267\u884c\u5de5\u5177\u8c03\u7528\uff0c\u8fd4\u56de\u89c2\u5bdf\u7ed3\u679c\n    result = []\n    for tool_call in state[\"messages\"][-1].tool_calls:\n        tool = tools_by_name[tool_call[\"name\"]]\n        observation = tool.invoke(tool_call[\"args\"])\n        result.append(ToolMessage(content=str(observation), tool_call_id=tool_call[\"id\"]))\n    return {\"messages\": result}\n\n# \u6dfb\u52a0\u8282\u70b9\uff08\u540d\u79f0 + \u51fd\u6570\uff09\nbuilder.add_node(\"llm_call\", llm_call)      # \u5fc5\u987b\u63d0\u4f9b\u540d\u79f0\u7528\u4e8e\u8fde\u63a5\u8fb9\nbuilder.add_node(\"tool_node\", tool_node)\n\n# \u6dfb\u52a0\u8fb9\u8fde\u63a5\u8282\u70b9\nbuilder.add_edge(START, \"llm_call\")\nbuilder.add_conditional_edges(\"llm_call\", should_continue, [\"tool_node\", END])\nbuilder.add_edge(\"tool_node\", \"llm_call\")\n\ngraph = builder.compile()  # \u7f16\u8bd1\u4e3a\u53ef\u6267\u884c\u56fe\n```\n\n## \u9ad8\u7ea7\u7528\u6cd5\uff1a\u7f13\u5b58\u3001\u5b50\u56fe\u548c\u8fd0\u884c\u65f6\u53c2\u6570\n\n\u652f\u6301**\u7f13\u5b58\u7b56\u7565**\u3001**\u5b50\u56fe**\u548c**\u8fd0\u884c\u65f6\u53c2\u6570**\uff08`config`\u3001`runtime`\uff09\uff1a\n\n```python\nfrom langgraph.types import CachePolicy\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# \u5e26\u7f13\u5b58\u7684\u8282\u70b9\uff08120\u79d2 TTL\uff09\nbuilder.add_node(\n    \"expensive_node\",\n    expensive_function,\n    cache_policy=CachePolicy(ttl=120)  # \u8282\u70b9\u7ea7\u7f13\u5b58\n)\n\n# \u6dfb\u52a0\u5df2\u7f16\u8bd1\u7684\u5b50\u56fe\u4f5c\u4e3a\u8282\u70b9\uff08\u5171\u4eab\u72b6\u6001\u952e\uff09\nsubgraph = sub_builder.compile()\nbuilder.add_node(\"subgraph\", subgraph)  # \u76f4\u63a5\u4f20\u9012\u7f16\u8bd1\u540e\u7684\u56fe\n\n# \u9ad8\u7ea7\u8282\u70b9\uff1a\u8bbf\u95ee\u8fd0\u884c\u65f6\u4e0a\u4e0b\u6587\ndef node_with_runtime(state: State, runtime):\n    print(f\"User ID: {runtime.context.user_id}\")  # \u8fd0\u884c\u65f6\u4e0a\u4e0b\u6587\n    return {\"result\": process(state)}\n\nbuilder.add_node(\"advanced\", node_with_runtime)\ngraph = builder.compile(checkpointer=MemorySaver())  # \u542f\u7528\u6301\u4e45\u5316\n```\n\n\u8282\u70b9\u540d\u79f0\u7528\u4e8e**\u8fb9\u8fde\u63a5**\uff0c\u72b6\u6001\u66f4\u65b0\u901a\u8fc7 reducer\uff08\u5982 `operator.add`\uff09**\u5408\u5e76**\u5230\u5168\u5c40\u72b6\u6001\u3002\n\n**Relevant docs:**\n- [Graph API Overview](https://docs.langchain.com/oss/python/langgraph/graph-api)\n- [Quickstart](https://docs.langchain.com/oss/python/langgraph/quickstart)\n- [LangGraph Overview](https://docs.langchain.com/oss/python/langgraph/overview)"
  },
  {
    "role": "user",
    "id": "ab95841c-8aae-4868-837a-9301c1886c7d",
    "content": "\u6211\u60f3\u8bf7\u95ee\u4e0b\u9762\u8fd9\u6bb5\u51fd\u6570\u4e2d\uff0c**set_entry_point**\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f\n    def _add_workflow_edges(self, workflow):\n        workflow.add_edge('browser', 'planner')\n        workflow.add_edge('planner', 'human')\n        workflow.add_edge('researcher', 'writer')\n        workflow.add_edge('writer', 'publisher')\n        workflow.set_entry_point(\"browser\")\n        workflow.add_edge('publisher', END)"
  },
  {
    "role": "assistant",
    "id": "lc_run--019b3a30-9126-71d1-853f-04e1b81b463e",
    "content": "**`set_entry_point()` \u6307\u5b9a\u56fe\u7684**\u5165\u53e3\u8282\u70b9**\uff0c\u51b3\u5b9a\u6267\u884c\u4ece\u54ea\u4e2a\u8282\u70b9\u5f00\u59cb\u3002**\n\n\u5728 LangGraph \u4e2d\uff0c\u56fe\u6267\u884c\u9700\u8981\u4e00\u4e2a\u8d77\u70b9\u3002`set_entry_point(\"browser\")` \u544a\u8bc9\u56fe**\u4ece `browser` \u8282\u70b9\u5f00\u59cb\u6267\u884c**\uff0c\u7136\u540e\u6309\u8fb9\u5b9a\u4e49\u7684\u987a\u5e8f\u6d41\u8f6c\u3002\n\n```python\nfrom langgraph.graph import StateGraph, START, END\n\nworkflow = StateGraph(State)\n\n# \u6dfb\u52a0\u8282\u70b9\nworkflow.add_node(\"browser\", browser_func)\nworkflow.add_node(\"planner\", planner_func)\nworkflow.add_node(\"human\", human_func)\nworkflow.add_node(\"researcher\", researcher_func)\nworkflow.add_node(\"writer\", writer_func)\nworkflow.add_node(\"publisher\", publisher_func)\n\n# \u6dfb\u52a0\u8fb9\u5b9a\u4e49\u6d41\u8f6c\u8def\u5f84\nworkflow.add_edge('browser', 'planner')      # browser \u2192 planner\nworkflow.add_edge('planner', 'human')        # planner \u2192 human  \nworkflow.add_edge('researcher', 'writer')    # researcher \u2192 writer\nworkflow.add_edge('writer', 'publisher')     # writer \u2192 publisher\nworkflow.add_edge('publisher', END)          # publisher \u2192 \u7ed3\u675f\n\n# \u26a0\ufe0f CRITICAL: \u6307\u5b9a\u5165\u53e3\u70b9 - \u6267\u884c\u4ece\u8fd9\u91cc\u5f00\u59cb\uff01\nworkflow.set_entry_point(\"browser\")          # \u4ece browser \u5f00\u59cb\n\ngraph = workflow.compile()\n```\n\n## \u6267\u884c\u6d41\u7a0b\u53ef\u89c6\u5316\n\n```\nSTART \u2192 browser \u2192 planner \u2192 human \u2192 ... \u2192 publisher \u2192 END\n         \u2191 (\u5165\u53e3\u70b9)\n```\n\n**\u6ca1\u6709 `set_entry_point()`**\uff1a\n- \u56fe\u65e0\u6cd5\u786e\u5b9a\u4ece\u54ea\u91cc\u5f00\u59cb\u6267\u884c\n- `graph.invoke(input)` \u4f1a\u5931\u8d25\n\n**\u66ff\u4ee3\u65b9\u6848**\uff08\u65e7\u7248\u672c\u517c\u5bb9\uff09\uff1a\n```python\n# \u7b49\u4ef7\u4e8e set_entry_point(\"browser\")\nworkflow.add_edge(START, \"browser\")  # \u4ece START \u76f4\u63a5\u5230 browser\n```\n\n## \u591a\u5165\u53e3\u70b9\u573a\u666f\n\n\u5bf9\u4e8e\u590d\u6742\u56fe\uff0c\u53ef\u4ee5\u6709\u591a\u4e2a\u5165\u53e3\uff1a\n\n```python\n# \u4e0d\u540c\u8f93\u5165\u6d41\u5411\u4e0d\u540c\u5165\u53e3\ndef router(state):\n    if state[\"task\"] == \"research\":\n        return \"researcher\"\n    return \"browser\"\n\nworkflow.add_conditional_edges(START, router, {\n    \"research\": \"researcher\",\n    \"browse\": \"browser\"\n})\n# \u65e0\u9700 set_entry_point - \u6761\u4ef6\u8fb9\u4ece START \u5206\u652f\n```\n\n**\u603b\u7ed3**\uff1a`set_entry_point(\"browser\")` \u786e\u4fdd**\u6267\u884c\u4ece `browser` \u8282\u70b9\u5f00\u59cb**\uff0c\u6309\u540e\u7eed\u8fb9\u987a\u5e8f\u6d41\u8f6c\u5230 `END`\u3002\n\n**Relevant docs:**\n- [Graph API Overview](https://docs.langchain.com/oss/python/langgraph/graph-api)\n- [Quickstart](https://docs.langchain.com/oss/python/langgraph/quickstart)"
  }
]